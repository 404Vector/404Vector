{"vault/Notion/DB/DB-Blog-Post/2023년-회고-ML-Engineer로서의-첫걸음":{"slug":"vault/Notion/DB/DB-Blog-Post/2023년-회고-ML-Engineer로서의-첫걸음","filePath":"vault/Notion/DB/DB Blog Post/2023년 회고-ML Engineer로서의 첫걸음.md","title":"2023년 회고-ML Engineer로서의 첫걸음","links":[],"tags":[],"content":"2023년도에 있었던 일, 이뤄낸 일들을 정리해 보았습니다.\n부스트캠프 AI Tech 4기\n\n저는 영상처리(Image Processing)를 전공했고, 지도교수님의 추천으로 어떤 기업에 입사하면서 검사 알고리즘(Inspection Algorithm)과 그 운용을 위한 SW 개발(Engine)을 했었습니다.\n하지만 SW Engineer로서 성장에 한계를 느낌과 동시에 업계가 AI를 적극적으로 도입하기 시작하면서 커리어를 전환하려고 마음먹고 2022년에 약간의 준비 후, 부스트캠프 AI Tech 4기(이하 부캠)를 지원했고 1월 말 즈음에 수료를 완료할 수 있었습니다.\n취업준비\n\n수료 후 바로 취업하고 싶었지만, 경기가 점점 안좋아지고, 채용시장도 안좋아지고, 희망적인 소식이 없었습니다. 취업 준비 기간은 지금 회상해보도 정말 쉽지 않았던 시기였습니다. 혼자 준비하다가는 페이스를 잃어버릴 것 같아서 미리 부캠 종료 전에 취업 스터디를 같이 할 사람들을 구했습니다.\n모두 좋은 분들이었고, 면접을 위해 서로가 분담하여 딥러닝과 관련된 여러 기술들을 조사하고 공유했습니다. 또한 Zoom으로 서로 조사한 기술에 대해 질문 및 답변도 해보면서 나중에는 모의 면접도 진행했습니다. 단순한 암기가 아니라, 질문받고 답변하면서 더 깊게 이해할 수 있었습니다.(특히 Swin Transform을 잘 설명할 수 있게 되었습니다.)\n입사\n\n1달 좀 넘게 준비했을 때, 서류를 넣고 면접을 보기 시작했습니다. 그러다 동영상 개선을 하는 회사에서 일 할 수 있는 기회를 얻게 되었고, 많은 데이터를 처리해야 하는 일이다 보니 재미있겠다는 생각(고난과 역경의 시작🙂)에 현재 회사에 합류하게 되었습니다.\nResearcher에서 Engineer로 급선회\n\n저는 회사에 AI Researcher(Vision)포지션으로 들어가게 되었습니다. 또한 팀원은 모두 Researcher였습니다.\n첫 출근 후, 저에게는 어떤 연구(비디오 화질 개선과 관련된 연구)를 할지 선택할 기회가 주어졌습니다. 어떤 연구를 할지 고민하고 있던 때, 팀이 생각보다 생긴지가 오래되었고 상품화를 준비하고 있다는 것을 알게 되었습니다.\n하지만 팀원들은 연구만을 해오던 분들이었고, 모델을 서비스로 만들기 위한 준비를 1부터 시작해야 하는 상황이었습니다. 따라서 엔지니어가 필요한 상황이었고, 저 일을 맡는 것이 더 많은 경험과 성장을 할 수 있겠다는 판단에 엔지니어로 나아가겠다고 이야기했습니다.\nEngineer로서 하게된 것 - 1. 가속화\n\n모델 뜯어보기\n팀은 현재 화질 개선 결과(동영상이 얼마나 좋아졌는지)에 모든 관심을 집중하고 있었고, 여러 실험을 진행하기 바빠서 모델의 코드 구현, 필요한 전처리 및 후처리을 제대로 살펴볼 여유가 없었습니다. 따라서 상용으로 검토하고 있는 모델들의 코드 구현과 전처리 및 후처리를 조사했습니다. 아직 배움이 부족해서인지 논문과 코드를 양방향으로 봐야 입출력과 흐름을 제대로 이해하고 내재화 시킬 수 있었습니다.\n\n이 과정에서 미션을 받게 되었는데, 바로 Denoise 모델을 최적화하여 Video to Video를 30FPS로 할 수 있게 하는 것이었습니다.\n\nONNX 변환하기\n모델 구조를 파악하고 난 뒤에는 모델을 ONNX로 변환하는 일을 했습니다. 물론 PyTorch 코드 그래도 가져가는 방법도 있지만, 모델마다 기괴하고 다양한 환경을 갖고 있는데 이것을 서비스마다 일일히 다 맞추고 싶지 않았습니다.\nONNX Runtime + TensorRT + 양자화\nONNX는 ONNX Runtime을 사용하여 인퍼런스를 수행합니다. ONNX Runtime은 인퍼런스를 위한 엔진이며, 내부에 여러 옵션들이 존재합니다. ONNX Runtime은 onnxruntime-gpu를 설치하여 테스트했습니다. 아무 설정을 하지 않고 그냥 사용한다면 PyTorch보다도 느릴 수 있고 호환되지 않는 CUDA버전이라면 정상적인 동작을 하지 않기 때문에 환경 구성에서 많이 해매게 되었습니다.\n30FPS를 위해서는 순수 인퍼런스 시간 자체로 줄여야 했습니다. 이 때, NVIDIA GPU를 사용하고 있었기 때문에 TensorRT를 조사하게 되었습니다. TensorRT는 ONNX Runtime과 마찬가지로 인퍼런스 엔진입니다. ONNX Runtime 공부해놓은 것이 아까워지는 찰나, ONNX Runtime이 Excution Provider로 TensorRT를 지원하여 ONNX Runtime의 코드로 내부 옵션만 조금 바꿔서 TensorRT를 사용할 수 있다는 것을 알게 되었습니다.\n\n기쁜 마음으로 사용하려고 했지만 생각보다 큰 장애물이 있습니다. ONNX Runtime, CUDA, CuDNN, TensorRT의 버전이 모두 서로 호환이 되어야 정상동작합니다. ==이곳==을 보고 반드시 Table에 기재된 조합으로 사용해야 저와 같은 고통을 겪지 않을 수 있습니다.\nDocker 등을 사용하지 않고 바로 구성하는 상황이라면 Nvidia Driver, CUDA 버전 등에 의해 사용해야 하는 TensorRT와 ONNX Runtime의 버전이 강제됩니다.\nTensorRT 빌드 자체도 쉽지 않습니다. 시행착오 끝에 실험적으로 알게 된 가장 편안한 방법은 사용하려는 ONNX Runtime 버전에 맞춰 NVIDIA에서 제공하는 TensorRT 이미지를 사용하는 것입니다.\n\n물론 이 때 TensorRT의 FP16, Int8 옵션도 사용할 수 있습니다. 하지만 Int8 경우 Post-Training Quantization의 특성상 바로 사용할 수는 없습니다. 이를 사용하기 위해서는 ONNX의 Quantization 기능을 통해 미리 양자화 Table을 준비해야 했습니다. 또한 TensorRT의 자체적인 최적화와 그 결과를 캐싱하는 과정을 기다려야 했습니다.\n하지만 모든 과정을 마친 뒤 성능을 시험한 결과, PyTorch 대비 순수 Inference 시간 기준으로 7.7배 빠르게 인퍼런스를 할 수 있게 되었습니다!\nHeap Memory Allocation Optimization\nImage Processing을 하다보니 얻게 된 기술인데, Memory의 생성을 최소화하면 속도를 향상시킬 수 있습니다. Python은 GC가 메모리의 소멸을 알아서 관리해주지만 GC가 할 일을 줄여준다면 당연히 성능이 향상될 수 있습니다. 따라서 전처리, 후처리 시에 메모리가 생성되는 함수들을 지양하고 최대한 복사해가도록 변경했습니다. 그 결과 총 56ms 걸리던 처리시간을 21ms로 줄일 수 있었습니다.\nMulti-Processing\n다음으로 눈이 가게 된 것은 병렬처리 입니다. 한 개의 GPU로는 더 큰 속도 향상이 어렵다고 판단했습니다. Python에서 Sub Processor를 띄우는 것은 쉽지만, 이미지를 전달하고 이미지를 받는 것을 편하게 하고 싶었고, IPC로는 Shared Memory가 가장 빠르고 좋지만 단순 실험을 위해 Python 환경에서 Shared Memory를 사용한 무언가를 개발하는 것은 부담스럽게 느껴졌고, 결과 적은 노력에 비해 편리하게 실험할 수 있겠다고 판단된 gRPC를 사용하게 되었습니다.\n결과 많은 것을 알게 되었는데, 첫 번째는 GPU 1개당 1개의 Inferecne Processor를 만들 때 속도가 선형적으로 증가하지 않는다는 것이었습니다.\n두 번째는 GPU 1개당 2개의 Inference Processor를 생성하여 속도 향상을 노려볼 수 있다는 것입니다. 테스트하던 환경에서는 VRAM이 충분해서 1개의 GPU에 대해서 여러 개의 Inference Processor를 생성할 수 있었는데, GPU Per Processor가 1일 때 보다 2일 때 총 인퍼런스 속도가 증가(약 1.44배의 속도로 인퍼런스 가능)하며 2와 3은 비슷했습니다.\n\n당시 비동기 처리를 하지 않고 있었기 때문에 위 방법으로 상당한 속도 향상을 이룰 수 있었습니다.\n\n세 번째는 고속 처리를 위해서는 HW 구성을 고려할 필요가 있다는 것이었습니다. 당시 Video to Image, Image to Video는 다른 팀의 엔지니어분이 담당하여 개발을 진행하고 있었는데 개발된 구조상 Disk에 이미지를 저장하고 로드하는 일들이 필수적이었습니다. FHD 기준 이미지 1장의 크기는 약 6.2MB입니다. 이를 30 FPS로 처리하려면 초당 186MB/s의 데이터를 디스크에서 읽고 쓸 수 있어야 합니다. 하지만 Video to Image와 Image to Video의 과정에서 이 값은 두 배가 됩니다. 따라서 초당 372MB/s를 읽고 쓰게 됩니다. SATA3의 경우 최대 약 500MB/s를 읽고 쓸 수 있으므로 당시 IO 성능의 75%를 사용하고 있었습니다. 구조 개선 및 최적화를 통해 성능을 높여도 HW적으로 최대 40fps 이상 빠르게 처리할 수 없습니다. 때문에 만약 처리속도를 더 늘리고 싶다면 NVMe등으로 인터페이스를 변경할 필요가 있다는 것을 알게 되었습니다.\n결과\n결과적으로 “인퍼런스를 최대한 많이, 빠르게 하는 방법”에 대해서 레벨 0부터 차근차근 레벨 업을 한 기분이었습니다.\n\n양자화 및 TensorRT를 적용하여 GPU당 순수 인퍼런스 시간을 7.7배 가속\n메모리 최적화를 통해 전처리와 후처리의 시간을 2.2배 가속\ngRPC+Multi-Processing으로 4개의 GPU를 사용하여 병렬 처리, 5.6(4x1.4)배 가속\n\n최종적으로 Video to Video의 FPS를 31.93fps를 달성해서 미션을 완수할 수 있었습니다.\nEngineer로서 하게된 것 - 2. Docker &amp; Docker Compose, AWS\n\nDocker 사용하기\n사내에는 서비스에 사용할 GPU가 없는 상황이었고, 점차 AWS등의 클라우드나 다른 서버에서 인퍼런스 서비스를 구성해야 하는 상황이었습니다. 당연히 코드를 zip파일로 압축해서 옮기기 + 각종 환경 구성하기를 하고싶지 않았고, 언제든 편하게 불러와서 실행시키고 싶었습니다. 이에 Video to Image, Image to Video를 담당하는 엔지니어분과 합의하에 제가 서비스를 Docker Image로 구성하기 시작했습니다. 부캠에서 배우고 실무에 처음 사용하는 것이라 걱정 반, 기대 반으로 시작했습니다.\nTensorRT 및 ONNX Runtime 때문에 초반엔 꽤 난항을 겪었지만(난이도가 어렵다기 보다는 제가 경험이 부족해서..) 결국 CUDA와 TensorRT 그리고 ONNX-Runtime 간의 버전을 맞추는 요령이 생겨 제대로 동작하는 환경을 꾸릴 수 있었습니다. 이후 Docker Image를 생성할 때 넣어야 할 것들, 넣지 말아야 할 것들을 정하면서 런타임 서비스에서 사용하기 위한 이미지를 생성할 수 있었습니다. 이미지는 Docker Hub가 아닌 Github에 업로드 하였습니다. 인증과 같이 조금 거처야 할 절차가 있긴 하지만 코드와 함께 이미지를 같이 관리할 수 있다는 것은 너무나 매력적인 부분 이었습니다.\nDocker Compose 사용하기\nDocker를 이용하여 빠르게 서비스를 올리는 것은 좋았지만 문제가 있었습니다.\n\n구조적으로 서비스를 올리는 노드에서 DB 컨테이너를 같이 올려줘야 했고,\n이를 위한 네트워크 구성을 해야 했으며\nIO가 매우 많아서(SATA3의 최대치 이상을 사용합니다…) 올릴 때 마다 nvme 디스크 마운트가 필요\n\n했습니다.\n때문에 이를 처음에는 셀스크립트를 만들다가, Docker Compose라는게 있다는걸 알게 되었고 이후에는 모든 환경에서 Docker Compose를 사용하여 서비스가 실행되도록 변경하였습니다.\nAWS 사용하기\nDocker 이미지 다음은 AWS 였습니다. 처음으로 AWS에 관해서는 아는 것이 거의 없었기 때문에 VPC 구성부터 하나하나 찾아보며 진행했습니다. EC2에 인스턴스를 생성하고 사용할 수 있게된 후에는 인퍼런스 서비스로 사용할 수 있는 인스턴스 종류를 찾기 시작했습니다.\n당시 GPU 가속을 지원하는 인스턴스 종류를 모두 찾고, 내장되어 있는 GPU의 FLOPS와 IOPS를 조사하고 시간당 비용을 따져보았습니다. 결론은, 추론용으로 나온 인스턴스가 결국 가성비가 좋고 신형이 비싸지만 더 빨리 끝나기 때문에 가격면에서 오히려 이득 일 수 있다는 것이었습니다. 다만, CPU아키텍처가 달라 지는 것은 추후 문제가 생길수 있다고 생각하여 g5 패밀리로 선택하게 되었습니다.\n결과\n실제 DL 인퍼런스 서비스를 구성하기 위해서, 차근차근 성장했다는 느낌을 받았습니다.\n\n추론 환경을 구성 가능\n이를 이용해 새롭게 Docker Image를 생성 가능\nDocker Compose를 사용하여 한번에 서비스 구성 가능\n\n목표한 바를 이루었지만, k8s까지 진도를 나아가지 못한 것은 아쉬웠습니다.\n마치며\n\n나름 바쁘게 지낸 한 해였다고 생각합니다. 새로운 기술들을 하나 하나 배우면서 RPG 캐릭터를 키우듯, 성장하는 재미가 있었습니다. 2023년에는 ML Engineer로서 기초를 닦았다고 생각하며, 올해에는 ML Engineer로서 기술적으로 더 성숙해지고 싶습니다."},"vault/Notion/DB/DB-Blog-Post/2D-Object-Detection/2D-Object-Detection":{"slug":"vault/Notion/DB/DB-Blog-Post/2D-Object-Detection/2D-Object-Detection","filePath":"vault/Notion/DB/DB Blog Post/2D Object Detection/2D Object Detection.md","title":"2D Object Detection","links":[],"tags":[],"content":"\n참조\n개요\nBirdNET: A deep learning solution for avian diversity monitoring\n\nAbstract\n소개\n방법\n데이터 증강\n모델 아키텍쳐\nTrain\n결과\n\n\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nreader.elsevier.com/reader/sd/pii/S1574954121000273\n\n\n\n개요\n같이 스터디를 하는 분들과 Kaggle - BirdCLEF2023에 참가하게 되었습니다. 이전에 개최되었던 비슷한 대회들을 찾아본 결과, BirdNET이 높은 성능을 보였음을 확인했습니다.\n이에 BirdNET을 기본으로 실험을 진행하고자 논문을 읽고 이곳에 정리합니다.\nBirdNET: A deep learning solution for avian diversity monitoring\nAbstract\n공간 및 시간에 따른 조류 다양성의 변화는 환경 변화를 평가하는 지표로 사용됩니다. 기존에는 전문가가 이러한 데이터를 수집했으나, 수동적으로 수집한 음향 데이터가 대체 조사 기법으로 떠오르고 있습니다. 하지만 대규모 Audio Dataset에서 정확한 종 풍부도 Data를 추출하는 것은 어려운 일이었습니다.\n그러나 최근 DNN의 발전은 음향 이벤트 감지 및 분류 분야에서 기존의 신호처리를 능가하는 성능을 발휘하는 경우가 많습니다.\n이에 저자는 BirdNET이라고 하는 984개의 북미 조류 및 유럽 조류를 소리로 분류할 수 있는 Network를 제안합니다.\nBirdNET은 ResNet에서 파생되었으며 광범위한 데이터 전처리, 증강 및 혼합을 사용하여 학습되었습니다.\nBirdNET은 3개의 Dataset에 대하여 평가되었습니다.\n\n22960개의 단일 종 녹음\n연구자들이 현장에서 조류 다양성을 측정하는데 사용하는 도구와 유사한 설계의 자율 녹음 장치 배열로 수집한 286시간 분량의 완전 Labeling된 사운드스케이프 Data\n전문 조류 관잘차들이 자주 찾는 4개의 eBird 핫스팟 근처에 배치한 단일 고품질 무지향성 미이크의 33,670 시간 분량의 사운드스케이프 데이터\n\n여기서 저자는 노이즈, 겹치는 발성에 대응하려면 도메인 별 데이터 증강이 핵심이라는 사실과 입력 Spectrogram을 고해상도로 만들수록 분류 성능을 올릴 수 있다는 것을 발견했습니다.\n소개\n생태계의 건강을 평가하는 것이 중요하며, 이에 조류는 대부분의 환경에 서식하기 때문에 모니터링 대상으로 널리 활용된다고 합니다.\n이를 기반으로 어디에 어느 종이 얼마나 사는지 파악하는 것의 중요성을 이야기하며, 이를 위해 환경 소리를 녹음하는 작업이 어떻게 이루어지며 어떤방식으로 발전했는지 소개합니다.\n그리고 딥러닝을 사용하기 이전에 BirdCLEF 챌린지에 어떤 솔루션들이 사용되었는지 소개합니다.\n마지막으로 스펙트로그램을 이용한 CNN 분류기를 사용하는 방식이 당시 가장 좋은 성능냈음을 보여주며, 몇 개의 레이어만 있는 잘 설계된 얕은 네트워크가 일반적인 이미지 분류에 사용되는 깊은 네트워크와 비슷한 성능을 내는 결과를 통해 조류의 발성과 같이 매우 많은 Sound Class를 분류하기 위해서는 새 소리의 전반적인 특성과 디테일한 특성들 모두 동등하게 중요하다고 주장합니다.\n방법\n저자가 훈련에 사용하기 위해 데이터를 선별하는 과정을 설명하고, 최종적으로 어떤 데이터들이 포함됬는지 소개합니다.\n추가로, 기존에 가장 일반적인 FP Detection 결과가 다른 동물이나 곤충의 소리 혹은 다른 환경 잡음이었으므로 구글(Stowell and Plumbley, 2013) 및 WarblR Dataset에서 다른 데이터셋에서 노이즈로 판단해야 하는 소리들을 가져와 ‘기타 동물’, ‘사람’, ‘환경 소음’ 세 가지로 분류하여 사용했습니다.\n스펙트로그램의 경우 해상도가 중요하기 때문에 아래 파라미터를 사용했습니다.\n\nFFT Window Size : 10.7ms(512 Samples at 48kHz Sampling Rate)\nOverlap : 25%\n1 Frame = 8ms\n\n또한 대부분의 새 발성 주파수는 250Hz ~ 8.3kHz 사이로 제한되는데, 스펙트로그램의 주파수 범위를 대부분의 조류 발성 주파수 범위를 포함하면서도 데이터 증강 중에 피치 이동을 위한 여지를 남겨두기 위해 150Hz~15kHz의 값으로 제한했습니다.\n또한 선행 연구 결과에 의거해 500Hz까지 대략적인 선형 스케일링을 달성하기 위해 64개 대역과 1750Hz의 중단 주파수를 가진 멜 스케일을 이용하여 주파수 압축을 수행하여 씨그러운 환경에서 새의 울음소리를 분류를 더 잘 되도록 하였습니다.\n데이터당 소리의 길이는 경험적으로 3초로 하였으며, 데이터에서 새들은 평균 1.94초 정도의 울음소리를 냅니다.\n데이터 증강\n저자는 아래 3개 방식으로 데이터를 증강 했습니다.\n\n\nVertical Roll (p=0.5)\n\n멜스펙트로그램이므로, 수직방향 이동은 주파수를 바꾸는 것을 의미합니다.\n모든 평가 시나리오에서 점수가 향상되었다고 합니다.\n\n\nWraping (p=0.5)\n\n사람의 음성 인식에서 널리 사용되는 기법입니다.\n\n\nNoise Sample Addition (p=0.5)\n\n\n새 소리가 없는 음원에서 추출한 BG Noise를 무작위 가중치로 합성하는 방식입니다.\n\n\n가장 강력한 증강 방법이라고 합니다.\n\n\n\n\n모델 아키텍쳐\nBirdNET은 ResNet을 기반으로 합니다. 저자는 K=4(네트워크 폭), N=3(네트워크 깊이)로 네트워크를 구축했습니다.\n\n\nPre-Processing Group : 5x5 단일 Conv와 1x2 max pooling을 수행합니다.\nResStack Group : Feature Extraction을 수행합니다.\n\nDownsampling Block은 여기에서 제안한 방법을 사용합니다.\nResBlock은 원본 Wide ResNet의 설계를 따릅니다.\n\n\nClassification Block : 분류를 수행합니다.\n\nTrain\nClass Imbalance를 해결하기 위해 Oversampling을 수행했습니다.\nFocal Loss의 경우 모델 성능을 향상시키지 못했습니다.\n동시에 발성하는 조류 종을 시뮬레이션 하기 위해 최대 3개의 스펙토그램을 하나의 샘플에 무작위로 결합하는 Mix-Up을 사용했습니다.\nOptimzier는 Adam을 사용했으며 Lr은 1e-3, Batch Size는 32를 사용했습니다.\nScheduler의 경우 valid loss가 감소하지 않을 경우 0.5배씩 낮추는 방식을 사용했습니다.\nDropout의 경우 0.5를 초기값으로 Scheduler가 Lr를 줄일 때 마다 0.1씩 줄였습니다.\n3 Epoch이상 성능 향상이 없다면 훈련을 종료하도록 Eearly Stoping을 적용했습니다.\n결과\n샘플별 평균 정밀도(mAP)와 Class 별 평균 정밀도(cmAP)를 Metric으로 사용하여 시스템을 평가했습니다. mAP는 일반적인 종에 더 높은 순위를 부여하므로 Class Imbalance에 대해 이점을 얻었지만, cmAP는 Class 별로 정밀도를 구한 것의 평균이기 때문에 모든 Class를 잘 맞춰야 합니다.\n(이하 생략, 저자의 모델로 데이터셋들에 대한 결과를 평가하고 결론을 맺습니다.)\n훈련 과정에서 저자가 성능향상을 얻은 포인트는 아래와 같습니다.\n\n입력 스펙트로그램의 높은 시간 분해능(짧은 FFT Window Length)를 사용하여 성능을 향상했습니다.\nMix-Up을 사용한 다중 Label 분류는 모든 Dataset에 대해 성능 향상을 얻었습니다.\n더 깊은 토폴로지(더 많은 레이어)가 더 넓은 토폴로지(더 많은 필터)보다 반드시 더 나은 성능을 발휘하는 것은 아닙니다.\nOver Sampling을 제외하면, Class Imbalance에 대해 전체 점수를 개선하지 못했습니다.\n"},"vault/Notion/DB/DB-Blog-Post/AWS-ECS+S3로-MLFlow-구축해보기-1/AWS-ECS+S3로-MLFlow-구축해보기-1":{"slug":"vault/Notion/DB/DB-Blog-Post/AWS-ECS+S3로-MLFlow-구축해보기-1/AWS-ECS+S3로-MLFlow-구축해보기-1","filePath":"vault/Notion/DB/DB Blog Post/AWS-ECS+S3로 MLFlow 구축해보기 1/AWS-ECS+S3로 MLFlow 구축해보기 1.md","title":"AWS-ECS+S3로 MLFlow 구축해보기 1","links":["vault/Notion/DB/DB-Blog-Post/ML-Flow---기본-사용/ML-Flow---기본-사용"],"tags":[],"content":"\n참조\n시작에 앞서\n시작하며\n어떻게 구성할까\n왜 저 구성일까\nAWS s3 Bucket\nAWS RDS(RDB로 사용)\nAWS ECR\nAWS ECS - Fargate\n마치며\n\n\n참조\naws.amazon.com/ko/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/\ndocs.aws.amazon.com/ko_kr/elasticloadbalancing/latest/application/introduction.html\ndingrr.com/blog/post/rds를-써야-하나요-ec2에-설치하면-안되나요\naws.amazon.com/ko/s3/\ndocs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/Welcome.html\naws.amazon.com/ko/fargate/\njsonobject.tistory.com/536\naws.amazon.com/ko/blogs/compute/building-deploying-and-operating-containerized-applications-with-aws-fargate/\naws.amazon.com/ko/ecr/\n시작에 앞서\n이 글은 2부로 나뉘어져 있습니다.\n\naws에 mlflow를 올리기 위해 필요한 각 구성요소에 대한 설명\naws에 실제로 mlflow를 올리기 위한 과정들\n\n시작하며\n팀 내에서 모델을 사용하여 서비스를 배포하기 위해 여러 준비를 하고 있었습니다. 그 중 고민이 몇 가지 있었는데 다음과 같습니다.\n\n\n모델 버전관리의 어려움\n기존에는 사람이 진행했지만 B2B 서비스라 같은 모델이어도 회사 별로, 버전 별로 관리할 필요가 있기 때문에 시스템으로 관리하고 싶었습니다.\n\n\nDocker Image와 분리\n모델 업데이트 될 때 마다 이미지를 다시 배포하고 싶지 않았습니다. 모델이 아직 안정적으로 서비스 할 수 있는 상황이라는 것을 장담할 수 없기 때문에 자주 업데이트하고, 편하게 업데이트 하고 싶었습니다.\n\n\n괜찮은 방법이 없나 찾아보던 중, MLFlow가 눈에 들어왔습니다. 마침 팀에서 공통적으로 사용할만한 tracking 툴도 없었겠다, 이 참에 구축해보자고 마음먹었습니다.\n\n\n                  \n                  MLFlow는 실험, 재현성, 배포 및 중앙 모델 레지스트리를 포함한 ML 수명 주기를 관리하는 오픈 소스 플랫폼입니다. MLFlow에 대한 기본적인 정보는 아래에 정리해 놓았습니다. \n                  \n                \n\n\nML Flow - 기본 사용\n\n\n\n어떻게 구성할까\nMLFlow 서비스를 구성하는 방법은 여러가지 있습니다. 당연하지만 On-premise로 구축하는 것이 가장 쉽고 저렴합니다. 하지만 관리 포인트를 줄이고 싶고, 모델 다운로드를 위해 사내 PC로 외부에서 접속하게 포트를 개방하는 것은 껄끄러운 부분이 있어 AWS에 구축하기로 했습니다.\n마침 AWS에서 예제를 제공해줘서 따라하면서 구성할 수 있었습니다. AWS 예제를 통해 구성되는 모양은 아래와 같습니다.\n\n이는 MLFlow 공식 문서의 시나리오 4에 해당하는 구성입니다.\n\nflowchart TD\n\tA[&quot;Inference Server&quot;] --&quot;&#039;Metric + Parameter Data&#039;&quot;--&gt; C;\n\tsubgraph AWS\n\tC[&quot;ALB&quot;]--&gt;D;\n\tD[&quot;ECS Instance(MLFlow Server)&quot;]--&gt;E;\n\tE[(&quot;MySQL (RDB)&quot;)]\n\tF[(&quot;s3 bucket&quot;)]\n\tend\n\tA--&quot;Artifact(model weight, image, ..etc)&quot;--&gt;F\n\t\n\n왜 저 구성일까\n위와같이 구성한다면, local에서 logging을 수행할 때 s3 bucket에 데이터를 올리기 위한 접근 권한 설정이 필요합니다. 사용자 입장에서는 tacking server로 artifact를 넘기고 tracking server가 s3 bucket에 데이터를 넘기면 사용하기 더 편할 것입니다.\n하지만 서버 입장에서는 artifact 같은 대형 데이터를 유저가 직접 업로드 해준다면, 부하가 매우 적어질 것입니다. 또한 위 그림처럼 중간에 ALB(Application Load Balancer)와 같이 이동한 데이터에 따라 과금이 되기 때문에 주의가 필요합니다.\n\nInstance 하나를 Tracking Service겸 DB겸 Artifact Storage로 사용한다면?\n\nInstance가 날라 갈 경우 위 데이터들이 사라지지 않도록 설정이 필요합니다. 또한 Artifact의 사용량에 따라 볼륨을 더 크게 확장하고 마이그레이션을 수행해야 합니다.\n\n\n\n→ 비용을 낼 것인가, 우리의 시간을 쓸 것 인가의 선택입니다.\nAWS s3 Bucket\n\ns3 Bucket은 비정형 데이터를 저장하기 위한 서비스입니다. 저 Bucket은 크기가 정해져 있지 않습니다. 우리는 계속 데이터를 넣을 수 있으며, 넣은 만큼만 요금을 내면 됩니다. 더 자세한 설명은 공식 페이지를 참조해주세요.\ns3의 장점은 ‘최대 크기에 제한이 없고 사용한 만큼 낸다’입니다. 따로 Storage를 잡는다면 artifact가 얼마나 생길지 예측해서 용량을 할당해야 합니다. 그러나 s3 Bucket을 쓰면 그럴 필요가 없습니다. 심지어 저렴합니다.\nAWS RDS(RDB로 사용)\n\nRDS는 AWS가 제공하는 DB 서비스입니다. 물론, ec2를 사용하여 직접 DB 서버를 만들 수도 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRDS• AWS가 모든 권한과 책임을 가지고 있습니다.  • AWS에서는 Oracle, MSSQL, MySQL, PostgreSQL, MariaDB 를 지원합니다.  • 뿐만 아니라, 자체 DB인 Aurora 도 제공합니다.  • Read Replica를 쉽게 만들고 각 AZ에 두고 사용할 수 있습니다.  • 마그네틱 스토리지는 3,000IOPS 까지 지원합니다.  • 프로비전된 SSD는 최대 10,000 IOPS 까지 지원합니다.DB on EC2• 완전한 권한이 AWS 사용자로 넘어옵니다. 모든 책임과 권한을 사용자가 가집니다.  • DB를 잘 알고 있다면 RDS를 뛰어넘을 튜닝의 여지가 있습니다.  • EC2 인스턴스의 프로비전된 SSD는 최대 8,000 IOPS를 지원합니다.  • 여기서도 데이터 암호화를 지원합니다. EC2 → EBS 통신간에도요!  • 모든 설정이 자유로우니, 포트도 원하는데로, 그리고 원하면 하나의 EC2에 여러개의 DB를 설치할 수 도 있습니다.  • 딱딱한 RDS 비용체계에 비해 유연하기 때문에 잘 설계하면 비용면에서 더 유리 할 수 있습니다.\n직접 만드는 것이 더 저렴합니다. 그러나 장기적으로 직접 유지보수를 해야 하므로 DB를 유지보수 할 인력이 없는 경우에는 RDS를 이용해보는 것도 괜찮다고 생각합니다.\n더 세부적인 비교는 이 글을 참조해주세요.\nAWS ECR\n\nECR(Elastic Container Registry)은 AWS만의 Docker hub입니다. public과 private으로 나뉘어져 있으며, public은 모두가 사용할 수 있게 공개되고 private는 제한된 계정만 접근할 수 있게 됩니다.\nECR의 경우 금액은 public, private 모두 리포지토리 저장된 데이터로 계산되며 GB당 0.10$씩 월마다 청구됩니다.\n\n다만 public의 경우, 월 50GB만큼은 요금을 부과하지 않습니다.\n\n더 자세한 설명과 사용법은 공식 문서와 이 정리글을 참조하는 것이 좋습니다.\nAWS ECS - Fargate\n\n예제는 AWS가 제공하는 ECS(Elastic Container Service)의 Fargate를 사용하여 MLFlow 서비스를 위한 Instance를 생성합니다. 기존에는 AWS에서 인스턴스를 생성하려면 ‘인스턴스 패밀리’ 테이블을 보고 목적에 맞는 HW 구성을 찾아야 했습니다.\nAWS Fargate는 우리가 선정한 사양(cpu n개, RAM xGB)에 맞춰 알아서 Instance를 생성합니다. 또한 필요에 따라 Instance 숫자를 마음대로 조절할 수 있습니다. 즉, 서버 크기 조정, 패치, 보안 및 관리의 운영 부담을 없애줍니다.\n실제로 사용하기 위해서는 2개의 과정이 필요합니다.\n\nAWS ECR에 Docker Image 등록\nAWS ECS에 Cluster 및 Service 생성\n\nFargate를 사용하면 ec2 instance 생성 → docker 설치 → image pull 등 의 작업을 할 필요 없습니다. 또한 만약 image를 업데이트했다면, ecs에서 자동으로 감지하고 instance를 업데이트합니다.\n또한 computing power가 부족하다면, 간단한 숫자 변경으로 동일 service instance를 scale-up 할 수 있고, 그 반대도 마찬가지 입니다.(cpu사용률 등을 조건으로 한 trigger에 따라 자동으로 scale-up ~ down 하는 기능도 존재합니다.)\n더 자세한 설명은 공식 문서1, 공식 문서2와 AWS Fargate 개념 및 사용법 정리 글을 보시는 것을 추천합니다. 또한 ECS 구성은 이 글을 참조하는 것이 좋습니다.\n마치며\nAWS에 MLFlow를 구성하기 위해 필요한 요소들을 알아보았습니다. 다만 위 구성은 절대적인 것이 아니며 사실 더 간단하고 쉽게 구축할 수도 있습니다. 그러나 그럼에도 불구하고 위 구성으로 글을 진행하는 이유는 두 가지 있습니다.\n\n비용에 대한 부담이 없다면 나쁘지 않은 구성이라고 생각하기 때문입니다.\nAWS에서는 위 구성을 예제로 제시하고 있는데, 생략되어 있는 부분이 많아 초심자에게는 생각보다 쉽지 않고 시행착오가 필요하기 때문에 정리글이 있으면 좋겠다고 생각했습니다.\n\n이상 글을 마치며 다음에는 실제 구축 과정을 다뤄보겠습니다."},"vault/Notion/DB/DB-Blog-Post/AlexNet/AlexNet":{"slug":"vault/Notion/DB/DB-Blog-Post/AlexNet/AlexNet","filePath":"vault/Notion/DB/DB Blog Post/AlexNet/AlexNet.md","title":"AlexNet","links":[],"tags":[],"content":"참조\nkr.mathworks.com/help/deeplearning/ref/alexnet.html\nen.wikipedia.org/wiki/AlexNet\narxiv.org/abs/1404.5997v2\nbskyvision.com/entry/CNN-알고리즘들-AlexNet의-구조\nm.blog.naver.com/PostView.naver\ntaeguu.tistory.com/29\nAlexNet이란?\n\nAlexNet는 Alex Krizhevsky, Geoffrey Hinton 및 Ilya Sutskever에 의해 설계된 매우 유명한 합성곱 신경망입니다.\n2012년 ImageNet 대규모 시각 인식 도전 과제 (ILSVRC)에 사용된 첫 번째 대규모 CNN이었습니다. AlexNet은 총 8 개의 계층으로 구성되어 있으며, 5 개의 합성곱 계층, 3 개의 완전 연결 계층 및 하나의 최종 출력 계층으로 구성됩니다.\nAlexNet에서 주목할 점\nAlexNet는 ReLU 활성화 함수를 사용하여 훈련 속도를 높이고 Dropout 계층을 사용하여 과적합을 줄이는 등의 여러 혁신적인 기능으로 설계되었습니다.\n\nParallel Network\n\nGPU 연산을 고려해 두 개의 벙렬 네트워크로 구성\n\n\nReLU(Rectified linear unit)\n\nLeNet-5에서 사용되었던 Tanh 함수 대신에 ReLU 함수가 사용되었습니다. 같은 정확도를 유지하면서 Tanh을 사용하는 것보다 6배나 빠르다고 하며, AlexNet 이후에는 활성화함수로 ReLU 함수를 사용하는 것이 선호되고 있습니다.\n\n\nLRN(Local Response Normalization)\n\nReLU의 특성상 Conv나 Pooling 시 매우 높은 하나의 픽셀 값이 주변 픽셀에 영향을 미치게 됩니다. 이런 부분을 방지하기 위해 Activation Map의 같은 위치에 있는 픽셀끼리 Normalize을 수행합니다. 현재는 Batch Normalization을 사용합니다.\n\n\nDropout\n\nDropout은 fully-connected layer의 뉴런 중 일부를 생략하면서 학습을 진행하는 것입니다. 무작위로 몇몇 뉴런의 값을 0으로 변경합니다. 따라서 그 뉴런들은 forward pass와 back propagation에 아무런 영향을 미치지 않습니다. Dropout은 훈련시에 적용하는 기법이며, 테스트시에는 모든 뉴런을 사용합니다.\n\n\nMax Pooling\n\nPooling은 컨볼루션을 통해 얻은 Feature Map의 크기를 줄이기 위해 사용하며, LeNet-5의 경우 Average Pooling을 사용한 반면, AlexNet에서는 Max Pooling을 사용했습니다.\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Analysis-of-model-behaviors/Analysis-of-model-behaviors":{"slug":"vault/Notion/DB/DB-Blog-Post/Analysis-of-model-behaviors/Analysis-of-model-behaviors","filePath":"vault/Notion/DB/DB Blog Post/Analysis of model behaviors/Analysis of model behaviors.md","title":"Analysis of model behaviors","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4"],"tags":[],"content":"\n참조\nAnalysis of model behaviors\n\n\n참조\nWeek 4\nAnalysis of model behaviors\n\n\nEmbedding feature analysis\n\n\nDataset들에 대한 Model의 고차원 high level feature vector를 수집(모델의 뒷부분, High level feature)\n알고싶은 이미지도 마찬가지로 inferrence 시켜서 high level feature vector를 수집\nfeature vector들끼리의 유사성으로 해당 이미지와 유사한 high level feature vector를 Dataset 내의 이미지들을 얻을 수 있음\n\n\n\n\nt-SNE(t-distributed stochastic neighbor embedding)\n\n고차원 백터를 저차원으로 표현할 수 있는 방법\n\n\n\n\nCAM(Class activation mapping)\n\n\nConv블록의 뒤에 3ch conv블록 생성, class에 대한 weighted sum\n\n\nhitmap 느낌으로 모델이 인지한 부분을 볼 수 있음\n\n\n\n모델이 처음부터 저 모양이라면 바로 쓸 수 있지만, 그렇지 않은 경우(FC layer 등이 있는 경우) 해당 부분을 제거하고 새롭게 추가한 신경망을 (GAP Layer + FC Layer)을 다시 훈련시켜야 함\n\n\n\n\nGrad-CAM\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1610.02391.pdf\n\n\n\n\n\n                  \n                  Grad-CAM - 새내기 코드 여행 \n인공지능은 이미 거의 모든 분야에서 다양한 용도로 사용되고 있습니다.\njoungheekim.github.io/2020/10/14/paper-review/\n                  \n                \n\n\n\nCAM을 일반화 한 방법\n\n\n"},"vault/Notion/DB/DB-Blog-Post/BCELoss(Binary-Cross-Entroby-Loss)":{"slug":"vault/Notion/DB/DB-Blog-Post/BCELoss(Binary-Cross-Entroby-Loss)","filePath":"vault/Notion/DB/DB Blog Post/BCELoss(Binary Cross Entroby Loss).md","title":"BCELoss(Binary Cross Entroby Loss)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\nBCELoss(Binary Cross Entroby Loss)\n\n\n참조\npytorch.org/docs/stable/generated/torch.nn.BCELoss.html\nWeek 2\nBCELoss(Binary Cross Entroby Loss)\n\n결과 값이 두개뿐인 시나리오에서 사용됨\n\nL=l_1(f(x_1),y_1)+l_2(f(x_2),y_2)\n\np : 예측 확률\ny : 지표(BCE의 경우 0 또는 1)\nt : n개의 class중 임의의 class의 index\n"},"vault/Notion/DB/DB-Blog-Post/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)":{"slug":"vault/Notion/DB/DB-Blog-Post/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)","filePath":"vault/Notion/DB/DB Blog Post/Basic Optimizer & Adam(Adaptive Moment Esimation)/Basic Optimizer & Adam(Adaptive Moment Esimation).md","title":"Basic Optimizer & Adam(Adaptive Moment Esimation)","links":[],"tags":[],"content":"\n참조\nAdam에 앞서 알아야 할 Optimizer\nAdam(Adaptive Moment Esimation)\nBias Correction\nAdam의 문제\n\n\n참조\n\n\n                  \n                  Adam: A Method for Stochastic Optimization \n                  \n                \n\n\nWe introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\narxiv.org/abs/1412.6980\n\n\n\nvelog.io/@freesky/Optimizer\nhazel01.tistory.com/36\nropiens.tistory.com/90\ntowardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c\nstats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for\nangeloyeo.github.io/2020/09/26/gradient_descent_with_momentum.html\nAdam에 앞서 알아야 할 Optimizer\n\n\n\n\nBasic Gradient Descent\n==\\theta_{t}=\\theta_{t-1}-\\eta \\nabla_{\\theta}J(\\theta_{t-1}) : Calculate Gradient &amp; Parameter Update==\n\n\\theta_t : model’s parameter at the step t\nJ(\\theta_{t-1}) : Loss Function\n\\eta : Learning Rate\n\\nabla_{\\theta} : Partial Derivative for theta\n\n\n\nMomentum : 이전의 Gradient를 곱하여 추가로 더합니다.\n하지만 이전 값을 기억해야 하기 때문에 기존 대비 2배의 메모리가 필요합니다.\n==v_{t}=\\gamma v_{t-1}+\\eta \\nabla J(\\theta_{t-1}) : Calculate Gradient==\n==\\theta_{t} =\\theta_{t-1}-v_{t} : Parameter Update==\n\n\\gamma : Momentum term, 0.9\nv_{t} : Momentum Update at the step t\n\n\n\nAdaGrad(Adaptive Gradient) : Parameter별로 다른 학습률을 적용합니다.\nGradient가 커서 변화가 많은 Parameter는 학습률을 감소시켜 다른 변수들이 잘 학습되도록 합니다.\n==g_{t}=g_{t-1}+({\\nabla J(\\theta_{t-1})})^2 : Calculate Size of the Gradient==\n→ 학습이 진행될 때 마다 계속해서 gradient를 누적해서 더하기 때문에, 분모항이 계속 커지게 됩니다. 따라서, 점점 학습이 이루어지지 않습니다.\n==\\theta_t = \\theta_{t-1} - {{\\eta} \\over \\sqrt{g_t + \\epsilon}} \\nabla f(\\theta_{t-1}) : Parameter Update==\n\ng_t : Gradient Size\n\\epsilon : A very small value to add to prevent \\sqrt{g_t + \\epsilon} from going to zero\n\n\n\nRMSProp : AdaGrad와 같은 컨셉이지만, 누적 합이 아닌 지수 평균을 사용합니다.\n변수 간의 상대적 학습 차이를 유지하면서, g가 무한정 커지는 것을 막아 오래 학습할 수 있습니다.\n==g_t=\\gamma g_{t-1}+(1-\\gamma)({\\nabla J(\\theta_{t-1})})^2 : Calculate Size of the Gradient==\n==\\theta_t = \\theta_{t-1} - {{\\eta} \\over \\sqrt{g_t + \\epsilon}} \\nabla f(\\theta_{t-1}) : Parameter Update==\n\n\\gamma : Update Rate, 0 ~ 1\n\n\n\nAdam(Adaptive Moment Esimation)\nAdam은 Momentum 와 RMSProp의 장점을 결합한 알고리즘 입니다.\n==v_t = \\beta_2 v_{t-1}+(1 - \\beta_2)(\\nabla_{\\theta}J(\\theta_{t-1}))^2 : Calculate Size of the Gradient==\n==m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)\\nabla_{\\theta}J(\\theta_{t-1}) : Calculate Gradient==\n==\\widehat{m_t}={m_t \\over {1 - \\beta^t_1}} : Bias Correction for m_t==\n==\\widehat{g_t}={g_t \\over {1 - \\beta^t_2}} : Bias Correction for g_t==\n==\\theta_t = \\theta_{t-1}-{{\\eta} \\over \\sqrt{\\widehat{v_t} + \\epsilon}}\\widehat{m_t} : Parameter Update==\nBias Correction\nAdam의 Bias Correction 은 update에 사용하는 각 Term이 0으로 편향되는 것을 보정하기 위해 사용합니다.\nGradient Descent with Momentum, RMSProp, ADAM은 모두 beta&lt;1 값을 이용해 이전 값들을 서서히 잊어가는 Exponentially Weighted Moving Average (EWMA)의 일종이라고 할 수 있습니다.\nEWMA는 일반적으로 아래와 같은 수식으로 쓸 수 있습니다.\n\nv_t=\\beta v_{t-1} + (1-\\beta)x(t)\n\nbeta는 이전 값을 얼마나 기억할지를 의미하므로 1에 가깝게 설정할수록 Smoothing이 많이 됩니다.\nraw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-09-26-gradient_descent_with_momentum/picN.mp4\n다만, 위 영상과 같이 Smoothing을 키울수록 초기 time의 Smoothing 결과가 원래 데이터들의 포인트에 비해 낮게 나오는 것을 알 수 있습니다. (0으로 편향됩니다)\n이 때, 아래 수식을 사용하여 보정할 수 있습니다.\n\n\\widehat{v_t}={{v(t)} \\over {1-\\beta^t}}\n\nraw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-09-26-gradient_descent_with_momentum/picM.mp4\nAdam에서, Bias Correction을 사용한 경우와 사용하지 않은 경우의 예시는 아래와 같습니다.\n\nAdam의 문제\nAdam의 뛰어난 성능에도 불구하고, 일부 경우에서 Adam이 최적 솔루션으로 수렴하지 않는다는 사실이 밝혀졌습니다.\n이를 해결하기 위해서는 SGD와 Mometerm을 사용해야 가능합니다. Adam과 SGD와의 격차를 좁히기 위해 여러 연구가 진행되었고, Nitish Shirish Keskar와 Richard Socher는 ‘Improving Generalization Performance by Switching from Adam to SGD’라는 논문을 통해 Adam으로 훈련 중에 SGD로 전환하는 방식으로 Adam만 사용할 때보다 더 나은 일반화 능력을 얻을 수 있음을 보여주었습니다.\n\n저자는 훈련의 초기 단계에서 Adam이 여전히 SGD를 능가하지만 나중에 학습이 포화된다는 것을 발견했습니다.\n따라서 Adam과 함께 심층 신경망 교육을 시작한 다음 특정 기준에 도달하면 SGD로 전환하는 SWATS라는 전략을 제안했습니다.\n"},"vault/Notion/DB/DB-Blog-Post/BiFPN(Neck,-EfficientDet)/BiFPN(Neck,-EfficientDet)":{"slug":"vault/Notion/DB/DB-Blog-Post/BiFPN(Neck,-EfficientDet)/BiFPN(Neck,-EfficientDet)","filePath":"vault/Notion/DB/DB Blog Post/BiFPN(Neck, EfficientDet)/BiFPN(Neck, EfficientDet).md","title":"BiFPN(Neck, EfficientDet)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10"],"tags":[],"content":"\n참조\nBiFPN(Neck, EfficientDet)\n\n\n참조\n\n\n                  \n                  EfficientDet: Scalable and Efficient Object Detection \n                  \n                \n\n\nModel efficiency has become increasingly important in computer vision.\narxiv.org/abs/1911.09070\n\n\n\nWeek 10\ngaussian37.github.io/dl-concept-bifpn/\nBiFPN(Neck, EfficientDet)\n\n\nTop-Down 방식만을 사용하는 FPN을 개선하기 위해 다양한 시도가 있었고 개선 방향의 핵심은 ‘어떻게 low-level feature와 high-level feature를 합치는 것인가’ 였습니다.\nEfficientDet은 위 (d)와 같이 top-down과 bottom-up 구조를 동시에 사용하는 bidirectional 구조를 사용하여 여러 단계의 BiFPN layer를 쌓는 방식을 제안했습니다.\n또한 Weighted Featrue Fusion을 제안했습니다.\n\n\nFPN과 같이 단순 Summantion이 아니라 feature 별 가중치를 부여해서 summantion\n\n\nFeature별 가중치를 통해 중요한 feature를 강조하여 성능 상승\nP^{td}_{6} = Conv({{w_1 \\cdot P^{in}_6 + w_2 \\cdot Resize({P^{in}_7})} \\over {w_1+w_2+\\epsilon}})\n→ \\epsilon : 분모가 0이 되지 않기 위한 매우 작은 값\n\n"},"vault/Notion/DB/DB-Blog-Post/Binary-Tree란-무엇일까/Binary-Tree란-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/Binary-Tree란-무엇일까/Binary-Tree란-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/Binary Tree란 무엇일까/Binary Tree란 무엇일까.md","title":"Binary Tree란 무엇일까","links":[],"tags":[],"content":"\n참조\nBinary Tree\nFull Binary Tree\nComplete Binary Tree\nBalanced Binary Tree\n\n\n참조\nratsgo.github.io/data structure&amp;algorithm/2017/10/21/tree/\nBinary Tree\nChild Node가 최대 두 개인 node들로 구성된 Tree\n정이진트리(full binary tree), 완전이진트리(complete binary tree), 균형이진트리(balanced binary tree) 등이 있음\nFull Binary Tree\n모든 레벨에서 노드들이 꽉 채워진(=잎새노드를 제외한 모든 노드가 자식노드를 2개 가짐) 이진트리\n\nComplete Binary Tree\n마지막 레벨을 제외한 모든 레벨에서 노드들이 꽉 채워진 이진트리\n\nBalanced Binary Tree\n모든 잎새노드의 깊이 차이가 많아야 1인 트리, 균형이진트리는 예측 가능한 깊이(predictable depth)를 가지며, 노드가 n개인 균형이진트리의 깊이는 log(n)을 내림한 값이 됨\n"},"vault/Notion/DB/DB-Blog-Post/Competition---Template":{"slug":"vault/Notion/DB/DB-Blog-Post/Competition---Template","filePath":"vault/Notion/DB/DB Blog Post/Competition - Template.md","title":"Competition - Template","links":[],"tags":[],"content":"==**[이곳에 Github Repos 링크를 추가합니다]**==\n\nTeam Member\n요약\n\nMy Contribute\nTask\nEvaluation Metric\nData\n\n\n개요\n본론\n\nEDA\nData\nTest\n\n\n결론\n\n\nTeam Member\n\n{name} - [{github profile link}]\n\n요약\nMy Contribute\nTask\nEvaluation Metric\nData\n\n개요\n==**[이곳에 대회 페이지 링크를 추가합니다]**==\n\n대회에 대한 개요, 간략한 설명을 추가합니다.\n\n\n본론\nEDA\n\n.\n\nData\n\n.\n\nTest\n\n\n.\n\n\n\n결론\n\n결과\n\n.\n\n\n느낀점\n\n.\n\n\n관련문서\n"},"vault/Notion/DB/DB-Blog-Post/Computational-Graph/Computational-Graph":{"slug":"vault/Notion/DB/DB-Blog-Post/Computational-Graph/Computational-Graph","filePath":"vault/Notion/DB/DB Blog Post/Computational Graph/Computational Graph.md","title":"Computational Graph","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\nComputational Graph\nDefine and Run\nDefine by Run(Dynamic Computational Graph, DCG)\n\n\n참조\nWeek 2\nComputational Graph\n\n\n연산의 과정을 그래프로 표현\n\n\n\n모든 신경망은 계산 그래프이다\n\nDefine and Run\n\n그래프를 먼저 정의, 실행 시점에 데이터를 feed\n\nDefine by Run(Dynamic Computational Graph, DCG)\n\n실행하면서 그래프 생성\n"},"vault/Notion/DB/DB-Blog-Post/Conditional-GAN(cGAN)/Conditional-GAN(cGAN)":{"slug":"vault/Notion/DB/DB-Blog-Post/Conditional-GAN(cGAN)/Conditional-GAN(cGAN)","filePath":"vault/Notion/DB/DB Blog Post/Conditional GAN(cGAN)/Conditional GAN(cGAN).md","title":"Conditional GAN(cGAN)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-5/Week-5"],"tags":[],"content":"\n참조\nConditional generative model\n\n\n참조\narxiv.org/abs/1411.1784\nWeek 5\nConditional generative model\n\n기존 GAN에 추가적으로 Conditional Vector를 추가해서 원하는 결과를 생성할 수 있게 한다.\n"},"vault/Notion/DB/DB-Blog-Post/Cross-Entropy-Loss":{"slug":"vault/Notion/DB/DB-Blog-Post/Cross-Entropy-Loss","filePath":"vault/Notion/DB/DB Blog Post/Cross Entropy Loss.md","title":"Cross Entropy Loss","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\nCross Entropy Loss\n\n\n참조\nwandb.ai/wandb_fc/korean/reports/---VmlldzoxNDI4NDUx\nWeek 2\nCross Entropy Loss\n\n\n주로 분류 모델이 얼마나 잘 분류했는가를 측정하는데 사용하는 metric\n\n\nloss(error)는 0에서 1 사이로 측정됨( 0일 시, 완벽한 모델)\nL=l_1(f(x_1),y_1)+...+l_n(f(x_n),y_n) \\\\ l_c=-(y_c\\log(p_c) + (1-y_c)log(1-p_c)) \\\\ =-{\\sum_{c=1}^{n}}{y_c\\log(p_c)}\n\n"},"vault/Notion/DB/DB-Blog-Post/CycleGAN/CycleGAN":{"slug":"vault/Notion/DB/DB-Blog-Post/CycleGAN/CycleGAN","filePath":"vault/Notion/DB/DB Blog Post/CycleGAN/CycleGAN.md","title":"CycleGAN","links":[],"tags":[],"content":"참조\narxiv.org/abs/1703.10593\nvelog.io/@sjinu/CycleGAN\nwikidocs.net/146366\n개요\nimage-to-image Translation은 보통 pair-image를 이용해 학습합니다.\n다만, 같은 이미지에 대해 2개의 특성을 갖는 이미지 쌍은 구하기 쉽지 않습니다.\nCycle-GAN은 이런 pair-image를 사용하지 않고 단지 X 도메인 데이터세트와 Y 도메인 데이터세트만을 이용해 두 도메인 간에 이미지를 변환하는 법을 학습합니다.\nCycleGAN이란?\n\nCycleGAN은 Jun-Yan Zhu 등에 의해 개발된 이미지-이미지 변환을 위한 비감독 기계 학습 접근 방식입니다.\nCycleGAN은 쌍을 이루는 예제가 필요 없이 이미지를 한 도메인에서 다른 도메인으로 변환할 수 있습니다.\n모델은 소스 도메인에서 대상 도메인으로 이미지를 변환하는 것을 배울 수 있으며, 대상 도메인은 관련 이미지 컬렉션이나 완전히 새로운 도메인 모두 사용할 수 있습니다.\nCycle GAN의 특징 및 구조\n\n\nCycleGAN은 기존의 Generator G 외에 Generator F를 추가한 순환구조를 사용하여 Mode collapse를 방지합니다.\n\n순환구조이므로 두 개의 Generator, 두 개의 Discriminator가 존재\n종래의 GAN 모델과 다른 점은 Paired Training Data를 사용하지 않고 학습\nMode collapse : input의 특징을 다 잊어버리고 똑같은 출력을 생성하는 경우\n\ninput image에서 output image로 맵핑하는 동작 과정을 forward consistency라고 하고, 반대의 과정을 backward consistency라고 합니다.\nGenerator G와 F를 거쳐서 한 바퀴 돌아오면 다시 자기 자신으로 돌아와야 하기 때문에 순환 일관성(Cycle Consistency) 이라고 했고, Input image와 Generator G, F 한 바퀴를 돌아 생성된 Output image 간의 차이를 cycle consistency loss라고 칭했습니다.\nCycle GAN의 Loss\n종래의 GAN을 토대로 Loss함수를 작성한다면 아래와 같은 식이 됩니다.\n{L_{X \\to Y}}(G,D_Y,X,Y)=\\Epsilon_{y\\sim p(y)}{\\log D_Y(y)}+\\Epsilon_{x\\sim p(x)}{\\log (1-D_Y(G(x)))} \\\\ {L_{Y \\to X}}(F,D_X,X,Y)=\\Epsilon_{x\\sim p(x)}{\\log D_X(x)}+\\Epsilon_{y\\sim p(y)}{\\log (1-D_X(F(y)))}\n모델의 네트워크 용량이 크다면, 우리가 원하는 시각적 결과를 얻었건 얻지 않았건 상관 없이, Dy 혹은 Dx의 결과 값이 우리가 원하는 확률 분포를 출력하도록 모델이 학습될 수 있습니다.\n그러므로 단순 adversarial loss만으로는 x→y로의 translation을 보장 할 수 없습니다.\n여기서 Circle Constant(순환 일관성)이라는 개념이 등장합니다. 만약 G와 F가 잘 동작한다면, X→Y로의 전환해서 나온 값 y’을 다시 Y→로의 전환을 했을 때 얻은 x’은 유사해야 합니다.\nG(x)=\\hat y \\\\ F(\\hat y)=\\hat x\\\\ x \\approx \\hat x\n마찬가지로 y의 경우도 아래와 같습니다.\nF(y)=\\hat x \\\\ G(\\hat x)=\\hat y\\\\ y \\approx \\hat y\n이를 Forward cycle consistency라고 저자는 표현합니다.\n모델이 이 결과를 낼 수 있도록 저자는 Cycle consistency loss를 사용합니다.\nL_{cycle}(G,F,X,Y)=\\Epsilon_{x\\sim p(x)}||F(G(x))-x||_1 +\\Epsilon_{y\\sim p(y)}||G(F(y))-y||_1\nL1 Norm을 사용하는 이유는 F와 G의 adversarial loss를 사용했을 때, 성능 향상이 없었기 때문입니다.\n최종목적함수는 아래와 같습니다.\nL(G,F,D_x,D_y)=L_{X \\to Y}(G,D_y,X,Y)+L_{Y \\to X}(F,D_x,Y,X)+\\lambda L_{cycle}(G,F,X,Y)\nLambda는 앞의 두 Loss와 Lcycle간의 중요도를 조절한다.\n\n이 방법은 다른 Loss들과 비교 했을 때, 좋은 성능을 보여주었습니다.\n한계\n\n\n\n이미지의 모양을 바꾸진 못합니다. CycleGAN은 주로 분위기나 색상을 바꾸는 것으로 스타일을 학습하여 다른 이미지를 생성합니다. 그러다보니 피사체의 모양 자체는 바꿀 수가 없었습니다.\n\nEX) 사과를 오렌지로 바꾸는 작업\n\n\n\n데이터셋의 분포가 불안정하면 이미지를 제대로 생성할 수 없게 됩니다.\n\nEX) 사람을 태운 말을 얼룩말로 바꿀 때 사람까지 얼룩말이 되는 것은 데이터셋에 사람이 말을 탄 데이터가 없기 때문\n\n\n"},"vault/Notion/DB/DB-Blog-Post/DACON-예술작품-화가-분류-AI-경진대회/DACON-예술작품-화가-분류-AI-경진대회":{"slug":"vault/Notion/DB/DB-Blog-Post/DACON-예술작품-화가-분류-AI-경진대회/DACON-예술작품-화가-분류-AI-경진대회","filePath":"vault/Notion/DB/DB Blog Post/DACON 예술작품 화가 분류 AI 경진대회/DACON 예술작품 화가 분류 AI 경진대회.md","title":"DACON 예술작품 화가 분류 AI 경진대회","links":[],"tags":[],"content":"\n\n                  \n                  GitHub - 404Vector/Competition.DACON.ArtWorksPainterClassification: Dacon 화가 이미지 분류 \n                  \n                \n\n\nDacon 예술 작품 화가 분류 AI 경진대회 model 폴더 .\ngithub.com/404Vector/Competition.DACON.ArtWorksPainterClassification\n\n\n\n\n요약\n\nMy Contribute\nTask\nScore\nData\n\n\n개요\n본론\n\nEDA\nData Sampling\nAugmentation\nLoss\nOptimizer\nScheduler\nModel\nEnsemble\n\n\n결론\n\n\n요약\n==My Contribute==\n\n==팀 내 최대 Code Contributor==\n==EDA==\n==Data download &amp; directory setting 자동화 Script 작성==\n==WandB 연동==\n==Train, Inferrence Script 작성==\n==Efficientnet v2실험==\n==ViT 모델 실험==\n\nTask\n\nClassification Task, 예술 작품을 화가 별로 분류하는 AI 모델 개발\n\nScore\n\nMacro F1 Score\n\nData\n\nTrain Data\n\n이미지, 화가, 화가별 장르 정보\n\n\nTest Data\n\n\n이미지(원본을 랜덤으로 1/4 crop)\n\n\n\n\n\n개요\n\n\n                  \n                  월간 데이콘 예술 작품 화가 분류 AI 경진대회 \n                  \n                \n\n\n안녕하세요 여러분!\ndacon.io/competitions/official/236006/overview/description\n\n\n\n\n화가 분류는 많은 연구가 이루어지고 있는 문제로, 주로 이미지 처리 및 기계 학습의 전통적인 접근 방식을 통해 꾸준하게 연구되어왔습니다. 더 나아가 현재에는 화가의 작품을 흉내내거나, 직접 예술 작품을 창조해내는 GAN을 활용한 생성 모델 연구까지 이루어지고 있습니다.\n이번 월간 데이콘 26은 예술 작품을 화가 별로 분류하는 대회입니다.\n더 나아가 예술 작품의 일부분만을 가지고도 올바르게 분류해낼 수 있어야합니다.\n예술 작품의 일부분만 주어지는 테스트 데이터셋에 대해 올바르게 화가를 분류해낼 수 있는 예술 작품의 전문가인 AI 모델을 만들어주세요.\n\n\n본론\nEDA\n\n\n장르와 화가의 상관 관계 → 화가별로 하나의 장르만 있는 사람도 있으나, 여러 장르를 그리는 화가도 있음\n\n\n\n화가와 그림 크기의 상관 관계 → 화가별로 그리는 그림의 크기가 어느정도 유사함\n\n\n\n화가 별 그림 개수 → 화가 별로 데이터 개수가 다름(Class imbalance) 존재\n\n\n\n화가 별 그림의 RGB Mean &amp; Std.(일부) → 화가별로 사용하는 색에 경향성을 보였음\n\n\n\n\n\nData Sampling\n\n\nTrain / Valid Split\nStratified sampling으로 Train과 Valid Set을 나눔\n\n\nClass Imbalance\nData Oversamplig을 사용해 부족한 화가 그림의 경우 더 많이 load되도록 설정\n\n\nAugmentation\n\n\nRandomCrop\n\n\nHorizontal &amp; Vertical Flip\n\n\nResize\n\n\nCutmix\n색조나 채도를 바꾸거나 Geometric Augmentation을 할 경우 화풍이나 색감이 왜곡될 것이라고 판단\n다른 기법은 사용하지 않음\n\n\nLoss\n\n\nCross Entropy Loss\n→ Oversampling을 사용했기 때문에 Focal loss 미사용\n\n\nOptimizer\n\n\nAdam\n\n\nScheduler\n\n\nStep LR\n\n\nModel\n\nResNeXt\nEfficientnet b4\nMax ViT\nSwinT\nRegnet\n\n\n→ Model 별로 Size나 RGB 평균, 표준 편차 값을 추가로 embed, 결과 모두 embed 한 것이 가장 성능이 좋았음\n단, ViT의 경우 사용하지 않은 것이 성능이 좋았음\nEnsemble\n\n\nCSV result hard voting\n각 모델(ResNeXt, Efficientnet b4, Max ViT, SwinT, Regnet)의 Best Case를 ensemble\n\n\nResult\nBest Case(Efficientnet b4, 0.7967) → ==Ensemble 0.8534==\n\n\n\n결론\n\n\nRanking\n\n\n\n느낀점\n\n\nEDA를 통해 얻은 인사이트(작가별 크기와 색에 따른 경향성)로 기존 Classification model에 정보를 추가로 concat하여 classification을 수행했을 때, 점수 상승이 있었고 EDA의 중요성을 느끼게 되었다.\n\n\nEDA 과정에서 Raw Data(이미지)를 많이 보게 되었고, 이를 토대로 Augmentation 의사결정 과정에서 색이나 그림 자체를 왜곡하는 기법들을 실험하지 않고 제외할 수 있었다. Raw Data를 눈으로 보는 것이 매우 중요하다고 느껴졌다.\n\n\n하나의 잘 된 모델보다 다양한 모델을 사용하는 것이 다양한 관점에서 볼 수 있다고 판단하여 다양한 모델을 훈련시켰다. 모델들의 결과는 0.7에서 0.79까지 다양했는데, Ensemble 결과 점수가 크게 상승하는 것을 보면서 비슷한 점수여도 모델마다 잘하는 것과 못하는 것이 있기 때문에 얻은 결과라고 생각했다.\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution":{"slug":"vault/Notion/DB/DB-Blog-Post/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution","filePath":"vault/Notion/DB/DB Blog Post/DBPN - Deep Back-Projection Networks for Single Image Super-resolution/DBPN - Deep Back-Projection Networks for Single Image Super-resolution.md","title":"DBPN - Deep Back-Projection Networks for Single Image Super-resolution","links":[],"tags":[],"content":"\n참조\n개요\n1. Introduction\n2. Related Work\n3. Deep Back-Projection Networks\n\n3.1 Projection Units\n3.2 Dense Projection Units\n3.3 Network Architecture\n\n\n4. Experimental Results\n\n4.1 Implementation and training details\n4.2 Model Analysis\n\n\n5. Conclusion\n\n\n참조\narxiv.org/abs/1803.02735\ngithub.com/alterzero/DBPN-Pytorch\nchat.openai.com/\n개요\n“Deep Back-Projection Networks For Super-Resolution” 논문은 딥 백프로젝션 네트워크를 사용하여 이미지 초해상도를 개선하는 새로운 방법을 제안합니다. 이 논문의 저자들은 딥 컨볼루션 신경망을 기반으로 한 이전 방법들이 인식 품질이 낮고 재구성된 이미지에서 고주파수 세부 정보가 부족하다는 한계점이 있다고 주장합니다.\n제안된 방법은 전방향 및 역방향 투영을 결합하여 초기 예측을 개선하는 데 역방향 투영이 사용됩니다. 이러한 접근 방식은 고주파수 세부 정보를 더 잘 재구성하고 재구성된 이미지의 인식 품질을 개선하는 데 도움이 된다는 것을 저자들은 주장합니다.\n실험 결과는 제안된 방법이 객관적인 측정 기준과 주관적인 인식 품질 측면에서 최첨단 방법보다 우수하다는 것을 보여줍니다. 또한, 저자들은 제안된 방법이 다양한 수준의 노이즈와 압축 아티팩트에 대해 강건하다는 것을 입증하였습니다.\n구조를 살펴보면, 내부적으로 Conv 연산을 이용하여 downscale, Deconv를 사용하여 upscale을 수행합니다. 그리고 이 결과에 반대 연산(Conv를 사용했다면 Deconv, Deconv를 사용했다면 Conv)를 다시 적용하여 입력과의 차이를 구합니다.(Feed-Back)\n이 차이 값에 다시 Conv 연산을 사용하여 Feature를 추출하며, 이 과정을 반복합니다. 이 때, 입력과 출력의 크기는 동일하게 유지되기 때문에 이미지 크기을 키울 수 없습니다.\n이에 저자는 x2, x4, x8을 하기 위해 각각 Kernel Size, Stride, Padding을 조정하여 영상의 크기를 키웁니다.\n1. Introduction\nSingle Image SR은 Low Resolution에서 High Resolution Image로의 비선형 Mapping을 구하는 과정입니다. 인간의 시각 시스템은 단순한 Feed-Forward가 아닌 Feed-Back 연결이 있다고 여겨집니다. 저자는 Feed-Forward SR 모델들의 성능이 부족한 이유가 이런 Feed-Back 연결의 부족으로 추측했습니다.\n반면 Feed-Back 형식의 연결은 초기 SR 알고리즘 중의 하나이며 좋은 성능을 내는 것이 입증되었지만 Iteration 수, Blur Operator 등의 파라메터 선택에 결과가 민감하게 변화합니다.\n\n저자는 ‘Improving Resolution by Image Registration’ 논문에서 영감을 받아 반복적인 Up, Down Sampling을 통해 Large Scaling Factor(4배, 8배 SR 등)에서도 잘 동작하는 End-to-End 모델을 만들었습니다.\n저자가 주장하는 Contribution은 아래와 같습니다.\n\n\nError Feedback\nSR을 위한 Up, Down Sampling 시 Error를 Feed-Back하는 구조\n\n\n상호 연결된 Up, Down Sampling Stage들\n기존 Feed-Forward 구조들은 SR 이미지를 매핑하는데 좋은 성능을 보여주지 못했습니다. 특히 Scaling Factor가 클수록 더욱 좋은 성능을 보여주지 못했습니다.\n이에 저자는 Upsampling Layer 뿐만 아니라 Downsampling Layer를 사용해 Feature Map을 다시 LR 해상도로 다시 Projection합니다.\n\n\nDeep concatenation\n다른 네트워크들과 다르게 Sampling Layer를 통과시키지 않습니다. 그 대신 위 Feature 2.의 붉은 선처럼 각 Upsampling Stage에서 얻은 HR Feature를 Concatenation하여 마지막에 Conv를 수행하는 방식으로 결과를 얻습니다.\n\n\nImprovement with dense connection\nUpsampling Stage에서 얻은 HR Feature를 다음 Stage에서 활용합니다.\n\n\n2. Related Work\n논문과 관련된 선행 연구들을 다루는 항목이므로 생략합니다.\n3. Deep Back-Projection Networks\nI^h : HR Image(M x N)\nI^l : LR Image(M’ x N’), M’ &lt; M, N’ &lt; N\n위와 같이 정의했을 때, DBPN은 LR → HR, HR → LR Projection을 모두 수행합니다.\n3.1 Projection Units\n\nUp Projection은 다음과 같이 정의됩니다.\n(Up Projection을 위한 수식 및 정의가 기술되어 있지만, 코드로 대체합니다)\nclass UpBlock(torch.nn.Module):\n    def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, num_stages=1, bias=True, activation=&#039;prelu&#039;, norm=None):\n        super(UpBlock, self).__init__()\n        self.conv = ConvBlock(num_filter*num_stages, num_filter, 1, 1, 0, activation, norm=None) if num_stages &gt; 1 else torch.nn.Identity()\n        self.up_conv1 = DeconvBlock(num_filter, num_filter, kernel_size, stride, padding, activation, norm=None)\n        self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride, padding, activation, norm=None)\n        self.up_conv3 = DeconvBlock(num_filter, num_filter, kernel_size, stride, padding, activation, norm=None)        \n \n    def forward(self, x):\n        x = self.conv(x)\n        h0 = self.up_conv1(x)\n        l0 = self.up_conv2(h0)\n        h1 = self.up_conv3(l0 - x)\n        return h1 + h0\nDown Projection은 다음과 같이 정의됩니다.\n(마찬가지로, 코드로 대체합니다)\nclass DownBlock(torch.nn.Module):\n    def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, num_stages=1, bias=True, activation=&#039;prelu&#039;, norm=None):\n        super(DownBlock, self).__init__()\n        self.conv = ConvBlock(num_filter*num_stages, num_filter, 1, 1, 0, activation, norm=None) if num_stages &gt; 1 else torch.nn.Identity()\n        self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride, padding, activation, norm=None)\n        self.down_conv2 = DeconvBlock(num_filter, num_filter, kernel_size, stride, padding, activation, norm=None)\n        self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride, padding, activation, norm=None)\n \n    def forward(self, x):\n        x = self.conv(x)\n        l0 = self.down_conv1(x)\n        h0 = self.down_conv2(l0)\n        l1 = self.down_conv3(h0 - x)\n        return l1 + l0\nUp Projection과 Down Projection Unit 한 쌍를 한 Stage로 표현하며, 이 Projection을 Unit들은 Projection Error를 얻을 수 있게 됩니다. 또한 이 Projection Unit들은 Projection Error를 반복적으로 Feed-Back함으로써, 스스로 자가교정 절차로 이해할 수 있습니다.\n또한 기존의 모델들은 수렴속도가 느려지고 최적의 결과를 주지 않는 문제들 때문에 8x8, 12x12 등의 큰 Filter Size를 지양했습니다. 그러나 이 Projection Unit은 이러한 문제들을 억제하고, 얕은 네트워크에서도 더 좋은 결과를 얻게 해줍니다.\n3.2 Dense Projection Units\n\nDenseNets를 통해 Dense Inter-Layer Connectivity Pattern이 Vanishing Gradient 문제를 완화하고 Feature 향상 등 장점을 가지는 것이 밝혀졌습니다. 저자는 이것에 영감을 받아 DBPN에 Dense Connection을 추가했으며, 이를 D-DBPN으로 명명했습니다.\n다만 기존 DenseNets에서 사용한 구조와 조금 다른데 Dropout과 Batch Norm을 사용하는 것은 SR Task에 적합하지 않아서 사용을 지양했습니다. (이유는 이 논문에 기술되어 있습니다)\n또한 1x1 Conv Layer를 사용하여 Feature를 Pooling하고 Dimension을 감소시킨 뒤 Projection을 수행했습니다.\n3.3 Network Architecture\n\nD-DBPN의 구조는 위 그림과 같고, 크게 3개의 구조로 나눌 수 있습니다.\n\n\nInitial Feature Extraction\nConv 연산을 통해 Feature를 추출합니다.\n\n\nBack-Projection Stages\nUp-Projection과 Down-Projection을 번갈아가며 수행하며 각각 HR, LR Feature들을 추출합니다.\n\n\nReconstruction\n각 HR Feature들을 Concatenation 하고, Conv를 사용하여 최종적인 결과 이미지를 생성합니다.\n\n\n4. Experimental Results\n4.1 Implementation and training details\n제안된 네트워크로 Up-Scaling을 하기 위해서는 Scaling factor(몇 배로 이미지를 키울 것인지)에 따라서 Filter Size와 기타 파라메터가 정의됩니다. Scaling factor에 따른 파라메터는 깃허브 코드를 참조했으며, 다음과 같습니다.\n\nScaling Factor : x2\n\nKernel Size : 6 x 6\nStride : 2\nPadding : 2\n\n\nScaling Factor : x4\n\nKernel Size : 6 x 6\nStride : 2\nPadding : 2\n\n\nScaling Factor : x8\n\n\nKernel Size : 6 x 6\n\n\nStride : 2\n\n\nPadding : 2\n\n\n\n\nWeight의 초기화는 카이밍 히(Kaiming He)가 제안한 He Initialization을 사용했습니다.\n훈련의 경우 DIV2K, Flickr, ImageNet dataset을 사용했으며 augmentation은 적용하지 않았습니다.\nLR 이미지의 생성은 Bicubic을 사용해서 데이터셋의 원본 이미지를 Downscaling하여 생성했습니다.\n훈련에 사용한 다른 설정들은 다음과 같습니다.\n\nBatch size : 20\nInput : 32 x 32\nOutput : Scaling Factor에 따라 변화. Factor가 2일 경우 64 x 64\nEpoch : 10\nOptimizer : Adam\n\nLr : 1e-4\n\n매 500000 Epoch 마다 0.1배로 감소시킴\n\n\nmomentum : 0.9\nweight decay : 1e-4\n\n\n\n4.2 Model Analysis\n\n성능을 확인하기 위하여 여러 크기의 네트워크(Small, Midium, Large)를 구성하였고, State-of-art Model 보다 우수한 결과를 보여주었습니다.\n\n추가로 DBPN-SS(Small보다 더 가벼운 버전)을 만들어 실험하였고 마찬가지로 우수한 성능을 보였습니다. 또한 기존 모델들보다 더 적은 Parameter로 더 좋은 결과를 얻을 수 있는 것을 실험적으로 입증하였습니다.\nD-DBPN의 경우, Parameter 수가 많이 증가하지만 제일 좋은 성능을 보였습니다. 특히 x8 Upscaling의 경우, 기존의 다른 방법보다 월등이 뛰어난 결과를 보여줍니다.\n5. Conclusion\n\nSingle Image Super Resolution을 위한 ‘DBPN’ 네트워크를 제안했고 기존 방법들과 다르게 여러번의 up sampling, down sampling step들을 통해 SR 결과 이미지를 생성합니다.\n또한 제안한 네트워크는 다른 State-of-the-art 방법보다 x8 Upscaling 요소에서 훨신 더 좋은 성능을 보여줍니다.\n아래 Table은 다른 State-of-the-art SR 알고리즘과의 성능 비교 표 입니다. 매우 많은 parameter가 필요한 EDSR에 근접하거나 혹은 그 이상의 성능을 보여줍니다.\n"},"vault/Notion/DB/DB-Blog-Post/DPIR---Plug-and-Play-Image-Restoration-with-Deep-Denoiser-Prior(작성중)":{"slug":"vault/Notion/DB/DB-Blog-Post/DPIR---Plug-and-Play-Image-Restoration-with-Deep-Denoiser-Prior(작성중)","filePath":"vault/Notion/DB/DB Blog Post/DPIR - Plug-and-Play Image Restoration with Deep Denoiser Prior(작성중).md","title":"DPIR - Plug-and-Play Image Restoration with Deep Denoiser Prior(작성중)","links":[],"tags":[],"content":"\n참조\nAbstract\n\n\n참조\narxiv.org/abs/2008.13751\ngithub.com/cszn/DPIR\nAbstract\nPlug-and-Play 영상복원에 대한 최근의 연구들은 Denoiser가 많은 역변환 문제들을 풀기 위한 모델 기반의 방법들의 Image Prior의 역할을 할 수 있음을 암묵적으로 보여주었습니다.\nDenoiser가 큰 모델링 용량을 가진 CNN에 의해 차별적으로 학습될 때, 이러한 속성들은 plug-and-play 영상 복원에 대해 고려할 수 있는 이점을 유도합니다.(예: 모델 기반 방법의 유연성과 학습 기반 방법의 효율성을 합칩니다)\n그러나 깊고 큰 CNN 모델이 빠르게 인기를 얻으면서, 기존의 plug-and-play 영상복원 모델들은 적절한 사전 denoiser의 부족으로 성능이 잘 향상되지 않았습니다.\n저자는 plug-and-play 영상복원의 한계를 뛰어넘기 위하여 매우 유연하고 효율적인 CNN denoiser를 훈련시키고, deep denoiser prior에 대한 밴치마크를 설정했습니다.\n\n이 논문에서 말하는 “Image Prior”란 이미지 복원 문제에서 이미지의 속성을 묘사하는 모델을 의미합니다. 이미지의 속성은 예를 들어 텍스처, 색상, 경계선 등이 있습니다. Image Prior 모델은 이러한 이미지 속성을 수학적으로 표현하여 이미지 복원 문제에서 활용됩니다.\nImage Prior 모델은 이미지 복원 작업에서 이미지의 원래 모습을 유추할 수 있도록 도와줍니다. 이 모델은 이미지가 가지고 있는 특정한 속성을 고려하여 이미지를 복원합니다. 예를 들어, 이미지 속성을 “부드러운 텍스처”로 설정하면, 복원된 이미지에서 텍스처가 부드럽게 처리됩니다.\n이 논문에서 제안한 Deep Denoiser Prior는 이미지가 가지고 있는 텍스처, 색상 등의 다양한 속성을 학습하여 이를 기반으로 이미지를 복원합니다. 이러한 Image Prior 모델을 이용하면 이미지 복원 작업에서 보다 정확하고 자연스러운 결과물을 얻을 수 있습니다.\n\n[contents]"},"vault/Notion/DB/DB-Blog-Post/Decision-Tree/Decision-Tree":{"slug":"vault/Notion/DB/DB-Blog-Post/Decision-Tree/Decision-Tree","filePath":"vault/Notion/DB/DB Blog Post/Decision Tree/Decision Tree.md","title":"Decision Tree","links":[],"tags":[],"content":"\n참조\nDecision Tree - WIKI\nDecision Tree - ML\n가지치기(Pruning)\n알고리즘: 엔트로피(Entropy), 불순도(Impurity)\n정보 획득 (Information gain)\n\n\n참조\nko.wikipedia.org/wiki/결정_트리\nratsgo.github.io/machine learning/2017/03/26/tree/\nbkshin.tistory.com/entry/머신러닝-4-결정-트리Decision-Tree\nDecision Tree - WIKI\n\n결정 트리(decision tree)는 의사 결정 규칙과 그 결과들을 트리 구조로 도식화한 의사 결정 지원 도구의 일종입니다.\n결정 트리는 운용 과학, 그 중에서도 의사 결정 분석에서 목표에 가장 가까운 결과를 낼 수 있는 전략을 찾기 위해 주로 사용됩니다.\nDecision Tree - ML\n결정 트리(Decision Tree, 의사결정트리, 의사결정나무라고도 함)는 분류(Classification)와 회귀Regression) 모두 가능한 지도 학습 모델 중 하나입니다.\n\n\nProcess\n\n\n아래와 같이 데이터를 가장 잘 구분할 수 있는 질문을 기준으로 나눕니다.\n\n\n\n나뉜 각 범주에서 또 다시 데이터를 가장 잘 구분할 수 있는 질문을 기준으로 나눕니다.\n\n\n\n이를 지나치게 많이 하면 아래와 같이 오버피팅이 됩니다. 결정 트리에 아무 파라미터를 주지 않고 모델링하면 오버피팅이 됩니다.\n\n\n\n\n\n가지치기(Pruning)\n오버피팅을 막기 위한 전략으로 가지치기(Pruning)라는 기법이 있습니다. 트리에 가지가 너무 많다면 오버피팅이라 볼 수 있습니다. 가지치기란 나무의 가지를 치는 작업을 말합니다.\n\nmin_sample_split\n\nmin_sample_split = 10 : 한 노드에 10개의 데이터가 있다면 그 노드는 더 이상 분기를 하지 않습니다.\n\n\nmax_depth\n\nmax_depth = 4 : 깊이가 4보다 크게 가지를 치지 않습니다. 가지치기는 사전 가지치기와 사후 가지치기가 있지만 sklearn에서는 사전 가지치기만 지원합니다.\n\n\n\n알고리즘: 엔트로피(Entropy), 불순도(Impurity)\n\n불순도(Impurity) : 해당 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지를 뜻합니다.\n순도(Purity) : 불순도와 반대되는 단어입니다.\n\n\n위 그림에서 위쪽 범주는 불순도가 낮고, 아래쪽 범주는 불순도가 높습니다.\n바꾸어 말하면 위쪽 범주는 순도(Purity)가 높고, 아래쪽 범주는 순도가 낮습니다.\n\n엔트로피(Entropy)는 불순도(Impurity)를 수치적으로 나타낸 척도입니다\n\np_i : 한 영역 안에 존재하는 데이터 가운데 범주 i에 속하는 데이터의 비율\nEntropy=-\\sum_i (p_i)\\log_2 (p_i)\n\n\n\n정보 획득 (Information gain)\n\nInformation gain\n\nE(p_i(t)) : Step t 시점에서의 Entropy\nInformation gain : G=E(p_i(t-1))-Mean([E_1(p_i(t)),E_2(p_i(t)),...,E_n(p_i(t))])\nStep t에서의 Entropy는 분기되어 있을 수 있습니다. 이 경우, 가중평균을 사용하여 Entropy를 계산합니다.\n\n\n"},"vault/Notion/DB/DB-Blog-Post/DeepLearning에서-가장-중요한-아이디어들(denny-britz,-2020-07-29)":{"slug":"vault/Notion/DB/DB-Blog-Post/DeepLearning에서-가장-중요한-아이디어들(denny-britz,-2020-07-29)","filePath":"vault/Notion/DB/DB Blog Post/DeepLearning에서 가장 중요한 아이디어들(denny britz, 2020-07-29).md","title":"DeepLearning에서 가장 중요한 아이디어들(denny britz, 2020-07-29)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-3"],"tags":[],"content":"\n참조\nDeepLearning에서 가장 중요한 아이디어들(denny britz, 2020-07-29)\n\n\n참조\nWeek 3\nDeepLearning에서 가장 중요한 아이디어들(denny britz, 2020-07-29)\n\nAlexNet(2012)\n\n고전적인 ML이 아닌, Deep Learning을 이용해서 처음으로 ImageNet 대회에서 우승\nML → DL로의 Paradigm Shift\n\n\nDQN(2013)\n\n딥마인드가 아타리를 DQN으로 Play\n후에 AlphaGo 탄생\n\n\nEncoder / Decoder(2014)\n\n기계어 번역의 Trend가 바뀜\n\n\nAdam Optimizer(2014)\n\nHyperparameter에 Tune의 부담이 크게 감소\nAdam을 쓰면 왠만하면 잘 되기 때문에 연구자들이 본인의 연구에 집중 할 수 있게 됨\n\n\nGAN(2015)\nResidual Networks(2015)\n\n기존보다 Network를 더 깊이 쌓을 수 있게 됨\n\n\nTransformer(2017)\nBERT(2018, Bidirectional Encoder Representations from Transformers)\nBIG Language Models(2019, GPT-X)\nSelf Supervised Learning(2020)\n"},"vault/Notion/DB/DB-Blog-Post/DetEval":{"slug":"vault/Notion/DB/DB-Blog-Post/DetEval","filePath":"vault/Notion/DB/DB Blog Post/DetEval.md","title":"DetEval","links":[],"tags":[],"content":"DetEval은, 이미지 레벨에서 정답 박스가 여러개 존재하고, 예측한 박스가 여러개가 있을 경우, 박스끼리의 다중 매칭을 허용하여 점수를 주는 평가방법 중 하나 입니다.\n평가가 이루어지는 방법은 다음과 같습니다.\n**1) 모든 정답/예측박스들에 대해서 Area Recall, Area Precision을 미리 계산해냅니다.**여기서 Area Recall, Area Precision은 다음과 같습니다. Area Recall = 정답과 예측박스가 겹치는 영역 / 정답 박스의 영역Area Precision = 정답과 예측박스가 겹치는 영역 / 예측 박스의 영역\n\n2) 모든 정답 박스와 예측 박스를 순회하면서, 매칭이 되었는지 판단하여 박스 레벨로 정답 여부를 측정합니다.\n박스들이 매칭이 되는 조건은 박스들을 순회하면서, 위에서 계산한 Area Recall, Area Precision이 0 이상일 경우 매칭 여부를 판단하게 되며, 박스의 정답 여부는 Area Recall 0.8 이상, Area Precision 0.4 이상을 기준으로 하고 있습니다.\n매칭이 되었는가 대한 조건은 크게 3가지 조건이 있습니다.\n\none-to-one match: 정답 박스 1개와 예측 박스 1개가 매칭 &amp;&amp; 기본조건 성립\none-to-many match: 정답 박스 1개와 예측 박스 여러개가 매칭되는 경우\nmany-to-one match: 정답 박스 여러개와 예측박스 1개가 매칭되는 경우\n\n여기서, one-to-many match 경우에 한해서, 박스 recall / precision 에 0.8 로 penalty가 적용됩니다.\n아래의 이미지를 통해 평가방법의 설명을 보충합니다.\n그림에서 왼쪽과 같이 다음과 같은 정답박스가 존재하는 이미지에 대해서, 오른쪽과 같이 예측하였다고 가정합니다.\n\n\n과정을 통하여, 모든 박스들 사이의 Area Recall, Area Precision을 계산해 놓습니다. 이해를 쉽게 하기 위해서, 왼쪽에 정답박스와 예측박스를 겹쳐서 그렸습니다.\n\n\nG1과 P1은 one-to-one match가 성립되었고, Area Recall, Area Precision 모두 0.99로 threshold 이상이므로, 정답으로 책정됩니다.\nG2, G3와 P2는 many-to-one match가 성립되었고, Area Recall 0.9(0.81+0.99)/2, Area Precision0.91(0.41+0.5) 로 threshold 이상으로 정답으로 책정되었습니다.\nG4와 P3, P4는 one-to-many match가 성립되었고, Area Recall 0.88(0.46+0.42), Area Precision 0.96(1.0+0.92)/2 로 threshold 이상으로 정답으로 책정되었습니다.\n따라서, 현재 이미지에서의 Recall, Precision, H-mean(F1 score)을 구해보면\nRecall = ( 1(G1) + 1(G2) + 1(G3) + 0.8(G4) ) / 4(len(gt)) = 0.95,\nPrecision = ( 1(P1) + 1(P2) + 0.8(P3) + 0.8(P4) ) / 4(len(prediction)) = 0.9,\nH-mean = 2 * 0.95 * 0.9 / (0.95 + 0.9) = 0.92\n가 되어, 해당 이미지에 대해서 최종적으로 0.92점을 부여하게 됩니다.\n3) 모든 이미지에 대하여 Recall, Precision을 구한 이후, 최종 F1-Score은 모든 이미지 레벨에서 측정 값의 평균으로 측정됩니다.\n테스트 셋은 여러장의 이미지로 구성되어있는데요, 위의 예시에서처럼 모든 이미지들에 대해서 Recall, Precision, 점수를 구한 이후, 모든 이미지들에 대해서 해당 값을 평균내어 최종 점수를 구하게 됩니다.\n예) image1, image2 두개의 테스트 이미지가 존재하고, 계산의 편의성을 위해서 image1은 위의 예시와 동일, image2는 정답/예측박스가 1개이고 맞았다고 가정하고 계산해보면\nFinal Recall = 1 + 1 + 1 + 0.8 + 1 / 5 = 0.96\nFinal Precision = 1 + 1 + 0.8 + 0.8 + 1 / 5 = 0.92\nFinal F1 = 2 * 0.96 * 0.92 / (0.96 + 0.92) = 0.94\n(분모가 5인 이유: image1에서 정답/예측박스 4개, image2에서 정답/예측박스가 1개이므로)\n해당 테스트 셋에서 최종 점수는 0.94 점이 되겠습니다."},"vault/Notion/DB/DB-Blog-Post/DetectroRS/DetectroRS":{"slug":"vault/Notion/DB/DB-Blog-Post/DetectroRS/DetectroRS","filePath":"vault/Notion/DB/DB Blog Post/DetectroRS/DetectroRS.md","title":"DetectroRS","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10"],"tags":[],"content":"\n참조\nDetectroRS\n\n\n참조\n\n\n                  \n                  DetectoRS: Detecting Objects with Recursive Feature Pyramid and... \n                  \n                \n\n\nMany modern object detectors demonstrate outstanding performances by using the mechanism of looking and thinking twice.\narxiv.org/abs/2006.02334\n\n\n\nWeek 10\ncdm98.tistory.com/49\nbetter-tomorrow.tistory.com/entry/Atrous-Convolution\ngaussian37.github.io/vision-segmentation-aspp/\nwww.notion.so/DetectoRS-Detecting-Objects-with-Recursive-Feature-Pyramid-and-Switchable-Atrous-Convolution-29969b0db009456e87941f27c71d7585\nWeek 10\nDetectroRS\n\n\n논문에서는 당시 객체탐지 모델들이 ‘Look and Think Twice’ 메커니즘을 차용하여 성능이 크게 개선되었다고 합니다. Look and Think Twice 메커니즘이란 일반적인 feed forward network에 feedback 레이어를 추가한 신경망(extra feedback network)을 의미합니다.\nDetectroRS은 FPN으로 부터 얻은 추가적인 패드백 연결(extra feedback connections)을 bottom-up backbone layer에 추가해주는 Recursive Feature Pyramid와 다양한 Atrous Rates로 얻은 Features를 Switch function을 통해 모아주는 Switchable Atrous Convolution을 제안합니다.\n아래 Recursive Feature Pyramid 구조를 여러번 반복하는 방식으로 Feedback을 구현합니다.\n\n\nRecursive Feature Pyramid\n\n\nFeature Pyramid Network(FPN)에 extra feedback을 추가한 네트워크\nFeature Pyramid 구조와 유사, 추가적으로 backbone에서 feature pyramid의 정보를 가지고 학습 수행\nFlops가 증가하는 단점\n\n\n\nASPP(Atrous Spatial Pyramid Pooling)\n\nDeeplab v2에서 처음 제안, Deeplab v3에서 개선\nFeature Map ‘f’를 Backbone으로 Feedback 할 때, f를 입력으로 받아 RFP Feature로 변환\nf를 그대로 그대로 RFP Feature로 사용하는 것 보다 더 좋은 성능\n\n\n\nUnrolled Iterations\n\n\nRecursive한 모델을 실제로 구현하기 위해서 iteration의 형태로 변경\nIteration unrolled step을 t라 하고 t=1, ..., T이며, 최대 iteration은 T번\n\n논문에서는 T를 2로 사용\n\n\nf_i^t = F_i^t(f^t_{f+1}, x_i^t), \\ x_i^t = B_i^t(x^t_{i-1}, R^t_i\\big(f_i^{t-1})\\big)\n\nB_i^0\\big(x_{i-1}^0,\\ 0\\big) : 그림 가장 왼쪽의 상황, iteration을 수행하지 않았으므로 당연히 feedback 할 것이 없음(f_i^0 = 0)\nB_i^1\\big(x_{i-1}^1,\\ R^1_i(f^0_i)\\big) : 1번째 iteration\nB_i^2\\big(x_{i-1}^2,\\ R^2_i(f^1_i)\\big) : 2번째 iteration\n\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/DockerERROR--Unexpected-bus-error-encountered-in-worker.-This-might-be-caused-by-insufficient-shared-memory-(shm)":{"slug":"vault/Notion/DB/DB-Blog-Post/DockerERROR--Unexpected-bus-error-encountered-in-worker.-This-might-be-caused-by-insufficient-shared-memory-(shm)","filePath":"vault/Notion/DB/DB Blog Post/DockerERROR- Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).md","title":"DockerERROR- Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm)","links":[],"tags":[],"content":"\n참조\nERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n\n\n참조\ncurioso365.tistory.com/136\njybaek.tistory.com/785\nERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\nOutput exceeds the size limit. Open the full output data in a text editor\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[44], line 1\n----&gt; 1 run_training()\n \nCell In[43], line 38, in run_training()\n     26 trainer = pl.Trainer(\n     27     # gpus=1,\n     28     val_check_interval=0.5,\n   (...)\n     34     precision=Config.PRECISION, accelerator=&quot;gpu&quot; \n     35 )\n     37 print(&quot;Running trainer.fit&quot;)\n---&gt; 38 trainer.fit(audio_model, train_dataloaders = dl_train, val_dataloaders = dl_val)                \n     40 gc.collect()\n     41 torch.cuda.empty_cache()\n \nFile ~/.local/share/virtualenvs/Competition.Kaggle.BirdCLEF2023-BGaCen5g/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:520, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    518 model = _maybe_unwrap_optimized(model)\n    519 self.strategy._lightning_module = model\n--&gt; 520 call._call_and_handle_interrupt(\n    521     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n    522 )\n \nFile ~/.local/share/virtualenvs/Competition.Kaggle.BirdCLEF2023-BGaCen5g/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\n...\n---&gt; 66     _error_if_any_worker_fails()\n     67     if previous_handler is not None:\n     68         assert callable(previous_handler)\n \nRuntimeError: DataLoader worker (pid 6865) is killed by signal: Bus error. It is possible that dataloader&#039;s workers are out of shared memory. Please try to raise your shared memory limit.\nlocal 환경의 docker 내부에서 model 훈련을 진행하던 도중, 위와 같은 오류가 발생하였습니다.\n도커로 컨테이너를 생성하게 되면 호스트와 컨테이너는 공유하는 메모리 공간이 생기게 되는데 이 공간에 여유가 없어서 발생되는 에러입니다.\n현재 shared memory 설정 값을 확인하려면 아래와 커맨드를 터미널에 입력합니다.\n\ndf -h\n\nFilesystem Size Used Avail Use% Mounted on\noverlay 251G 4.9G 234G 3% /\ntmpfs 64M 0 64M 0% /dev\ntmpfs 13G 0 13G 0% /sys/fs/cgroup\nshm 64M 30M 35M 46% /dev/shm\nR:\\ 932G 579G 354G 63% /root\n/dev/sdc 251G 4.9G 234G 3% /etc/hosts\ndrivers 931G 452G 479G 49% /usr/bin/nvidia-smi\nlib 931G 452G 479G 49% /usr/lib/x86_64-linux-gnu/libcuda.so.1\nnone 13G 0 13G 0% /dev/dxg\ntmpfs 13G 0 13G 0% /proc/acpi\ntmpfs 13G 0 13G 0% /sys/firmware\n\n\n\n제 경우, 64MB로 설정되어 있습니다.\n값을 늘리는 방법은 다음과 같습니다.\n\n컨테이너 생성 시 —shm-size 옵션을 사용하여 필요한 값을 입력합니다.\n컨테이너 생성 시 -ipc=host를 입력하여 특정한 세그먼트만 메모리에 연결되지 않게 합니다.\n"},"vault/Notion/DB/DB-Blog-Post/EDA(Exploratory-Data-Analysis,-탐색적-데이터-분석)는-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/EDA(Exploratory-Data-Analysis,-탐색적-데이터-분석)는-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/EDA(Exploratory Data Analysis, 탐색적 데이터 분석)는 무엇일까.md","title":"EDA(Exploratory Data Analysis, 탐색적 데이터 분석)는 무엇일까","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-6/Week-6"],"tags":[],"content":"\n참조\nEDA(Exploratory Data Analysis, 탐색적 데이터 분석)란?\n\n\n참조\nen.wikipedia.org/wiki/Exploratory_data_analysis\nWeek 6\nEDA(Exploratory Data Analysis, 탐색적 데이터 분석)란?\n통계 그래픽 및 기타 데이터 시각화 방법을 사용하여 Dataset을 분석하여 주요 특성을 요약하는 접근 방식입니다."},"vault/Notion/DB/DB-Blog-Post/FPN(Feature-Pyramid-Network)/FPN(Feature-Pyramid-Network)":{"slug":"vault/Notion/DB/DB-Blog-Post/FPN(Feature-Pyramid-Network)/FPN(Feature-Pyramid-Network)","filePath":"vault/Notion/DB/DB Blog Post/FPN(Feature Pyramid Network)/FPN(Feature Pyramid Network).md","title":"FPN(Feature Pyramid Network)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10"],"tags":[],"content":"\n참조\nFPN(Feature Pyramid Network)\n\nPipeline\nStage Mapping\n\n\n\n\n참조\n\n\n                  \n                  Feature Pyramid Networks for Object Detection \n                  \n                \n\n\nFeature pyramids are a basic component in recognition systems for detecting objects at different scales.\narxiv.org/abs/1612.03144\n\n\n\njonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\nherbwood.tistory.com/18\nWeek 10\nFPN(Feature Pyramid Network)\n\nFeature Pyramid Network(FPN)은 Object Detection에서 Semantic 정보를 고유한 비율로 추출할 수 있도록 하는 기술입니다.\nPyramid 구조를 통해 Low Level에서 High Level로 정보를 전달하며, Top-Down 방식과 Bottom-Up 방식을 사용합니다.\n또한, Stage Mapping을 통해 ROI의 w와 h 값으로 Stage를 추정하여 각 Stage에서 Feature map을 추출합니다.\n\nLow Level은 Semantic한 정보가 없기 때문에 Pyramid 구조를 사용하여 High Level에서 Low Level로 Semantic 정보를 전달\n\nLow level = Early stage = bottome\nHigh level = Late Stage = Top\n\n\n\n\nPipeline\n\nBottom-Up\n\nimage ~~ High Level까지 feature 전달됨, 일발적인 CNN Backbone 통과 과정을 의미\n\n\nTop-Down\n\nHigh Level ~~ Low Level까지 feature 전달\nFeature Map의 Dimension이 맞지 않음\n\n\nTop-Down Path : Up Convolution 진행(h,w 피팅)\n\n\n기존 level Path : 1x1 Convolution 진행(c 피팅)\n\n\n\n\n\n\nStage Mapping\nFaster R-CNN 구조에 FPN을 적용하려면 Region Proposal을 어떤 scale의 feature map과 매칭시킬지 결정해야 합니다.\nFast R-CNN은 single-scale feature map만을 사용한 반면, FPN을 적용한 Faster R-CNN은 multi-scale feature map을 사용하기 때문입니다.\n저자는 아래 공식을 사용하여 k번째 feature map과 매칭시킵니다.\nk=[k_0 + \\log_2({{\\sqrt {wh}} \\over 224})] \\\\ k_0 : 4(default)"},"vault/Notion/DB/DB-Blog-Post/GAN(Vanilla-GAN)/GAN(Vanilla-GAN)":{"slug":"vault/Notion/DB/DB-Blog-Post/GAN(Vanilla-GAN)/GAN(Vanilla-GAN)","filePath":"vault/Notion/DB/DB Blog Post/GAN(Vanilla GAN)/GAN(Vanilla GAN).md","title":"GAN(Vanilla GAN)","links":[],"tags":[],"content":"\n참조\nGAN\nGAN의 Loss(Objective Function)\nMode-Collapse\n\n\n참조\nproceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf\npseudo-lab.github.io/Tutorial-Book/chapters/GAN/Ch1-Introduction.html\nyamalab.tistory.com/98\nprocess-mining.tistory.com/169\nraon1123.blogspot.com/2019/10/gan-model-collapse.html\nGAN\n\nGAN(Generative Adversarial Network)은 인공신경망(Artificial Neural Network)을 사용하여 데이터의 분포를 모방하는 새로운 방식의 기계학습 기법입니다. 데이터 분포를 모방하는 것은 신경망의 핵심 기능입니다.\n\nGAN은 데이터를 생성하는 generator와 데이터를 구별하는 Discriminator로 이루어져 있습니다. \n\n\nGenerator는 최대한 실제처럼 보이는 데이터를 생성하여 Discriminator를 속이려고 시도합니다.\n\n\nDiscriminator는 실제 데이터와 만들어진 가짜 데이터를 구별하려고 시도합니다.\n\n\n두 네트워크는 서로 경쟁하며, 결과적으로 Generator가 데이터의 분포를 잘 모방하게 됩니다.\n\n\n\nGAN의 Loss(Objective Function)\n\n\n\nDiscriminator는 위 식에서, D(x)를 최대한 1에 가깝게 만들고 D(G(z))를 최대한 0에 가깝게 만듦으로써 Objective Function을 최대화하는 것을 목적으로 합니다.\n\n\nGenerator는 위 식에서 D(G(z))를 최대한 1에 가깝게 만듦으로써 Objective Function을 최소화하는 것을 목적으로 한다.\n\n\nGAN을 학습하면 실제로는 학습이 잘 되지 않는 경우가 있습니다.\nGenerator의 성능이 안 좋을 때, 위 Loss를 사용하면 Gradient가 굉장히 작아서 학습이 빠르게 될 수 없기 때문입니다.\n\n위 그래프를 보면, log(1−D)이 0에 가까울 때(=D(G(z))가 0에 가까울 때) Gradient가 0에 가깝기 때문에 학습이 잘 되지 않을 수 있습니다.\n이 경우, \\log(1-D_\\phi (G_\\theta (z))) 대신에 -\\log(D_\\phi (G_\\theta (z)))를 loss로 사용합니다. 기존 loss와 유사한 의미를 갖고 있으면서도 Generator가 학습되지 않았을 때(=D(G(z))가 0에 가까울 때)의 gradient가 크기 때문에 학습이 비교적 잘 될 수 있습니다.\nMode-Collapse\n\nMode-Collapse는 학습의 불균형으로 발생합니다. Gernerator와 Discriminator중 하나가 학습이 너무 잘 되서 다른 하나의 학습이 안되는 것을 의미합니다.\n\ndiscriminator가 너무 학습이 잘 되어서 완벽하게 generate된 이미지를 구분할 수 있는 경우를 생각해보도록 하겠습니다. generator는 어떠한 이미지를 내더라도 discriminator를 속일 수 없고, 더이상 학습이 진행이 되지 않을 것입니다.\n반대의 경우도 마찬가지 입니다. 한쪽이 너무 잘 되어버리면 다른쪽은 학습이 더이상 진행이 되지 않고 멈추어버리게 됩니다. 이렇게 되면 generator는 한 종류의 이미지만 계속 생성하게 됩니다.\n"},"vault/Notion/DB/DB-Blog-Post/Github-잔디-Notion에-Embed하기":{"slug":"vault/Notion/DB/DB-Blog-Post/Github-잔디-Notion에-Embed하기","filePath":"vault/Notion/DB/DB Blog Post/Github 잔디 Notion에 Embed하기.md","title":"Github 잔디 Notion에 Embed하기","links":[],"tags":[],"content":"개요\n이 글은 내 github 잔디를 Notion Page에 embed하는 방법을 소개합니다.\nHTML 링크 생성\n\n\n아래 링크에 자신의 github username을 넣습니다.\nghchart.rshah.org/{your github user name}\n \n# example\nghchart.rshah.org/404Vector\n\n\n링크를 복사합니다.\n\n\n내가 원하는 노션 공간에 임베드 블록을 생성하고, 복사한 링크를 링크임베드 칸에 붙여넣습니다.\n\n\n"},"vault/Notion/DB/DB-Blog-Post/GoogLeNet(Inception-v1)/GoogLeNet(Inception-v1)":{"slug":"vault/Notion/DB/DB-Blog-Post/GoogLeNet(Inception-v1)/GoogLeNet(Inception-v1)","filePath":"vault/Notion/DB/DB Blog Post/GoogLeNet(Inception v1)/GoogLeNet(Inception v1).md","title":"GoogLeNet(Inception v1)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4"],"tags":[],"content":"\n참조\nGoogLeNet(Inception v1)\n특징\n\n\n참조\ndeepai.org/machine-learning-glossary-and-terms/inception-module\ndatacrew.tech/inception-v1-2014/\nWeek 4\nvelog.io/@woojinn8/CNN-Networks-3.-GoogLeNet\nGoogLeNet(Inception v1)\n\nGoogLeNet은 ILSVRC-2014에서 우승한 CNN Network입니다.GoogLeNet의 개발자들은 2014년 당시 딥러닝 모델의 수치적인 성능 향상이라는 수치적인 점 보다는 Convolution을 이용한 딥러닝 네트워크 구조의 혁신에 고무되었고, 더 깊으면서 더 효과적인 네트워크 구조 설계를 위해 노력했습니다.\n그 결과 Inception module이라는 구조의 네트워크 구조를 설계했고, 이 Inception module을 활용해 AlexNet보다 더 깊지만 파라미터 수는 121인 GoogLeNet을 설계했습니다.\n이렇듯 GoogLeNet은 효율적인 구조설계로 이전보다 파라미터 수와 성능면에서 좋은 성능 보여주는 네트워크였지만, 복잡한 구조와 그로인한 적용의 어려움 때문에 같은 대회에서 2위한 VGGNet보다 덜 활용되는 네트워크입니다.\n특징\n\n\nInception module\n\n\n여러 스케일의 Conv layer의 병렬 사용\n파라미터 수 감소를 위해 1x1 Covolution 사용\n1x1, 3x3, 5x5 세 개의 Conv layer와 1개의 Max-pooling을 사용\n각각의 결과를 연결해(concat) 하나의 output을 생성\n\n\n\nAuxiliary Classifier\n\nVanishing gradient를 해결하기 위해 도중에 값을 injection\n낮은 단계에 해당하는 layer에서도 backpropagation되는 gradient signal을 증폭시킴\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Graph란-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/Graph란-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/Graph란 무엇일까.md","title":"Graph란 무엇일까","links":[],"tags":[],"content":"\n참조\n그래프\n그래프 종류\n그래프 용어\n\n\n참조\nleejinseop.tistory.com/43\n그래프\n\n그래프(Graph)는 연결되어있는 원소간의 관계를 표현한 자료구조입니다.\n· 그래프는 연결할 객체를 나타내는 정점(Vertext)과 객체를 연결하는 간선(Edge)의 집합으로 구성됩니다.\n· 그래프 G를 G=(V, E)로 정의하는데, V는 정점의 집합, E는 간선들의 집합을 의미합니다.\n그래프 종류\n\n무방향 그래프\n\n\n무방향 그래프(Undirected Graph)는 두 정점을 연결하는 간선에 방향이 없는 그래프.\n\n무향방 그래프에서 정점 Vi와 Vj를 연결하는 간선을 (Vi, Vj)로 표현하는데, 이때 (Vi, Vj)와 (Vj, Vi)는 같은 간선을 나타냅니다.\nV(G1)={A,B,C,D}, E(G1)={(A,B), (A,D), (B,C), (B,D), (C,D)}\n\n\n방향 그래프\n\n방향 그래프(Directed Graph)는 간선에 방향이 있는 그래프.\n\n\n방향 그래프에서 정점 Vi와 Vj를 연결하는 간선을 &lt;Vi, Vj&gt;로 표현하는데 Vi를 꼬리(tail), Vj를 머리(head)라고 합니다. 이때 &lt;Vi, Vj&gt;와 &lt;Vj, Vi&gt;는 서로 다른 간선입니다.\nV(G1)={A,B,C,D} E(G1)={&lt;A,B&gt;, &lt;A,D&gt;, &lt;B,C&gt;, &lt;B,D&gt;, &lt;C,D&gt;}\n\n\n완전 그래프\n\n완전 그래프(Complete Graph)는 한 정점에서 다른 모든 정점과 연결되어 최대 간선 수를 갖는 그래프.\n\n· 정점이 n개인 완전 그래프에서 무방향 그래프의 최대 간선 수는 n(n-1)/2이고 방향 그래프의 최대 간선 수는 n(n-1)입니다.\n\n부분 그래프\n\n부분 그래프(Subgraph)는 기존의 그래프에서 일부 정점이나 간선을 제외하여 만든 그래프.\n\n\n가중 그래프\n\n가중 그래프(Weight Graph)는 정점을 연결하는 간선에 가중치(weight)를 할당한 그래프.\n\n\n유향 비순환 그래프(DAG, Directed Acyclic Graph)\n\n방향 그래프에서 사이클이 없는 그래프.\n\n\n연결 그래프(Connected Graph)\n\n떨어져 있는 정점이 없는 그래프.\n\n연결 그래프 G9\n\n단절 그래프(Disconnected Graph)\n\n연결되지 않은 정점이 있는 그래프.\n\n그래프 용어\n그래프에서 두 정점 Vi와 Vj가 연결되어 간선 (Vi, Vj)가 있을 때, 두 정점 Vi와 Vj를 인접(adjacent)한다 하고, 간선 (Vi, Vj)는 정점 Vi와 Vj에 부속(incident)되어 있다고 말합니다.\n\n차수(Dgree): 정점에 부속되어 있는 간선의 수\n\n진입차수(in-degree): 정점을 머리로 하는 간선의 수\n진출차수(out-degree): 정점을 꼬리로 하는 간선의 수\n\n\n경로(Path): 정점 Vi에서 Vj까지 간선으로 연결된 정점을 순서대로 나열한 리스트\n\n단순 경로(Simple Path): 모두 다른 정점으로 구성된 경로\n\n\n경로 길이(Path Length): 경로를 구성하는 간선의 수\n사이클(Cycle): 단순 경로 중에서 경로의 시작 정점과 마지막 정점이 같은 경로\n"},"vault/Notion/DB/DB-Blog-Post/Heap이란-무엇일까/Heap이란-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/Heap이란-무엇일까/Heap이란-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/Heap이란 무엇일까/Heap이란 무엇일까.md","title":"Heap이란 무엇일까","links":[],"tags":[],"content":"\n참조\nHeap이란?\nHeap의 종류\nHeap의 구현\n\n\n참조\ngmlwjd9405.github.io/2018/05/10/data-structure-heap.html\nHeap이란?\n\n완전 이진 트리의 일종으로 우선순위 큐를 위하여 만들어진 자료구조\n여러 개의 값들 중에서 최댓값이나 최솟값을 빠르게 찾아내도록 만들어진 자료구조\n힙은 일종의 반정렬 상태(느슨한 정렬 상태) 를 유지\n큰 값이 상위 레벨에 있고 작은 값이 하위 레벨에 있다는 정도\n\n부모 노드의 키 값이 자식 노드의 키 값보다 항상 크게(작게) 정렬한 이진 트리\n\n\n힙 트리에서는 중복된 값을 허용\n\n이진 탐색 트리에서는 중복된 값을 허용하지 않음\n\n\n\nHeap의 종류\n\n\n최대 힙(max heap)\n\n부모 노드의 키 값이 자식 노드의 키 값보다 크거나 같은 완전 이진 트리\nkey(부모 노드) &gt;= key(자식 노드)\n\n\n최소 힙(min heap)\n\n부모 노드의 키 값이 자식 노드의 키 값보다 작거나 같은 완전 이진 트리\nkey(부모 노드) ⇐ key(자식 노드)\n\n\n\nHeap의 구현\n\n\n\n힙을 저장하는 표준적인 자료구조는 배열\n\n\n구현을 쉽게 하기 위하여 배열의 첫 번째 인덱스인 0은 사용하지 않음\n\n\n특정 위치의 노드 번호는 새로운 노드가 추가되어도 변하지 않음\n\n예를 들어 루트 노드의 오른쪽 노드의 번호는 항상 3\n\n\n\n힙에서의 부모 노드와 자식 노드의 관계\n\n왼쪽 자식의 인덱스 = (부모의 인덱스) * 2\n오른쪽 자식의 인덱스 = (부모의 인덱스) * 2 + 1\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Kaggle---Intro-to-Machine-Learning-번역":{"slug":"vault/Notion/DB/DB-Blog-Post/Kaggle---Intro-to-Machine-Learning-번역","filePath":"vault/Notion/DB/DB Blog Post/Kaggle - Intro to Machine Learning 번역.md","title":"Kaggle - Intro to Machine Learning 번역","links":[],"tags":[],"content":"\n참조\nKaggle - Intro to Machine Learning\n\n[[#How Models Work []]\n[[#Basic Data Exploration []]\n[[#Your First Machine Learning Model []]\n[[#Model Validation []]\n[[#Underfitting and Overfitting []]\n[[#Random Forests []]\n\n\n\n\n참조\nwww.kaggle.com/code/dansbecker/how-models-work\nwww.kaggle.com/code/dansbecker/basic-data-explorationhttps://www.kaggle.com/code/dansbecker/your-first-machine-learning-model\nwww.kaggle.com/code/dansbecker/model-validation\nwww.kaggle.com/code/dansbecker/underfitting-and-overfitting\nwww.kaggle.com/code/dansbecker/random-forests\nKaggle - Intro to Machine Learning\nHow Models Work [link]\nIntroduction\n기계 학습 모델의 작동 방식과 사용 방법에 대한 개요부터 시작하겠습니다. 이전에 통계 모델링이나 머신 러닝을 해본 적이 있다면 기본적으로 느껴질 수 있습니다. 걱정하지 마십시오. 곧 강력한 모델을 구축할 것입니다.\n이 과정에서는 다음 시나리오를 진행하면서 모델을 구축하게 됩니다.\n당신의 사촌은 부동산 투기로 수백만 달러를 벌었습니다. 데이터 과학에 대한 귀하의 관심 때문에 그는 귀하와 비즈니스 파트너가 되겠다는 제안을 받았습니다. 그는 돈을 공급할 것이고 당신은 다양한 주택의 가치를 예측하는 모델을 제공할 것입니다.\n당신은 당신의 사촌에게 그가 과거에 부동산 가치를 어떻게 예측했는지 물었고 그는 단지 직감일 뿐이라고 말했습니다. 그러나 더 많은 질문은 그가 과거에 본 주택에서 가격 패턴을 식별했으며 이러한 패턴을 사용하여 그가 고려 중인 새 주택에 대한 예측을 한다는 것을 보여줍니다.\n기계 학습은 동일한 방식으로 작동합니다. 의사 결정 트리라는 모델로 시작하겠습니다. 더 정확한 예측을 제공하는 더 멋진 모델이 있습니다. 그러나 의사 결정 트리는 이해하기 쉽고 데이터 과학에서 일부 최고의 모델을 위한 기본 빌딩 블록입니다.\n단순화를 위해 가능한 가장 간단한 의사 결정 트리부터 시작하겠습니다.\n\n주택을 두 가지 범주로만 나눕니다. 고려 중인 모든 주택의 예상 가격은 같은 범주에 있는 주택의 과거 평균 가격입니다.\n우리는 데이터를 사용하여 주택을 두 그룹으로 나누는 방법을 결정한 다음 다시 각 그룹의 예상 가격을 결정합니다. 데이터에서 패턴을 캡처하는 이 단계를 피팅 또는 훈련 모델이라고 합니다. 모델을 적합하는 데 사용되는 데이터를 학습 데이터라고 합니다.\n모델이 어떻게 적합했는지에 대한 세부 사항(예: 데이터 분할 방법)은 나중에 저장하기 위해 충분히 복잡합니다. 모델이 적합해지면 새로운 데이터에 적용하여 추가 주택의 예측 가격을 산출할 수 있습니다.\nImproving the Decision Tree\n다음 두 의사결정 트리 중 어느 것이 부동산 교육 데이터를 피팅한 결과일 가능성이 더 높습니까?\n\n왼쪽의 결정 트리(결정 트리 1)는 침실이 더 많은 주택이 침실이 더 적은 주택보다 더 높은 가격에 판매되는 경향이 있다는 현실을 포착하기 때문에 아마도 더 의미가 있을 것입니다. 이 모델의 가장 큰 단점은 욕실 수, 부지 크기, 위치 등과 같이 주택 가격에 영향을 미치는 대부분의 요소를 포착하지 못한다는 것입니다.\n더 많은 “분할”이 있는 트리를 사용하여 더 많은 요인을 캡처할 수 있습니다. 이를 “더 깊은” 트리라고 합니다. 각 주택 부지의 전체 크기도 고려하는 의사 결정 트리는 다음과 같습니다.\n\n의사 결정 트리를 추적하고 항상 해당 주택의 특성에 해당하는 경로를 선택하여 주택 가격을 예측합니다. 집의 예상 가격은 트리 맨 아래에 있습니다. 하단에서 예측을 수행하는 지점을 리프라고 합니다.\n리프의 분할 및 값은 데이터에 의해 결정되므로 작업할 데이터를 확인해야 합니다.\nBasic Data Exploration [link]\nUsing Pandas to Get Familiar With Your Data\n기계 학습 프로젝트의 첫 번째 단계는 데이터에 익숙해지는 것입니다. 이를 위해 Pandas 라이브러리를 사용합니다. Pandas는 데이터 과학자가 데이터를 탐색하고 조작하는 데 사용하는 기본 도구입니다. 대부분의 사람들은 코드에서 팬더를 pd로 축약합니다. 우리는 명령으로 이것을합니다.\nimport pandas as pd\nPandas 라이브러리의 가장 중요한 부분은 DataFrame입니다. DataFrame은 테이블로 생각할 수 있는 데이터 유형을 보유합니다. 이는 Excel의 시트나 SQL 데이터베이스의 테이블과 비슷합니다.\nPandas에는 이러한 유형의 데이터로 수행하려는 대부분의 작업에 대한 강력한 방법이 있습니다.\n예를 들어 호주 멜버른의 주택 가격에 대한 데이터를 살펴보겠습니다. 실습에서는 아이오와의 주택 가격이 있는 새 데이터 세트에 동일한 프로세스를 적용합니다.\n예제(멜버른) 데이터는 **../input/melbourne-housing-snapshot/melb_data.csv** 파일 경로에 있습니다.\n다음 명령을 사용하여 데이터를 로드하고 탐색합니다.\n# save filepath to variable for easier access\nmelbourne_file_path = &#039;../input/melbourne-housing-snapshot/melb_data.csv&#039;\n# read the data and store data in DataFrame titled melbourne_data\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# print a summary of the data in Melbourne data\nmelbourne_data.describe()\nOut[2]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoomsPriceDistancePostcodeBedroom2BathroomCarLandsizeBuildingAreaYearBuiltLattitudeLongtitudePropertycountcount13580.0000001.358000e+0413580.00000013580.00000013580.00000013580.00000013518.00000013580.0000007130.0000008205.00000013580.00000013580.00000013580.000000mean2.9379971.075684e+0610.1377763105.3019152.9147281.5342421.610075558.416127151.9676501964.684217-37.809203144.9952167454.417378std0.9557486.393107e+055.86872590.6769640.9659210.6917120.9626343990.669241541.01453837.2737620.0792600.1039164378.581772min1.0000008.500000e+040.0000003000.0000000.0000000.0000000.0000000.0000000.0000001196.000000-38.182550144.431810249.00000025%2.0000006.500000e+056.1000003044.0000002.0000001.0000001.000000177.00000093.0000001940.000000-37.856822144.9296004380.00000050%3.0000009.030000e+059.2000003084.0000003.0000001.0000002.000000440.000000126.0000001970.000000-37.802355145.0001006555.00000075%3.0000001.330000e+0613.0000003148.0000003.0000002.0000002.000000651.000000174.0000001999.000000-37.756400145.05830510331.000000max10.0000009.000000e+0648.1000003977.00000020.0000008.00000010.000000433014.00000044515.0000002018.000000-37.408530145.52635021650.000000\nInterpreting Data Description\n결과에는 원본 데이터 세트의 각 열에 대해 8개의 숫자가 표시됩니다. 첫 번째 숫자인 개수는 누락되지 않은 값이 있는 행 수를 보여줍니다.\n누락된 값은 여러 가지 이유로 발생합니다. 예를 들어 침실이 1개인 집을 측량할 때 침실 2의 크기는 수집되지 않습니다. 누락된 데이터 주제로 다시 돌아오겠습니다.\n두 번째 값은 평균인 평균입니다. 그 아래 std는 값이 수치적으로 얼마나 퍼져 있는지를 측정하는 표준 편차입니다.\nmin, 25%, 50%, 75% 및 max 값을 해석하려면 각 열을 최저값에서 최고값으로 정렬한다고 상상해 보세요. 첫 번째(가장 작은) 값은 최소값입니다. 목록을 4분의 1로 이동하면 값의 25%보다 크고 값의 75%보다 작은 숫자를 찾을 수 있습니다. 이것이 25% 값(‘25번째 백분위수’라고 발음함)입니다. 50번째 및 75번째 백분위수는 유사하게 정의되며 최대는 가장 큰 숫자입니다.\n(Exercise: Explore Your Data)\nYour First Machine Learning Model [link]\nSelecting Data for Modeling\n데이터 세트에 변수가 너무 많아 머리를 감싸거나 멋지게 인쇄할 수 없었습니다. 이 엄청난 양의 데이터를 이해할 수 있는 것으로 어떻게 줄일 수 있습니까?\n직관을 사용하여 몇 가지 변수를 선택하여 시작하겠습니다. 이후 과정에서는 변수의 우선 순위를 자동으로 지정하는 통계 기술을 보여줍니다.\n변수/열을 선택하려면 데이터 세트의 모든 열 목록을 확인해야 합니다. 이는 DataFrame의 columns 속성으로 수행됩니다(아래 코드의 맨 아래 줄).\nIn [1]:\nimport pandas as pd\nmelbourne_file_path = &#039;../input/melbourne-housing-snapshot/melb_data.csv&#039;\nmelbourne_data = pd.read_csv(melbourne_file_path) \nmelbourne_data.columns\nOut[1]:\n`Index([&#039;Suburb&#039;, &#039;Address&#039;, &#039;Rooms&#039;, &#039;Type&#039;, &#039;Price&#039;, &#039;Method&#039;, &#039;SellerG&#039;,\n       &#039;Date&#039;, &#039;Distance&#039;, &#039;Postcode&#039;, &#039;Bedroom2&#039;, &#039;Bathroom&#039;, &#039;Car&#039;,\n       &#039;Landsize&#039;, &#039;BuildingArea&#039;, &#039;YearBuilt&#039;, &#039;CouncilArea&#039;, &#039;Lattitude&#039;,\n       &#039;Longtitude&#039;, &#039;Regionname&#039;, &#039;Propertycount&#039;],\n      dtype=&#039;object&#039;)`\nIn [2]:\n# The Melbourne data has some missing values (some houses for which some variables weren&#039;t recorded.)\n# We&#039;ll learn to handle missing values in a later tutorial.  \n# Your Iowa data doesn&#039;t have missing values in the columns you use. \n# So we will take the simplest option for now, and drop houses from our data. \n# Don&#039;t worry about this much for now, though the code is:\n \n# dropna drops missing values (think of na as &quot;not available&quot;)\nmelbourne_data = melbourne_data.dropna(axis=0)\n데이터의 하위 집합을 선택하는 방법에는 여러 가지가 있습니다. Pandas 과정에서 이를 자세히 다루지만 지금은 두 가지 접근 방식에 중점을 둘 것입니다.\n\n“예측 대상”을 선택하는 데 사용하는 점 표기법\n“기능”을 선택하는 데 사용하는 열 목록으로 선택\n\nSelecting The Prediction Target\n점 표기법으로 변수를 꺼낼 수 있습니다. 이 단일 열은 시리즈에 저장되며, 이는 단일 데이터 열만 있는 DataFrame과 대체로 유사합니다.\n점 표기법을 사용하여 예측 대상이라고 하는 예측하려는 열을 선택합니다. 규칙에 따라 예측 대상은 y라고 합니다.\n멜버른 데이터에 집값을 저장하는 데 필요한 코드는 다음과 같습니다.\nIn [3]:\ny = melbourne_data.Price\nChoosing “Features”\n모델에 입력되고 나중에 예측에 사용되는 열을 “특성”이라고 합니다. 이 경우에는 주택 가격을 결정하는 데 사용되는 열이 됩니다. 경우에 따라 대상을 제외한 모든 열을 기능으로 사용합니다. 다른 경우에는 더 적은 기능으로 더 나을 것입니다.\n지금은 몇 가지 기능만 있는 모델을 빌드합니다. 나중에 다른 기능으로 빌드된 모델을 반복하고 비교하는 방법을 볼 수 있습니다.\n대괄호 안에 열 이름 목록을 제공하여 여러 기능을 선택합니다. 해당 목록의 각 항목은 문자열(따옴표 포함)이어야 합니다.\n다음은 예입니다.\nIn [4]:\nmelbourne_features = [&#039;Rooms&#039;, &#039;Bathroom&#039;, &#039;Landsize&#039;, &#039;Lattitude&#039;, &#039;Longtitude&#039;]\n관례적으로 이 데이터를 X라고 합니다.\nIn [5]:\nX = melbourne_data[melbourne_features]\n상위 몇 행을 표시하는 describe 방법과 head 방법을 사용하여 주택 가격을 예측하는 데 사용할 데이터를 빠르게 검토해 보겠습니다.\nIn [6]:\nX.describe()\nOut[6]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoomsBathroomLandsizeLattitudeLongtitudecount6196.0000006196.0000006196.0000006196.0000006196.000000mean2.9314071.576340471.006940-37.807904144.990201std0.9710790.711362897.4498810.0758500.099165min1.0000001.0000000.000000-38.164920144.54237025%2.0000001.000000152.000000-37.855438144.92619850%3.0000001.000000373.000000-37.802250144.99580075%4.0000002.000000628.000000-37.758200145.052700max8.0000008.00000037000.000000-37.457090145.526350\nIn [7]:\nX.head()\nOut[7]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoomsBathroomLandsizeLattitudeLongtitude121.0156.0-37.8079144.9934232.0134.0-37.8093144.9944441.0120.0-37.8072144.9941632.0245.0-37.8024144.9993721.0256.0-37.8060144.9954\n이러한 명령으로 데이터를 시각적으로 확인하는 것은 데이터 과학자의 업무에서 중요한 부분입니다. 데이터 세트에서 추가 검사가 필요한 놀라움을 자주 발견하게 될 것입니다.\n\nBuilding Your Model\nscikit-learn 라이브러리를 사용하여 모델을 생성합니다. 코딩 시 이 라이브러리는 샘플 코드에서 볼 수 있듯이 sklearn으로 작성됩니다. Scikit-learn은 일반적으로 DataFrames에 저장되는 데이터 유형을 모델링하기 위한 가장 인기 있는 라이브러리입니다.\n모델을 구축하고 사용하는 단계는 다음과 같습니다.\n\n정의: 어떤 유형의 모델이 될까요? 의사 결정 트리? 다른 유형의 모델? 모델 유형의 일부 다른 매개변수도 지정됩니다.\nFit: 제공된 데이터에서 패턴을 캡처합니다. 이것이 모델링의 핵심입니다.\n예측: 소리 그대로\n평가: 모델의 예측이 얼마나 정확한지 확인합니다.\n\n다음은 scikit-learn으로 의사 결정 트리 모델을 정의하고 기능 및 대상 변수에 피팅하는 예입니다.\nIn [8]:\nfrom sklearn.tree import DecisionTreeRegressor\n \n# Define model. Specify a number for random_state to ensure same results each run\nmelbourne_model = DecisionTreeRegressor(random_state=1)\n \n# Fit model\nmelbourne_model.fit(X, y)\nOut[8]:\nDecisionTreeRegressor(random_state=1)\n많은 기계 학습 모델은 모델 교육에서 임의성을 허용합니다. ‘random_state’에 숫자를 지정하면 각 실행에서 동일한 결과를 얻을 수 있습니다. 이것은 좋은 습관으로 간주됩니다. 임의의 숫자를 사용하면 선택한 값에 따라 모델 품질이 크게 달라지지 않습니다.\n이제 예측에 사용할 수 있는 적합한 모델이 있습니다.\n실제로는 이미 가격이 책정된 주택이 아니라 시장에 출시되는 새 주택에 대한 예측을 원할 것입니다. 하지만 학습 데이터의 처음 몇 행에 대해 예측을 수행하여 예측 기능이 어떻게 작동하는지 확인합니다.\nIn [9]:\nprint(&quot;Making predictions for the following 5 houses:&quot;)\nprint(X.head())\nprint(&quot;The predictions are&quot;)\nprint(melbourne_model.predict(X.head()))\nMaking predictions for the following 5 houses: Rooms Bathroom Landsize Lattitude Longtitude 1 2 1.0 156.0 -37.8079 144.9934 2 3 2.0 134.0 -37.8093 144.9944 4 4 1.0 120.0 -37.8072 144.9941 6 3 2.0 245.0 -37.8024 144.9993 7 2 1.0 256.0 -37.8060 144.9954 The predictions are [1035000. 1465000. 1600000. 1876000. 1636000.]\nModel Validation [link]\n모델을 만들었습니다. 하지만 얼마나 좋은가요?\n이 단원에서는 모델 검증을 사용하여 모델의 품질을 측정하는 방법을 배웁니다. 모델 품질을 측정하는 것은 모델을 반복적으로 개선하는 핵심입니다.\nWhat is Model Validation\n구축한 거의 모든 모델을 평가하고 싶을 것입니다. 전부는 아니지만 대부분의 애플리케이션에서 모델 품질의 관련 척도는 예측 정확도입니다. 즉, 모델의 예측이 실제로 발생하는 것과 비슷할 것입니다.\n많은 사람들이 예측 정확도를 측정할 때 큰 실수를 합니다. 그들은 _학습 데이터_로 예측을 하고 이러한 예측을 _학습 데이터_의 목표 값과 비교합니다. 이 접근 방식의 문제와 해결 방법을 잠시 후에 볼 수 있지만 먼저 이 작업을 수행하는 방법에 대해 생각해 봅시다.\n먼저 모델 품질을 이해할 수 있는 방식으로 요약해야 합니다. 10,000채의 주택에 대한 예상 주택 가격과 실제 주택 가격을 비교하면 좋은 예측과 나쁜 예측이 혼합되어 있음을 알 수 있습니다. 10,000개의 예측 및 실제 값 목록을 살펴보는 것은 무의미합니다. 우리는 이것을 하나의 지표로 요약해야 합니다.\n모델 품질을 요약하는 많은 측정항목이 있지만 Mean Absolute Error(MAE라고도 함)라는 항목부터 시작하겠습니다. 마지막 단어인 오류부터 시작하여 이 메트릭을 분석해 보겠습니다.\n각 주택의 예측 오차는 다음과 같습니다.\nerror = actual−predicted\n따라서 집값이 150,000달러이고 100,000달러가 될 것이라고 예측한 경우 오류는 50,000달러입니다.\nMAE 메트릭을 사용하여 각 오류의 절대값을 취합니다. 이렇게 하면 각 오류가 양수로 변환됩니다. 그런 다음 이러한 절대 오류의 평균을 취합니다. 이것은 모델 품질의 척도입니다. 평이한 영어로는 다음과 같이 말할 수 있습니다.\n\n평균적으로, X에 대한 우리의 예측은 조금씩 빗나갑니다.\n\nMAE를 계산하려면 먼저 모델이 필요합니다. 그것은 아래의 숨겨진 셀에 내장되어 있으며 코드 버튼을 클릭하여 검토할 수 있습니다.\nIn [1]:\n# Data Loading Code Hidden Here\nimport pandas as pd\n \n# Load data\nmelbourne_file_path = &#039;../input/melbourne-housing-snapshot/melb_data.csv&#039;\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing price values\nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and features\ny = filtered_melbourne_data.Price\nmelbourne_features = [&#039;Rooms&#039;, &#039;Bathroom&#039;, &#039;Landsize&#039;, &#039;BuildingArea&#039;, \n                        &#039;YearBuilt&#039;, &#039;Lattitude&#039;, &#039;Longtitude&#039;]\nX = filtered_melbourne_data[melbourne_features]\n \nfrom sklearn.tree import DecisionTreeRegressor\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(X, y)\nOut[1]:\nDecisionTreeRegressor()\n모델이 있으면 평균 절대 오차를 계산하는 방법은 다음과 같습니다.\nIn [2]:\nfrom sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\nOut[2]:\n434.71594577146544\nThe Problem with “In-Sample” Scores\n방금 계산한 측정값을 “샘플 내” 점수라고 합니다. 우리는 모델을 구축하고 평가하기 위해 주택의 단일 “샘플”을 사용했습니다. 이것이 나쁜 이유가 여기에 있습니다.\n대규모 부동산 시장에서 문 색상이 주택 가격과 관련이 없다고 상상해 보십시오.\n그러나 모델을 구축하는 데 사용한 데이터 샘플에서 녹색 문이 있는 모든 집은 매우 비쌌습니다. 모델의 임무는 주택 가격을 예측하는 패턴을 찾는 것이므로 이 패턴을 확인하고 항상 녹색 문이 있는 주택의 높은 가격을 예측합니다.\n이 패턴은 교육 데이터에서 파생되었으므로 모델은 교육 데이터에서 정확하게 나타납니다.\n그러나 모델이 새 데이터를 볼 때 이 패턴이 유지되지 않으면 실제로 사용할 때 모델이 매우 부정확합니다.\n모델의 실질적인 가치는 새로운 데이터에 대한 예측에서 나오므로 모델을 구축하는 데 사용되지 않은 데이터에 대한 성능을 측정합니다. 이를 수행하는 가장 간단한 방법은 모델 구축 프로세스에서 일부 데이터를 제외하고 이를 사용하여 이전에 본 적이 없는 데이터에 대한 모델의 정확도를 테스트하는 것입니다. 이 데이터를 검증 데이터라고 합니다.\nCoding It\nscikit-learn 라이브러리에는 데이터를 두 부분으로 나누는 ‘train_test_split’ 함수가 있습니다. 이 데이터 중 일부는 모델에 맞는 학습 데이터로 사용하고 다른 데이터는 검증 데이터로 사용하여 ‘mean_absolute_error’를 계산합니다.\n코드는 다음과 같습니다.\nIn [3]:\nfrom sklearn.model_selection import train_test_split\n \n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n \n# get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))\n258930.03550677857\nWow!\n샘플 내 데이터에 대한 평균 절대 오차는 약 500달러였습니다. 아웃 오브 샘플은 250,000 달러 이상입니다.\n이것은 거의 정확히 맞는 모델과 대부분의 실용적인 목적에 사용할 수 없는 모델의 차이입니다. 참고로 검증 데이터의 평균 집값은 110만 달러다. 따라서 새 데이터의 오류는 평균 집값의 약 4분의 1입니다.\n더 나은 기능이나 다른 모델 유형을 찾기 위한 실험과 같이 이 모델을 개선하는 방법은 많습니다.\n(Exercise: Model Validation)\nUnderfitting and Overfitting [link]\nAt the end of this step, you will understand the concepts of underfitting and overfitting, and you will be able to apply these ideas to make your models more accurate.\nExperimenting With Different Models\n이 단계가 끝나면 과소적합 및 과적합의 개념을 이해하고 이러한 아이디어를 적용하여 모델을 더 정확하게 만들 수 있습니다.\nscikit-learn의 문서에서 결정 트리 모델에 많은 옵션(오랫동안 원하거나 필요로 하는 것 이상)이 있음을 확인할 수 있습니다. 가장 중요한 옵션은 트리의 깊이를 결정합니다. 이 과정의 첫 번째 강의에서 나무의 깊이는 예측에 도달하기 전에 만드는 분할 수의 척도임을 기억하세요. 이것은 비교적 얕은 나무입니다\n\n실제로 나무가 최상위 수준(모든 집)과 잎사귀 사이에 10개의 분할을 갖는 것은 드문 일이 아닙니다. 트리가 깊어질수록 데이터 세트는 더 적은 수의 집이 있는 리프로 분할됩니다. 트리가 1개만 분할된 경우 데이터를 2개의 그룹으로 나눕니다. 각 그룹이 다시 분할되면 4개의 하우스 그룹이 생성됩니다. 다시 각각을 나누면 8개의 그룹이 생성됩니다. 각 수준에서 더 많은 분할을 추가하여 그룹 수를 계속 두 배로 늘리면 10단계에 도달할 때까지 210210개의 집 그룹이 생깁니다. 1024개의 잎사귀입니다.\n우리가 많은 잎사귀 사이에서 집을 나눌 때, 각 잎사귀에는 더 적은 수의 집이 있습니다. 집이 거의 없는 잎사귀는 해당 집의 실제 가치에 매우 가까운 예측을 하지만 새 데이터에 대해서는 매우 신뢰할 수 없는 예측을 할 수 있습니다(각 예측은 몇 개의 집에만 기반하기 때문).\n모델이 학습 데이터와 거의 완벽하게 일치하지만 검증 및 기타 새로운 데이터에서는 제대로 작동하지 않는 과적합이라고 하는 현상입니다. 반대로 나무를 매우 얕게 만들면 집을 매우 뚜렷한 그룹으로 나누지 않습니다.\n극단적으로 나무가 집을 2~4개로 나누더라도 각 그룹에는 여전히 다양한 집이 있습니다. 결과 예측은 훈련 데이터에서도 대부분의 주택에서 멀리 떨어져 있을 수 있습니다(동일한 이유로 유효성 검사에서도 나쁠 것입니다). 모델이 데이터에서 중요한 차이점과 패턴을 포착하지 못해 학습 데이터에서도 성능이 떨어지는 경우를 과소적합이라고 합니다.\n검증 데이터에서 추정한 새 데이터의 정확도에 관심이 있기 때문에 과소적합과 과적합 사이의 적절한 지점을 찾고자 합니다. 시각적으로 우리는 아래 그림에서 (빨간색) 검증 곡선의 낮은 지점을 원합니다.\n\nExample\n트리 깊이를 제어하기 위한 몇 가지 대안이 있으며 많은 경우 트리를 통과하는 일부 경로가 다른 경로보다 더 깊은 깊이를 가질 수 있습니다. 그러나 max_leaf_nodes 인수는 과적합과 과소적합을 제어하는 매우 합리적인 방법을 제공합니다. 모델이 만들 수 있는 잎이 많을수록 위 그래프의 과소적합 영역에서 과적합 영역으로 더 많이 이동합니다.\n유틸리티 함수를 사용하여 max_leaf_nodes에 대한 다양한 값의 MAE 점수를 비교할 수 있습니다.\nIn [1]:\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n \ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n데이터는 이미 본 것과 같고 이미 작성한 코드를 사용하여 train_X, val_X, train_y 및 val_y에 로드됩니다.\nIn [2]:\n# Data Loading Code Runs At This Point\nimport pandas as pd\n    \n# Load data\nmelbourne_file_path = &#039;../input/melbourne-housing-snapshot/melb_data.csv&#039;\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing values\nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and features\ny = filtered_melbourne_data.Price\nmelbourne_features = [&#039;Rooms&#039;, &#039;Bathroom&#039;, &#039;Landsize&#039;, &#039;BuildingArea&#039;, \n                        &#039;YearBuilt&#039;, &#039;Lattitude&#039;, &#039;Longtitude&#039;]\nX = filtered_melbourne_data[melbourne_features]\n \nfrom sklearn.model_selection import train_test_split\n \n# split data into training and validation data, for both features and target\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\nfor-loop를 사용하여 max_leaf_nodes에 대해 다른 값으로 빌드된 모델의 정확도를 비교할 수 있습니다.\nIn [3]:\n# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(&quot;Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d&quot; %(max_leaf_nodes, my_mae))\nMax leaf nodes: 5 Mean Absolute Error: 347380 Max leaf nodes: 50 Mean Absolute Error: 258171 Max leaf nodes: 500 Mean Absolute Error: 243495 Max leaf nodes: 5000 Mean Absolute Error: 254983\n나열된 옵션 중에서 최적의 리프 수는 500입니다.\n\nConclusion\n요점은 다음과 같습니다. 모델은 다음과 같은 문제를 겪을 수 있습니다.\n\n과적합: 미래에 반복되지 않는 가짜 패턴을 캡처하여 덜 정확한 예측으로 이어짐\n과소적합: 관련 패턴을 캡처하지 못하여 예측 정확도가 떨어집니다.\n\n후보 모델의 정확도를 측정하기 위해 모델 학습에 사용되지 않는 검증 데이터를 사용합니다. 이를 통해 많은 후보 모델을 시도하고 최상의 모델을 유지할 수 있습니다.\n(Exercise: Underfitting and Overfitting)\nRandom Forests [link]\nIntroduction\n결정 트리는 어려운 결정을 내리게 합니다. 잎이 많은 깊은 나무는 각 예측이 잎에 있는 몇 채의 집의 과거 데이터에서 나오기 때문에 과대적합됩니다. 그러나 리프가 적은 얕은 트리는 원시 데이터에서 많은 차이를 캡처하지 못하기 때문에 성능이 좋지 않습니다.\n오늘날의 가장 정교한 모델링 기술조차도 과소적합과 과적합 사이의 긴장에 직면해 있습니다. 그러나 많은 모델에는 더 나은 성능으로 이어질 수 있는 영리한 아이디어가 있습니다. 랜덤 포레스트를 예로 들어 보겠습니다.\n랜덤 포레스트는 많은 트리를 사용하며 각 컴포넌트 트리의 예측을 평균하여 예측합니다. 일반적으로 단일 의사 결정 트리보다 예측 정확도가 훨씬 뛰어나며 기본 매개변수와 잘 작동합니다. 모델링을 계속하면 더 나은 성능으로 더 많은 모델을 학습할 수 있지만 많은 모델이 올바른 매개변수를 얻는 데 민감합니다.\nExample\n이미 몇 번 데이터를 로드하는 코드를 보았습니다. 데이터 로딩이 끝나면 다음과 같은 변수가 있습니다.\n\ntrain_X\nval_X\ntrain_y\nval_y\n\nIn [1]:\nimport pandas as pd\n    \n# Load data\nmelbourne_file_path = &#039;../input/melbourne-housing-snapshot/melb_data.csv&#039;\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing values\nmelbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and features\ny = melbourne_data.Price\nmelbourne_features = [&#039;Rooms&#039;, &#039;Bathroom&#039;, &#039;Landsize&#039;, &#039;BuildingArea&#039;, \n                        &#039;YearBuilt&#039;, &#039;Lattitude&#039;, &#039;Longtitude&#039;]\nX = melbourne_data[melbourne_features]\n \nfrom sklearn.model_selection import train_test_split\n \n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\nscikit-learn에서 결정 트리를 구축한 방법과 유사하게 랜덤 포레스트 모델을 구축합니다. 이번에는 DecisionTreeRegressor 대신 RandomForestRegressor 클래스를 사용합니다.\nIn [2]:\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n \nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))\n191669.7536453626\nConclusion\n추가 개선의 여지가 있을 수 있지만 이는 250,000의 최상의 의사 결정 트리 오류에 비해 크게 개선된 것입니다. 단일 결정 트리의 최대 깊이를 변경한 만큼 Random Forest의 성능을 변경할 수 있는 매개 변수가 있습니다. 그러나 Random Forest 모델의 가장 좋은 기능 중 하나는 이러한 튜닝 없이도 일반적으로 합리적으로 작동한다는 것입니다.\n(Exercise: Random Forests)\n(Exercise: Machine Learning Competitions)"},"vault/Notion/DB/DB-Blog-Post/Knowledge-distillation/Knowledge-distillation":{"slug":"vault/Notion/DB/DB-Blog-Post/Knowledge-distillation/Knowledge-distillation","filePath":"vault/Notion/DB/DB Blog Post/Knowledge distillation/Knowledge distillation.md","title":"Knowledge distillation","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4"],"tags":[],"content":"\n참조\nKnowledge distillation\n\n\n참조\nen.wikipedia.org/wiki/Knowledge_distillation\nWeek 4\nKnowledge distillation\n\n\n\n기계학습에서 Knowlkege distillation은 큰 모델에서 작은 모델(일반적으로) 지식을 이전하는 프로세스\n모델압축, unlabeled dataset에 대한 pseudo-label(가짜 라벨) 생성에 사용\nStudent Model이 Teacher Model의 결과를 흉내내게(mimic) 함\n"},"vault/Notion/DB/DB-Blog-Post/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding/LayoutLM-Paper-번역/LayoutLM-Paper-번역":{"slug":"vault/Notion/DB/DB-Blog-Post/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding/LayoutLM-Paper-번역/LayoutLM-Paper-번역","filePath":"vault/Notion/DB/DB Blog Post/LayoutLM- Pre-training of Text and Layout for Document Image Understanding/LayoutLM Paper 번역/LayoutLM Paper 번역.md","title":"LayoutLM Paper 번역","links":[],"tags":[],"content":"논문 번역(GPT-4)\nABSTRACT\nLayoutLM: 텍스트와 레이아웃의 사전 학습을 통한 문서 이미지 이해\n최근 문서 이미지에서 텍스트와 레이아웃 정보를 모두 활용하는 것이 중요하다고 인식되고 있습니다. LayoutLM은 이러한 정보를 통합하여 문서 이미지를 이해하기 위해 텍스트와 레이아웃을 사전 학습하는 새로운 방법을 제안합니다. 이 방법은 텍스트와 함께 문서의 레이아웃 정보를 입력으로 활용하여 더 나은 문서 분류, 내용 추출 및 질의 응답 성능을 달성합니다. 이 연구에서 LayoutLM은 여러 벤치마크에서 기존 모델들을 뛰어넘는 성능을 보여 주었습니다.\nCCS CONCEPTS\n\n정보 시스템→정보 추출: 문서로부터 유용한 정보를 추출하는 기술에 관한 연구입니다.\n컴퓨팅 방법론→기계 학습: 문서 이미지를 이해하기 위해 텍스트와 레이아웃 정보를 활용하는 사전 학습 기법을 다룹니다.\n\n이 섹션은 문서에서 정보를 추출하고 이해하는 데 필요한 컴퓨팅 기술과 방법론에 초점을 맞춥니다.\nKEYWORDS\n\n문서 이미지 이해: 스캔된 문서나 디지털 문서 형식에서의 정보 추출 및 해석 • 텍스트 및 레이아웃 사전 학습: 문서의 텍스트와 레이아웃 정보를 함께 사용하여 사전 학습을 수행 • 딥 러닝: 문서 분석과 처리를 위해 깊은 신경망을 사용하는 기술 • NLP (자연어 처리): 자연어 데이터의 이해 및 처리를 위한 기술 및 방법론\n\n이 키워드들은 논문에서 다루는 주요 기술과 방법론의 범위를 나타냅니다.\n1. INTRODUCTION\n이 연구에서는 문서 이미지 이해를 위한 새로운 사전 학습 모델인 LayoutLM을 소개합니다. 최근 문서의 텍스트와 레이아웃 정보를 통합하여 학습하는 것의 중요성이 강조되고 있습니다. LayoutLM은 이러한 정보를 결합하여 문서의 의미를 더 정확하게 파악할 수 있도록 합니다. 본 논문에서는 모델의 구조와 사전 학습 방법, 그리고 여러 벤치마크 데이터셋에서의 성능 평가 결과를 제시하며, 이 모델이 문서 이미지 처리 및 분석 작업에 어떻게 유용하게 적용될 수 있는지를 설명합니다.\n2. LAYOUTLM\nLayoutLM은 문서 이미지 이해를 위해 설계된 모델로, 텍스트와 레이아웃 정보를 통합하여 사전 학습합니다. 이 모델은 BERT 기반 아키텍처를 확장하여, 문서 내에서 각 단어의 2D 위치 정보를 고려합니다. 이는 문서의 시각적 구조를 이해하는 데 중요한 역할을 합니다. LayoutLM은 특히 문서 분류, 정보 추출, 질의응답 태스크에서 높은 성능을 보이며, 이를 통해 더 정확하고 효율적인 문서 처리가 가능함을 보여줍니다.\n\n2.1 The BERT Model\nBERT(Bidirectional Encoder Representations from Transformers)는 트랜스포머 구조를 기반으로 하며, 대량의 텍스트 데이터로부터 단방향이 아닌 양방향의 문맥을 학습할 수 있습니다. 이를 통해 자연어 처리(NLP)에서 다양한 태스크의 성능을 향상시킬 수 있습니다. BERT 모델은 또한 다양한 다운스트림 NLP 태스크에 대한 사전 훈련과 미세 조정 단계를 포함합니다. 이러한 특징 덕분에 BERT는 텍스트의 복잡한 의미를 더 효과적으로 이해하고 처리할 수 있습니다.\n2.2 The LayoutLM Model\nLayoutLM은 BERT 모델을 기반으로 하여, 텍스트 데이터와 함께 문서의 레이아웃 정보를 입력으로 사용합니다. 이 모델은 문서의 시각적 요소와 구조적 위치를 고려하여, 단어의 의미뿐만 아니라 해당 단어가 문서 내에서 어떤 역할을 하는지도 파악할 수 있도록 합니다. LayoutLM은 문서 분류, 정보 추출, 질의응답과 같은 태스크에서 더욱 향상된 성능을 보여주는 것이 목표입니다. 이를 통해, 문서의 텍스트와 레이아웃 정보를 모두 활용하는 종합적인 이해가 가능해집니다.\n2.3 Model Architecture\n이 모델은 텍스트 인코딩을 위한 BERT의 트랜스포머 아키텍처를 사용합니다. 여기에 문서의 레이아웃 정보를 처리할 수 있는 위치 인코딩 계층을 추가하여, 문서 내의 각 단어 위치가 모델에 입력됩니다. 또한, 시각적 특성을 포함시키기 위해 이미지 처리 서브모듈이 통합되어 있습니다. 이러한 아키텍처는 텍스트와 레이아웃 데이터 모두를 포괄적으로 활용하여, 문서 이해 작업에 있어 더욱 깊이 있는 분석을 가능하게 합니다.\n2.4 Pre-training LayoutLM\nLayoutLM은 먼저 BERT의 구조를 사용하여 대규모 텍스트 데이터에서 언어 모델링을 수행합니다. 여기에 문서의 레이아웃 정보를 추가로 결합하여, 모델이 텍스트와 함께 해당 텍스트가 위치한 문서 내의 공간적 정보도 학습하도록 합니다. 이 과정은 문서의 의미뿐 아니라 구조적 맥락을 이해하는데 중요한 역할을 합니다. 이렇게 사전 학습된 모델은 후속 작업에서 미세 조정을 통해 특정 문서 이해 작업에 적용됩니다.\n2.5 Fine-tuning LayoutLM\n사전 학습된 LayoutLM 모델은 특정 NLP 작업에 맞춰 추가적으로 학습됩니다. 이 과정에서, 문서의 종류나 태스크에 특화된 데이터를 사용하여 모델이 더욱 정밀하게 튜닝됩니다. 이를 통해 모델은 다양한 문서 분석 작업에서 보다 높은 성능을 발휘할 수 있으며, 특정 문서의 내용을 더욱 정확하게 이해하고 처리할 수 있습니다.\n3 EXPERIMENTS\n이 실험들은 주로 문서 분류, 정보 추출, 질의응답 등의 NLP 태스크를 대상으로 하며, 여러 벤치마크 데이터셋을 사용하여 LayoutLM의 성능을 다른 최신 모델들과 비교합니다. 결과적으로 LayoutLM은 텍스트와 레이아웃 정보를 통합하는 접근 방식이 문서 이미지 이해 작업에서 효과적임을 보여줍니다.\n3.1 Pre-training Dataset\n이 데이터셋은 다양한 유형의 문서 이미지를 포함하며, 각 이미지에는 텍스트와 레이아웃 정보가 풍부하게 담겨 있습니다. 이를 통해 모델은 문서의 구조와 공간적 배치를 이해하는 방법을 학습하게 됩니다. 데이터셋은 대규모로 구성되어 있어, 모델이 다양한 문서 형태와 스타일을 경험하며 더 강력한 일반화 능력을 개발할 수 있도록 돕습니다.\n3.2 Fine-tuning Dataset\n논문의 3.2절에서는 LayoutLM 모델의 미세 조정에 사용된 데이터셋에 대해 설명합니다. 이 데이터셋은 특정 NLP 태스크에 맞추어 세밀하게 구성되어 있으며, 문서 분류, 정보 추출, 질의응답 등 다양한 문서 처리 작업에 필요한 레이블이 포함되어 있습니다. 미세 조정 과정에서는 이 데이터셋을 활용하여 모델이 사전 학습된 지식을 특정 작업에 적용하고, 실제 성능을 개선하는 방법을 학습합니다. 이를 통해 모델은 실제 응용 프로그램에서의 유용성을 검증받게 됩니다.\n3.3 Document Pre-processing\n이 과정은 문서에서 텍스트와 레이아웃 정보를 추출하고, 이를 모델이 처리할 수 있는 형태로 변환하는 작업을 포함합니다. 텍스트는 OCR(Optical Character Recognition) 기술을 사용하여 디지털 텍스트로 변환되며, 레이아웃 정보는 문서의 각 요소 위치를 나타내는 좌표로 변환됩니다. 이러한 전처리 단계는 모델이 문서의 시각적 및 구조적 정보를 보다 효과적으로 학습하도록 돕습니다.\n3.4 Model Pre-training\n이 과정에서는 대규모 문서 데이터셋을 사용하여 텍스트와 레이아웃 정보의 통합 학습이 이루어집니다. 모델은 문서 내에서 텍스트의 위치와 관련된 컨텍스트를 학습하여, 단순한 텍스트 해석을 넘어서 문서의 구조적 이해도를 높입니다. 이를 통해 모델은 복잡한 문서 이미지에서 정보를 효과적으로 추출하고 이해할 수 있는 기반을 마련합니다.\n3.5 Task-specific Fine-tuning\n여기서는 문서 분류, 정보 추출, 질의응답과 같은 다양한 NLP 태스크를 위한 미세 조정 방법을 구체적으로 다룹니다. 각 태스크에 대한 성능 향상을 위해, 사전 학습된 모델을 해당 태스크의 데이터셋으로 추가 학습시키면서, 텍스트와 레이아웃 정보를 최적화된 방식으로 결합하는 방법이 중점적으로 연구됩니다. 이 과정을 통해 모델은 특정 문서 처리 작업에 보다 효과적으로 적용될 수 있습니다.\n3.6 Results\n실험 결과는 모델이 문서 분류, 정보 추출, 질의응답 등의 다양한 NLP 태스크에서 기존 모델들보다 우수한 성능을 보였음을 보여줍니다. 또한, 텍스트와 레이아웃 정보의 통합적 활용이 모델의 문서 이해 능력을 크게 향상시킨다는 것을 입증합니다. 이러한 결과는 LayoutLM이 실제 응용 프로그램에서도 효과적으로 활용될 수 있음을 시사합니다.\n\n\n4. RELATED WORK\n이 섹션에서는 문서 이미지 분석, 자연어 처리(NLP), 컴퓨터 비전 분야에서의 연구를 중점적으로 다루며, 특히 텍스트와 시각적 정보의 통합을 시도한 다양한 연구들을 검토합니다. 이를 통해 LayoutLM의 혁신적인 접근 방식이 기존의 어떤 문제점들을 해결할 수 있는지, 그리고 어떻게 이전의 연구들을 발전시켰는지를 설명합니다. 이러한 비교 분석은 LayoutLM의 기술적 배경과 연구의 필요성을 강조합니다.\n4.1 Rule-based Approaches\n이러한 방식들은 특정 규칙이나 패턴을 사용하여 문서의 구조를 분석하고, 텍스트와 레이아웃 정보를 분리하는 데 집중합니다. 이 접근법은 구조화되지 않은 데이터를 처리하는 데는 한계가 있지만, 명확한 규칙이 적용될 수 있는 상황에서는 효과적일 수 있습니다. 이 섹션에서는 규칙 기반 방식의 기존 한계와, 이를 해결하기 위해 도입된 머신 러닝 기반 접근법의 필요성을 설명합니다.\n4.2 Machine Learning Approaches\n이러한 접근법은 규칙 기반 방식의 한계를 극복하고자 다양한 데이터에서 패턴을 학습할 수 있는 알고리즘을 사용합니다. 특히, 딥 러닝 모델이 많이 사용되며, 이들은 문서의 텍스트뿐만 아니라 레이아웃과 같은 비정형 데이터를 처리하는 데 뛰어난 성능을 보입니다. 이 섹션에서는 특정 머신 러닝 기법들과 그들이 문서 이미지 처리에서 어떻게 적용되는지를 다룹니다.\n4.3 Deep Learning Approaches\n이러한 접근법은 컨볼루션 신경망(CNN)과 트랜스포머 아키텍처를 포함하여, 복잡한 문서 구조와 다양한 레이아웃의 텍스트를 효과적으로 인식하고 분석하는 데 중점을 둡니다. 이 섹션은 특히 비정형 문서 데이터에서 텍스트와 시각적 요소를 동시에 처리할 수 있는 딥 러닝 모델의 발전에 초점을 맞추며, 이러한 기술이 어떻게 실제 응용 프로그램에 효과적으로 적용될 수 있는지 설명합니다.\n5. CONCLUSION AND FUTURE WORK\nLayoutLM은 문서 이미지 이해 분야에서 텍스트와 레이아웃 정보를 통합하여 사전 학습하는 새로운 접근 방식을 제시하며, 다양한 NLP 태스크에서 우수한 성능을 보였습니다. 또한, 미래 연구에서는 더 다양한 데이터셋과 복잡한 문서 유형에서의 성능 개선을 목표로 하며, 이를 위해 모델의 아키텍처를 더욱 발전시킬 계획입니다. 이러한 노력은 문서 이미지 분석 기술의 발전을 가속화할 것입니다."},"vault/Notion/DB/DB-Blog-Post/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding":{"slug":"vault/Notion/DB/DB-Blog-Post/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding","filePath":"vault/Notion/DB/DB Blog Post/LayoutLM- Pre-training of Text and Layout for Document Image Understanding/LayoutLM- Pre-training of Text and Layout for Document Image Understanding.md","title":"LayoutLM- Pre-training of Text and Layout for Document Image Understanding","links":["vault/Notion/DB/DB-Blog-Post/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding/LayoutLM-Paper-번역/LayoutLM-Paper-번역"],"tags":["ML--and--MLOps"],"content":"\n\n                  \n                  참조 \n                  \n                \n\n\narxiv.org/abs/1912.13318\nwww.kaggle.com/code/ritvik1909/layoutlmv1\nwww.kaggle.com/code/ritvik1909/layoutlmv1-information-extraction\ndsba.korea.ac.kr/seminar/\n\n\n\nLayoutLM Paper 번역\ngithub.com/404Vector/Study.LayoutLMv1.git"},"vault/Notion/DB/DB-Blog-Post/Linux-Command-Collection":{"slug":"vault/Notion/DB/DB-Blog-Post/Linux-Command-Collection","filePath":"vault/Notion/DB/DB Blog Post/Linux Command Collection.md","title":"Linux Command Collection","links":[],"tags":[],"content":"이 문서는 각 명령어를 예제와 함께 간단한 설명과 함께 자주 사용되는 리눅스 명령어의 종합 목록을 제공합니다.\nLinux Command\n\n\n                  \n                  Important\n                  \n                \n\n\n==Index==\n\nLinux Command\n\nChange directory\nPrint working directory\nList\n\n상세보기\n시간순 정렬 상세보기 (최근이 가장 위)\n시간순 역순정렬 상세보기 (최근이 가장 아래)\n시간순 역순정렬 파일명만 + 출력 갯수 제한\n\n\nCopy\n\nCopy file\nCopy directory\n\n\nProcess\n\n실행중인 프로세스를 확인할 때\n실행중인 프로세스를 강제 종료할 때\n백그라운드로 실행하면서 작업 로그 남기기\n\n\nLogging\n\n로그 파일 스트림으로 보기\n\n\nGPU Monitoring\n\nnvidia-smi 로 확인하는 방법 (일정 시간마다)\nwatch 명령어로 일정 주기마다 명령어 실행하기\n\n\n\n\n\n\n\n\n\nChange directory\ncd [targetDir]\nPrint working directory\npwd\nList\n상세보기\nls -al\n시간순 정렬 상세보기 (최근이 가장 위)\nls -alt\n시간순 역순정렬 상세보기 (최근이 가장 아래)\nls -alrt\n시간순 역순정렬 파일명만 + 출력 갯수 제한\nls -1rt | head -COUNT_NUM ## ex) ls -alrt | head -3\nCopy\nCopy file\ncp [source] [target]\nCopy directory\ncp -r [source] [target]\n\nProcess\n실행중인 프로세스를 확인할 때\nps -ef\nps -ef | grep [PID | NAME]\n실행중인 프로세스를 강제 종료할 때\nkill PID\npkill NAME\n백그라운드로 실행하면서 작업 로그 남기기\nnohup COMMAND 1&gt;LOG_FILE_NAME 2&gt;&amp;1 &amp;\n\nLogging\n로그 파일 스트림으로 보기\ntail -f LOG_FILE_NAME\n\nGPU Monitoring\nnvidia-smi 로 확인하는 방법 (일정 시간마다)\nnvidia-smi -i SECOND\nwatch 명령어로 일정 주기마다 명령어 실행하기\nwatch -n SECOND COMMAND\nex)\nwatch -n 1 nvidia-smi\nwatch -n 10 date"},"vault/Notion/DB/DB-Blog-Post/Loss-Function-for-task":{"slug":"vault/Notion/DB/DB-Blog-Post/Loss-Function-for-task","filePath":"vault/Notion/DB/DB Blog Post/Loss Function for task.md","title":"Loss Function for task","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-3"],"tags":[],"content":"\n참조\nLoss Function for task\n\n\n참조\nWeek 3\nLoss Function for task\n손실 함수는 수행하고자 하는 작업 종류에 따라 변합니다.\n\n\nRegression Task\nMSE = {1 \\over N}{\\sum_{i=1}^{N}}{\\sum_{d=1}^{D}}{(y_{i}^{d} - \\hat y_{i}^{d})}^2\n\n\nClassification Task\n\n\nCE = -{1 \\over N}{\\sum_{i=1}^{N}}{\\sum_{d=1}^{D}}{(y_{i}^{d} \\log \\hat y_{i}^{d})}\n\nProbabilistic Task\n\nMLE = {1 \\over N}{\\sum_{i=1}^{N}}{\\sum_{d=1}^{D}}{\\log N( y_{i}^{d} ; \\hat y_{i}^{d},1)} = MSE"},"vault/Notion/DB/DB-Blog-Post/ML-Flow---Tracking-Server-Docker-Image-만들기":{"slug":"vault/Notion/DB/DB-Blog-Post/ML-Flow---Tracking-Server-Docker-Image-만들기","filePath":"vault/Notion/DB/DB Blog Post/ML Flow - Tracking Server Docker Image 만들기.md","title":"ML Flow - Tracking Server Docker Image 만들기","links":[],"tags":[],"content":"ML Flow Tracking Server Image 만들기에 대해\nML Flow Tracking Server Image 만들기는 머신러닝 작업을 관리하기 위해 도커 이미지를 이용하는 과정입니다. 이를 통해 여러 작업의 상태를 쉽게 관리하고 기록할 수 있습니다.\n준비물\nML Flow Tracking Server Image를 만들기 위해 도커를 설치하고, 필요한 라이브러리를 준비해야 합니다. 또한 배포하기 위해 설정 파일들이 필요합니다.\n\nDocker\ngithub.com/404Vector/Docker.MLFlow.TrackingServer\n\n진행 방법\nML Flow Tracking Server Image를 만들기 위해 아래 과정을 진행합니다.\n\n\ngit clone\n위 github 저장소의 main branch를 clone\n\n\nchange directory\nclone한 local 저장소로 이동\n\n\ninput command\ndocker build -t mlflow:2.1.1 . → mlflow:{TAG}이므로 원하면 다른 값 입력\n\n\ninput command\ndocker run --name mlflow -p 15000:5000 -v $(pwd):/mlflow --rm mlflow:2.1.1\n\n\n결론\nML Flow Tracking Server Image 만들기는 도커를 이용하여 머신러닝 작업을 관리할 수 있는 방법입니다. 필요한 도구를 준비하고, 위 진행 과정을 거치면 별도의 서버 이미지를 생성할 수 있습니다."},"vault/Notion/DB/DB-Blog-Post/ML-Flow---기본-사용/ML-Flow---기본-사용":{"slug":"vault/Notion/DB/DB-Blog-Post/ML-Flow---기본-사용/ML-Flow---기본-사용","filePath":"vault/Notion/DB/DB Blog Post/ML Flow - 기본 사용/ML Flow - 기본 사용.md","title":"ML Flow - 기본 사용","links":[],"tags":[],"content":"\nWhat is ML Flow\nML Flow 기본 구조\n\nProject\nLogging, Running\n\nLogging\nExecute\n\n\n\n\nML Flow 사용\n\nlocalhost 단일 사용(Running &amp; Tracking)\nlocalhost(Running) + localhost Docker(Tracking)\nlocalhost(Running) + remotehost(Tracking)\n\n\n\n\n\n\n                  \n                  MLflow - A platform for the machine learning lifecycle \n                  \n                \n\n\nAn open source platform for the machine learning lifecycle MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry.\nmlflow.org\n\n\n\n\n\n                  \n                  mlflow \n                  \n                \n\n\nThe module provides a high-level “fluent” API for starting and managing MLflow runs.\nwww.mlflow.org/docs/latest/python_api/mlflow.html\n\n\n\ngithub.com/404Vector/Docker.MLFlow\n\nWhat is ML Flow\nMLflow는 실험, 재현성, 배포 및 중앙 모델 레지스트리를 포함한 ML 수명 주기를 관리하는 오픈 소스 플랫폼\n\n\nMLflow Tracking\nRecord and query experiments: code, data, config, and results\nRead more\n\n\nMLflow Projects\nPackage data science code in a format to reproduce runs on any platform\nRead more\n\n\nMLflow Models\nDeploy machine learning models in diverse serving environments\nRead more\n\n\nModel Registry\nStore, annotate, discover, and manage models in a central repository\nRead more\n\n\nML Flow 기본 구조\nExperiment &gt; Run &gt; Prameter, Metric, Model, Result Image, …etc\nProject\n\n\nPrameter, Metric, Model, Result Image, …etc\n\nML Flow에 기록하고자 하는 정보들(ex: f1 score, accuracy, miou, … etc)\n\n\nRun\n\npython code(entry point) 1회 실행 시, 모아지는 정보들(위에 prameter, metric)의 집합\n예를 들어, run 하나 하나는 train.py 를 한 번 돌렸을 때 나오는 여러가지 기록들을 가지고 있음\n\n\nExperiment\n\n\nML Flow로 기록하는 정보의 가장 큰 단위\n\n\n같은 종류의 Run들의 집합\n\n\n예를 들어, Mask &amp; Gender &amp; Age 를 각각 분류한다면 Mask Experiment &amp; Gender Experiment &amp; Age Experiment로 구분할 수 있음\n\n\n\n\nLogging, Running\n훈련을 하기 위해서는 python으로 code를 만듬 → train.py\n# train.py\n \nimport argparse\nimport sys\n \nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n \nimport mlflow\n \nfrom mlflow import log_metric, log_param, log_artifacts\n \nif __name__ == &quot;__main__&quot;:\n    X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n    y = np.array([0, 0, 1, 1, 1, 0])\n \n    lr = LogisticRegression(solver=sys.argv[1], penalty=sys.argv[2], l1_ratio=float(sys.argv[3]))\n \n    with mlflow.start_run() as run:\n        lr.fit(X, y)\n \n\t\t    log_param(&quot;solver&quot;, sys.argv[1])\n\t\t    log_param(&quot;penalty&quot;, sys.argv[2])\n\t\t    log_param(&quot;l1_ratio&quot;, sys.argv[3])\n \n\t\t    score = float(lr.score(X, y))\n\t\t    log_metric(&quot;score&quot;, score)\n \n\t\t    print(&quot;Score: %s&quot; % score)\nLogging\nmlflow.start_run() 으로 기록을 시작, mlflow.end_run() 으로 기록을 종료\n\\#---- 방법 1----\nwith mlflow.start_run() as run:\n\t\t#~logging\n \n\\#---- 방법 2----\n \nmlflow.start_run()\n# ~ logging\nmlflow.end_run()\n기록하고 싶은 항목들은 위와 같이 mlflow.log_* 등의 메소드를 통해 코드 내부에서 지정\nExecute\n그럼 저 코드를 mlflow를 통해 실행 및 기록하려면 어떻게 해야 할까?\n이를 위해서는 mlflow에게 어떤 파일을 어떤 파라메터를 넣고 어떤 파일을 실행할지 알려줘야 할 필요가 있음\nml flow는 Dockerfile처럼 ‘MLProject’라는 파일을 통해 이 정보를 기록\n\n\n                  \n                  some-project-folder \n                  \n                \n\n\n\nMLProject # mlflow에게 넘겨줘야 할 정보들이 담겨있는 파일\n\n\ntrain.py # 훈련을 위해 만든 코드\n\n\n\n(예시 파일)\n# MLProject\nname: tutorial\n \nentry_points:\n  main:\n    parameters:\n      solver:\n        type: string\n        default: &quot;saga&quot;\n      penalty:\n        type: string\n        default: &quot;l2&quot;\n      l1_ratio:\n        type: float\n        default: 0.1\n    command: &quot;python train.py {solver} {penalty} {l1_ratio}&quot;\n실행을 위해서는 아래와 같이 어떤 파일이 아닌, MLProject가 들어있는 폴더의 경로를 입력해줌\nmlflow run ./some-project-folder --experiment-name &quot;some-experiment&quot;\n이 때, ‘some-experiment’ 자리에 입력하는 변수가 앞서 언급했던 Experiment를 의미\nML Flow 사용\nlocalhost 단일 사용(Running &amp; Tracking)\n\n\n                  \n                  Important\n                  \n                \n\nlocal에서 사용 시(mlflow run ~), 현재 경로의 [mlruns]폴더를 만들어서 기록을 저장\n따라서 같은 경로에서 tracking server를 (mlflow ui 를 입력)실행\nlocalhost(Running) + localhost Docker(Tracking)\n\n\n                  \n                  localhost(Running) \n                  \n                \n\n\n\n[some working directory]\n— [mlruns]\n\nlocalhost docker(Tracking)\n\n[some working directory] # docker command를 사용해서 local의 경로를 mount\n\n\n\n\ndocker를 이용해서 tracking server를 따로 가져간다고 해도 mount를 통해 working directory를 공유하면 되기 때문에 사용이 크게 다르지 않다\nlocalhost(Running) + remotehost(Tracking)\n\n\n                  \n                  local(Running) \n                  \n                \n\n\n\n[some working directory]\n— [mlruns]\n\nremotehost(Tracking Server)\n\n[some working directory]\n— [mlruns]/mlflow.db\n\n\n\n\n\nmlflow server -h 0.0.0.0 —backend-store-uri sqlite:///mlflow.db\ndailyheumsi.tistory.com/260 읽어보기\n\n성공!\n\n\n\n                  \n                  localhost(Running &amp; Tracking) - [some working directory] — [mlruns]\n                  \n                \n"},"vault/Notion/DB/DB-Blog-Post/MLOps란-무엇일까/MLOps란-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/MLOps란-무엇일까/MLOps란-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/MLOps란 무엇일까/MLOps란 무엇일까.md","title":"MLOps란 무엇일까","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-8/Week-8"],"tags":[],"content":"\n참조\nMLOps란 무엇일까?\nML Ops Component\n\n\n참조\nblogs.nvidia.com/blog/2020/09/03/what-is-mlops/\nen.wikipedia.org/wiki/MLOps\nzzsza.github.io/mlops/2018/12/28/mlops/\nzzsza.github.io/data/2018/01/28/hidden-technical-debt-in-maching-learni\nWeek 8\nMLOps란 무엇일까?\n\nMLOps는 프로덕션에서 기계 학습 모델을 안정적이고 효율적으로 배포하고 유지 관리하는 것을 목표로 하는 일련의 사례이며, 이 단어는 “기계 학습”과 DevOps의 합성어입니다.\nMLOps를 통해 기업이 달성하고자 하는 목표\n\n\n배포 및 자동화\n\n\n모델 및 예측의 재현성\n\n\n진단\n\n\n거버넌스 및 규정 준수\n\n\n확장성\n\n\n협업\n\n\n상업적 사용\n\n\n모니터링 및 관리\n\n\nML Ops Component\n\n\n                  \n                  What is MLOps? - Databricks \n                  \n                \n\n\nBack to glossary MLOps stands for Machine Learning Operations.\nwww.databricks.com/glossary/mlops\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Machine-Learning-Notation(Shan-Hung-Wu)-번역":{"slug":"vault/Notion/DB/DB-Blog-Post/Machine-Learning-Notation(Shan-Hung-Wu)-번역","filePath":"vault/Notion/DB/DB Blog Post/Machine Learning Notation(Shan-Hung Wu) 번역.md","title":"Machine Learning Notation(Shan-Hung Wu) 번역","links":[],"tags":[],"content":"\n참조\n주의사항\nNumbers &amp; Arrays\nIndexing\nFunctions\nCalculus\nLinear Algebra\nProbability &amp; Info. Theory\nMachine Learning\n\n\n참조\nnthu-datalab.github.io/ml/slides/Notation.pdf\n주의사항\n가끔가다 모르는 수식이나 표현이 있었는데, 참조하면 좋겠다고 생각하여 가져와 한글로 정리합니다.\n원문은 Shan-Hung Wu라는 분이 작성하신 자료이며 본인의 수업에서 사용하는 notation을 정리한 자료로 보입니다.\n따라서 아래 기호들의 의미는 절대적인 기준이 아니므로, 이 점 유의하여 참조하여 주시기 바랍니다.\n필요없다고 판단한 부분은 임의로 제외하였으니, 모든 내용을 보고 싶으신 분들은 원본 pdf를 참조하여 주시기 바랍니다.\nNumbers &amp; Arrays\nI_n : n x n 단위 행렬(identity matrix)\nD : diagonal matrix\ndiag(a) : vector a의 요소를 주 대각선 요소로 하는 diagonal matrix\nIndexing\na_i : vector a의 i번째 element(1부터 indexing 했을 때)\na_{-i} : vector a의 i번째 element를 제외한 모든 element\nA_{i,j} : matrix A의 (i, j) element(i : row, j : column)\nFunctions\nf:\\Bbb{A}\\rightarrow \\Bbb{B} : 영역(domain) A와 범위(range) B를 갖는 함수 f\nf \\circ g : 함수 f와 함수 g의 합성 함수\nf(x;\\theta) : \\theta로 매개변수화 된 x의 함수(\\theta는 때때로 생략됩니다)\nf(x,\\theta)와 수학적으로 차이는 없으나, 중요도를 표현한 것입니다.\n위 식에서 theta는 단순히 함수 f(x)를 만들기 위해 사용한 변수이며 관심있는 값은 x라고 표현해 준 것입니다.\n이\n링크를 들어가보시면 더 좋은 설명을 볼 수 있습니다.\n\\ln x : x의 자연로그\n\\sigma(x) : Logistic sigmoid 함수 ={1 \\over 1+e^{-x}}={{e^x}\\over{e^x +1}}\n\\zeta(x) : Softplus 함수 =\\ln(1+\\exp(x))\nRelu 함수에 부드럽게 근사한 함수입니다.\n이 링크를 들어가보시면 더 자세한 설명을 볼 수 있습니다.\n\\|x\\|_p : L^p norm of x\nx^+ : x의 양수 part\n1(x;cond) : indicator 함수, condition이 true이면 x = 1, false이면 x = 0\ng[f;x] : f를 f(x)에 mapping하는 함수\n함수 f를 f(x)로 사용할 수 있도록 mapping해주는 함수입니다.\n예를들어 f가 단일 element에 대한 함수라고 했을 때, vector x에 f를 사용하려면, x의 각 element에 element-wise하게 f를 각각 적용해야 합니다. 여기서 g는 이 mapping을 해주는 함수입니다.\nCalculus\n{\\partial f \\over \\partial x_i}(a) : 입력 a의 요소 x_i에 대한 f의 편미분\n\\R^n \\rightarrow \\R x_i에 대한 편미분이므로 차원이 유지되지 않습니다.\n\\nabla f(a) : 입력 a에 대한 gradient\n\\R^n \\rightarrow \\R 편미분이 사용되므로 차원이 유지되지 않습니다.\nLinear Algebra\nA^T : A의 Transpose matrix\nA^\\dagger : 무어-펜로즈 유사 역행렬(Moore-Penrose pseudo-inverse matrix)\nA\\odot B : A와 B의 Element-wise 연산 = Hadamard product\n\\det(A) : A의 Determinant\ntr(A) : A의 대각 합(trace) = Trace of A\ne^{(i)} : i번째 표준 기저 vector(one-hot vector) = standard basis vector\nProbability &amp; Info. Theory\na \\bot b : 확률 변수 a와 b는 독립\na \\bot b |c : given c에 의해 조건부로 a와 b는 독립\nP_a(a) : 이산 확률 변수 a에 대한 확률 질량 함수\np_a(a) : 연속 확률 변수 a에 대한 확률 밀도 함수\nP(\\theta) : Theta로 Pameterized된 확률 분포\nN(\\mu, \\sigma^2) : 평균이 mu고 표준편차가 sigma인 gaussian 분포\nx\\sim P(\\theta) : 확률변수 x는 분포는 P를 갖는다\nE_{x\\sim P}[f(x)] : P에 대한 f(x)의 기대\nVar[f(x)] : f(x)의 분산\nCov[f(x),g(x)] : f(x)와 g(x)의 공분산\nH(x) : 확률변수 x에 대한 shannon entropy\nD_{KL}(P||Q) : 분포 Q로의 KL Divergence\nMachine Learning\n\\Bbb{X} : Training set\nN : Training set의 크기\n(x^{(i)},y^{(i)}) : 지도학습에서 Training set의 i번째 Sample pair(data &amp; label)\nx^{(i)} : 비지도학습에서 Training set의 i번째 Sample pair(data)\nD : Data의 Dimension\n오역의 우려되어 원문을 같이 적습니다.\n원문 = Dimension of a data point x^{(i)}\nK : Label의 Dimension\n오역의 우려되어 원문을 같이 적습니다.\n원문 = Dimension of a label y^{(i)}\nX \\in \\R^{N\\times D} : Design matrix, X_i는 x^i를 의미합니다.\nP(x,y) : Data 생성 분포\n\\Bbb{F} : 학습할 함수의 가설공간(Hypothesis space)\nC[f] : F에서 f의 비용함수(loss function)\nC(\\theta) : F에서 f로 매개변수화 된 theta의 비용함수(loss function)\n(x&#039;,y&#039;) : testing pair\n\\hat{y} : function f에 의해 예측된 label"},"vault/Notion/DB/DB-Blog-Post/Metric(in-machine-learning)":{"slug":"vault/Notion/DB/DB-Blog-Post/Metric(in-machine-learning)","filePath":"vault/Notion/DB/DB Blog Post/Metric(in machine learning).md","title":"Metric(in machine learning)","links":[],"tags":[],"content":"\n참조\nMetric(in machine learning)\n\n\n참조\nmachine-learning.paperspace.com/wiki/metrics-in-machine-learning\nen.wikipedia.org/wiki/Metric\nMetric(in machine learning)\n\n\n머신러닝에서의 메트릭(metric)은 머신러닝 시스템의 최적화를 위해 우리가 관심을 갖는 숫자를 의미\n\nAccuracy, Loss, Confusion Matrix, AUC, MAE, RMSE, … etc\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Monocular-3D-Object-Detection을-위한-KITTI-Dataset의-이해(작성중)/Monocular-3D-Object-Detection을-위한-KITTI-Dataset의-이해(작성중)":{"slug":"vault/Notion/DB/DB-Blog-Post/Monocular-3D-Object-Detection을-위한-KITTI-Dataset의-이해(작성중)/Monocular-3D-Object-Detection을-위한-KITTI-Dataset의-이해(작성중)","filePath":"vault/Notion/DB/DB Blog Post/Monocular 3D Object Detection을 위한 KITTI Dataset의 이해(작성중)/Monocular 3D Object Detection을 위한 KITTI Dataset의 이해(작성중).md","title":"Monocular 3D Object Detection을 위한 KITTI Dataset의 이해(작성중)","links":[],"tags":[],"content":"\n\n                  \n                  The KITTI Vision Benchmark Suite \n                  \n                \n\n\nWe thank Karlsruhe Institute of Technology (KIT) and Toyota Technological Institute at Chicago (TTI-C) for funding this project and Jan Cech (CTU) and Pablo Fernandez Alcantarilla (UoA) for providing initial results.\nwww.cvlibs.net/datasets/kitti/\n\n\n\n\n개요\nDownload Dataset\nHardware 구성\nKITTI 3D BBox Coordinate\nKITTI Object’s Property\n\nCalibration data\n\n\n\n\n개요\nKITTI Dataset은 단독 카메라와 범위 센서를 사용하여 자율주행 연구를 위해 제공되는 데이터 셋입니다.\nKITTI 데이터 셋은 비디오 캡처, 전방 카메라가 장착된 차량, 라이다로 Scan한 Point Cloud, 그리고 상세한 제공 라벨과 메타데이터를 갖고 있습니다.\nKITTI 데이터 셋은 고해상도 가로 사진과 정교한 인식 및 추적 데이터 셋, 그리고 자율주행 연구에 사용되는 상세한 라벨과 메타데이터를 갖고 있습니다.\nKITTI 데이터 셋의 상세한 라벨과 메타데이터는 3D 객체의 속성, 카메라 및 범위 센서 교정 데이터, 그리고 카메라 매트릭스와 투영 행렬 데이터를 포함하고 있습니다.\n\nKITTI의 홈페이지를 들어가보면 여러가지 Task들이 있고, 그에 따라 다르게 dataset이 주어집니다.\n이 글에서는 Monocular 3D Object를 수행하기 위해 필요한 부분만을 다룹니다.\n\nDownload Dataset\n\n3D Object Detection Task에 들어가보면 위와 같이 데이터를 다운로드할 수 있는 링크를 제공합니다.\n여기서 필요한 항목은 아래와 같습니다.\n\n\nleft Color Images of object data set (12GB)\n\n\ncamera Calibration matrices of object data set (16MB)\n\n\ntraining labels of object data set (5 MB)\n\n\n\nHardware 구성\n\nKITTI는 위와 같이 HW를 구성하고 주행하며 데이터를 취득했습니다.\n라이다 센서(Velodyne, 이하 라이다 센서)과 카메라 센서의 좌표계 축이 서로 다르기 때문에 주의가 필요합니다.\n위 구성을 보면, 카메라들과 센서의 위치가 서로 다른 것을 알 수 있습니다.\n각 센서들은 취득하는 데이터의 종류도 Scale도 좌표계 기준도 모두 다릅니다.\n만약 라이다 센서로 Scan해서 얻은 3D 공간에서 어떤 물체가 있을 때, 이 물체가 카메라 기준에서의 값은 얼마인지 궁금하다면 좌표계 변환이 필요합니다.\nKITTI에서는 아래 논문에서 제시한 방법으로 Camera간의 좌표계 변환과 Camera와 라이다 간의 좌표계 변환 등에 필요한 계수를 추정했습니다.\n→ Automatic Camera and Range Sensor Calibration using a single Shot\nKITTI 3D BBox Coordinate\n\n\n\nx-axis → right (length), y-axis → bottom (height), z-axis → forward (width)\n3B bb corner coordinates in camera coordinate frame, coordinate system is at the bottom center of the box.\n\nKITTI Object’s Property\n\nlabel (str)\n\n차, 보행자 등의 라벨 정보 문자열\n‘Car’, ‘Pedestrian’, …\n\n\ntruncation (float)\n\n이미지상에서 물체가 잘려있는 정도\n0.0 : 전혀 잘리지 않음 ~ 1.0 완전히 잘림\n\n\nocclusion (int)\n\n폐섹 수준(camera 시야 기준으로 추측 됨), 물체가 가려진 정도\n0:전혀 가려지지 않음 | 1:부분적으로 가려짐 | 2:완전히 가려짐 | 3:알 수 없음\n\n\nalpha (float)\n\n관측각, 관측자(자율주행자동차) 기준 물체가 위치한 각도\n-pi:좌측 ~ 0:정면 ~ pi:우측\n\n\nxmin (float)\n\nimage 상에서 물체를 감싸는 2d bbox의 left-x\n\n\nymin (float)\n\nimage 상에서 물체를 감싸는 2d bbox의 top-y\n\n\nxmax (float)\n\nimage 상에서 물체를 감싸는 2d bbox의 right-x\n\n\nymax (float)\n\nimage 상에서 물체를 감싸는 2d bbox의 bottom-y\n\n\nHeight (float) (y)\n\nCamera 좌표계 상에서 물체의 높이(in meters)\n\n\nWidth (float) (z)\n\nCamera 좌표계 상에서 물체의 너비(in meters)\n\n\nLength (float) (x)\n\nCamera 좌표계 상에서 물체의 길이(in meters)\n\n\ntx (float)\n\nCamera 좌표계 상에서 물체의 x(in meters)\n\n\nty (float)\n\nCamera 좌표계 상에서 물체의 y(in meters)\n\n\ntz (float)\n\nCamera 좌표계 상에서 물체의 z(in meters)\n\n\nry (float)\n\n\nCamera 좌표계 상에서 물체의 yaw\n\n\n-pi:좌측 ~ 0:정면 ~ pi:우측\n\n\n\n\nVision meets Robotics: The KITTI Dataset\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.cvlibs.net/publications/Geiger2013IJRR.pdf\n\n\n\nCalibration data\nCalibration data는 촬영 날짜 별로 존재\n\n\ns(i) ∈ N2 : 원본 이미지 크기 (1392×512)\n\n\nK(i) ∈ R3×3 : 교정 매트릭스(unrectified)\n\n\nd(i) ∈ R5 : 왜곡 계수(unrectified)\n\n\nR(i) ∈ R3×3 : 카메라 0에서 카메라 i로 회전\n\n\nt(i) ∈ R1×3 : 카메라 0에서 카메라 i로 변환\n\n\ns(i) ∈ N2 : 정류 후 이미지 크기 rect\n\n\nR(i) rect ∈ R3×3 : 회전 행렬 정류\n\n\nP(i) rect ∈ R3×4 : 정류 후 투영 행렬\n\n\n\n\n                  \n                  KITTI Coordinate Transformations \n                  \n                \n\n\nA guide on how to navigate between different sensor coordinate systems of KITTI.\ntowardsdatascience.com/kitti-coordinate-transformations-125094cd42fb\n\n\n\n\n\n                  \n                  GAC3D: improving monocular 3D object detection with ground-guide model and adaptive convolution \n                  \n                \n\n\nMonocular 3D object detection has recently become prevalent in autonomous driving and navigation applications due to its cost-efficiency and easy-to-embed to existent vehicles.\npeerj.com/articles/cs-686/\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Multi-process-vs-Multi-thread":{"slug":"vault/Notion/DB/DB-Blog-Post/Multi-process-vs-Multi-thread","filePath":"vault/Notion/DB/DB Blog Post/Multi-process vs Multi-thread.md","title":"Multi-process vs Multi-thread","links":[],"tags":[],"content":"\n\n                  \n                  멀티 프로세스(Multi Process)와 멀티 스레드(Multi Thread) \n                  \n                \n\n\n프로세스는 운영체제로부터 자원을 할당받는 작업의 단위이고 스레드는 프로세스가 할당받은 자원을 이용하는 실행의 단위이다.\nwooody92.github.io/os/%EB%A9%80%ED%8B%B0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80-%EB%A9%80%ED%8B%B0-%EC%8A%A4%EB%A0%88%EB%93%9C/\n\n\n\n설명\n\nMulti-process\n\n두개 이상 다수의 프로세서(CPU)가 협력적으로 하나 이상의 작업(Task)을 동시에 처리하는 것\n각 프로세스 간 메모리 구분이 필요하거나 독립된 주소 공간을 가져야 할 경우 사용\n장점\n\n하나의 프로세스가 죽더라도 다른 프로세스에 영향을 주지 않아 안정성이 높음\n\n프로세스 중 하나에 문제가 생겨도 다른 프로세스에 영향을 주지 않아, 작업속도가 느려지는 손해정도는 생기지만 정지되거나 하는 문제는 발생하지 않음\n여러개의 프로세스가 처리되어야 할 때 동일한 데이터를 사용하고, 이러한 데이터를 하나의 디스크에 두고 모든 프로세서(CPU)가 이를 공유하면 비용적으로 저렴\n\n\n\n\n단점\n\n멀티 스레드보다 많은 메모리공간과 CPU 시간을 차지\nContext Switching 과정에서 캐시 메모리 초기화 등 무거운 작업이 진행되고 시간이 소모되는 등 오버헤드가 발생\n\n\n\n\nMulti-thread\n\n하나의 프로세스에 여러 스레드로 자원을 공유하며 작업을 나누어 수행하는 것\n장점\n\n멀티 프로세스보다 적은 메모리 공간을 차지하고 Context Switching이 빠름\n\n프로세스를 생성하여 자원을 할당하는 시스템 콜이 줄어 자원을 효율적으로 관리\n프로세스 내 작업이므로, 캐시 메모리를 비울 필요가 없음\n프로세스 간의 통신보다 스레드 간의 통신 비용이 적음\n\n\n\n\n단점\n\n하나의 스레드 장애로 전체 스레드가 종료 될 위험\n자원을 공유하기에 동기화 문제가 발생할 수 있음\n\n병목현상\n데드락\n\n\n주의 깊은 설계가 필요, 디버깅의 어려움\n\n\n\n\nContext Switching\n\nCPU는 한번에 하나의 프로세스만 실행 가능\nCPU에서 여러 프로세스를 돌아가면서 작업을 처리하는 데, 이 과정이 Context Switching\n프로세스가 대기를 하면서 해당 프로세스의 상태(Context)를 보관하고, 대기하고 있던 다음 순서의 프로세스가 동작하면서 이전에 보관했던 프로세스의 상태를 복구하는 작업\n\n\n"},"vault/Notion/DB/DB-Blog-Post/NAVER-Connect-마스크-착용-상태-분류/NAVER-Connect-마스크-착용-상태-분류":{"slug":"vault/Notion/DB/DB-Blog-Post/NAVER-Connect-마스크-착용-상태-분류/NAVER-Connect-마스크-착용-상태-분류","filePath":"vault/Notion/DB/DB Blog Post/NAVER Connect 마스크 착용 상태 분류/NAVER Connect 마스크 착용 상태 분류.md","title":"NAVER Connect 마스크 착용 상태 분류","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---WrapUp-Report/마스크-착용-상태-분류---WrapUp-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---멘토-피드백","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---개인-회고"],"tags":[],"content":"\n\n                  \n                  GitHub - 404Vector/Competition.AIStages.MaskWearingStatusClassification: 카메라로 촬영한 사람 얼굴 이미지의 마스크 착용 여부, 성별, 나이를 판단하는 Task \n                  \n                \n\n\nAI Stages / Naver Boost Camp AI Tech 4th COVID-19의 확산으로 우리나라는 물론 전 세계 사람들은 경제적, 생산적인 활동에 많은 제약을 가지게 되었습니다.\ngithub.com/404Vector/Competition.AIStages.MaskWearingStatusClassification\n\n\n\n\nTeam member\n요약\n\nMy Contribute\nTask\nEvaluation Metric\nData\n\n\n개요\n본론\n\nEDA\nData\nTest\n\n\n결론\n\n\n==이 대회는 비공개 대회이며, Bootcamp AI Tech 4기 교육의 일환으로 진행되었습니다.==\nTeam member\n\n김형석[github.com/404Vector]\n박시형[github.com/sihyeong671]\n정혁기[github.com/22eming]\n노순빈[github.com/Haz25]\n장국빈[github.com/JKbin]\n\n요약\nMy Contribute\n\n팀 내 최대 Code Contributor\nGithub Repos 구성 및 Baseline Code 작성\nEDA\n\n전체 이미지에 대한 RGB 평균 및 표준편차 시각화\nClass Histogram 시각화\nClass별 평균 이미지, 전체 평균 이미지 시각화\n\n\nModel(Multi-head)\n\nGender 단일 분류 모델 학습, Accuracy 98%\n\n\nModel(Single-head)\n\nViT Model 훈련, 팀 내 Best Score 달성\n\n\nFocal Loss 추가\nSGD ↔ Adam 성능 비교\n\nTask\n\n마스크 상태 Classification, 성별 Classification, 나이 Classification\n\nEvaluation Metric\n\n\nF1 Score\n\n\n\nData\n\n전체 사람 명 수 : 4500\n한사람당사진의개수: 7 (마스크착용5장, 이상하게착용(코스크, 턱스크) 1장, 미착용 1장)\n이미지 크기 : 512 x 384\nClass Distribution : Total 18 Class(마스크 상태 x 성별 x 나이)\n\n마스크 상태 : [Wear, Incorrent, Not Waer]\n성별 : [Male, Female]\n나이 : [x&lt;30, 30≤x&lt;60, 60&lt;x ]\n\n\n\n\n개요\n\n\n                  \n                  Info\n                  \n                \n\n\nstages.ai/competitions/206/overview/description\n\n\n\n\nCOVID-19의 확산으로 우리나라는 물론 전 세계 사람들은 경제적, 생산적인 활동에 많은 제약을 가지게 되었습니다. 우리나라는 COVID-19 확산 방지를 위해 사회적 거리 두기를 단계적으로 시행하는 등의 많은 노력을 하고 있습니다. 과거 높은 사망률을 가진 사스(SARS)나 에볼라(Ebola)와는 달리 COVID-19의 치사율은 오히려 비교적 낮은 편에 속합니다. 그럼에도 불구하고, 이렇게 오랜 기간 동안 우리를 괴롭히고 있는 근본적인 이유는 바로 COVID-19의 강력한 전염력 때문입니다.\n감염자의 입, 호흡기로부터 나오는 비말, 침 등으로 인해 다른 사람에게 쉽게 전파가 될 수 있기 때문에 감염 확산 방지를 위해 무엇보다 중요한 것은 모든 사람이 마스크로 코와 입을 가려서 혹시 모를 감염자로부터의 전파 경로를 원천 차단하는 것입니다. 이를 위해 공공 장소에 있는 사람들은 반드시 마스크를 착용해야 할 필요가 있으며, 무엇 보다도 코와 입을 완전히 가릴 수 있도록 올바르게 착용하는 것이 중요합니다. 하지만 넓은 공공장소에서 모든 사람들의 올바른 마스크 착용 상태를 검사하기 위해서는 추가적인 인적자원이 필요할 것입니다.\n따라서, 우리는 카메라로 비춰진 사람 얼굴 이미지 만으로 이 사람이 마스크를 쓰고 있는지, 쓰지 않았는지, 정확히 쓴 것이 맞는지 자동으로 가려낼 수 있는 시스템이 필요합니다. 이 시스템이 공공장소 입구에 갖춰져 있다면 적은 인적자원으로도 충분히 검사가 가능할 것입니다.\n\n\n본론\nEDA\n\n\n나이 분포 시각화\n\n\n\n\nImage RGB 분포 시각화\n\n\nR - mean: 142.84912575185194, std: 25.110128252411393\nG - mean: 133.64627702635642, std: 24.40858433130365\nB - mean: 127.87051157552729, std: 24.741864071778853\n\n\n\n\n\n최종 Class 분포 시각화\n\n\n\nClass별 평균 이미지시각화, 전체 평균 이미지 시각화\n\n\n\n\nInsight\n\n고령자(60&lt;x)에 대한 class imbalance가 심하다\n.RGB 분포는 의외로 매우 크고, Class에 따른 경향성은 보이지 않는다\n\n\n\nData\n\nData Pre-processing\n\n나이 분류 모델 훈련 시 데이터 샘플링 시도 : 60대 이상의 데이터를 복제하여분포를 맞춤\n\n\n\nTest\n\nMulti-Head Classification(Class별 modeling &amp; training)\n\n\nMask model\n\nFocalLoss\npretrained_ResNext50 → Epoch 10\nF1-score : 0.98\nBest model 찾는 기준을 classification_report를 이용, 여러 F1-score 기준으로 고름\n\n\n\nAge model\n\nData augmentation : 사진에서 보통 사람들이 중앙에 위치하기 때문에center crop을 통해 얼굴에 집중할 수 있도록 함\n나이를 3개의 카테고리로 나눠서 진행\nresnet101, resnext, regnet_x_16f, efficientnet_b3 → Epoch 10~15\nF1Loss, CrossEntropyLossCutMix 적용했으나 성능 저하를 야기\nSGD, ADAM 사용해서 실험\nGriddropout 사용하여 머리카락 등 여러 일반화 성능이 낮아질 가능성을낮추고자 함\n나이 class를 여러 단위로 바꿔보면서 비교\n\n\n\nGender model\n\nPretrained ResNext(224x224)\nNo Augmentation, BCE Loss, ADAM, lr : 0.001, Epoch20\naccuracy : 98%\n\n→ 각 모델을 Assemply한 성능이 너무 낮았음.\nAge Model의 문제로 추정 되지만 정확히 어떤 이유로 성능이 낮은건지 Trace를 하지 못했음\n내부적으로 방법을 찾지 못해 Multi-Head Model을 폐기\n\n\n\nSingle-Head Classification\n\nMask+Age+Gender Model(baseline code)\n\nCase 1\n\nAugmentation=‘BaseAugmentation’, batch_size=64,\ncriterion=‘cross_entropy’, epochs=5, lr=0.001, lr_decay_step=20,\nmodel=‘VIT’, optimizer=‘SGD’, resize=[224, 224], valid_batch_size=1000,\nPretrained=True\nf1 score : 0.7231\naccuracy : 75.0635%\n\n\nCase 2\n\naugmentation=‘BaseAugmentation’, batch_size=64, criterion=‘f1’,\nepochs=10, lr=0.001, lr_decay_step=20, model=‘VIT’, optimizer=‘SGD’,\nresize=[224, 224], valid_batch_size=1000,\nPretrained=True\nf1 score : 0.5916\naccuracy : 71.6825%\n\n\nCase 3\n\nmodel : Pretrained ViT(224x224)\nCriterion : CE, batch_size : 64, optimizer : SGD, Scheduler : StepLR,\nLearining rate : 0.001, Dataset : BaseDataset, Augementation - train,\nevaluation 시 다르게 적용\nF1 score : 0.6974\nAccuracy : 75.3810\n\n\nCase 4(Best)\n\nPretrained ViT(384x384), SGD, Crop((50,50),(334,400))\n==F1 Score : 0.7417==\nAccuracy : 79.7460%\n\n\nensemble\n\n\n제출 한 결과 값들을 Hard voting→유의미한 결과를 얻지 못함\n\n\nvoting에 사용한 모델들의 성능이 너무 낮아서라고 추정\n\n\n\n\n\n\n\n\n\n결론\n\n\n결과\n\n\nPublic - f1 score : 0.7417, accuracy : 79.7460 (12위)\n\n\nprivate - f1 score : 0.7227, accuracy : 78.2857 (14위)\n\n\n\n\n느낀점\n\nTeam\n\n나이,성별,마스크 착용 여부 각각의 task로 나누어 분류하고 결과를 도출하면 더 좋은 성능이 나올 것이라고 기대했으나, 그렇지 못했다.\n코드 자동화와 실험 로그 추적의 필요성을 느꼈다.\n실험 전 사전계획에서 변인통제와 독립변수를미리정해야함을 깨달았다.\n좀 더 체계적이고 지속적으로 결과 및 hyper parameters를 기록해야 되겠다고 느꼈다.\n전체적으로 데이터 분석을 세밀하게 하지 못했던 점과 데이터 불균형을 해결하지 못해 아쉬웠다.\n\n\n개인\n\nGender, Age, Mask를각각분류하는모델을만들고결과를 합치는방식이 좋은결과 를도출할 것이라고 생각했다. 개별적으로 훈련을 진행한 뒤, 잘 예측하는 항목은 고정 하고 잘예측하지 못하는 항목에 집중하면 더 좋을 것이라고 생각했기 때문이다. 하지만 결과는 좋지않았다. 한번에 예측하는 모델이라면 제출해서 정량적으로 판단할 수 있다. 그러나 개별 모델로나누고 이를 합쳐서 제출하면 어떤 모델을 수정해야하는지 알 수 없게 되어버렸다.\n어떤 계획을 세웠을 때, 실제로 실현이 가능한지 면밀이 검토가 필요하다고 느꼈다. 또한 개별분류 방식에서 한번에 분류하는 방식으로 넘어갔을 때, 기록의 중요성을 느꼈다. 개별 분류방식에서 했던 많은 시행착오를 기록해놓지 않았기 때문에, 0부터 다시 시 작하는 기분이었다.그래서 한번에 분류하는 방식으로 훈련할 때는 지속적으로 기록했다.\n성별 분류 모델은 기본적으로 성능이 잘 나와서 크게 새로운 것은 없었다. 한번에 분류 하는모델의 경우, 처음으로 ViT를 사용해서 분류해보았다. ViT는 Res-net, Efficient- net등과는구조가 조금 다르다. 그래서 처음에는 어떻게 끝단을 수정해야 할지 알 수 없어서 직접 모델 내부를 분석해 보았다. 결과, timm 라이브러리를 사용하지 않고torchvision에 있는 ViT의 Head를 직접 수정해서 사용할 수 있었다.\nViT를 사용해 HyperParameter와 ViT Model Type 및Argmentation을 변경해가며 실험했고,이전과 같은 실수를 반복하지 않기 위하여 모두 기록으로 남겼다. 결과, 실험적 으로 성능이올라가는 조건들을 하나씩 찾을 수 있었고, 단일 제출로는 가장 높은 점수 를 얻을 수있었다.\n\n\n\n\n\n관련 문서\n마스크 착용 상태 분류 - WrapUp Report\n마스크 착용 상태 분류 - 멘토 피드백\n마스크 착용 상태 분류 - 개인 회고\n\n"},"vault/Notion/DB/DB-Blog-Post/NAVER-Connect-재활용-품목-분류를-위한-Object-Detection/NAVER-Connect-재활용-품목-분류를-위한-Object-Detection":{"slug":"vault/Notion/DB/DB-Blog-Post/NAVER-Connect-재활용-품목-분류를-위한-Object-Detection/NAVER-Connect-재활용-품목-분류를-위한-Object-Detection","filePath":"vault/Notion/DB/DB Blog Post/NAVER Connect 재활용 품목 분류를 위한 Object Detection/NAVER Connect 재활용 품목 분류를 위한 Object Detection.md","title":"NAVER Connect 재활용 품목 분류를 위한 Object Detection","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---개인-실험-일지/재활용-품목-분류를-위한-Object-Detection---개인-실험-일지","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---개인회고","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---멘토-피드백"],"tags":[],"content":"\n\n                  \n                  GitHub - 404Vector/Competition.AIStages.TrashObjectDetection: 사진에서 Object Detection을 사용하여 쓰레기 객체를 검출하고 분류하는 Task \n                  \n                \n\n\n환경 구성에 필요한 package 설치 mmcv 설치(pipenv로 직접설치 안됨) pipenv install pipenv run mim install mmcv-full [train] python train.\ngithub.com/404Vector/Competition.AIStages.TrashObjectDetection\n\n\n\n\nTeam Member\n요약\n\nMy Contribute\nTask\nEvaluation Metric\nData\n\n\n개요\n본론\n\nEDA\nData\nTest\n\n\n결론\n\n\n==이 대회는 비공개 대회이며, Bootcamp AI Tech 4기 교육의 일환으로 진행되었습니다.==\nTeam Member\n\n김형석 - [github.com/404Vector]\n이동훈 - [github.com/teedihuni]\n전지용 - [github.com/Jiyong-Jeon]\n정원국 - [github.com/jungwonguk]\n한상준 - [github.com/jphan32]\n\n요약\n==My Contribute==\n\n==팀 내 최대 Code Contributor==\n==EDA==\n\n==Category Histogram Analysis 및 시각화==\n==Category RGB Component Analysis 및 시각화==\n==Annotation per Image Analysis 및 시각화==\n==Category Size Analysis 및 시각화==\n\n\n==Model==\n\n==SSD 훈련 및 평가==\n\n\n==Ensemble 기능 개발==\n==조건부 Annoation 삭제 기능 개발==\n==Submission 생성 자동화 기능 개발==\n==Confusion Matrix 생성 자동화 기능 개발==\n\nTask\n\n쓰래기 2D Object Detection\n\nBBox 좌표, Category, Score\n\n\n\nEvaluation Metric\n\nmAP50(Mean Average Precision)\n\n\n\n\nData\n\n\n전체 이미지 개수 : 9754장\n\ntrain : 4883\ntest : 4871\n\n\n\n이미지 크기 : (1024, 1024)\n\n\n10 Class : General trash, Paper, Paper pack, Metal, Glass, Plastic, Styrofoam, Plastic bag, Battery, Clothing\n\n\nAnnotation Format : COCO\n\n\n\n개요\n\n\n                  \n                  Info\n                  \n                \n\n\nstages.ai/competitions/218/overview/description\n\n\n\n\n바야흐로 대량 생산, 대량 소비의 시대. 우리는 많은 물건이 대량으로 생산되고, 소비되는 시대를 살고 있습니다. 하지만 이러한 문화는 ‘쓰레기 대란’, ‘매립지 부족’과 같은 여러 사회 문제를 낳고 있습니다.\n\n분리수거는 이러한 환경 부담을 줄일 수 있는 방법 중 하나입니다. 잘 분리배출 된 쓰레기는 자원으로서 가치를 인정받아 재활용되지만, 잘못 분리배출 되면 그대로 폐기물로 분류되어 매립 또는 소각되기 때문입니다.\n따라서 우리는 사진에서 쓰레기를 Detection 하는 모델을 만들어 이러한 문제점을 해결해보고자 합니다. 문제 해결을 위한 데이터셋으로는 일반 쓰레기, 플라스틱, 종이, 유리 등 10 종류의 쓰레기가 찍힌 사진 데이터셋이 제공됩니다.\n여러분에 의해 만들어진 우수한 성능의 모델은 쓰레기장에 설치되어 정확한 분리수거를 돕거나, 어린아이들의 분리수거 교육 등에 사용될 수 있을 것입니다. 부디 지구를 위기로부터 구해주세요! 🌎\n\n\n본론\nEDA\n\n\nCategory Histogram Analysis\n\n\n\nRGB Component Analysis by Category\n\n\nAverage, Std by Channel for red\n\n\n\n\n\n\nAverage, Std by Channel for green\n\n\n\n\n\n\nAverage, Std by Channel for blue\n\n\n\n\n\n\nRGB Average\n\n\n\nRGB Mean\n\n\n\nRGB Std.\n\n\n\n\n\nAnnotation per Image Analysis\n\n\n\n\n\n\nSize Analysis by Category\nWidth &amp; Height\n\nCategory ID &amp; W/H Ratio\n\n\n\nData\n\nAugmentation\n\n모델이 Overfitting 되는 경향을 파악, 추가되는 Augmentation의 장점과 모델에 가해지는 단점을 전부 생각한 후 적용 유무를 결정\n\nShiftScaleRotate → Overfitting을 막기위해 데이터 증강의 방법으로서 진행\nRandomBrightnessContrast → 이미지 EDA결과 밝기와 대조가 올라가면 이미지 구분이 더 잘 될 것이라는 판단 하에 진행.\nRGBShift → RGB 값을 조정하여 원하는 Object가 더 눈에 띄고 모델이 잘 찾아낼 수 있게 함\n\n다만 값을 크게 줄 경우 오히려 방해가 되므로 적정 한 값을 테스트를 통해 찾아냄\n\n\nHueSaturationValue → HSV 값을 조정하여 원하는 Object가 더 눈에 띄고 모델이 잘 찾아낼 수 있게 함\n\nRGBShift와 마찬가지로 적정 한 값을 테스트를 통해 찾아냄\n\n\nJpegCompression → 이미지 품질을 낮춰서 강선성을 높이고자 함, 품질이 낮아지는 면이 걱정되었지만 단일 이미지에 적용 후 확인해보니 괜찮다고 판단\nChannelShuffle → 무작위로 채널을 섞어줘서 강건성을 높이고자 함\nBlur → Drop out 과 같은 효과를 주어 모델의 강건성을 높이고자 함.\nMedianBlur → Noise를 어느정도 제거하고자 하였음, 다만 화질이 뭉쳐 있는 부작용이 일어날 수 있으나 Task를 수행하는 데는 큰 영향이 없다고 판단\n\n\n\n\n\nTest\n\n개별 최적 모델 탐색\n\nBackbone\n\nSwin 계열, Resnet50, Resnext, Darknet, …\n\n\nModel\n\nRetinaNet, YOLOX, FPN, R-CNN, Cascade R-CNN, Deformable, DETR, SSD…\n\n\n\n\n모델 Top-2\n\nDeformable DETR (Backbone : Resnet 50)\n\n\n모델 구조\n\n\n\n학습 결과\n\n\n\n모델에 대한 고찰\n\n백본으로 Resnet-50을 사용하고, Neck / Head 구조는 Deformable DETR 을 채택하였고, Two-stage 와 One-stage가 모두 가능한 구조여서 본 실험에서는 Two-stage로 실험을 하게 되었다.\nMMDetection에서 Pre-trained model weight 를 제공하고 있어서, weight 를 불러오지 않았을 때에 비하여 모델의 빠른 수렴을 얻을 수 있었다.\nSoft NMS 를 적용함에 따라 최종 추론하는 Boundary Box를 유효하게 늘릴 수 있었고, mAP의 향상이 있었다.\n\n\n\n\nFaster RCNN (Backbone : Swin-S)\n\n\n모델 구조\n\n\n\n학습 결과\n\n\n\n모델에 대한 고찰\n\n가장 좋은 결과를 내고 있었던 Faster-RCNN 모델에 백본을 Swin Transformer S 로 변경하여 학습을 진행하였고, Resnet-50 과 비교하여 Overfitting이 지연되고, 학습 셋의 특징을 더 잘 수렴할 수 있었다.\nMMDetection에서 Pre-trained model weight 를 제공하고 있어서, weight 를 불러오지 않았을 때에 비하여 모델의 빠른 수렴을 얻을 수 있었다.\nSoft NMS 를 적용함에 따라 최종 추론하는 Boundary Box를 유효하게 늘릴 수 있었고, mAP의 향상이 있었다.\n\n\n\n\n\n\nOverlapping Object Detection Strategy\n\nSoft NMS\n\n기존 NMS 방식을 Soft NMS 방식으로 바꿔 Overlapping Object를 탐지 할 수 있도록 변경\n\n\nConfidence Score\n\nConfidence thresold를 낮춰 많은 BBox를 검출하여 Overlapping Object를 검사할 수 있게 추론\n\n\nNMS IoU thresold\n\nIoU thresold를 낮춰 가장 점수가 높은 Bbox와 근처의 Bbox까지 더 찾아질 수 있도록 변경\n\n\n\n\nEnsemble\n\n\nEnsemble Method\n\nNMS(None Maximum Suppression) 사용\n\n\n\n\nEnsemble Parameter : IOU Threshold\n\nIOU가 해당 Threshold이상인 것만 Ensemble 수행\nParameter Search : 이진 탐색 기법으로 접근, Submission을 통해 최적 Threahold 추정\n\n\nIOU 0.5 → IOU 0.9 → IOU 0.7 → IOU 0.6 → IOU 0.55(Best)\n\n\n\n\n\n\n\n\n\n결론\n\n\n결과\n\n\nPublic 0.5897 → Private 0.5739 (19위)\n\n\n\nDeformable DETR (R50) 과 Faster-RCNN (Swin-S), Faster-RCNN (800x600, soft_nms,treshold 0.1 ) 모델을 IoU Threshold 0.55 로 NMS 하여 앙상블한 결과를 제출한 것이 최종 제출에서 가장 좋은 결과를 얻게 되었다.\n\n\n이는, Neck이 트랜스포머인 경우, 백본이 트랜스포머인 경우 등 서로 다른 특성을 가지는 모델을 앙상블 하였기에 다양성을 이끌어 낼 수 있었으며, mAP 를 계산하는데 있어 Precision을 높을 수 있었다고 평가하고 있다.\n\n\n\n\n느낀점\n\nTeam\n\nBackbone 만 Pretrained 모델을 사용한 것 보다 전체 Architecture의 Pretrained 모델을 가져오는 것이 더 학습에 효과적이였다.\nBackbone 만이 아닌 Neck, head 부분도 Task에 적합한 Pretrained 모델을 사용했다면 더 좋은 결과를 얻을 수 있을 것이라 생각된다.\nmAP가 높아도 결과 이미지 시각화 시 원하는 모습을 보이지 않았다.\n\n실생활에 사용할 모델을 학습할 때는 사용 목적과 의미에 따라 평가 지표를 더 정확하게 사용해야 함을 알게되었다.\n\n\n외부 경진대회 참가로 인하여 일주일 정도 늦은 출발을 하게되었는데, 실험 횟수의 부족이 최종 결과 도출에서도 큰 악영향으로 다가왔다.\n대회 후반부에 들어선 후, 오르지 않는 성능과 타 팀과의 리더보드 랭킹 격차, 부족한 시간으로 체계적인 실험을 진행하는데 큰 부담이 되었다.\n하루도 허투루 쓰지 않고 최대한 많은 실험을 진행하여 온전히 대회에 집중하는것이 필요하다고 느꼈다.\n본 대회처럼 실험 관리를 위하여 WandB 등 을 적극적으로 활용해야 할 것이라고 생각하며, 제공되는 기본 기능 이외에도 우리가 수집해야 할 Metric 이 있다면 추가하거나 이미지도 수집하는 등 커스텀도 필요하다고 느꼈다.\n\n\n개인\n\n한 모델에 매몰되지 말고, 더욱 다양한 실험이 필요하다고 느꼈다.\n\nSSD라는 모델로 많은 실험을 했으나, 태생적인 한계로 결과가 좋지 못했다.\n\nSSD는 paperswithcode 기준으로 성능이 별로 좋은 모델이 아니다\n\nmAP50 : 48.5\n\n\n그런데 이건 생각하지 못하고 모델이 낼 수 있는 성능한계에 가까워졌다는 생각을 하지 못하고 SSD의 성능 향상에 계속 매몰되어 있었다\n\nBest mAP50 : 0.4538\n\n\n\n\n\n\n실험을 통해 Hyperparameter를 튜닝하는 것은 필요하지만, 시간을 효율적으로 사용하기 위해서는 0에서부터 하려고 하지 말고 모델의 논문 등을 참조하여 레퍼런스 얻어야 한다고 느꼈다.\n막히는 부분을 찾다보면 다시 부트캠프의 강의를 돌아보게 되는 일이 많았고, 부트캠프에서 가르쳐주는 기본기에 더 충실해져야겠다고 생각했다.\n여러 EDA를 해보려고 노력했지만, 그 결과에서 Insight을 도출해 내는 작업에는 소흘했다.\nConfusion Matrix를 그려놓고 막상 적극적으로 사용하지 않았다.\n이전 Mask Classification 대회에서 아쉬웠던 부분을 이번 프로젝트에서의 내 목표로 삼았고, 리더보드 상위권을 제외한 모든 목표를 이루었다.\n\n그러나 이 목표 중, ‘라벨링을 맹신하지 않기’는 역으로 성능 향상을 위한 사고 실험에 방해되었다.\n\n\n마지막에 Ensemble 자동화 코드를 만들어 팀의 성적을 최대한 끌어낼 수 있었다.\n대회라는 것을 망각하고 ‘올바른 예측’을 고민했다.\n우리는 라벨링을 다시 하는 단계에서 ‘어떤 기준으로 라벨링을 다시 해야 옳을까?’를 고민했다. 그러나 이것은 실무가 아닌 대회이며, 우리가 해야 할 일은 ‘옳은 예측’을 하는 것이 아니라 주최측이 준비해 놓은 정답에 최대한 근접하는 예측을 할 수 있는 모델을 만드는 것이었다.\n\n\n\n\n\n관련문서\n재활용 품목 분류를 위한 Object Detection - Wrapup Report\n재활용 품목 분류를 위한 Object Detection - 개인 실험 일지\n재활용 품목 분류를 위한 Object Detection - 개인회고\n재활용 품목 분류를 위한 Object Detection - 멘토 피드백\n\n"},"vault/Notion/DB/DB-Blog-Post/NAVER-Connect-재활용-품목-분류를-위한-Semantic-Segmentation/NAVER-Connect-재활용-품목-분류를-위한-Semantic-Segmentation":{"slug":"vault/Notion/DB/DB-Blog-Post/NAVER-Connect-재활용-품목-분류를-위한-Semantic-Segmentation/NAVER-Connect-재활용-품목-분류를-위한-Semantic-Segmentation","filePath":"vault/Notion/DB/DB Blog Post/NAVER Connect 재활용 품목 분류를 위한 Semantic Segmentation/NAVER Connect 재활용 품목 분류를 위한 Semantic Segmentation.md","title":"NAVER Connect 재활용 품목 분류를 위한 Semantic Segmentation","links":["vault/Notion/DB/DB-Blog-Post/mIoU(Mean-Intersection-over-Union)","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---개인회고","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---멘토-피드백"],"tags":[],"content":"\n\n                  \n                  Info\n                  \n                \n\n\ngithub.com/404Vector/Competition.AIStages.TrashSemanticSegmentation\n\n\n\n\nTeam Member\n요약\n\nMy Contribute\nTask\nEvaluation Metric\nData\n\n\n개요\n본론\n\nEDA\nTest\n\n\n결론\n\n\n==이 대회는 비공개 대회이며, Bootcamp AI Tech 4기 교육의 일환으로 진행되었습니다.==\nTeam Member\n\n김형석 - [github.com/404Vector]\n이동훈 - [github.com/teedihuni]\n전지용 - [github.com/Jiyong-Jeon]\n정원국 - [github.com/jungwonguk]\n한상준 - [github.com/jphan32]\n\n요약\n==My Contribute==\n\n==팀 내 최대 Code Contributor==\n==SMP Baseline code 개발==\n==SMP WandB 연동==\n==SMP Scheduler 적용==\n==SMP AMP 적용==\n==SMP 실험결과 Notion API로 자동 기록 기능 개발==\n==SMP Focal Loss + Label Smoothing 구현==\n==SMP Model Test==\n==SMP SwinTv2 encoder 구현==\n==SMP UpperNet decode 구현==\n==Hard voting Ensemble 구현==\n\nTask\n\n이미지 내 쓰래기에 대한 Semantic Segmentation Task\n\nEvaluation Metric\n\nmIoU(Mean Intersection over Union)\n\nData\n\n\n이미지 크기 : (512, 512)\n\n\nCOCO Format\n\n\nTrain : 3,272\n\n\nTest : unknown\n\n\n\n개요\n\n\n                  \n                  Info\n                  \n                \n\n\nstages.ai/competitions/227/overview/description\n\n\n\n\n바야흐로 대량 생산, 대량 소비의 시대. 우리는 많은 물건이 대량으로 생산되고, 소비되는 시대를 살고 있습니다. 하지만 이러한 문화는 ‘쓰레기 대란’, ‘매립지 부족’과 같은 여러 사회 문제를 낳고 있습니다.\n\n분리수거는 이러한 환경 부담을 줄일 수 있는 방법 중 하나입니다. 잘 분리배출 된 쓰레기는 자원으로서 가치를 인정받아 재활용되지만, 잘못 분리배출 되면 그대로 폐기물로 분류되어 매립 또는 소각되기 때문입니다.\n따라서 우리는 사진에서 쓰레기를 Segmentation하는 모델을 만들어 이러한 문제점을 해결해보고자 합니다. 문제 해결을 위한 데이터셋으로는 배경, 일반 쓰레기, 플라스틱, 종이, 유리 등 11 종류의 쓰레기가 찍힌 사진 데이터셋이 제공됩니다.\n여러분에 의해 만들어진 우수한 성능의 모델은 쓰레기장에 설치되어 정확한 분리수거를 돕거나, 어린아이들의 분리수거 교육 등에 사용될 수 있을 것입니다. 부디 지구를 위기로부터 구해주세요! 🌎\n\n\n본론\nEDA\n\n\nData Imbalance 문제\n\n\n카테고리 별 annotation 분포\n\n\npaper, plastic bag, plastic, General trash에 비해 Styrofoam, Paper pack, Glass, Metal, Clothing, Battery의 annotation 수가 적음\n\n\n\n\n\nAnnotation 특징\n\n\n카테고리 별 Annotation 크기\n\n\n픽셀 단위의 아주 작은 Annotation 다수 존재\n사진의 크기와 유사한 매우 큰 Object 존재\nClothing, Battery의 경우 크기가 안정되어있지만 Styrofoam, Paper pack, Glass, Metal의 경우 크기마저 불안정한 모습을 보임\n\n\n\n카테고리 별 Annotation 위치 및 크기 비교\n\n\nobject는 대부분 중앙에 크게 존재하고 가장자리로 갈수록 크기가 작아짐\n크기가 작은 Annotation 중 이미지의 가장자리에 속한 것들은 물체가 잘린 것으로 추정\n\n\n\n\n\n데이터 시각화\n\n\n데이터 셋 분석\n\n라벨링 기준이 모호하다.\n\n일반 쓰레기의 기준, 박스와 종이, 테이프 기준\n얼마나 세부적으로 라벨링을 할 것인지\n한 물체에 포함된 물체에 대한 기준 (ex. 페트병에 감싸진 비닐)\n\n\n투명한 이미지\n\n병, 페트병, 비닐 등 투명한 이미지에 대한 오류가 심할 것 같음\n\n\nSmall Object\n\n사람 눈으로도 인식이 되지 않는 작은 물체가 존재\n\n\n라벨링 오류\n\n데이터 라벨링이 잘못 되어있거나 심한 경우 라벨링이 되어있지 않은 경우도 존재\n\n\n\n\n\n\n\n결론\n\n데이터 클린징 작업은 의미가 없을 것 이라 추정\n\nTest 데이터도 비슷하다고 가정\n\n\nData Imbalance 문제를 해결하기 위한 기법 연구 필요\nSegmentation 문제에서 물체의 모양을 잘 맞추는 것 보다 물체의 클래스를 잘 맞추는 것이 우선이 되어야 함\n라벨링 오류와 같은 Noise에 대한 해결 방안이 필요\n\n\n\nTest\n\nModel Search\n\n\nSMP와 MMSegmentation Baseline Code를 작성하고 모델을 찾음\n\n\nSMP (Segmentation Models Pytorch)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitectureBackboneValidation ScoreTest Score특이사항UNetEfficientb20.49890.4844Color Aug 사용UNetEfficientb20.62440.5452Color Aug 미사용UNetEfficientb30.63560.5715Color Aug 미사용UNetEfficientb40.63350.5698Color Aug 미사용MAnetResnext500.5495--PSPNetEfficientb30.4296--LinknetEfficientb30.3661--Deeplabv3Resnet1010.5831--FPNResnet1010.5962--FPNEfficientb30.65470.5995-FPNEfficientb3(Timm)0.62860.6073-FPNSwinTv20.2856-가중치 미사용FPNSwinTv2 Crs2240.5757--FPNSwinTv2 w24i3840.7130.6212-UpperNetSwinTv2 w24i3840.7083--PANSwinTtiny0.7290.68PANSwinTtiny0.720.66\n==→ Backbone의 경우 Swin Transfomer v2가 가장 우수했고, Segmentation Model의 경우 PAN이 가장 우수한 성능을 보였다.==\n\n\nMMSegmentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitectureBackboneValidation ScoreTest Score특이사항SegformerMIT-b00.6158-UperNetSwin_Tiny0.6217-PointRendResNet1010.6394-Architecture 전체 Pretrained 가중치KNet+UperNetSwin_Large0.750.7009UperNetSwin_Large0.73690.711Augmenation 적용PointRendSwin_Large0.750.7118Segmentorvit_tiny0.66매우 빠른 학습시간Segmentorvit_small0.690.60UperNetbeit0.13매우 긴 학습시간\n==→ Backbone의 경우 Swin Transfomer Large가 가장 우수했고, UperNet 모델과 PointRend의 장단점이 확실히 나뉘어졌다.==\n\n\n\nAugmentation\n\nNoise Augmentation\n\nNoise의 경우 파라미터가 적고 학습이 쉬운 Efficient model의 경우 큰 의미가 없었다.\n그러나 파라미터가 크고 많은 feature를 보게 되는 경우 학습을 지연 시키면서 더 많은 부분을 학습하게 만들어 큰 효과를 이루었다.\n\n\n\n\nLoss Function\n\nDiceCE Loss\n\nDice Loss\n\n데이터 불균형 특징이 존재하는 semantic segmentation network 구조에 많이 사용되는 손실 함수\n데이터 특성 상 Background가 많이 존재하며 데이터 불균형이 심하게 보이므로 Dice Loss 사용\n\n\nCross Entropy loss와 Dice loss를 가중치를 두고 혼합하여 DiceCE Loss 구현\n\nCross Entropy loss (0.75) + Dice loss (0.25)\nCross Entropy loss를 활용하여 정확도를 향상시키고 Dice loss를 활용하여 Class Imbalance 문제를 해결\n\n\n\n\n\n\nSweep\n\nlr , batch, optimizer, Augmentation\n\nSweep을 통해 파라미터 튜닝을 진행하여 성능 향상을 이루었다. miou 0.58 → 0.61\n\n\n\n\nModel Ensemble\n\nMMSegmentation Model Ensemble\n\nMMSegmenation으로 학습된 모델에 대한 앙상블 기법\n앙상블 할 모델을 모두 추론하여 픽셀 별 Confidence score가 높은 Class를 선택\n\n\nHard voting Ensemble\n\nInference를 통해 생성된 csv파일을 Hard voting 방식으로 Ensemble, 동률의 투표 결과가 나왔다면 Submission Score가 높은 모델의 결과를 반영\n결과\n\n==[0.7118]SwinL&amp;Pointrend + [0.7057]SwinL&amp;Upernet + [0.6800]SwinTtiny&amp;PAN → 0.7278(Best)==\n[0.7093]SwinL&amp;Pointrend + [0.7110]SwinL&amp;Upernet + [0.6800]SwinTtiny&amp;PAN → 0.7247\n\n\n\n\n\n\n\n\n결론\n\n\n결과\n\npublic mIoU : 0.7278 → private mIoU : 0.7185\nAugmentaion이나 Hyperparameter Tuning등의 기법보다 성능이 좋은 SOTA Model를 찾는 것이 더 좋은 결과를 얻었다\nEnsemble의 경우, 여러 Model의 결과를 Ensemble한 것이 미소하지만 더 좋은 결과를 얻었다.\n\n\n\n느낀점\n\nSOTA Model을 선정하는 것이 가장 큰 성능 향상 요소였는데, 다른 실험에 몰입하여 많은 실험을 진행하지 못했다.\nSegmentation Models PyTorch와 MMSegmentation 이외에도 Detectron2, PaddleSeg 또는 커스텀 모델들을 더 적극적으로 활용하여 SOTA 모델을 포함한 많은 모델을 실험해보았어야 했지만 이런 부분에서 협업이 원활하지 못하였다.\n프로젝트 진행을 원활하게 하기 위해 PM과 팀원의 역할을 명확히 정의하고, 협업 방식을 개선하여야 하겠고, Github Flow 등 협업에 관련한 컨벤션을 정하여 팀원 모두가 적극적으로 참여할 수 있도록 해야하겠다.\n모델의 학습 결과를 평가함에 있어서 논리 근거를 충분히 하기 위하여, 각 실험에 관련한 논문을 읽고, 다양한 가설을 세워 어떤 문제 때문에 학습이 잘 되지 않는지 실험하여 근거와 결과를 바탕으로 의사 결정을 하는 자세를 지향해야 하겠다.\n\n\n\n관련문서\n재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report\n재활용 품목 분류를 위한 Semantic Segmentation - 개인회고\n재활용 품목 분류를 위한 Semantic Segmentation - 멘토 피드백\n\n"},"vault/Notion/DB/DB-Blog-Post/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선":{"slug":"vault/Notion/DB/DB-Blog-Post/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선","filePath":"vault/Notion/DB/DB Blog Post/NAVER Connect 학습 데이터 추가 및 수정을 통한 이미지 속 글자 검출 성능 개선/NAVER Connect 학습 데이터 추가 및 수정을 통한 이미지 속 글자 검출 성능 개선.md","title":"NAVER Connect 학습 데이터 추가 및 수정을 통한 이미지 속 글자 검출 성능 개선","links":["vault/Notion/DB/DB-Blog-Post/DetEval","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/데이터-제작-프로젝트---Wrapup-Report/데이터-제작-프로젝트---Wrapup-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/데이터-제작-프로젝트---발표자료/데이터-제작-프로젝트---발표자료"],"tags":[],"content":"\n\n                  \n                  GitHub - 404Vector/Competition.AIStages.WordAreaDetection: 이미지에서 Object Detection을 사용하여 글자 영역을 검출하는 Task \n                  \n                \n\n\nYou can’t perform that action at this time.\ngithub.com/404Vector/Competition.AIStages.WordAreaDetection\n\n\n\n\nTeam Member\n요약\n\nMy Contribute\nTask\nEvaluation Metric\nData\n\n\n개요\n본론\n\nEDA\nData\nTest\n\n\n결론\n\n\n==이 대회는 비공개 대회이며, Bootcamp AI Tech 4기 교육의 일환으로 진행되었습니다.==\nTeam Member\n\n김형석 - [github.com/404Vector]\n이동훈 - [github.com/teedihuni]\n전지용 - [github.com/Jiyong-Jeon]\n정원국 - [github.com/jungwonguk]\n한상준 - [github.com/jphan32]\n\n요약\n==My Contribute==\n\n==팀 내 최대 Code Contributor==\n==Dataset Polygon to rotated rect 변환 기능 개발==\n==Raw Image Caching 최적화==\n==Geometric Augmentation 기능 개발==\n==Geometric Augmentation 실험==\n==Input Size 실험==\n\nTask\n\n이미지 상에 있는 단어 영역을 Detection\n\n어떤 단어인지는 검출하지 않음\n\n\n일반 대회들과 다르게 모델을 변경하지 않고 Data Augmentation과 추가 Dataset으로만 성능을 향상시켜야 함\n\nEvaluation Metric\n\n\nDetEval\nDetEval\n\n\nData\n\nTrain Dataset\n\nICDAR17_Korean 사용(UFO Format)\n\nICDAR17-MLT 데이터셋에서 언어가 한글인 샘플들만 모아서 재구성한 Dataset\n총 536개\n\n\n\n\nTest Dataset\n\n\nWeb에서 Crolling된 이미지(손글씨, 간판, 책표지 등)\n\n총 300개\n\n\n\n이미지 내 단어 수 평균 : 17.5개\n\n\n이미지 내 단어 수 최대 : 약 400개\n\n\n가로쓰기 글자, 세로쓰기 글자, 진행방향이 불규칙한 글자, 휘어진 글자 등 존재\n\n\n테스트셋은 대회 참가자에게 별도로 노출되지 않음\n\n\n\n\n\n개요\n\n\n                  \n                  Info\n                  \n                \n\n\nstages.ai/competitions/224/overview/description\n\n\n\n\n스마트폰으로 카드를 결제하거나, 카메라로 카드를 인식할 경우 자동으로 카드 번호가 입력되는 경우가 있습니다. 또 주차장에 들어가면 차량 번호가 자동으로 인식되는 경우도 흔히 있습니다. 이처럼 OCR (Optimal Character Recognition) 기술은 사람이 직접 쓰거나 이미지 속에 있는 문자를 얻은 다음 이를 컴퓨터가 인식할 수 있도록 하는 기술로, 컴퓨터 비전 분야에서 현재 널리 쓰이는 대표적인 기술 중 하나입니다.\n\n(출처 : 위키피디아)\nOCR task는 글자 검출 (text detection), 글자 인식 (text recognition), 정렬기 (Serializer) 등의 모듈로 이루어져 있습니다. 본 대회는 아래와 같은 특징과 제약 사항이 있습니다.\n\n본 대회에서는 ‘글자 검출’ task 만을 해결하게 됩니다.\n예측 csv 파일 제출 (Evaluation) 방식이 아닌 model checkpoint 와 inference.py 를 제출하여 채점하는 방식입니다. (Inference) 상세 제출 방법은 AI Stages 가이드 문서를 참고해 주세요!\n대회 기간과 task 난이도를 고려하여 코드 작성에 제약사항이 있습니다. 상세 내용은 베이스라인 코드 탭 하단의 설명을 참고해주세요**.**\nInput : 글자가 포함된 전체 이미지\nOutput : bbox 좌표가 포함된 UFO Format (상세 제출 포맷은 평가 방법 탭 및 강의 5강 참조)\n\n\n\n본론\nEDA\n\n\nICDAR19(MLT, LSVT, ArT)\n\n\n곡선과 원형 패턴의 글자 이미지들이 많이 들어가 있음\nImage size (2000, 2000) ~ (4000, 4000) 사이의 다양한 사이즈로 구성\nICDAR19 - MLT\n\n학습 시간을 고려하여 Korean only 최종 채택 (약 1,000장)\n\n\nICDAR19 - LSVT\n\n학습 시간을 고려하여 사용하지 않음\n\n\nICDAR19 - ArT\n\n캠퍼 제작 데이터셋과 가장 유사한 Annotation rule\n학습 시간을 고려하여 사용하지 않음\n\n\n\n\n\nCamper Dataset\n\n\nAnnotation 실습으로 Camper들이 직접 제작한 Dataset\nAnnotation이 깔끔한 좋은 데이터가 있는 반면 어노테이션이 불안정한 데이터도 포함\nAnnotation 보정 없이 그대로 전체 사용\n\n일부 실험에서 제외하였을 경우도 평가함\n본 대회의 annotation 기준과 가장 유사하다고 판단\n\n\n\n\n\nHierText\n\nCVPR 2022 / Towards End-to-End Unified Scene Text Detection and Layout Analysis @Google Research 에서 발표\nScene Text Detection Task 와 Layout Analysis 을 통합하기 위한 모델과 관련한 논문과 데이터셋을 공개\n특징\n\n\n이미지당 평균 100단어 이상\n\n\n현재 가장 밀도가 높은 TextOCR(2021년)보다 두 배 더 밀도가 높음\n\n\n\n\n\n\n\nData\n\n\nJPEG 이미지 메타정보(exif) Orientation 변환\n\n\n\n다각형 Annotation → Rect 형식으로 변환\n\n\n\nTest\n\n\nAugmentation\n\n다양한 실험을 기반으로 학습 시 최적의 Augmentation을 찾음\nGeometric Augmentation\n\nRotate : ±30º\nRandom Crop : 20% ~ 100%\n\n\nChannel Augmentation\n\nColor Jitter\n성능 하락으로 인해 사용하지 않음\n\nGauss Noise, ISO, Channel Shuffle, CHALE, ImageCompression 등\n\n\n\n\nInput Augmentation Visualization\n\nInput 이미지를 시각화 함으로써 각 Augmentation 적용 시 어떤 효과가 있는지 눈으로 확인\n\n\n\n\n\n\nModel Ensemble\n\n\nEnsemble Method\n\n\nNMS(None Maximum Suppression) 사용\n\n\n\n각 모델에서 나온 추론 결과를 모아 NMS를 진행하여 최종 Output 결정\n\n\n\nOutput Visualization\n\n\n모델 추론 결과를 시각화하여 모델이 어떤 것들을 잘 예측하고 어떤 문제가 있는지 파악\n\n\n이를 활용하여 다음 스텝을 계획하거나 앙상블에 필요한 모델의 장점을 파악\n\n\n\n\n\nMulti Scale Inference\n\n입력 이미지 크기에 따라 성능이 달라짐\n장단점을 확인하고 입력 이미지의 크기를 다르게 하여 Multi Scale Inference 진행\n\n\n\nModel Ensemble\n\n여러 실험 결과의 장점을 살리고자 앙상블 진행\n\n\n\n\n\nEnsemble Flow Diagram\n\n\n\n\n\n\n결론\n\n\n결과\n\n\npublic f1: 0.6901 → private f1: 0.7967, 1위\n\n\n\n\n\n느낀점\n\nAI 모델 학습에서 데이터가 얼만큼 중요한지 경험을 통해 알게 되었다.\n강건한 모델은 좋은 데이터로부터 만들 수 있다고 느꼈다.\n\nHierText dataset을 통해 일반화 성능을 높인것이 Public rank 6위에서 Private rank 1위를 할 수 있게 만들어 주었다고 판단\n\n\n많은 데이터보다도, 잘 구성된 웰메이든 데이터셋이 더 큰 성능을 보였다.\n\n다른 캠퍼들에 비해 적은 데이터를 사용하였지만, 더 높은 성능을 얻었다.\n\n\n\n\n\n관련 문서\n데이터 제작 프로젝트 - Wrapup Report\n데이터 제작 프로젝트 - 발표자료\n\n"},"vault/Notion/DB/DB-Blog-Post/NMS(Non-Maximum-Suppression)":{"slug":"vault/Notion/DB/DB-Blog-Post/NMS(Non-Maximum-Suppression)","filePath":"vault/Notion/DB/DB Blog Post/NMS(Non-Maximum Suppression).md","title":"NMS(Non-Maximum Suppression)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-9/Week-9"],"tags":[],"content":"\n참조\nNMS(Non-Maximum Suppression)\n\n\n참조\nwikidocs.net/142645\nWeek 9\nNMS(Non-Maximum Suppression)\n\n모든 Bounding box는 자신이 해당 객체를 얼마나 잘 잡아내는지 나타내는 Confidence Score를 가짐\nThreshold 이하의 confidence score를 가지는 Bounding Box는 제거\n남은 Bounding Box들을 Confidence score 기준 모두 내림차순 정렬\n맨 앞에 있는 Bounding box 하나를 기준으로 잡고, 다른 bounding box와 IoU 값을 계산\nIoU가 threshold 이상인 Bounding box들은 제거(같은 물체를 검출하고 있다고 판단)\n해당 과정을 순차적으로 시행, 모든 Bounding Box를 비교 및 제거\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Boostcamp-AI-Tech-4기-후기/Boostcamp-AI-Tech-4기-후기":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Boostcamp-AI-Tech-4기-후기/Boostcamp-AI-Tech-4기-후기","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Boostcamp AI Tech 4기 후기/Boostcamp AI Tech 4기 후기.md","title":"Boostcamp AI Tech 4기 후기","links":[],"tags":[],"content":"\nBoostcamp AI Tech는 무엇인가요?\n\n어떤걸 배우나요?\n팀은 어떻게 나눠지나요?\n각 대회는 어떻게 진행되나요?\n최종 프로젝트는 무엇인가요?\n또 무엇이 있나요?\n이 교육의 장단점은 무엇인가요?\n\n\nBoostcamp AI Tech에서 무엇을 얻었나요?\n글을 마치며…\n\n\n\n\n                  \n                  본 글을 AI Tech 4기 교육 내용을 기준으로 작성되었으며, 다른 기수와 다를 수 있습니다. \n                  \n                \n\nBoostcamp AI Tech는 무엇인가요?\n\n\n                  \n                  부스트캠프 \n                  \n                \n\n\n개발자의 지속 가능한 성장을 위한 학습 커뮤니티\nboostcamp.connect.or.kr/program_ai.html\n\n\n\n\n“ AI 기술과 함께하는 미래를 향한 여정 ”\n\nBoostcamp AI Tech는 Naver Connect 재단에서 진행하는 프로그램으로 약 5개월동안 몰입하여 AI 기초부터 심화과정, 그리고 Product Serving까지 학습하는 End to End 교육입니다.\n어떤걸 배우나요?\n교육이 시작되면 기본적으로 5명으로 이루어진 팀 단위로 학습을 진행합니다.\n\nAI 기초 과정에서는 AI를 학습하는데 필요한 기본적인 수학지식(확률과 통계, 미적분, 선형대수학)을 학습합니다.\nAI 심화과정에서는 CV, NLP, RecSys 3개의 Track으로 나뉘고 각 Track에 따라 특화된 AI 교육을 받게 됩니다.\nProduct Serving 과정에서는 Streamlit, FastAPI, BentoML, AirFlow, MLFlow 등 훈련한 모델을 웹 기반으로 유저가 사용할 수 있게 하거나 스케줄러를 통해 훈련을 자동화 할 수 있게 하는 툴 등을 학습합니다.\n\nCV Track의 경우 대략적으로 아래 순서로 교육이 진행됬습니다.\n\nAI 기초\n데이터 시각화\nCV 이론\nCV 대회 - Image Classification\nCV 대회 - Object Detection\nCV 대회 - OCR\nCV 대회 - Semantic Segmantation\nProduct serving\n최종 프로젝트\n\n팀은 어떻게 나눠지나요?\n처음 시작할 때는 운영진 분들이 팀을 배정합니다.(CV는 CV 별로, NLP는 NLP별로 배정됩니다)\n그러나 AI 심화과정 도중에 원하는 사람들과 팀 결성할 수 있는 기회가 한 번 있습니다.\n\n팀 결성을 한 뒤에는 마지막(최종 프로젝트)까지 팀원들과 같이 가게 됩니다.\n팀 결성 이전에 자기소개 및 PR할 수 있는 기회가 있습니다.\n최종 프로젝트는 각자가 하고싶은 것이 매우 다를 수 있으므로 잘 고민하셨다가 자기소개 및 PR에서 본인이 하고 싶은 방향을 기재하시고, 뜻을 함께하는 팀원들을 찾는 것을 추천합니다.\n\n각 대회는 어떻게 진행되나요?\n\nTrack별로 별도의 대회를 진행합니다.(CV Track과 NLP Track은 서로 다른 대회를 진행합니다)\n부스트캠프 참가자들끼리만 경쟁합니다.(내부 대회)\n대회 중에는 AI Stages에서 V100(!)이 장착된 서버를 원격으로 대여해주며, 참가자는 ssh 또는jupyter를 기반으로 서버에 접근하여 모델을 훈련시키고 경쟁합니다.\n대회는 기본적으로 baseline 코드를 제공합니다. 이것을 그대로 사용할 지, 새롭게 코딩할 지는 자유입니다.\n대회마다 GitHub Repository를 제공받으며, 해당 Repository를 사용하여 팀원들과 협업하여 대회를 진행합니다.\n\n최종 프로젝트는 무엇인가요?\n최종 프로젝트는 두 종류로 나뉩니다. 첫 번째는 기본형이고 두 번째는 기업연계형입니다.\n\n대부분의 팀들은 기본형 프로젝트를 진행하며, 배운 내용들을 토대로 직접 데이터를 수집하고 모델을 훈련한 뒤 유저가 사용할 수 있도록 Serving하는 작업을 진행합니다.\n\n직접 주제를 정해서 원하는 작업을 하는 것은 좋지만, 어떤 주제를 해야할지 막막할 수 있습니다.\n\n\n‘기업 연계’ 프로젝트의 경우 기업과 연계해서 기업이 원하는 주제와 내용으로 프로젝트를 진행합니다.\n\n주제 선정에 대한 고민은 없어지지만, 결과물이 본인이 원하는것과 매우 다를 수 있습니다.\n\n\n\n또 무엇이 있나요?\n교육 말고도 중간중간 교육생을 위한 여러 프로그램들이 준비되어 있었습니다.\n(아래 프로그램이 전부는 아니며, 제가 인상깊게 생각한 것들을 리스트업 했습니다.)\n\n멘토링\n\n팀별로 멘토 한 사람이 배정됩니다.\n멘토는 AI 업계 종사자입니다.\n멘토링은 기본적으로 매 주 1회 진행되며 내용은 멘토님마다 다릅니다.\n\n궁금한 점, 알고 싶은 점에 대한 질의응답\n논문 리뷰\n실무 관련 경험 및 알아야 할 기술 공유\n\n\n\n\n두런두런\n\nAI 관련 직군들은 어떤 일들을 하는지\n고민상담, 이력서 피드백\n면접에서 준비해야 할 것들\n\n\n네트워킹데이\n\n기업 인사담당자 혹은 실무자와의 면담 기회\n\n\n\n이 교육의 장단점은 무엇인가요?\n개인적으로 생각하는 Boostcamp AI Tech의 장단점은 아래와 같습니다.\n\n장점\n\nTrack별로 특화된 AI 기술을 익힐 수 있습니다.\n기초부터 심화, 그리고 Product serving까지 end to end를 교육받을 수 있습니다.\n같은 목표를 가진 팀원들과의 함께하며 프로젝트를 통해 협업 능력을 기를 수 있습니다.\nAI 업계 종사자들에게 멘토링을 받을 수 있습니다.\nTrack별 대회 참여를 통해 배운 것들을 실습하며 빠르게 실력을 늘릴 수 있습니다.\n파트너 기업들과의 취업 연계 과정이 있습니다. 다만, 모두가 취업연계에 성공하는 것은 아니기 때문에 맹신하지 말아야 합니다. 또한 연계가 되더라고 정규직이 아닌 인턴일 수도 있습니다.\n\n\n단점\n\n힘듭니다. 교육과정을 순수하게 다 소화하려면 정말 열심히 노력해야 합니다.\n5개월의 긴 기간동안 수강할 수 있게 금전적으로, 시간적으로 환경을 확보해야 합니다.\n스스로 5개월 동안 아침부터 저녁까지 공부할 수 있어야 합니다.\n취업이 된다고 장담할 수 없습니다. 좋은 정보들과 기회를 제공해주지만 결국 스스로의 몫이며, 당연하지만 파트너 기업들도 ‘부캠 출신’을 뽑고 싶은게 아닌 ‘잘하는 사람’을 뽑고싶어합니다.\nTrack별 대회 내용이 기수마다 같아서 수 많은 팀들이 등수를 높이기 위해 이전 기수들의 대회 내용을 찾아보며 후반이 될 수록 심해집니다. 하지만 이렇게 등수를 높여봤자 남는게 없습니다. 대회마다 목표를 잡고 등수에 얾매이지 말고 개인의 성장에 초점을 맞추는 것을 추천합니다.\n\n\n\nBoostcamp AI Tech에서 무엇을 얻었나요?\n\n\n‘기초’를 잘 쌓았다고 느꼈습니다.\n수료 후의 저는 AI와 관련하여 새롭게 공부해야 하는 것이 있다면, 스스로 자료를 찾아보고 공부해나갈 수 있습니다.\n\n\nAI Engineer로 나아갈 수 있다는 자신감을 얻었습니다.\n저는 비전공자로, AI에 대해서 아무것도 모르고 파이썬도 잘 하지 못하는 상태로 시작했습니다.\n그러나 교육 마친 뒤에는 스스로 모델 훈련을 위한 baseline 코드를 직접 만들낼 수 있게 되었고, pre trained model을 불러와 사용하는 것은 물론 모델을 수정할 수 도 있게 되었습니다.\n최종 프로젝트를 통해 Model Serving을 경험해보았고, 간단한 Task라면 직접 훈련시켜 Model을 Web으로 Serving 할 수 있다는 자신감을 얻게 되었습니다.\n\n\nAI 라는 기술과 많이 친숙해졌습니다.\n여러 대회와 프로젝트를 거치면서 AI 관련 작업들이 생소함에서 친숙함으로 변했습니다. 모델 훈련도, 데이터 분석도, 구현을 위한 코딩도 더 이상 생소하지 않고 친숙해졌습니다.\n\n\n‘커뮤니티’를 얻었습니다.\n오랜 기간 함께한 팀원들과 수료 후 1기부터 4기까지 모든 캠퍼들이 소속되어 있는 커뮤니티 까지, 정보와 의견을 교류할 수 있으며 프로젝트나 대회 참여를 하고 싶다면 이야기를 나눠볼 수 있는 값진 커뮤니티를 얻었습니다.\n\n\n글을 마치며…\n\n\n정말 빠르게 지나간 5개월\n\n처음 시작할 때는 _‘5개월이나 교육을 받아야 한다니……’_라는 생각으로 시작했습니다. 그러나 끝날 때는 _‘5개월이 벌써 지나갔다니……’_라는 생각이 들었습니다.\n\n\n기존의 CV 지식이 큰 도움이 되지 않음\n저는 머신비전 업계에서 일했던 경험이 있었기에 ‘이미지’라는 데이터에 친숙한 상태였습니다. 그러나 기존에 제가 가지고 있던 지식(Rule-Base 기술)들은 Deep Learning을 이용한 Computer Vision을 이해하는 것에 별다른 도움이 되지 않았습니다.\n\n\n기존 지식이 도움이 안되었기에, 오히려 가치있게 느껴진 교육\n하지만 그렇기 때문에 캠프에 참여하길 잘 했다는 생각이 들었습니다. 사회, 그리고 산업 전반에 AI 기술이 들어오고 있는 시점에서 기존의 Rule-Base 기술들만 고집하며 살아가기에는 AI 기술은 너무 빠르게 발전하고 있다고 느껴집니다. 그런데 이런 상황에서 기존의 기술로 기존의 직무에 머무르려고 했다면, 어느순간 AI에게 대체되었을 것이라고 느껴졌습니다.\n\n\nAI 대회에 참가하면서 느낀 기존 실무자 vs AI Engineer에 대한 생각\n예전에 머신비젼 실무를 진행하면서, ‘기존 도메인 지식이 매우 중요하기 때문에, 회사 입장에서 AI Engineer를 새로 고용하는 것 보다 기존 실무자에게 AI를 배우게 할 수 있지 않을까?’라는 생각을 한 적이 있습니다.\n실제로, 도메인 지식은 AI 기술을 적용하기 위해 넘어야 할 큰 벽이라고 생각합니다. 하지만 캠프를 수료하고 나서 기존의 실무자들이 자신의 도메인으로 진입하는 AI Engineer들을 막을 수 있는 벽은 아니라고 생각하기 됬습니다.\n회사의 입장에서, 기존의 실무자에게 AI를 교육시켜서 ‘AI 기술’을 도메인 지식과 실무에 연결해줄 사람을 키우는 것은 분명 가치있는 일이라고 생각합니다. 하지만 기존의 모든 실무자들이 이것을 담당할 필요가 없고 그 일부만이 필요하며, 나머지는 대체될 것이라고 느꼈습니다. AI 팀이 조직에 자리잡고 나면, 그 다음부터는 새롭게 AI Engineer를 영입하는 것이 더 높은 성과를 기대할 수 있다고 생각합니다.\n이것은 AI 대회에 참가하면서 느끼게 되었는데, 저를 포함한 대회 참가자들은 대회측이 제시한 Baseline 코드를 기반으로 AI 기술들을 사용하여 모델의 성능을 높이는 경쟁을 합니다. 하지만, 경쟁에서 도메인 지식이 크게 작용하지 않는다고 느꼈습니다.\n데이콘, 캐글에 나오는 대회 주제들을 보면서 이러한 생각은 더욱 확고해졌습니다.\n\n\n힘들지만, 그래도 보람찼던 교육\nBoostcamp AI Tech 교육과정을 순수하게 다 소화하려면 많은 노력이 필요합니다. 그래도 이 과정을 통해 AI와 관련하여 많은 것을 배우고, AI라는 기술에 친숙해질 수 있습니다.\n수료를 한다고 무조건 취업이 된다고 장담할 수 없지만, 좋은 정보들과 기회를 제공해주므로 스스로의 노력과 끈기가 있다면 AI 엔지니어로 나아갈 수 있는 길을 열어줄 것입니다.\n\n\n만약 참여한다면, 적극적으로..\nBoostcamp AI Tech를 만약 참여하게 된다면, 여러 활동들을 적극적으로 참여해 보는 것을 추천합니다. 뿌듯함을 느끼는 것은 물론, 소소한 보상이 기다릴 수도 있습니다.\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Naver-Connect---Boostcamp-AI-Tech-4기":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Naver-Connect---Boostcamp-AI-Tech-4기","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Naver Connect - Boostcamp AI Tech 4기.md","title":"Naver Connect - Boostcamp AI Tech 4기","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Boostcamp-AI-Tech-4기-후기/Boostcamp-AI-Tech-4기-후기","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/초보-운전자를-위한-안전-주행-보조-시스템---개인-회고","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/초보-운전자를-위한-안전-주행-보조-시스템---Wrapup-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-18/Week-18","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-17","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-2/팀-프로젝트-개인-아이디어-정리-2","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-1/팀-프로젝트-개인-아이디어-정리-1","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---멘토-피드백","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---개인회고","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14/Week-14","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/데이터-제작-프로젝트---발표자료/데이터-제작-프로젝트---발표자료","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/데이터-제작-프로젝트---Wrapup-Report/데이터-제작-프로젝트---Wrapup-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-12/Week-12","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---멘토-피드백","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---개인회고","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---개인-실험-일지/재활용-품목-분류를-위한-Object-Detection---개인-실험-일지","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-9/Week-9","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-8/Week-8","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-6/Week-6","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-5/Week-5","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---멘토-피드백","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---개인-회고","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---WrapUp-Report/마스크-착용-상태-분류---WrapUp-Report","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-3","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\nLevel - 3\nLevel - 2\nLevel - 1\n\n\nBoostcamp AI Tech 4기 후기\nLevel - 3\n\n초보 운전자를 위한 안전 주행 보조 시스템 - 개인 회고\n초보 운전자를 위한 안전 주행 보조 시스템 - Wrapup Report\n🚫 Week 21 Skip! Wrapup Report로 대체\n🚫 Week 20 Skip! Wrapup Report로 대체\n🚫 Week 19 Skip! Wrapup Report로 대체\nWeek 18\nWeek 17\n팀 프로젝트 개인 아이디어 정리 2\n팀 프로젝트 개인 아이디어 정리 1\nLevel - 2\n재활용 품목 분류를 위한 Semantic Segmentation - 멘토 피드백\n재활용 품목 분류를 위한 Semantic Segmentation - 개인회고\n재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report\n🚫 Week 16 Skip! Wrapup Report로 대체\n🚫 Week 15 Skip! Wrapup Report로 대체\nWeek 14\n데이터 제작 프로젝트 - 발표자료\n데이터 제작 프로젝트 - Wrapup Report\nMemo : 데이터 제작 프로젝트는 개인회고와 멘토 피드백이 생략됨\n🚫 Week 13 Skip! Wrapup Report로 대체\nWeek 12\n재활용 품목 분류를 위한 Object Detection - 멘토 피드백\n재활용 품목 분류를 위한 Object Detection - 개인회고\n재활용 품목 분류를 위한 Object Detection - Wrapup Report\n재활용 품목 분류를 위한 Object Detection - 개인 실험 일지\nWeek 11\nWeek 10\nWeek 9\nLevel - 1\nWeek 8\nWeek 7\nWeek 6\nWeek 5\nWeek 4\n마스크 착용 상태 분류 - 멘토 피드백\n마스크 착용 상태 분류 - 개인 회고\n마스크 착용 상태 분류 - WrapUp Report\nWeek 3\nWeek 2\nWeek 1"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 1.md","title":"Week 1","links":[],"tags":[],"content":"\nNorm\nL1 Norm\nL2 Norm\n선형회귀의 목적식\n이산형 확률변수와 연속형 확률변수\n조건부 확률\n기대값\n범함수(functional)\n샘플링(Samiling)\n몬테카를로 방법\n마르코프 연쇄 몬테카를로(MCMC, Markov Chain Monte Carlo) 🛑 이해low..\n\n\nNorm\n크기의 일반화로 벡터의 길이 혹은 크기를 측정하는 방법(함수)\nL1 Norm\nTaxicab Norm 혹은 맨허튼 노름(Manhattan norm)으로 불림\n벡터 요소에 대한 절대값의 합을 의미\n|+|-3|+|4|+|5|) = 15$$\n\n### L2 Norm\n\n$$x=[-1,2,3]\\\\||x||_2 = sqrt((-1)^2+(2)^2+(3)^2)$$\n\n### 선형회귀의 목적식\n\n입력 데이터 $X = [x_1,x_2,..,x_n]$과 가중치 $B=[b_1,..., b_n]$ 그리고 출력 데이터 $Y=[y_1, ..., y_n]$가 있을 때, $y_t \\simeq x_t*b_t$가 되도록 행렬 B를 최적화해보자.\n\n예측값 $r_t=x_tb_t$일 때, 실제 값과의 차이는 아래와 같다.\n\n$$L = \\sqrt{{1\\over n}\\sum_{t=1}^{n}||y_t - r_t||_2}$$\n\n$$L(B) =L(b_1,b_2,...,b_n)= \\sqrt{{1\\over n}\\sum_{t=1}^{n}||y_t - x_tb_t||_2}$$\n\n$${L}={({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))^{0.5}}$$\n\nL(loss)를 기준으로 얼마나 가중치모델 B를 통해 도출한 예측 값 R이 실제 값 Y에 근접하는지의 척도를 알 수 있다.\n\nL은 R에 대한 함수이므로 위 함수에 B를 대입하면 함수 L에 대한 결과 값도 변화한다.\n\n따라서 L 대하여 편미분 예측 값 R로 편미분을 해서 나온 기울기를 통해 L값이 낮아지는 방향으로 B를 업데이트 할 수 있다.\n\n  \n\n우선 R로 L을 편미분 해보자.\n\n$${\\delta L \\over \\delta R}={1 \\over 2}({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))\\prime{({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))^{-0.5}}\\\\={1 \\over 2}{({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))\\prime \\over \\sqrt{({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))}}\\\\={1 \\over {2L}}{({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))}\\prime\\\\={1 \\over {2nL}}{(-2(y_1-r_1)+...+-2(y_n-r_n))}\\\\={-{1 \\over {nL}}}{((y_1-r_1)+...+(y_n-r_n))}$$\n\n  \n\nR로 먼저 편미분을 한 이유는 B는 R의 함수이고 R은 L의 함수이기 때문이다.\n\n$${\\delta r_t \\over \\delta b_t} = x_t$$\n\n$${\\delta l_t \\over \\delta b_t}={\\delta l_t \\over \\delta r_t}{\\delta r_t \\over \\delta b_t}$$\n\n$${\\delta L \\over \\delta B} = {\\delta L \\over \\delta R}{\\delta R \\over \\delta B}\\\\={-{1 \\over nL}}{((y_1-r_1)x_1+...+(y_n-r_n)x_n)}$$\n\n### 이산형 확률변수와 **연속형 확률변수**\n\n&gt; [!info]  \n&gt; \n&gt;  \n&gt; [namu.wiki/w/%ED%99%95%EB%A5%A0%EB%B3%80%EC%88%98](namu.wiki/w/%ED%99%95%EB%A5%A0%EB%B3%80%EC%88%98)  \n\n- $P(X = x)$ : 확률변수가 x 값을 가질 확률\n- 이산형 확률변수(discrete random variable)\n    - 확률 변수 X가 취할 수 있는 모든 값을 x1, x2, x3, ... 처럼 셀 수 있을 때 X를 이산확률변수라고 한다.\n\n$$P(X \\in A) =  \n{\\sum_{x \\in A}}P(X = x)$$\n\n- 연속형 확률변수(continuous random variable)\n    - 적절한 구간 내의 모든 값을 취하는 확률 변수이다.\n    - 연속적인 범위의 값을 지니는 확률변수. 예를 들어, &#039;핸드폰으로 나무위키를 보는 사람의 수&#039;는 셀 수 있으므로 이산확률변수이나, &#039;핸드폰으로 나무위키를 보는 사람이 일요일에 나무위키를 본 시간&#039;은 셀 수 없으므로 연속확률변수이다.\n\n$$P(X \\in A) =  \n{\\int_{A}}P(x)dx$$\n\n### 조건부 확률\n\n- 조건부확률 $P(y|x)$ : 입력변수 x에 대해 정답이 y일 확률\n\n  \n\n### 기대값\n\n- 데이터를 대표하는 통계량\n- 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용\n    - 분산, 첨도, 공분산 등 여러 통계량 계산에 사용\n\n  \n\n### 범함수(functional)\n\n&gt; [!info]  \n&gt; \n&gt;  \n&gt; [namu.wiki/w/%EB%B2%94%ED%95%A8%EC%88%98](namu.wiki/w/%EB%B2%94%ED%95%A8%EC%88%98)  \n\n- 함수를 입력받아 스칼라(즉 수 하나)를 내어놓는 함수\n\n  \n\n### 샘플링(Samiling)\n\n&gt; [!info] 샘플링 - 위키백과, 우리 모두의 백과사전  \n&gt; \n&gt; 샘플링(sampling)은 어떤 자료에서 일부 값을 추출하는 것을 의미한다.  \n&gt; [ko.wikipedia.org/wiki/%EC%83%98%ED%94%8C%EB%A7%81](ko.wikipedia.org/wiki/%EC%83%98%ED%94%8C%EB%A7%81)  \n\n- 어떤 자료에서 일부 값을 추출하는 것\n\n  \n\n### 몬테카를로 방법\n\n&gt; [!info] 몬테카를로 방법 - 위키백과, 우리 모두의 백과사전  \n&gt; \n&gt; 목차 토글 몬테카를로 방법(Monte Carlo method) (또는 몬테카를로 실험)은 반복된 무작위 추출(repeated random sampling)을 이용하여 함수의 값을 수리적으로 근사하는 알고리즘을 부르는 용어이다.  \n&gt; [ko.wikipedia.org/wiki/%EB%AA%AC%ED%85%8C%EC%B9%B4%EB%A5%BC%EB%A1%9C_%EB%B0%A9%EB%B2%95](ko.wikipedia.org/wiki/%EB%AA%AC%ED%85%8C%EC%B9%B4%EB%A5%BC%EB%A1%9C_%EB%B0%A9%EB%B2%95)  \n\n- 반복된 무작위 추출(repeated random sampling)을 이용하여 함수의 값을 수리적으로 근사하는 알고리즘\n- 대체적으로, 몬테카를로 방법은 확률론적 해석을 가진 문제를 해결하기 위해 사용될 수 있음\n- 대수의 법칙에 의해 어떤 확률 변수의 기댓값으로 설명되는 적분은 랜덤표본(ramdom sample)의 표본 평균을 취함으로써 근사치를 구할 수 있음\n\n![[Untitled 47.png|Untitled 47.png]]\n\n### 마르코프 연쇄 몬테카를로(MCMC, **Markov Chain Monte Carlo)** 🛑 이해low..\n\n&gt; [!info] 마르코프 연쇄 몬테카를로 - 위키백과, 우리 모두의 백과사전  \n&gt; \n&gt; (무작위 행보 몬테 카를로 방법 포함)은 마르코프 연쇄의 구성에 기반한 확률 분포로부터 원하는 분포의 정적 분포를 갖는 표본을 추출하는 알고리즘의 한 부류이다.  \n&gt; [ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EC%97%B0%EC%87%84_%EB%AA%AC%ED%85%8C%EC%B9%B4%EB%A5%BC%EB%A1%9C](ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EC%97%B0%EC%87%84_%EB%AA%AC%ED%85%8C%EC%B9%B4%EB%A5%BC%EB%A1%9C)  \n\n&gt; [!info] Markov Chain Monte Carlo 샘플링의 마법  \n&gt; \n&gt; 이번 포스트에서는 강력한 샘플링 기법 중 하나인 Markov Chain Monte Carlo(MCMC)에 대해 알아보겠습니다.  \n&gt; [www.secmem.org/blog/2019/01/11/mcmc/](www.secmem.org/blog/2019/01/11/mcmc/)  \n\n&gt; [!info] 마르코프 연쇄 - 위키백과, 우리 모두의 백과사전  \n&gt; \n&gt; 마르코프 연쇄는 시간에 따른 계의 상태의 변화를 나타낸다.  \n&gt; [ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EC%97%B0%EC%87%84](ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EC%97%B0%EC%87%84)  \n\n- 마르코프 연쇄의 구성에 기반한 확률 분포로부터 원하는 분포의 정적분포를 갖는 표본을 추출하는 알고리즘의 한 부류\n- 어떤 함수 $f(x)$ 를 계산할 수 있고 확률분포 $p(x)$를 샘플링 할 수 있을 때, 아래와 같이 적분 결과를 근사시킬 수 있다.\n\n$${\\int f(x)p(x)dx} \\approx {\\sum_{i=1}^{n}{f(X_i) \\over N}}$$\n\n- $p(x)$가 없어진 이유\n    - $X_i$는 표본이며, 표본은 확률 분포를 반영한다. 그러므로 $p(x)$를 곱한 것과 같은 효과가 나타난다.\n- Markov Chain\n    \n    - MCMC는 표본만을 이용하지만 필요한 만큼 많이 뽑게 된다면 현재까지 뽑은 전체 표본이 확률 분포를 거의 모방한다. 다만, 이 결과를 보장하기 위해 Markov Chain을 사용\n    - 확률론에서 마르코프 연쇄는 이산 시간 확률 과정을 의미\n    - 시간에 따른 계의 상태의 변화를 나타냄\n    - 마르코프 성질을 가지고 있음\n        - 과거와 현재의 상태가 주어졌을 때, 미래 상태의 조건부 확률 분포가 과거 상태와는 독립적으로 현재 상태에 의해서만 결정됨\n    \n      \n    \n    ### 모집단(population)\n    \n    &gt; [!info]  \n    &gt;  \n    &gt; [namu.wiki/w/%ED%86%B5%EA%B3%84%ED%95%99?from=%EB%AA%A8%EC%A7%91%EB%8B%A8#s-2](namu.wiki/w/%ED%86%B5%EA%B3%84%ED%95%99?from=%EB%AA%A8%EC%A7%91%EB%8B%A8#s-2)  \n    \n    - 관측 대상이 되는 전체 집단\n    \n    ### 표본(sample)\n    \n    &gt; [!info]  \n    &gt;  \n    &gt; [namu.wiki/w/%ED%86%B5%EA%B3%84%ED%95%99?from=%EB%AA%A8%EC%A7%91%EB%8B%A8#s-2](namu.wiki/w/%ED%86%B5%EA%B3%84%ED%95%99?from=%EB%AA%A8%EC%A7%91%EB%8B%A8#s-2)  \n    \n    - 모 집단에서 일부만 조사한 것\n    \n    ### 모 평균(population mean, μ)\n    \n    &gt; [!info]  \n    &gt;  \n    &gt; [namu.wiki/w/%ED%86%B5%EA%B3%84%ED%95%99?from=%EB%AA%A8%EC%A7%91%EB%8B%A8#s-2](namu.wiki/w/%ED%86%B5%EA%B3%84%ED%95%99?from=%EB%AA%A8%EC%A7%91%EB%8B%A8#s-2)  \n    \n    - 모 집단의 평균, 확률 변수의 기댓값\n    \n    ### 기댓값(expectation)\n    \n    &gt; [!info]  \n    &gt;  \n    &gt; [namu.wiki/w/%EA%B8%B0%EB%8C%93%EA%B0%92](namu.wiki/w/%EA%B8%B0%EB%8C%93%EA%B0%92)  \n    \n    - 어떤 확률 과정을 무한히 반복했을 때, 얻을 수 있는 값의 평균으로써 기대할 수 있는 값\n    \n    ### 모수\n    \n    &gt; [!info]  \n    &gt;  \n    &gt; [namu.wiki/w/%EB%AA%A8%EC%88%98](namu.wiki/w/%EB%AA%A8%EC%88%98)  \n    \n    - 모집단(population)의 수\n    - 통계학에서 모 평균, 모 표준편차, 모 분산 등 모집단의 데이터를 의미\n    - 통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표\n    - 그러나 유한한 개수의 데이터로 정확한 분포를 구할 수는 없음\n    - 따라서 근사적으로 확률분포 추정 필요\n        - ex) 정규분포, 베르누이분포,… etc\n    \n    ### 모수적(parametric) 방법론\n    \n    - 유한한 개수의 데이터를 특정 확률 분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법론\n    \n    ### Sample Distribution vs Sampling Distribution🛑 이해low.\n    \n    &gt; [!info] 표본 분포 - 위키백과, 우리 모두의 백과사전  \n    &gt; 표본 분포(sampling distribution 또는 finite-sample distribution) 또는 표집분포는 크기 n의 확률 표본(random sample)의 확률 변수(random variable)의 분포(distribution)이다.  \n    &gt; [ko.wikipedia.org/wiki/%ED%91%9C%EB%B3%B8_%EB%B6%84%ED%8F%AC](ko.wikipedia.org/wiki/%ED%91%9C%EB%B3%B8_%EB%B6%84%ED%8F%AC)  \n    \n    &gt; [!info] 표본분포와 표집분포 (Sampling distribution vs. Sample distribution)  \n    &gt; 표본분포와  \n    &gt; [velog.io/@regista/%ED%91%9C%EB%B3%B8%EB%B6%84%ED%8F%AC%EC%99%80-%ED%91%9C%EC%A7%91%EB%B6%84%ED%8F%AC-Sampling-distribution-vs.-Sample-distribution](velog.io/@regista/%ED%91%9C%EB%B3%B8%EB%B6%84%ED%8F%AC%EC%99%80-%ED%91%9C%EC%A7%91%EB%B6%84%ED%8F%AC-Sampling-distribution-vs.-Sample-distribution)  \n    \n    - 표본분포(sample distribution)\n        - 확률표본(확률표본, random sample)의 함수\n    - 표집분포(sampling distribution)\n        - 표본통계량이 이론적으로 따르는 확률분포\n    \n      \n    \n    ### RNN 내부에서 sigmoid 또는 tanh를 쓰는 이유\n    \n    &gt; [!info] 호다닥 공부해보는 RNN 친구들(2) - LSTM &amp; GRU  \n    &gt; 호다닥 공부해보는 RNN 친구들(1) - RNN(Recurrent Neural Networks)에서 이어지는 글입니다.  \n    &gt; [gruuuuu.github.io/machine-learning/lstm-doc2/](gruuuuu.github.io/machine-learning/lstm-doc2/)  \n    \n    rnn은 구조상 반복된 곱연산이 발생한다. 따라서 뒤로 갈수록 값이 무한정 커질 수 밖에 없다.\n    \n    하지만 값을 tanh 또는 sigmoid를 거쳐가게 하면 sigmoid의 경우 항상 [0,1]사이의 값 되도록 보장할 수 있고 tanh의 경우 [-1,1]사이의 값이 되도록 보장할 수 있다. 그러므로 계속 곱연산이 일어나도 결과 값이 발산하지 않을 수 있다.\n    \n    ### **주변 분포(Marginal Probability Distribution)**\n    \n    &gt; [!info] 주변 분포 - 위키백과, 우리 모두의 백과사전  \n    &gt; 확률론과 통계학에서 확률 변수들의 부분 집합의 주변 분포 (周邊分布) 란 그 부분 집합에 속한 확률 변수들의 확률 분포를 뜻한다.  \n    &gt; [ko.wikipedia.org/wiki/%EC%A3%BC%EB%B3%80_%EB%B6%84%ED%8F%AC](ko.wikipedia.org/wiki/%EC%A3%BC%EB%B3%80_%EB%B6%84%ED%8F%AC)  \n    \n    - 확률론과 통계학에서 확률 변수들의 부분 집합의 주변 분포(周邊分布)란 그 부분 집합에 속한 확률 변수들의 확률 분포를 의미\n    \n      \n    \n    ### RNN(**Recurrent Neural Networks)**\n    \n    &gt; [!info] RNN과 LSTM을 이해해보자!  \n    &gt; 이번 포스팅에서는 Recurrent Neural Networks(RNN)과 RNN의 일종인 Long Short-Term Memory models(LSTM) 에 대해 알아보도록 하겠습니다.  \n    &gt; [ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/](ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)  \n    \n    ![[Untitled 27.png|Untitled 27.png]]\n    \n    - 녹색 박스는 hidden state를 의미\n    - 빨간 박스는 input data를 의미\n    - 파란 박스는 output data를 의미\n    - 현 시점의 hidden state($h_t$)는 이전 시점의 hidden state($h_{t-1}$)을 받아 갱신됨\n    - 첫 번째 RNN의 경우, 이전 hidden state가 존재하지 않으므로 0을 입력\n    \n    ![[Untitled 1 21.png|Untitled 1 21.png]]\n    \n    ![[Untitled 2 16.png|Untitled 2 16.png]]\n    \n    ### 하이퍼볼릭 탄젠트(Hyperbolic Tangent)\n    \n    &gt; [!info] tanh 미분 정리  \n    &gt; Deep Learning을 대표적인 활성화 함수인 Hyperbolic Tangent 함수의 미분 절차를 정리합니다.  \n    &gt; [taewan.kim/post/tanh_diff/](taewan.kim/post/tanh_diff/)  \n    \n    - Sigmoid의 대체제로 사용될 수 있는 활성화 함수\n    - 출력 범위는 -1에서 1사이\n    - 출력 범위가 더 넓고 경사면이 큰 범위가 더 크기 때문에 더 빠르게 수렴하여 학습\n    - Sigmoid의 치명적인 단점인 **Vanishing gradient problem** 문제를 그대로 갖고 있음\n    \n    $$tanh(x)={{e^{x}-e^{-x}} \\over {e^{x}+e^{-x}}}$$\n    \n    ### 표본평균(Sample mean)\n    \n    &gt; [!info] Sample mean and covariance - Wikipedia  \n    &gt; The sample mean (or &quot;empirical mean&quot;) and the sample covariance are statistics computed from a sample of data on one or more random variables.  \n    &gt; [en.wikipedia.org/wiki/Sample_mean_and_covariance](en.wikipedia.org/wiki/Sample_mean_and_covariance)  \n    \n    - 표본 평균은 모집단에서 가져온 표본의 평균 값\n    \n    $$X \\in \\Omega \\\\ \\bar{X} = {1 \\over N}{\\sum_{i=1}^{N}X}$$\n    \n    ### 표본분산(sample variance)\n    \n    &gt; [!info] 분산 - 위키백과, 우리 모두의 백과사전  \n    &gt; 확률론과 통계학에서 어떤 확률변수의 분산(分散, 영어: variance,) 또는 &#039;변량 [출처 필요]&#039;은 그 확률변수가 기댓값으로부터 얼마나 떨어진 곳에 분포하는지를 가늠하는 숫자이다.  \n    &gt; [ko.wikipedia.org/wiki/분산](ko.wikipedia.org/wiki/분산)  \n    \n    - 표본(sample)의 분산\n    - 관측값에서 표본 평균을 빼고 제곱한 값을 모두 더한 것을 n-1로 나눈 값\n    \n    $$s^2 ={1 \\over {N-1}}{\\sum_{i=1}^{N}(X_i-\\bar X)}$$\n    \n    ### 표본편차(sample standard deviation )\n    \n    &gt; [!info] 표준 편차 - 위키백과, 우리 모두의 백과사전  \n    &gt; 표준 편차(標準 偏差, 영어: standard deviation, SD)는 통계집단의 분산의 정도 또는 자료의 산포도를 나타내는 수치로, 분산의 음이 아닌 제곱근 즉, 분산을 제곱근한 것으로 정의된다.  \n    &gt; [ko.wikipedia.org/wiki/표준_편차](ko.wikipedia.org/wiki/표준_편차)  \n    \n    - 표본의 표준 편차\n    \n    $$s =\\sqrt{{1 \\over {N-1}}{\\sum_{i=1}^{N}(X_i-\\bar X)}}$$\n    \n      \n    \n    ### 가능도 함수(Likelihood function, likelihood, **우도 함수, 우도**)\n    \n    &gt; [!info] Maximum Likelihood, clearly explained!!!  \n    &gt;  \n    &gt; [www.youtube.com/watch](www.youtube.com/watch)  \n    \n    &gt; [!info] Likelihood function - Wikipedia  \n    &gt; The likelihood function (often simply called the likelihood) is the joint probability of the observed data viewed as a function of the parameters of the chosen statistical model.  \n    &gt; [en.wikipedia.org/wiki/Likelihood_function](en.wikipedia.org/wiki/Likelihood_function)  \n    \n    - 확률 분포의 모수가, 어던 확률변수의 표집값과 일관되는 정도를 나타내는 값\n    - 선택한 parameter에 대한 statistical model에서 관측값들이 나올수 있는 확률, 즉 해당 statistical model과 관측 값들의 결합확률(Joint probability)을 의미\n    - 그러므로 가능도는 parameter에 대한 함수임을 강조하기 위해 아래와 같이 표현\n        - $L(\\theta | X)$ ( $=P(X|\\theta)$ )\n    \n    ### 최대가능도 추정법(MLE, **Maximum likelihood estimation)**\n    \n    - 최대가능도 추정법(MLE, **Maximum likelihood estimation**)은 모수 theta가 주어졌을 때 데이터 X에 대한 가능도 함수(likelihood function)의 최대값을 찾는 것을 말한다.\n    - 따라서 최대가능도 추정법은 확률 분포의 모수 theta를 추정하는 기법으로, 관측된 데이터의 가능도가 최대가 되는 모수값을 찾는 것이다.\n    - 즉, MLE는 관찰된 데이터가 주어졌을 때, 모수 theta가 가장 잘 관찰된 데이터를 나타내는 값이 되는 값을 찾는 것이다."},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/9강-Checkpoint":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/9강-Checkpoint","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 10/9강 Checkpoint.md","title":"9강 Checkpoint","links":[],"tags":[],"content":"[1강] mAP\n\nmAP가 어떻게 계산될 수 있는 지 완벽하게 이해하고 있는가?\n\n[2강] 2 Stage detectors – RCNN, SPP, FastRCNN, FasterRCNN\n\nSPP(RoI Pooling)에 대해 완벽하게 이해하고 있는가?\nRoI projection에 대해 완벽하게 이해할 수 있는가?\nFasterRCNN중 RPN에 대해 완벽하게 이해하고 있는가?\nAnchorbox에 이해하고 있는가?\nRPN의 역할에 대해 이해하고 있는가?\n\n[3강] MMDetection, Detectron2\n\nMMDetection이든 Detectron2이든 Scratch든 새로운 모델에 대해 코드를 짤 수 있는가? Through Competition !!!\n\n[4강] FPN, PANet, RFP, BiFPN, NasFPN, AugFPN\n\nNeck의 역할에 대해 완벽하게 이해하고 있는가?\nFPN, PANet에 대해 완벽하게 이해하고 있는가?\n\n[5강] Yolo, SSD, RetinaNet\n\n2 stage와는 다르게 RPN이 없는 1 stage에서 어떻게 박스를 예측하는지 이해하고 있는가?\nYolo v1에 대해 완벽하게 이해하고 있는가?\n\n[6강] EfficientDet\n\nEfficientDet의 등장 배경 및 Compound scaling의 중요성에 대해 설명할 수 있는가?\n\n[7강] Cascade, Deformable, Transformer\n\n(Optional) Cascade, Deformable, Swin에 대해 설명할 수 있는가?\n\n[8강] Yolov4, M2Det, CornerNet\n\n(Optional) M2Det에 대해 설명할 수 있는가?\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Object-detection-꿀팁/Object-detection-꿀팁":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Object-detection-꿀팁/Object-detection-꿀팁","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 10/Object detection 꿀팁/Object detection 꿀팁.md","title":"Object detection 꿀팁","links":[],"tags":[],"content":"\n1Stage + 2Stage\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 10/Week 10.md","title":"Week 10","links":["tags/class","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/9강-Checkpoint","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Object-detection-꿀팁/Object-detection-꿀팁"],"tags":["class"],"content":"\nObject Detection Library\n\nOverview\nMMDetection\n\nTwo-Stage Detector의 Pipeline\nMMDetection 2Stage Model Pipeline\n\n\nDetectron2\n\nPipeLine\n\n\n\n\nNeck\n\nOverview\n\nNeck은 무엇인가?\n2-Stage Pipline\n왜 마지막 feature map 만을 사용해야하는가?\n\n\nFPN(Feature Pyramid Network)\n\nPipeline\nStage Mapping\n\n\nPANet(Path Aggregation Network)\n\nFPN의 단점\nAdaptive Feature Pooling\n\n\nDetectroRS\n\nMotivation\nRFP(Recursive Feature Pyramid)\nAtrous Convolution(or dilated convolution)\nASPP(Atrous Spatial Pyramid Pooling)\n\n\nBiFPN(EfficientDet)\n\nWeighted Featrue Fusion\n\n\nNasFPN\nAugFPN\n\n기존 FPN의 문제점\nResidual Featrue Augmenmtation\nSoft ROI Selection\n\n\n\n\n1 Stage Detectors\n\nBackground &amp; History\n\n2 Stage Detector\n1 Stage Detector\nHistory\n\n\nYolo v1(You Only Look Once)\n\nHistory\nPipeline\nNetwork Architecture\nPipeline\nLoss\n장점\n단점\n\n\nSSD\n\nYolo vs SSD\nPipeline\nTraining\nLoss\n\n\nYOLO Follow-up\n\nYolo v2\nYolo v3\n\n\nRetinaNet\n\n1 Stage Detector의 고질적 문제\nConcept\n\n\n\n\nEfficientDet\n\nEfficientNet\n\n등장배경\nModel Scaling\nBetter Accuracy &amp; Efficiency\n\n\nEfficientDet\n\nMotivation\n기존 Detection Model들의 문제와 해결방법\n\n\n\n\n\n\nObject Detection Library\nOverview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMMDetectionDetectron2특징- 전체 프레임워크를 모듈 단위로 분리해 관리할 수 있음  - 많은 프레임워크를 지원함  - 다른 라이브러리에 비해 빠름- 전체 프레임워크를 모듈 단위로 분리해 관리할 수 있음  - OD 외에도 Segmentation, Pose Prediction 등의 알고리즘을 지원함지원 모델- Fast R-CNN  - SSD  - YOLO v3  - DETR  ..etc- Faster R-CNN  - RetinaNet  - Mask R-CNN  - DETR  ..etc\nMMDetection\nPytorch 기반의 Object Detection 오픈소스 라이브러리\n일정 수준의 코딩실력이 된다면 쉽게 사용 가능\n커스터마이징 어려움\nTwo-Stage Detector의 Pipeline\nInput→Backbone→Neck(일종의 Feature Map)→Dense Prediction→Prediction\nMMDetection 2Stage Model Pipeline\n➡️ Backbone➡️Neck➡️—————➡️ROI Head\n↘️ ↗️\nDenseHead\n\n\nBackbone : 이미지를 Feature Map으로 변형\n\n\nNeck : Backbone과 Head를 연결, Feature Map을 재구성(ex. FPN)\n\n\nDense head : Feature Map의 dense location을 수행\n\n\nROI Head : ROI의 Feature를 받아 box classification, 좌표 회귀 등을 예측\n\n\nDetectron2\nFacebook AI Research의 Pythorch 기반 라이브러리\nObject detection, segmentation, pose prediction 등 알고리즘 제공\nPipeLine\n\nSetup Config\nSetup Trainer\nSatrt Training\n\n\nNeck\nOverview\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1905.05055.pdf\n\n\n\n\nNeck은 무엇인가?\n\n\n                  \n                  Object Detection에서 말하는 Backbone, Neck, Head \n                  \n                \n\n\nBackbone, neck, head는 무엇을 의미하는 걸까?\nvelog.io/@peterkim/Object-Detection%EC%97%90%EC%84%9C-%EB%A7%90%ED%95%98%EB%8A%94-Backbone-Neck-Head\n\n\n\n2-Stage Pipline\n\nInput → Backbone → RPN→ Prediction\n\n기존에는 Backbone에서 마지막 Feature Map을 사용해왔음\n왜 마지막 feature map 만을 사용해야하는가?\n\n연구를 통해 중간 과정의 Feature map들도 사용할 수 있겠다는 결론\n중간단계의 Feature map들도 활용 시작\n크기 별로 feaute를 추출하기 때문에 검출에 유리\n다양한 크기의 객체를 더 잘 탐지하기 위해 필요\n\nLow Level의 Feature는 Semantic 정보가 약하고, Local한 정보가 강함\nHigh Level의 Feature는 Semantic 정보는 강하나 Local한 정보가 약함\n\n\n서로간의 Feuatre 교환 필요\n\n\n\n\nFPN(Feature Pyramid Network)\n\n\n                  \n                  Understanding Feature Pyramid Networks for object detection (FPN) \n                  \n                \n\n\nDetecting objects in different scales is challenging in particular for small objects.\njonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\n\n\n\n\nHigh Level에서 Low Level로 Semantic 정보 전달 필요\nPyramid 구조를 통해 High level 정보를 low level에 순차적으로 전달\n용어\n\nLow level = Early stage = bottome\nHigh level = Late Stage = Top\n\n\nPipeline\n\nBottom-Up\n\nimage ~~ High Level까지 feature 전달됨, 일발적인 CNN Backbone 통과 과정을 의미\n\n\nTop-Down\n\nHigh Level ~~ Low Level까지 feature 전달\nDimm이 맞지 않음\n\n\nTop-Down Path : Up Convolution 진행(h,w 피팅)\n\n\n기존 level Path : 1x1 Convolution 진행(c 피팅)\n\n\n\n\n\n\nStage Mapping\n\nROI가 어느 Stage에서 온 것인지 알아야 함, roi의 w와 h 값으로 stage를 추정\n\nk=[k_0 + \\log_2({{\\sqrt {wh}} \\over 224})] \\\\ k_0 : 4(default)\nCode\n\nBuild laterals :각 feature map 마다 다른 채널을 맞춰주는 단계\nBuild Top-down : channel을 맞춘 후 top-down 형식으로 featuremap 교환\nBuild outputs : 최종 3x3 CNN을 통과하여 RPN을 들어갈 feature 완성\n\nPANet(Path Aggregation Network)\n\n\n                  \n                  Path Aggregation Network for Instance Segmentation \n                  \n                \n\n\nThe way that information propagates in neural networks is of great importance.\narxiv.org/abs/1803.01534\n\n\n\nFPN의 단점\n\nBottom-Up의 과정에서 실제로는 매우 많은 CNN Layer를 거치기 때문에 상위 Level의 Layer로 Feature를 재대로 전달하는지 장담 할 수 없음(ex : ResNet의 긴 CNN 구조)\n\n\nPANet의 Solution : Bottom-Up Path augmentation(첨가)를 위한 Network를 추가\nAdaptive Feature Pooling\n\n기존의 stage mapping(k 값 구하기)의 문제 : 수 pixel 차이로 stage가 변하는 경계선상에 있는 roi들이 존재\nSolution : 모든 Stage에서 ROI Pooling 수행\n\nDetectroRS\n\n\n                  \n                  DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution \n                  \n                \n\n\nMany modern object detectors demonstrate outstanding performances by using the mechanism of looking and thinking twice.\narxiv.org/abs/2006.02334\n\n\n\nMotivation\n\nLooking and thinking twice\n\nRPN\nCascade R-CNN\n반복적인 작업을 수행하면 성능이 올라갈까?\n\n\n\n\n\nRFP(Recursive Feature Pyramid)\n\n\nFeature Pyramid 구조와 유사, 추가적으로 backbone에서 feature pyramid의 정보를 가지고 학습 수행\n\n\nFlops가 증가하는 단점\n\n\nAtrous Convolution(or dilated convolution)\n\n\n                  \n                  Atrous Convolution \n                  \n                \n\n\nAtrous Convolution 1.\nbetter-tomorrow.tistory.com/entry/Atrous-Convolution\n\n\n\n\n\nReceptive field를 키우는 테크닉\n\n\nASPP(Atrous Spatial Pyramid Pooling)\n\n\n                  \n                  ASPP(Atrous Spatial Pyramid Pooling) \n                  \n                \n\n\ngaussian37’s blog\ngaussian37.github.io/vision-segmentation-aspp/\n\n\n\n\n\n다른 크기로 Atrous Colvolution을 수행한 뒤, Concat하여 사용\n\nBiFPN(EfficientDet)\n\n\n                  \n                  EfficientDet: Scalable and Efficient Object Detection \n                  \n                \n\n\nModel efficiency has become increasingly important in computer vision.\narxiv.org/abs/1911.09070\n\n\n\n\n\n효율성을 위해, feature map이 한 곳에서만 오는 node들을 삭제(flops 감소)\nWeighted Featrue Fusion\n\n\nFPN과 같이 단순 Summantion이 아니라 feature 별 가중치를 부여해서 summantion\n\n\nFeature별 가중치를 통해 중요한 feature를 강조하여 성능 상승\nP^{td}_{6} = Conv({{w_1 \\cdot P^{in}_6 + w_2 \\cdot Resize({P^{in}_7})} \\over {w_1+w_2+\\epsilon}})\n→ \\epsilon : 분모가 0이 되지 않기 위한 매우 작은 값\n\n\n더 가볍고, 더 높은 성능 구현\n\nNasFPN\n\n\n                  \n                  NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection \n                  \n                \n\n\nCurrent state-of-the-art convolutional architectures for object detection are manually designed.\narxiv.org/abs/1904.07392\n\n\n\n기존에는 사람이 구조를 찾았음\nFPN 구조에 따라서 성능이 변화\n→ NAS를 사용해서 가장 성능이 높은 구조만을 채택해보면 어떨까?\n\n\n같은 inference time 대비 높은 성능\n\nCOCO Dataset, ResNet 기준으로 찾은 Architecture, 범용적이지 못함\n\nParameter가 많이 소요\n\n\nHigh serch cost\n\n다른 Architecture, 다른 backbone에 적용하려면 새롭게 구해야 함\n\n\n\nAugFPN\n\n\n                  \n                  AugFPN: Improving Multi-scale Feature Learning for Object Detection \n                  \n                \n\n\nCurrent state-of-the-art detectors typically exploit feature pyramid to detect objects at different scales.\narxiv.org/abs/1912.05384\n\n\n\n기존 FPN의 문제점\n\n\n서로 다른 Level의 feature 간의 semantic 차이\n\n\nHighest feature map의 정보 손실\n\n가장 위의 layer는 다른 layer와 다르게 위쪽에서 전달해주는 feature가 없음\n\n\n\n1개의 feature map에서 roi 생성\n\n\nResidual Featrue Augmenmtation\n\n\n\n기존의 FPN의 문제점 중, “Highest feature map의 정보 손실”를 해결하기 위해 기존 backbone에서 Residual Feature Augmentation을 사용해 C5 Feature를 M5 Feature로 전달\n\n\nRatio-invariant Adaptive Pooling\n\nTarget Feature Map을 Pyramid 형태로 다양한 Scale의 feature map 생성\n\n\n\n\nAdaptive Spatial Fusion\n\nRatio-invariant Adaptive Pooling을 통해 생성한 다양한 크기의 N개의 Feature Map을 Upsample을 통해 동일 크기로 변경, Channel의 경우 256 Channel 고정\nUpsample된 N개의 Feature Map를 Concat → Nx256xHxW\n1x1 Conv → CxHxW\n3x3 Conv → NxHxW\nSigmoid → Nx(1xHxW)\n\n일종의 featuremap당 pixel별 weight로 볼 수 있음\n\n\nWeighted Sum → 256xHxW\n\n\nN개의 Feature Map과 위 Sigmod 결과로 가중합 수행\n\n\n\n\n\n\nSoft ROI Selection\n\nFPN : 하나의 FeatureMap에서 ROI 계산, Sub-optimal\nPANet : 모든 Feature map 사용, 하지만 max pool을 사용했기에 정보손실 가능성 존재\n\n모든 Scale의 Feature에서 ROI Projection 진행 후 ROI Pooling 수행\nChannel-wise 가중치 계산 후 가중합을 사용\n\nPANet의 Max Pooling을 학습가능한 가중 합으로 대체\n\n\n1 Stage Detectors\nBackground &amp; History\n\n\n                  \n                  An overview of object detection: one-stage methods. \n                  \n                \n\n\nIn this post, I’ll discuss an overview of deep learning techniques for object detection using convolutional neural networks.\nwww.jeremyjordan.me/object-detection-one-stage/\n\n\n\n2 Stage Detector들의 단점 : 속도가 매우 느림\nReal World에서 사용 가능한 Object Detector는 없을까?\n2 Stage Detector\nInput → Region Proposal → Classification → Multi-class classification &amp; Bounding regression For each Proposed Region\n1 Stage Detector\nInput → Conv Layers → Feature Maps → Multi-class classification &amp; Bounding regression For each Grid or Spatial Location\n\nLocalization, Classification이 동시에 진행됨\n전체 이미지에 대해 특징 추출, 객체 검출이 이루어짐\n속도가 매우 빠름\n영역추출을 하지 않고 이미지를 보기 때문에 맥락적 이해가 높음\n\nBackground Error 낮음\n\n\nYolo, SSD, RetinaNet, …etc\n\nHistory\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1905.05055.pdf\n\n\n\n\nYolo v1(You Only Look Once)\n\n\n                  \n                  You Only Look Once: Unified, Real-Time Object Detection \n                  \n                \n\n\nWe present YOLO, a new approach to object detection.\narxiv.org/abs/1506.02640\n\n\n\nHistory\n\n\nv1 : 이미지의 Bbox와 Classification을 동시에 예측하는 1 Stage Detector 등장\n\n\nv2 : 빠르고 강력하고 더 좋게 향상\n\n\nv3 : multi-scale feature maps 사용\n\n\nv4 : 최신 딥러닝 기술(BagOf Freebies=BOF, Bag of Specials=BOS) 사용\n\n\nv5 : 크기별로 모델 구성(Small, Medium, Lage, Xlarge)\n\n\nRegion Proposal 단계가 없음\nGooLeNet의 변형구조\nPipeline\n\n\n입력 이미지를 SxS 그리드 영역으로 나누기\n각 그리드 영역마다 B개의 Bounding box와 Confidence score 계산\n\nConfidence = Pr(Object) x IOU(truth;pred)\n\n\n각 그리드 영역마다 C개의 Class에 대한 확률 계산(C=20)\n\n\nConditional class probability = Pr(Class_i | Object)\n\n\n\n\nNetwork Architecture\n\nOutput channel : 7(H)x7(W)x30(Info)\n\nCell(Grid로 나눈 영역)마다 30개의 정보가 저장됨\n\n\n5개 : 1번 Bonding Box 정보(center x, center y, width, height, confidence score)\n\n\n5개 : 2번 Bonding Box 정보(x, y, width, height, confidence score)\n\n\n20개 : 각 Class일 확률\n\n\n\n\nPipeline\n\n\n각 Cell의 Bbox의 Confidence score와 Class 확률 Multiply → 20개의 Class별 Score를 담고 있는 7x7x2개의 Bbox가 생성됨\n\n\nScore Thresholding → 너무 작은 Score는 0으로 Drop\n\n\nSort Descending → 내림차순으로 정렬\n\n\nNMS\n\n\nOut\n\n\nLoss\n\n장점\nFaster R-CNN에 비해 6배 빠른 속도\n다른 real-time detector에 비해 2배 높은 정확도\n물체의 일반화된 표현을 학습(학습하지 않은 다른 도메인의 이미지에서도 좋은 성능을 보임)\n단점\nGrid 보다 작은 크기의 물체 검출 불가능\n마지막 feature만을 사용\nSSD\n\n\n                  \n                  SSD: Single Shot MultiBox Detector \n                  \n                \n\n\nWe present a method for detecting objects in images using a single deep neural network.\narxiv.org/abs/1512.02325\n\n\n\nYolo vs SSD\n\n\nYolo\n\n448 x 448 input\nfc layer\nuse only last feature map\n\n\nSSD\n\n\n300 x 300 input\n\n\nno fc layer\n\n\nuse 6 featrue map\n\nearly stage : small object detection\nlate stage : big object detection\n\n\n\nDefault Box(=anchor box)\n\n\n\n\n\nPipeline\n\n\nMulti-scale feature maps 사용\n\n\nFeature Map(HxWx256)을 3x3 Conv로 5x5xC로 변환\n\nC = Number of Bbox * (cx, cy, w, h, background, class_1,class_2,…class_n)\nNumber of Bbox(default box) : 6\n\n\n\n각 Feature Map에 3x3 Conv(stride 1, padding1)연산으로 n * n * (#default box * (offset + class))의 output 생성 → 8732개의 bounding box\n\n\nTraining\n\n\nHard negative mining\n\n\nNMS\n\n\nLoss\n\n\nLoss Function\n\n\n\nLocalization Loss\n\n\n\nConfidence Loss\n\n\n\nYOLO Follow-up\nYolo v2\n\nBetter : 정확도 향상\n\nBatch normalization : mAP 2% up\nHigh resolution classifier : mAP 4% up\n\nApply 224x224 Pretrained VGG to 448 x 448 Task\nFine Tuning using 448x448 image\n\n\nConv with anchor boxes : mAP 5% up\n\nremove fc layer\nanchor box 도입\n5개의 anchor box\n\n\nPassthrough Layer\n\nEarly feature map과 late feature map을 합쳐주는 Layer\n\n\n\n\nFatser : 속도 향상\n\nGoogLeNet을 버리고 Darknet-19 아키텍쳐 적용\n\n\nStronger : 더 많은 class 예측 80 → 9000\n\n\nImageNet, Coco Dataset 사용\n\n\n\n\nYolo v3\n\nDarknet-53 사용\n\nResnet-101, resnet 152와 비슷한 성능 및 높은 FPS 달성\n\n\nSkip Connection 적용\nMax pooling x, Conv stride 2 사용\nMulti-scale feature map 사용(52x52, 26x26, 13x13)\n\nRetinaNet\n\n\n                  \n                  Focal Loss for Dense Object Detection \n                  \n                \n\n\nThe highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations.\narxiv.org/abs/1708.02002\n\n\n\n1 Stage Detector의 고질적 문제\n\nClass imbalance\n\nGrid로 나눠서 Cell 마다 모두 Bbox를 추정하게 함\nPositive Sample(Object area) &lt; Negative Sample(BG Area)\n\n\nAnchor Box 대부분은 Negative Sample\n\n\n2 Stage detector의 경우 RPN에서 BG 제거\n\n\nHard Negative mining\n\n\n\n\nConcept\n\nFocal Loss 제안 : Cross entropy loss + scaling factor\n\n쉬운 예제에 작은 가중치, 어려운 예제에 큰 가중치\n\n\n\n\n\nEfficientDet\nEfficientNet\n\n\n                  \n                  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks \n                  \n                \n\n\nConvolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available.\narxiv.org/abs/1905.11946\n\n\n\n등장배경\n점점 빠르고 작은 모델에 대한 요구 증가\n효율성과 정확도의 trade-off를 통해 모델 사이즈를 줄이는 것이 일반적\n하지만 모델을 어떻게 압축해야 하는지 불분명\n더 높은 정확도와 효율성을 가지면서 ConvNet의 크기를 키우는 방법(Scale Up)은 없을까?\nModel Scaling\n\n\nModel이 커질수록, 성능이 증가 → 모델의 크기를 키움\n모델의 크기를 키우다보면, 더 이상 성능 이점이 없어짐 → ‘잘’ 쌓는 방법이 있을 것이다\n어떻게 잘 쌓아야 할까?\n\n\nWidth Scaling : Channel을 크기 주는 것\n\nex) Wide ResNet\n작은 모델에서 주로 사용됨(MobileNet, MnasNet)\n더 wide한 네트워크는 미세한 특징을 잘 잡아내는 경향이 있음\n그러나 극단적으로 얕은 모델은 High-level의 특징을 잘 잡아내지 못하는 경향이 있음\n\n\n\nDepth Scaling : CNN을 더 깊게 쌓는 것\n\nex) ResNet\n많은 ConvNet에서 쓰이는 방법(DenseNet, Inception-v4)\n깊은 ConvNet은 더 풍부하고 복잡한 특징들을 잡아낼 수 있고 새로운 Task에도 잘 일반화 됨\n그러나 Gradient vanishing 문제로 깊어질수록 학습이 어려움\n\n\n\nResolution Scaling : 더 큰 Image를 사용하는 것\n\n고화질의 이미지를 사용하면 ConvNet은 미세한 패턴을 잘 잡아낼 수 있음\n\n\n\nCompound Scaling : 위 3개의 Scaling을 모두 사용하는 것\n\n\n\n\nBetter Accuracy &amp; Efficiency\n\nModel의 Accuracy를 최대화하는 d, w, r을 찾는게 목표\n\n\nScale Factors\n\nd : Model의 depth를 조절하는 Factor\nw : Model의 Channel을 조절하는 Factor\nr : Model의 Resolution을 조절하는 Factor\n\n\n\n\nModel의 Memory는 Target의 Memory보다 작거나 같아야한다\n\nModel의 FLOPS는 Target의 FLOPS보다 작거나 같아야 한다\n\n각 Factor를 증가시킬 때(다른 Factor는 통제) FLOPS 증가 대비 ImageNet Top-1 Accuracy(%)\n\nd,r을 변화시키면서 확인한 FLOPS 증가 대비 Top1 Accuracy(%)\n\n효율적인 Model을 만들기 위해서는 d, w, r을 모두 적절히 변화시키는 Compound Scaling이 필요함을 실험적으로 증명\n\nEfficientDet\n\n\n                  \n                  EfficientDet: Scalable and Efficient Object Detection \n                  \n                \n\n\nModel efficiency has become increasingly important in computer vision.\narxiv.org/abs/1911.09070\n\n\n\nMotivation\n자원의 자약이 있는 상태에서 더 높은 정확도와 효율성을 가진 Detection 구조를 만드는 것이 가능할까?\nEfficientNet의 아이디어는 Detection Task에도 적용 가능\nYolo나 SSD같이 1 Stage Detector에 속함\n기존 Detection Model들의 문제와 해결방법\n\n\nEfficient Multi-scale featrue fusion\n\n\nNeck에서 Channel별 Resolition을 맞춰 Simple Summantion만 했음\n\n\n단순합을 하는 것이 올바를까? → BiFPN 구조 제안\n\n\n\n\n\nModel Scaling\n\n\n큰 Backbone, 큰 Image Size에 집중했음\nEfficientDet은 정확도와 효율을 모두 높일 수 있는 모델을 찾고자 함\n\nEfficientNet과 같은 Compound Scaling 방식 제안\n\n\nEfficientNet B0~B6을 Backbone으로 사용\nBiFPN Network\n\n\n네트워크의 width(=#channels)와 depth(=#layers)를 compound 계수에 따라 증가시킴(grid search)\n\n\n\n\n\n\n\n\n9강 Checkpoint\nObject detection 꿀팁"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 11/Week 11.md","title":"Week 11","links":[],"tags":[],"content":"\nAdvanced Object Detection\n\nCasecade RCNN\n\nContribution\nMethod\n\n\nDeformable Convolution Networks(DCN)\n\nContribution\nMethod\n\n\nViT\nEnd to End Object Detection with Transformer\n\nContibution\nArchitecture\n\n\nSwin Transformer\n\nVIT의 문제점\nContibution\nArchitecture\n\n\nYOLO v4\n\nOverview\nRelated work\nSelection of architecture\nAdditional Improvements\n\n\nM2Det\n\nOverview\nArchitecture\n\n\nCornerNet\n\nOverview\nArchitecture\n\n\n\n\n\n\nAdvanced Object Detection\nCasecade RCNN\nContribution\nFaster RCNN\nRPN을 사용한 최초의 End to End network\nIoU 0.7 이상을 positive, 0.3 이하를 negative로 훈련시켰다\n그 후, 0.5를 기준으로 True / False를 판별하여 성능을 측정했다\n→ 왜 0.5로 했는가? 0.6이면 어떻게 변화하는가? 에 대한 고찰이 없었다\nCasecade RCNN\nFaster RCNN에서 하지 않았던 고찰들을 실험을 통해 명확히 하였음\n또한 실험 결과를 기반으로 새로운 모델을 제시, 성능 향상을 이뤄냄\nMethod\n\n\n                  \n                  Cascade R-CNN: Delving into High Quality Object Detection \n                  \n                \n\n\nIn object detection, an intersection over union (IoU) threshold is required to define positives and negatives.\narxiv.org/abs/1712.00726\n\n\n\n\ninput iou : RPN의 결과 값으로 그린 bbox와 GT 간의 IOU\noutput iou : box-head를 통과한 최종 bbox와 GT 간의 IOU\n훈련에서 사용한 iou 기준이 높을수록, rpn이 bbox를 잘 예측했을 때(input iou가 클수록) box-head를 통과한 최종 bbox 예측 성능(output iou)도 좋아짐\n그러나, rpn이 iou를 잘 예측하지 못한다면, 낮은 iou threshold로 훈련하는 것이 더 좋은 성능을 보여준다는 것을 알 수 있다\n나아가서 detection 성능 또한 0.5로 훈련시킨 것이 전반적으로 좋은 map를 얻을 수 있는 것을 실험적으로 증명했다\n→ high quality detection이 필요하다면, 높은 iou 기준으로 훈련 및 높은 iou threshold로 분류(map 기준에서 성능 하락)\n→ low quality detection이 필요하다면, 낮은 iou 기준으로 훈련 및 낮은 Iou threshold로 분류\n\n선행연구(interactive bbox at inference, integral loss)는 약간의 성능 향상이 있었음\n\n그러나 이를 조합한다면(Casecade R-CNN), 더 큰 성능 향상을 얻을 수 있었음\nDeformable Convolution Networks(DCN)\nContribution\nCNN의 문제\nCNN은 Geometric Transformation들에 한계를 지님\n기존 해결방법\nGeometric augmentation을 수행\n하지만, 그러므로 휴리스틱하게 사람이 넣어준 augmentation에 대해서만 학습을 수행하므로 사람이 잘 넣어줘야 함\nDeformable Convolution\ntask에 따라 CNN을 변형시켜서 학습\nMethod\n\n\n                  \n                  Deformable Convolutional Networks \n                  \n                \n\n\nConvolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules.\narxiv.org/abs/1703.06211\n\n\n\n\n\n                  \n                  DCN \n                  \n                \n\n\n(Deformable Convolutional Networks) Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the ﬁxed geometric structures in their building modules.\njjeamin.github.io/posts/DCN/\n\n\n\nDeformable Convolution\n\n기존 kernel size를 유지한 체, 각 sampling position에 대한 offset을 파라미터에 추가한다\n\n\nCNN을 통해 Offset Field R을 생성하고, R을 참조하여 Conv 시 Sampling Position을 결정한다 offset은 아주 작은 값이기 때문에 소수점이 될 수 있고, 이 경우 bilinear 보간을 통해 값을 얻는다\noffset filed는 왜 2N일까?\n이미지 공간은 2차원이으로 offset을 표현하기 위해서는 dx, dy가 필요하다 즉, Offset Filed R의 크기가 N이라면, 사실 2N의 크기가 필요하다\n\n\nFaster R-CNN등에서는 큰 성능 향상이 없었다 그러나 DeepLab 등의 segmantation task에서는 큰 성능향상이 있다\nObject detection, Segmentation에서 좋은 효과를 보인다\nViT\n\n\n                  \n                  An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \n                  \n                \n\n\nWhile the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited.\narxiv.org/abs/2010.11929\n\n\n\nNLP에서 long range dependanc를 해결\nvision에 적용한 case가 ViT\n이전에 학습한 내용과 크게 상이하지 않으므로 생략\nEnd to End Object Detection with Transformer\n\n\n                  \n                  End-to-End Object Detection with Transformers \n                  \n                \n\n\nWe present a new method that views object detection as a direct set prediction problem.\narxiv.org/abs/2005.12872\n\n\n\nContibution\nTrainsformer를 처음으로 Object Detection에 적용\n기존의 Object Detection의 Hand-Crafted post process 단계(ex: NMS)를 Transformer를 사용해 없앰\nArchitecture\n\nCNN을 backbone으로 feature map 추출, 지정한 개수 만큼 transformer를 사용해서 bbox 생성\n\nTransformer 특성상 많은 연산이 필요하기 때문에, Highest level feature map을 사용해서 입력을 감소시킴\ndecoder를 통과한 결과 값을 Feed forward network를 통과시켜 bbox를 얻음\ndecoder 1개당 1개의 ffn과 1개의 bbox 및 class 예측을 수행하기 때문에 이미지에 존재하는 오브젝트의 총 개수보다 디코더 개수가 많아야 함\n\nSwin Transformer\n\n\n                  \n                  Swin Transformer: Hierarchical Vision Transformer using Shifted Windows \n                  \n                \n\n\nThis paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision.\narxiv.org/abs/2103.14030\n\n\n\nVIT의 문제점\n많은 양의 data 학습 필요\ntransformer 특성상 computational const가 큼\n일반적인 backbone으로 사용이 어려움\nContibution\nCNN과 유사 구조로 설계\nWindow라는 개념을 활용하여 cost 감소\nArchitecture\n\n\n\nPatch Partitioning(이미지를 patch 크기로 나눔)\n\n\nLinear Embeding(Detection task를 목적으로 하기 때문에 CLS Token 없음)\n\n\nSwin Transformer Block\n\n\nWindow Multi-head self attention\n\n\n\nShifted window Multi-head attention\n\n\n\n\n\n\nPatch Merging\n\n\n매우 큰 성능 향상\n적은 data에도 학습이 잘됨\nwindow단위로 computation cost 감소\ncnn과 비슷한 구조로 object detction backbone으로 사용 가능\nYOLO v4\nOverview\nContribution\n하나의 GPU에서 훈련할 수 있는 빠르고 정확한 Object detector\nBOF(Bag of Freebies), BOS방법들을 실험을 통해서 증명하고 조합을 찾음\n\nBOF : Inference 비용을 늘리지 안호 정확도를 향상시키는 방법\nBOS : Inference 비용을 조금 높이지만 정확도가 크게 향상되는 방법\n\nGPU 학습에 더 효율적이고 적합하도록 방법들을 변형\nRelated work\nBag of freebies\n\n\nData augmentation\n\nPhotometric Distortions\nGeometric Distortions\nCutOut\nRandom Erase\nMixUp\nCutMix\nGAN\n\n\n\nSemantic Distribution Bias(Dataset 불균형을 해결하기 위한 방법)\n\nHard Negative Mining\n\n어려운 배경을 강제로 batch에 많이 포함시킴\n\n\nOHEM\nFocal Loss\nLabel Smoothing\n\n0→0.1 , 1→ 0.9로 smoothing\n잘못된 label이 많을 경우, 효과적\noverfitting을 막아주고 regularization 효과 기대\n\n\n\n\n\nBounding Box Regression\n\n\nMSE\n\nGT 와 bbox의 거리에 대한 loss, 허나 iou 오차를 대변하지 못하기 때문에 iou관련 loss를 추가로 사용\n\n\n\nGIOU\n\n\nDIoU\n\n\nCIoU\n\n\n\n\nBag of specials\n\nEnhancement of Receptive field\n\nSPP(Spatial Pyramid Pooling)\nASPP(Atrous SPP)\nRFB(Receptive field block)\n\n\nAttension mudle\n\nSE(Squeeze and excitation)\nSAM(spatial attension module)\n\n\nFeature integration(=Neck)\n\nFPN(Feature Pyramid network)\nSFPM(Scale-wise feature aggregation module)\nASFF(Adaptively spatial feature)\nBiFPN\n\n\nActivation Function\n\nReLU\nLeaky ReLU\nParametric ReLU\nReLU6\nSwish / Mish\n\n약간의 음수를 허용하기 때문에 ReLU의 zero bound 보다 gradient흐름에 좋은 영향\n모든 구간 미분 가능\n\n\n\n\nPost-processing method\n\nNMS(Non maximum suppression)\nSoft NMS\nDIoU NMS\n\n\n\nSelection of architecture\n\n\n                  \n                  YOLOv4: Optimal Speed and Accuracy of Object Detection \n                  \n                \n\n\nThere are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy.\narxiv.org/abs/2004.10934\n\n\n\n\n\n\n기존 Yolo Architecture에 CSPNet을 추가\n\n\nCSPNet(Cross Stage Partial Network)\n\n\n정확도 유지 + 경량화\n\n\n메모리 cost 감소\n\n\n다양한 backbone에서 사용 가능\n\n\n연산 bottleneck 제거\n\n\n기존 DenseNet의 문제\n\n\n가중치 업데이트 시, gradient 정보를 재사용하게 됨\n\n\n\n\n                  \n                  CSPNet: A New Backbone that can Enhance Learning Capability of CNN \nNeural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection.\narxiv.org/abs/1911.11929\n                  \n                \n\n\n\n\n모든 feature map을 사용하는 방식에서 일부 feature map을 사용하는 방식으로 변경\n\n\n\nAdditional Improvements\n\n\nNew data augmentation\n\nMosaic\n\n4장의 이미지를 합처버림\n\n\nSAT(Self-Adversarial Training)\n\n모델의 오검출을 유도\n\n\n\n\n기존 방법 변형\n\nmodified SAM\nmodified PAN\nCmBN(Cross mini-batch normalization)\n\n\n\nM2Det\n\n\n                  \n                  M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network \n                  \n                \n\n\nFeature pyramids are widely exploited by both the state-of-the-art one-stage object detectors (e.\narxiv.org/abs/1811.04533\n\n\n\nOverview\nBackground\n물체에 대한 scale 변화는 object detection의 과제\n\n\nImage Pyramid\n\n\nFeature Pyramid(Neck의 기본 구조)\n\n\nFeature Pyramid의 한계점\n\n\nbackbone으로부터 feature pyramid 구성\nClassification task를 위해 설계된 backbone은 object detection task를 수행하기에 충분하지 않음\nbackbone network는 single-level layer로, singe-level 정보만을 나타냄\n일반적으로, low-level feature는 간단한 외형을, high-level feature는 복잡한 외형을 나타내는 것에 적합\n\nArchitecture\n\nMLFPN(Multi-level, multi-scale feature pyramid) 제안\nSSD에 합쳐서 M2Det이라는 One Stage Detector 제안\n\nOverall archtecture\n\nUnet 구조의 TUM을 통과시키고 나온 Feature Map에서 가장 큰 FeatureMap(마지막 Featrue map)을 가져와 FFMv2 연산으로 concat하는 것을 반복\n이후, Level별 Multi-scale Feature Map을 SFAM을 사용해 scale 별로 concat\n마지막으로 Prediction Layers를 통과\nFFM(Feature Fusion Module)\n\n서로 다른 scale의 2 feature map\nFFMv1 : 서로 다른 scale의 두 feature map을 channel wise로 conca해서 Base feature 생성(서로 다른 scale의 2 feature map을 합쳐 semantic 정보가 풍부)\nFFMv2 : FFMv1 결과에 추가로 TUM의 가장 큰 feature map을 concat\nTUM(Thinned U-shape Module)\nEncoder-decoder 구조\n구조상, 가장 마지막 feature map(디코더 출력)은 U Shape의 구조상 다른 scale의 feature map들의 정보가 모두 반영되어 있으므로 Multi-scale feature map으로 볼 수 있다\nSFAM(Scale-wise Feature Aggregation Module)\n\n동일 크기를 가진 feature map끼리 concat\n각각의 scale의 feature들은 multi-level 정보를 포함\nScale-wise로 concat된 블록들은 SE block을 통해 추가적으로 가중치를 조절하고(채널 별 중요도 조정) SSD에 입력(이미 multi scale feature map이 있기 때문에, SSD Header 연산만을 사용)\n\nCornerNet\n\n\n                  \n                  CornerNet: Detecting Objects as Paired Keypoints \n                  \n                \n\n\nWe propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network.\narxiv.org/abs/1808.01244\n\n\n\nOverview\nAnchor box의 단점\nAnchor box의 수가 너무 많음(feature map의 각 pixel마다 n개의 anchor box가 생성됨)\n\nPositive Sample(객체)가 적고, 대부분이 Negative Sampe(배경)임\n\nClass Imbalance가 존재\n\n\n\nAnchor box를 사용할 때, 하이퍼 파라미터를 고려해야 함\n\nAnchor box 개수, 사이즈, 비율\n\nCornerNet\nAnchor box가 없는 1stage detector\nCenter가 아닌 Corner를 사용하는 이유\n\n중심점을 잡게 되면 4개의 면을 모두 고려해야 하는 반면, corner를 사용하면 2개만 고려할 수 있음\n\nArchitecture\n\nHourglass를 사용해서 feature map을 생성\nHourgalss\nHuman pose estimation task에서 사용\nGlobal, Local 정보 모두 추출 가능\nEncoder(Feature 추출)-Decoder(Reconstruct) 구조\nPrediction Module\n2개의 Prediction Module을 사용\n\nTop-Left 담당 모듈\nBottom-Right 담당 모듈\n\n결과\n\nHeatmaps\nEmbeddings\nOffsets\n\n두 heatmap을 통해서 예측\nH x W x C(number of category)로 구성\nCorner loss의 경우, focal loss를 변형해서 사용\n정답에 가까울 수록 적은 loss 적용\nDetecting Corner\n\nConv를 통과하면서 heatmap에 floating point loss 발생\nHeatmap에서 이미지로 위치를 다시 mapping 시킬 때 차이 발생\nOffset을 사용하여 예측한 위치를 조정\nSmooth L1 Loss 사용\n\nGrouping Corner\n한 이미지 안에서 동일 클래스 오브젝트가 존재할 수 있음\n\nTop-Left 코너와 Bottom-Right 코너의 짝을 맞춰주는 과정\nTop-Left 코너와 Bottom-Right 코너의 임베딩값 차이에 따라서 그룹을 지어줌\n\nEmbedding 값 사이의 거리가 작으면 같은 물체의 bbox에 속한다고 가정\n\n\n\nCorner Pooling\n\n코너에는 특징적인 부분이 없음\n코너를 결정하기 위해서 feature map에 corner pooling 수행\n\nHorizontal : Right to left, Horizontal max pooling\nVertical : bottom to top Vertical max pooling\n이후 embedding, offset 계산에도 사용\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-12/Week-12":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-12/Week-12","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 12/Week 12.md","title":"Week 12","links":["tags/"],"tags":[""],"content":"\nSoftware 1.0 vs Software 2.0\n\nSoftware 1.0(딥러닝이 아닌 SW)\nObject Detection Accuract Improvements\nSoftware 2.0\nSoftware 1.0 + Software 2.0\n\n\nLifecycle of an AI Project\n\nAI Research VS AI Production\n\n서비스향 AI 모델 개발 과정\n\n\nProduction Process of AI Model\n\n\nData! Data! Data!\n\nData-related tasks\nData engine / Flywheel\n\n\nOCR Technology and service\n\nOCR Technology\n\nOCR\nText Detector\nText Recognizer\nSerializer\nText Parser\n\n\nOCR Services\n\nText Extractor\nText Extractor + Natural Language Processing\nKey-Value Extractor\n\n\n\n\nIntroduce of Text Detection\n\nBasics\n\n일반 객체 영역 검출 vs 글자 영역 검출\n\n\nTaxonomy\n\nRegression-based vs Segmentation-based\nCharacter-based vs Word-based\n\n\nEAST(An Efficient add Accurate Scene Text Detector(CVPR 2017)\n\n\n데이터 소개\n\nData Collection\n\nOCR 학습 및 평가 데이터는 어디에서 오는가?\n\n\nPublic Dataset\n\nICDAR 15 : Incidental Scene Text Dataset\nICDAR 17 : Multi-Lingual Scene Text Dataset\n\n\nUFO(Upstage Format for OCR)\n\nUFO란?\nUFO Format\n\n\nChallenge Dataset\n\n\nAnnotation Guide\n\n좋은 데이터셋의 선결조건, 가이드라인\n\n가이드라인이란?\n학습 데이터셋 제작 파이프라인\n\n\nGeneral OCR Dataset 예제로 알아보는 가이드라인 작성법\n\n개요\n노하우\n가이드라인으로 데이터셋 구축하기\n\n\n\n\n성능평가 개요\n\n성능 평가의 중요성\n정량평가 &amp; 정성평가\n글자 검출 모델 평가\n\n\n데이터의 중요성\n\n양질의 데이터를 확보하는 방법\n오픈소스 Labeling Tool\n\n\nAdvanced Text Detection Models\n\nDBNet(Real-Time Scne Text Detection with Differentiable Binarization, AAAI 2020)\n\nIdea : Adaptive thresholding\nStrategy\nDifferentiable Binarization\nLoss\n\n\nMOST(A Multi-Oriented Scene Text Detector with Localization Refinement, CVPR 2021)\n\n기존 EAST 모델의 특징\n\n\nTextFusionNet(Scne Text Detection with Richer Fused Features, IJCAI 2020)\n\nPipeline\nTraining\n\n\n\n\nBag of Tricks\n\nSynthetic Data\n\nSynthText\nSynthText3D\nUnrealText\nHow to Use\n\n\nData Augmentation\n\nGeometric Transformation\n실전\n\n\nOthers\n\nLarge Scale Variation\n\n\n\n\n\n\nSoftware 1.0 vs Software 2.0\nSoftware 1.0(딥러닝이 아닌 SW)\n\n문제 정의\n큰 문제를 작은 문제들의 집합으로 분해\n개별 문제 별로 알고리즘 설계\n솔루션들을 합쳐 하나의 시스템으로\n\n\n예시\n\nTCP/IP Stack\nAndroid Stack\n\n\n\nObject Detection Accuract Improvements\n\n(SW 1.0)2008, DPM-v1 : mAP 21.0%\n(SW 1.0)2014, DPM-v5 : 33.7%\n(SW 2.0)2020, RCNN : 58.50%\n\nSW 2.0으로 바뀌면서 매우 빠른 발전 및 성능 향상이 이루어짐\nSW 1.0 → SW 2.0으로 변화하면서, 사람이 거의 개입하지 않게 바뀜\nSoftware 2.0\n뉴럴넷 구조에 의해 검색 영역이 정해짐\n따라서 SW 1.0은 사람이 고민하여 프로그램을 만든다면, SW 2.0은 AI 모델의 구조로 프로그램의 검색 범위를 한정하고, 데이터와 최적화 방법을 통해 최적의 프로그램을 찾는다\nSoftware 1.0 + Software 2.0\n전체 시스템은 SW 1.0으로 이루어저 있음\n여기서 AI는 그 중 일부만(모듈)을 SW 2.0으로 이루어짐\n\nLifecycle of an AI Project\nAI Research VS AI Production\n수업, 학교, 연구에는 정해진 데이터셋 및 평가 방식에서 더 좋은 모델을 찾는 일을 함\n서비스향 AI 모델 개발 과정\n\n\nProject Setup\n모델 요구사항 확정(처리 시간, 목표 정확도, 목표 qps, Serving 방식, 장비 사양)\n\n\nData Preparation\n데이터 셋 준비(종류, 수량, 정답)\n\n\nModel Training\n모델 학습 및 디버깅(데이터 관련 피드백, 요구사항 달성)\n\n\nDeploying\n설치 및 유지보수(성능 모니터링, 이슈 해결)\n\n\nProduction Process of AI Model\nData-Centric → 데이터만 수정하여 모델 성능 끌어올리기\nModel-Centric → 데이터는 고정시키고 모델 성능 끌어올리기\n\n모델 성능 달성에서 데이터와 모델의 비중은?\n\n\n첫 릴리즈 이전\n\nData-Centric 50%\nModel-Centric 50%\n\n\n\n이미 사용 중인 모델의 성능 개선 시\n\nData-Centric 80%\nModel-Centric 20%\n\n→ 서비스 출시 후 성능 개선 요구가 많음. 그러나 모델 구조 개선은 처리속도, qps, 메모리 크기 등에 대한 요구사항에 검증도 필요해서 비용이 크게 발생\n\n\n\n\n\nData! Data! Data!\nData-related tasks\n\n\n왜 데이터와 관련된 업무가 많을까?\n라벨링 결과에 대한 노이즈(작업의 일관되지 않은 정도)\n잘못 작업된 라벨링 결과를 학습 시, 무시하기 위해서는 적어도 깨끗이 라벨링된 결과가 2배 이상 필요\n\n\n자연스럽게 데이터를 모았을 때, 일반적인 샘플들은 작업자들이 자주 보기 때문에 라벨링 노이즈가 약하고 희귀한 샘플일 수록 라벨링 노이즈가 크다\n\n\nData engine / Flywheel\nSW 2.0의 IDE는 무엇이 필요할까\n\n\n데이터셋 시각화\n데이터, 라벨 분포 시각화, 라벨 시각화, 데이터 별 예측값 시각화, … etc\n\n\n데이터 라벨링\n라벨링 UI, 태스크 특화 기능, 라벨링 일관성 확인, 라벨링 작업 효율 확인, Auto 라벨링\n\n\n데이터셋 정제\n반복 데이터 제거, 라벨링 오류 수정\n\n\n데이터셋 선별\n모델의 성능개선을 위해서 어떤 데이터를 가져오고 라벨링해야 할까?\n\n\n\nOCR Technology and service\nOCR Technology\nOCR\n\n\nOCR : Optical Charactor Recognition\nInput Image → Text Detection → Text Recognition\nRecognizer는 Computer Vision과 NLP의 교집합 영역\nInput은 Image, Output은 Text\n\n\nSTR : Scene Text Recognition\n\n\nText Detector\n\n글자 영역 다수 객체 검출 모델\n\n이미지 입력에 글자 영역 위치들이 출력인 모델\n내부 내용과 상관 없이, 글자 영역인지 아닌지를 판단하기 때문에 단일 클래스 문제\n\n\n\nText Recognizer\n\n글자 영역 내 문자열 검출 모델\n\n내부 문자열을 읽으므로 멀티 클래스 문제\n\n\nCV와 NLP의 교집합 영역(Input : Image, Output : Text)\n\nSerializer\n\nRecognizer를 사용해서 나온 결과(문자열)을 (사람이 읽는 순서로)정렬\n딥러닝으로 하는 경우도 있으나, 일반적으로 Rule-base로 알고리즘을 만든다\n\nText Parser\n자연어 처리 모듈 중 가장 많이 사용되는 것은 기 정의된 key들에 대한 value 추출\n\nEx : 명함을 OCR 했을 때, [Name] Key에 대한 Value, [Phone Number]에 대한 Value 추출\n\n\n\n토큰화\nEx : 해,리,포,터,보,러,가,자\n\n\nBIO(Begin / Inside / Outside) Tagging\n\nBegin : 관심 있는 개채의 시작\nInside : 관심 있는 개채의 중간\nOutside : 상관 없는 요소\n\nEx\n해(Begin-movie)\n리&amp;포&amp;터(Inside-movie)\n보&amp;러(Outside)\n메(Begin-movie)\n가&amp;박&amp;스(Inside-Threater)\n가&amp;자(Outside)\n\n\nOCR Services\nText Extractor\nCopy &amp; Paste : 이미지의 글자를 OCR, 문자열로 Copy해서 Paste 할 수 있게 해줌\nText Extractor + Natural Language Processing\nSearch : 사진의 OCR 결과를 가지고 있다면, 이미지 내 글자를 ‘검색’ 할 수 있음(ex: google photo)\nMatching : Music playlist capture → copy &amp; paste\n금칙어 처리\n번역\nKey-Value Extractor\n신용카드 인식 : [카드번호], [유효기간] Key에 대한 Value를 이미지에서 추출\n신분증 인식 : [이름], [주민번호] Key에 대한 Value를 이미지에서 추출\n\nIntroduce of Text Detection\nBasics\n일반 객체 영역 검출 vs 글자 영역 검출\n예측 정보의 차이\n\n\n일반 객체 검출 : 클래스와 위치를 예측하는 문제\n\n\n글자 검출 : Text라는 단일 클래스 위치만을 예측하는 문제\n\n\n객체의 특징\n\n매우 높은 밀도\n극단적 종횡비\n특이 모양(구겨짐, 휘어짐)\n모호한 객체 영역\n크기 편차\n\nTaxonomy\nSW 1.0 → SW 2.0의 과정에서 비약적인 발전이 있었음, 사람의 개입이 최소화됨\nRegression-based vs Segmentation-based\nRegression-based : 이미지를 입력 받아 글자 영역 표현값들을 바로 출력\n\n\n주로 사각형 형상의 글자에 잘 작동함 bbox 표현 방식의 한계\n\n\nAnchor box보다 더 큰 종횡비를 갖는 경우, 검출률 떨어짐\n\n\nSegmentation-based : 이미지를 입력 받아 글자 영역 표현값들에 사용되는 화소 단위 정보를 뽑고, 후처리를 통해서 최종 글자 영역 표현 값들을 확보\n\n\n복잡하고 시간이 오래걸리는 post-processing이 필요할 수 있음\n\n\n서로 간섭이 있거나 인접한 개체 간의 구분이 어려움\n\n\nHybrid(Regression + Segmentation) : Regression으로 대략적인 영역을 구하고, Segmentation으로 해당 영역에서의 화소 정보 추출\nCharacter-based vs Word-based\nCharacter-based : Character 단위로 검출 및 조합하여 word instance를 예측\n\nCharacter-level GT 필요\n\nWord-based : Word 단위로 예측\n\n대부분의 모델이 해당\n\nEAST(An Efficient add Accurate Scene Text Detector(CVPR 2017)\n\n\n                  \n                  EAST: An Efficient and Accurate Scene Text Detector \n                  \n                \n\n\nPrevious approaches for scene text detection have already achieved promising performances across various benchmarks.\narxiv.org/abs/1704.03155\n\n\n\nIdea\n\n네트워크가 2개의 정보를 Pixel-wise로 출력\n\nScore Map : 글자 영역 중심에 해당하는지\nGeometry Map : Bounding Box의 위치는 어디인지\n\n\n\nPipeline\n\n\n\n기존의 pipeline들과 비교했을 때, 매우 단순하다\n\n\nStructure\n\n\n\nUNet 구조로 되어있다\n\n\nLocality-Aware NMS\n\n\n기존의 NMS : 복잡도가 O(N^2)으로 Dense Prediction 상황에 부적합\n\n\nIdea : 인접한 픽셀에서 예측한 Bbox들은 같은 Text instance일 가능성이 높음\n\n\n위치순서(행 우선) 탐색으로 비슷한 Bbox를 먼저 통합(IOU 기준)\n\n\n통합 시, Score Map 값으로 Weighted merge\n\n\n\n\nLoss Terms\n\n\nLoss Function = Loss for score map + loss for geometry map\nL=L_S + \\lambda_gL_g, \\lambda_g=1\nPaper : Class balanced cross entropy\n\n\n\nRealtime 까지는 아니어도, 초당 17장 처리 가능\n\n데이터 소개\nData Collection\nOCR 학습 및 평가 데이터는 어디에서 오는가?\nPublic Dataset\n\n\n라벨링된 실제 이미지를 손쉽게 확보 가능\n\n\n원하는 데이터가 없을 수 있다(이미지 도메인, 라벨링 방식 등)\n\n\n수량이 적다\n\n\nCreated Dataset\n\nSynthetic Image\n\n라벨링 작업이 필요 없다\n원하는 데이터를 빠르게 확보 가능\n실제 데이터와 얼마나 다른지 확인 필요\n\n\nReal Image\n\nCrawled Image\n\n빠르게 이미지를 모을 수 있다\n고화질 이미지 확보 가능\n다양한 샘플을 모으기 힘듬\n라이센스에 신경써야 함\n\n\nCrowd-sourced Image\n\n비용이 크고 시간이 오래 걸림\n원하는 고품질 데이터를 얻를 수 있음\n\n\n\n\n\nPublic Dataset\n서비스향 AI 모델 개발 시, 빠르게 답을 얻어야 하는 질문들\n\n\n몇 장을 학습시키면 어느 정도 성능이 나오는가?\n\n\n어떤 경우가 일반적이고, 어떤 경우가 희귀 케이스인가?\n\n\n최신 모델의 한계는 무엇인가?\n\n\nDataset 얻기(OCR의 경우)\n\n대회\n\nKaggle OCR 관련 대회\nRRC(Robust Reading) : 2년마다 열리는 OCR 전문 대회\n\n\n논문\n\nOCR Dataset 논문\nArixv(모든 ai 논문), cvpr, iccv, aaai, icdar(OCR 전문 학회)\n\n\n전문 사이트\n\n\nGoogle Datasearch(데이터 전용 검색 플랫폼)\n\n\nZenodo.org\n\n\nDatatang(데이터 유료 구매)\n\n\n\n\nOCR Data에 포함되는 것들\nBbox\nText\ndon’t care area(학습시 사용 안함)\nImage File Name\nImage Width\nImage Height\nICDAR 15 : Incidental Scene Text Dataset\n풍경 이미지 속에 우연히 글자가 잡힌 경우\n\n총 1500장의 이미지와 그에 해당하는 GT\nCare, don’t care로 구분하여 전사\n\ncare : 검출할 영역\ndon’t care : 검출하지 않을 영역(육안상 알아보기 힘들거나 라틴문자가 아닌 글자)\n\n\nx1,y1,x2,y2,x3,y3,x4,y4,transcription 형태로 이루어짐\n\ndon’t care의 경우 transcription : \n\n\n\nICDAR 17 : Multi-Lingual Scene Text Dataset\n\nMultilingual\n\n9가지 언어: Chinese, Japanese, Korean, English, French, Arabic, Italian, German and Indian\n6가지 문자: Arabic, Latin, Chinese, Japanese, Korean, Banla + Symbols, Mixed\n\n\n총 18000장\nTrain 9000(각 언어별 1000장), test 9000\nFocused Intentional Scene Text\n\n우연히 찍힌 글자가 아닌, 글자 영역을 위주로 촬영된 이미지\n글거리 표지판, 광고판, 가게 간판, 지나가는 자동차 및 웹 microblog에 올라간 유저 사진 등\n\n\nGT는 ICDAR 15와 유사\n\nUFO(Upstage Format for OCR)\nUFO란?\n각각의 Public dataset의 파일 형식(json, txt, xml, csv 등)을 하나로 통합\ndetector, recognizer, paser 등 서로 다른 모듈에서 모두 쉽게 사용할 수 있어야 함\n모델 개선을 위해 필요한 case에 대한 정보를 데이터에 포함 시킬 수 있음\nUFO Format\njson 파일 안에서 element 탐색에 쉽도록 graph structure를 기반으로 만들었음\nChallenge Dataset\npublic 150장 + private 150장\n\n데이터 source\n\n네이버 및 구글에서 크롤링된 데이터 사용\n상업적 활용이 가능한 license\n\n\n데이터 annotation 방식\n\nUFO 포맷 사용\nImage, Word 레벨 정보만 Tagging 되어 있음\n\n\n\n\nAnnotation Guide\n좋은 데이터셋의 선결조건, 가이드라인\n가이드라인이란?\n가이드라인 : 좋은 데이터를 확보하기 위한 과정을 정리해 놓은 문서\n좋은 데이터 : 골고루 모여 있고, 일정하게 라벨링된 데이터\n일관성을 위해, 원하는 작업을 명확하게 언급하는 것이 좋다\n신경써야 할 세 가지 요소\n\n특이 케이스 : 가능한면 특이케이스를 다 고려한 가이드라인이 좋음\n단순함 : 장황하게 하지 말고, 단순하게 해야 함\n명확함 : 작업자 별로 해석이 달라지지 않도록 명확해야 함\n\n학습 데이터셋 제작 파이프라인\n데이터셋 제작 파이프라인\n\n서비스 요구 사항 → 제작 목적 설정 → 가이드 라인 제작 → Raw Image 수집 → 어노테이션(라벨링) → 모델링 → 성능 및 평가 분석\n\nRaw Image 수집\n\nCrawl Images(좋은 키워드 선정)\n\nLicense 확인\n출처도 같이 크롤링\n되도록 큰 이미지 수집(나중에 얼마든지 줄일 수 있음)\n\n\nFilter Images(필터링)\n\n글자가 없는 이미지 필터링\n너무 작은 이미지 필터링\n중복 이미지 필터링\n디지털 이미지 필터링(실사가 아닌 이미지)\n배경이 투명인 경우 후처리\n\n\n\n\n\n\n\nGeneral OCR Dataset 예제로 알아보는 가이드라인 작성법\n개요\n가이드라인 제작 과정\n\n가이드 작성 → 가이드 교육 → 라벨링 → 라벨링 검수 → 데이터 검수 → 가이드 작성(업데이트)\n\n라벨링 검수 : 라벨링 노이즈 관점에서의 검수\n데이터 검수 : 데이터 분포와 라벨링 노이즈 관점에서의 검수\n\n\n초반에는 소수의 데이터를 가지고 라벨링하면서 커뮤니케이션 빈도수를 갖고 가이드라인의 틀을 어느정도 갖춘 뒤, 그 후 커뮤니케이션 빈도수를 낮추고 훈련에 사용하는 라벨 수를 높이는게 좋음\n\n노하우\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n용어설명HOLD작업을 진행하지 않고, 이미지 전첼을 제외하는 처리Points글자 영역에 대한 표시 방법TranscriptionPoints 안에 존재하는 글자 시퀸스Illegibility글자 번짐, 잘림 들으로 인해 글자를 정확히 알아보기 힘들 경우 모델이 의도적으로 무시하도록 표시  영역 단위의 처리로, Transcription 대상이 되는 Points처럼 타이트하게 영역을 지정할 필요는 없음Image_Tags이미지 자체에 특이사항이 있는 경우, 내용 표시Word_Tags글자 영역의 특이사항이 있는경우, 내용 표시글자 영영 내에 글자가 있지만, 매우 특수하거나 실제로 인식하여 출력하기 어려운 글자의 경우 입력해주는 값  인식 대상이 아닌 글자(특수기호,한글, 알파벳이 아닌 글자)를 Transcription할 때 표시\n가이드라인으로 데이터셋 구축하기\n라벨링이 잘 되고 있는지 검수하는 방법\n\n\n감독자 전수 검사\n한 감독자가 본인에게 할당된 작업자의 결과물 모두 시각화하에 문제가 없는지 확인하고 문제가 있을 시에는, 문제 있는 부분을 기록하여 “다른” 작업자에게 살당\n\n\nPeer check\n끝난 작업물을 다른 작업자에게 할당하여 틀린 부분을 찾아서 고치게 함\n\n\n다수결\n여러 사람이 동일한 작업을 진행하고, 그 결과를 프로그래밍저긍로 하나로 합침\n\n\n가이드 자체가 잘 동작하고 있는지를 검수\n\n\n초기에 소량씩 완성본 받아서 품질을 확인\n\n\n작업자 QnA 활용\n\n\n추가 수정을 위한 비용과 시간이 크다면 어느정도 포기\n\n\n가이드라인 만들기는 끝없는 의사소통과 수정의 연속\n\n\n충분한 Tagging을 바탕으로 가이드 제작\n\n\n가이드라인 수정 시 Versioning 필요, 기존 내용과 충돌 없도록 최소한의 변경만 할 것\n\n\n최대한 명확하고 객관적인 표현을 사용\n\n\n일관성 있는 데이터가 가장 잘 만들어진 데이터\n\n\n우선순위를 알고, 필요하다면 포기하는 것도 중요\n\n\n\n성능평가 개요\n성능 평가의 중요성\n성능평가 = 새로운(학습되지 않은) 데이터가 들어왔을 때 얼마나 잘 동작하는가?\nTrain - Test로 Dataset을 분리함\nTrain Set의 일부는 Validation으로 사용하기도 함\n\n그러나 Validation Set을 고정할 경우, 데이터 셋의 크기가 작으면 성능 평가의 신뢰성이 떨어지 게됨 만약 Validation Set을 어떻게 잡느냐에 따라 성능이 달라진다면, 우연의 효과로 인해 모델 평가 지표에 편향이 생겨버림\n\n\nK-Fold Cross Validation\n\n\nTrainSet을 TrainSet + ValidationSet으로 나우기 위해 K개의 Fold로 나누고 아래와 같이 k번의 Cross-Validation을 수행\nK번 수행하므로, 총 K개의 성능 결과가 나오며, K의 평균을 학습 모델의 성능으로 본다\n\n\n\n\n\n정량평가 &amp; 정성평가\n정량평가 : 모델의 성능을 알고리즘을 통해 수치적으로 표현\n정성평가 : 사람이 직접 수치적으로 평가(ex : bbox를 잘 맞춘 정도는? 0~1)\n→ 해당 모델이 서비스로 출시 되었을 때 사람들이 체감하는 품질의 정도\n글자 검출 모델 평가\n두 영역 간의 매칭 판단 방법(매칭 행렬 계산) + 매칭 행렬에서 유사도 수치 계산 방법(유사도 계산)이 정의되야 함\n\n\n두 영역 간 매칭 판단(Task에 따라 선정해야 함)\n\n\nOne-to-One Match : GT Bbox 1개에 1개가 Bbox가 Predict 됨\n\n\nOne-to-Many Match : GT Bbox 1개에 여러 개의 Bbox가 Predict 됨\n\n\nMany-to-One Match : 여러 GT Bbox를 1개의 Bbox가 Predict로 합쳐져서 Predict 됨\n\n\nex : IOU, DetEval[link], TIoU, CLEval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne-to-OneOne-to-ManyMany-to-OneIoU허용허용지양 (0.8로 Penalty)DetEval허용(IoU&gt;0.5만 1)지양 (0으로 Penalty)지양 (0으로 Penalty)\n\n\nCLEval(Character-Level Evaluation)\n얼마나 많은 글자(Character)를 맞추고 틀렸는지로 평가\nDetection 뿐 아니라 end-to-end, recognition에 대해서도 평가 가능\n\n\n평가를 위해 Character 별 Center Position이 필요\n\n\nPCC(Pseudo Character Centers) 알고리즘으로 글자 수를 사용해서 획득, 꽤 정확하게 구할 수 있음\n\n\n정답영역 기준 Score : (CorrectNum - GranualPenalty) / TotalNum\n일종의 Recall로 볼 수 있음\n\nCorrectNum : 정답 영역 내 PCC 중 어느 예측 영역이라도 속하게 된 PCC의 개수\nGranualPenalty : 정답 영역 내 PCC를 포함하는 예측 영역의 개수 - 1\nTotalNum : 정답 영역 내 PCC 개수\n\n\n\n예측영역 기준 Score : (CorrectNum - GranualPenalty) / TotalNum\n일종의 Precision으로 볼 수 있음\n\nCorrectNum : 이 예측영역이 포함하고 있는 PCC 별로, 해당 PCC를 포함하는 예측 영역의 개수로 나눈 값들의 합\nGranualPenalty : 예측 영역과 연관된 정답 영역의 개수 - 1\nTotalNum : 이 예측 영역이 포함하고 있는 PCC의 개수\n\n\n\n\n\n\n\n\n\n\n\n두 영역 간 유사도 판단\nex : Area Recall, Area Precision\n\n\n\n데이터의 중요성\n양질의 데이터를 확보하는 방법\n\nPeople\n\n가이드 숙지력\n일관된 작업\n작업 효율성\n특이 케이스에 대한 대응력(태스크, 모델에 대한 이해가 있는 사람들은 처음 보는 경우도 거의 동일하게 작업)\n\n\nProcess\n\n일관된 작업을 보장하기 위한 프로세스 정립\n작업마다 최적의 프로세스가 다를 수 있으므로 유연성도 필요\n\n\nTool\n\n\n작업 효율성을 올리기 위한 UX / 자동화 / 부가 기능\n\n\n커뮤니케이션 효율화를 위한 게시판, 댓글 기능\n\n\n\n\n오픈소스 Labeling Tool\n\nLabelMe\n\nMIT CSAIL(Computer Science Artificial Intelligence Laboratory)에서 공개한 Image data annotation 도구를 참고하여 만든 오픈소스\nPolygon, Circle, Rectangle, Line, Point의 Annotation 수행 가능\n장점 : 설치의 편함(Python), 커스터마이징 쉬움\n단점 : 공동작업 불가, Object 및 Image에 대한 속성 부여 불가\n\n\nCVAT(Computer Vision Annotation Tool)\n\nIntel OpenVINO 팀에서 제작한 공개 CV 데이터 제작 도구\nImage Video 등 일반적인 CV Task에서 필요한 Annotation 기능을 모두 포함\n주로 Object Detection, Image Segmentation, Image Classification 등에 사용\n장점\n\n다양한 Annotation 지원\nAutomatic annotation\n온라인 사용 가능, 오픈소스이므로 on-premise로 사용도 가능\nMulti-user 기반 annotation 가능, assignee, reviewer 기능 제공\n\n\n단점\n\nmodel inference가 느림(CPU)\nobject, image에 대한 속성을 부여하기 까다로움\n\n\n\n\nHasty Labeling Tool\n\nCVAT와 유사, 그러나 Annotation은 전체 솔루션의 일부\n데이터 제작 / 모델학습 / 서빙 / 모니터링까지 전체를 쉽게 할 수 있는 솔루션 제공\n장점\n\n다양한 Annotation을 지원\nSemi-Automated Annotation 기능 지원\nCloud Storage 활용 가능\nMulti-user 기반 annotation 가능, assignee, reviewer 기능 제공\n\n\n단점\n\n\nFree credit 소진 후에는 과금 필요\n\n\nAnnotator가 수동으로 이미지마다 review, stats로 변경 필요\n\n\nHasty 플랫폼에 강하게 연결되어 있어, Annotation 도구에 대한 커스터마이징 불가능\n\n\n\n\n\n\nUpstage Labeling Tool과 비교\n\n\nAdvanced Text Detection Models\nDBNet(Real-Time Scne Text Detection with Differentiable Binarization, AAAI 2020)\n\n\n                  \n                  Real-time Scene Text Detection with Differentiable Binarization \n                  \n                \n\n\nRecently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text.\narxiv.org/abs/1911.08947\n\n\n\nIdea : Adaptive thresholding\n→ 글자 영역 구분에 Threshold 값을 이미지 별로 모델이 일아서 정하도록 해보자\n\nStrategy\n\n글자 영역 경계 부분에서만 높은 임계치를 적용, 나머지 영역은 낮은 임계치를 적용\n글자영역은 경계부분을 제외한 영역으로 정의\n\n\n그러나 기존의 Threshold는 미분 불가능\n→ Differentiable Binarization 제안\nDifferentiable Binarization\nB_{i,j}={1 \\over {1+e^{-k(P_{i,j}-T_{i,j})}}}\nLoss\n\n\nBCE Loss\n\nSegmentation map + Binarization ↔ GT Probability map\n\n\nL1 Loss(Threshold Map ↔ GT Threshold Map\n\nGT Probability Map에서 image processing을 사용하여 생성\n\n\n\nMOST(A Multi-Oriented Scene Text Detector with Localization Refinement, CVPR 2021)\n\n\n                  \n                  MOST: A Multi-Oriented Scene Text Detector with Localization Refinement \n                  \n                \n\n\nOver the past few years, the field of scene text detection has progressed rapidly that modern text detectors are able to hunt text in various challenging scenarios.\narxiv.org/abs/2104.01070\n\n\n\n기존 EAST 모델의 특징\n\n단순하고 빠름\nExtreme aspect ratio sample에 대해 성능이 많이 떨어짐\n\n\nReceptive Field의 한계\n\n\nLA-NMS의 문제점\n\n\n\n\nMost : EAST의 보완된 버전\n\n\n\nTFAM(Text Feature Aligned Module)을 추가 Receptive Field를 극복\n\nDeformable Convolution 사용\n\n\n\nPA-NMS(Position-Aware NMS) NMS에 필요한 글자 영역 내 화소들의 상대 위치 정보를 받아서, 검출 결과에 PA-NMS 적용\n겹치는 부분, 침범하는 부분 등 많은 문제에 대해서 성능 향상을 이룸\n\n\nTextFusionNet(Scne Text Detection with Richer Fused Features, IJCAI 2020)\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.ijcai.org/proceedings/2020/0072.pdf\n\n\n\nPipeline\n\n\n\n\nSemantic Segmentation Branch : 이미지 전체에서 글자 영역을 추출, Global Level Feature 획득\n\n\nDetection Branch : 글자 영역별로 Global / Word Level Feature를 활용, 글자영역 검출과 글자 위치 검출\n\n\nMask Brahcn : Global / Word / Character Level Feature를 활용하여 글자영역 추출과 글자 추출\n\n\nTraining\nText Fusion의 Detection Branch 학습을 위해서는 글자 단위의 Labeling이 필요\n→ Weakly supervised learning 사용\n\n\nSynthTest(Full-supervision) pretraining\n글자단위 label 확보가 가능한 합성 데이터로 먼저 학습\n\n\nPseudo Labeling\nGT 단어영역 넓이에 비해 예측 영역과 GT 단여 영역 간의 IOU가 0.8 보다 큰 Pseudo Label만 학습에 사용\n\n\nFine-tuning\n\n\n\nBag of Tricks\nSynthetic Data\n성능 향상에 있어서 가장 중요한 것은 Data\n그러나 Real Data를 확보하는 것은 어려운 일\n→ Real Data 확보 : Public Data 가져오기 + 직접 만들기\n직접 만들 경우, Annotation을 직접 만들어야 함\n→난이도가 높고, 비용이 많이 들어가는 작업\nSynthetic Data(합성 데이터) : Real Data 수집에 대한 부담을 덜어준다\n\n장점 : 비용이 훨씬 적게 든다\n개인정보나 라이센스에 관한 제약으로부터 자유롭다\n더 세밀한 수준의 annotation도 쉽게 얻을 수 있다(Character-level, pixel-level)\n\nSynthText\n\n\n                  \n                  Synthetic Data for Text Localisation in Natural Images \n                  \n                \n\n\nIn this paper we introduce a new method for text detection in natural images.\narxiv.org/abs/1604.06646\n\n\n\nSynthetic Data for Text Localisation in Natural Images, CVPR 2016 논문과 함께 데이터셋 발표\n800K Dataset + 글자 이미지 합성 코드 제공\n현실에 있을법한 위치에만, 표면 모양에 맞춰 글자를 합성\n→ Depth Estimation 수행\nSynthText3D\n\n\n                  \n                  SynthText3D: Synthesizing Scene Text Images from 3D Virtual Worlds \n                  \n                \n\n\nWith the development of deep neural networks, the demand for a significant amount of annotated training data becomes the performance bottlenecks in many fields of research and applications.\narxiv.org/abs/1907.06007\n\n\n\n3D 가상 세계를 이용한 텍스트 이미지 합성\n이미 Geometry 정보를 갖고 있기 때문에 더 쉽게 합성 가능\nWorld 선택 → View Setting → Illumination Setting으로 다양한 Synthetic Image 생성 가능\nUnrealText\n\n\n                  \n                  UnrealText: Synthesizing Realistic Scene Text Images from the Unreal World \n                  \n                \n\n\nSynthetic data has been a critical tool for training scene text detection and recognition models.\narxiv.org/abs/2003.10608\n\n\n\nSynthText3D와 매우 유사, UnrealText는 기존 SynthText3D에서 View의 설정을 자동화\n\n기존 SynthText3D에서 사람이 직접 View를 생성한 이유는 현실에 존재할 수 없는 View가 생기는 것을 방지하기 위함\nUnrealText는 View 설정을 자동화 하면서도, 현실에 존재할법한 View를 생성\n\nUnreal Text는 존재할만한 View를 자동으로 생성\n\nHow to Use\n\nPretraining\n\nTarget Dataset만 존재할 때\n\nIamgeNet pretrained model로 부터 backbone load\nTaget dataset에 대해 fine-tuning 진행\n\n\n합성 데이터가 주어졌을 때\n\nImageNet pretrained model로 부터 backbone load\nSynthetic Dataset으로 한번 더 pretraining\nTarget dataset에 대해 fine-tuning 진행\n\n\n\n\nWeakliy Supervised Learning\n\nCharactor-level detection을 수행하는 모델\n\nCRAFT, TextFuseNet 등\nReal Dataset은 대부분 Word-level Annotation만 포함하기 때문에 Full supervision이 힘듬\n\n\n\n\n\nData Augmentation\nGeometric Transformation\nAugmentation이 모델의 False-Positive를 유발하지 않기 위한 Rule 예시\n\nPositive ratio 보장 : 최소 1개의 개체를 포함해야 한다\n\n발생하는 문제 : 글자와 멀리 있는 배경에서는 hard negative sampling이 잘 되지 않는다\n시도해볼 수 있는 해결방법 : pretrained model이 false positive를 발생시키는 patch를 수집한다(=Hard negative mining)\n\n\n개체 잘림 방지 : 잘리는 개체가 없어야 한다\n\n발생하는 문제 : 글자들이 많이 밀집된 곳에서는 Sampling이 잘 되지 않는다\n시도해볼 수 있는 해결방법 : 최소 1개의 개체는 잘리지 않고 포함하게 한다, 잘린 것들은 Masking을 통해 학습에서 무시한다\n\n\n\n실전\n도메인 특징에 따라 다양한 문제가 발생할 수 있음\n\n실제로 모델에 입력되는 이미지 관찰 필요\nLoss가 크게 발생하는 영역들을 분석해서 Rule을 업데이트하는 작업이 필요\n\nOthers\nLarge Scale Variation\n이미지에서 글자는 매우 다양한 크기로 나타남\nScale Variation은 난이도를 높이는 주요 원인\n작은 글자들 : Miss detection\n큰 글자들 : Broken / partial detection 발생\nSNIP : Scale Normalization for image Pyramid\n\n\nScale augmentation을 적용하나 개체의 크기가 적정 범위를 벗어나지 않도록 제한\n\n\n크기가 적정 범위를 벗어난 개체들에 대해서는 gradient 전파를 하지 않는 방식으로 학습에서 제외\n\n\nAdaptive Scaling\n\n\n                  \n                  It&#039;s All About The Scale -- Efficient Text Detection Using Adaptive Scaling \n                  \n                \n\n\n“Text can appear anywhere”.\narxiv.org/abs/1907.12122\n\n\n\n\n이미지에서 글자가 있을법한 영역만, 글자가 적정 크기로 나타나도록 조정해서 모델에 입력\n\n\n\n글자의 위치와 크기를 대략적으로 예측\n\nSeg Mask 예측 : 글자가 있는 위치에 대한 Pixel wise map\nScale Mask 예측 : 글자 Scale에 대한 Pixel wise map\n\n\n\n그 결과를 기반으로 재구성한 이미지를 다시 입력\n\n\nCanonical knapsacks을 생성(텍스트들만들 다시 짜집기하여 만든 새로운 이미지)\n\n\n\nCanonical knapsacks 이미지에서 예측 수행\n\nCanonical knapsacks 이미지는 적은 배경만을 갖기 때문에 매우 경제적\n글자들의 크기가 적정 크기로 통일되기 때문에 Scale Variation으로 인한 성능 저하가 없음\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14/Week-14":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14/Week-14","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 14/Week 14.md","title":"Week 14","links":[],"tags":[],"content":"\nSegmentation Overview\n\nSegmentation History\nCOCO(Common Objects in COntext)\n\n\nSemantic Segmentation 기초와 이해\n\nTransposed Convolution\nFCN(Fully Convolutional networks for semantic segmentation)\nFCN의 성능을 향상시키기 위한 방법\n\n\nFCN의 한계와 극복한 모델들 1\n\nFCN의 한계점\nDecoder를 개선한 Model들\n\nDeconvNet\nSegNet\nDeconvNet vs SegNet\n\n\nSkip Connection을 적용한 Model들\n\nFC DenseNet\nUnet\n\n\nReceptive Field를 확장시킨 Model들\n\nDeepLab v1\nDilatedNet\n\n\n\n\nFCN의 한계와 극복한 모델들 2\n\nReceptive Field를 확장시킨 Model들\n\nDeepLab v2\nPSPNet\nDeepLab v3\nDeepLab v3+\n\n\n\n\nHigh Performance를 자랑하는 UNet 계열의 모델들\n\nUNet\n\nUNet 개요\nUNet Architecture\nData Augmentation\n한계점\n\n\nUNet++\n\nUNet++ 개요\n한계점\n\n\nUNet3+\n\nUNet, UNet++의 한계점\nUNet3+에 적용된 Techniques\n\n\nAnother Version of the UNet\n\n\nSemantic Segmentation 대회에서 사용하는 방법들\n\nEfficientUNet Baseline\n\nModel 불러오기\n\n\nBaseline 이후에 실험해봐야할 사항들\n\nAugmentation\nSOTA Model\nScheduler\nOptimizer / Loss\n\n\nEnsemble\nPseudo Labeling\n그 외\n\n\nSemantic Segmentation 연구 동향\n\nHRNet\n\nHRNet의 구성 요소\n\n\nWSSS\n\n\n\n\nSegmentation Overview\nSegmentation History\n\n\n                  \n                  Image Segmentation Using Deep Learning: A Survey \n                  \n                \n\n\nImage segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others.\narxiv.org/abs/2001.05566\n\n\n\n\nCOCO(Common Objects in COntext)\n\n\n                  \n                  COCO - Common Objects in Context \n                  \n                \n\n\ncocodataset.org/#home\n\n\n\n\n\n                  \n                  Introduction to the COCO Dataset - OpenCV \n                  \n                \n\n\nWith applications such as object detection, segmentation, and captioning, the COCO dataset is widely understood by state-of-the-art neural networks.\nopencv.org/introduction-to-the-coco-dataset/\n\n\n\n \ndataset = {\n    “info”: {\n        “year”: 2021,\n        “version”: 1.2,\n        “description:” “Pets dataset”,\n        “contributor”: “Pets inc.”,\n        “url”: “sampledomain.org”,\n        “date_created”: “2021/07/19” \n    },\n    “licenses”: [{\n        “id”: 1,\n        “name”: “Free license”,\n        “url:” “sampledomain.org”\n    }],\n    “categories”: [\n        {“id”: 1, \n         “name”: ”poodle”, \n         “supercategory”: “dog”, \n         “isthing”: 1, \n         “color”: [1,0,0]},\n        {“id”: 2, \n         “name”: ”ragdoll”, \n         “supercategory”: “cat”, \n         “isthing”: 1, \n         “color”: [2,0,0]}\n    ],\n    “image”: [\n        {\n        “id”: 934,\n        “width”: 640,\n        “height”: 640,\n        “file_name: “84.jpg”,\n        “license”: 1,\n        “date_captured”: “2021-07-19  17:49”\n        }\n        ]\n“annotations”: [\n        {\n        ”segmentation”:\n            {\t\n            “counts”: [34, 55, 10, 71]\n            “size”: [240, 480]\n            },\n        “area”: 600.4,\n        “iscrowd”: 1,\n        “Image_id:” 122214,\n        “bbox”: [473.05, 395.45, 38.65, 28.92],\n        “category_id”: 15,\n        “id”: 934\n        },\n        {\n        ”segmentation”: [[34, 55, 10, 71, 76, 23, 98, 43, 11, 8]],\n        “area”: 600.4,\n        “iscrowd”: 1,\n        “Image_id:” 122214,\n        “bbox”: [473.05, 395.45, 38.65, 28.92],\n        “category_id”: 15,\n        “id”: 934\n        }\n        ]\n}\nSemantic Segmentation 기초와 이해\nTransposed Convolution\n\n\n                  \n                  딥러닝에서 사용되는 여러 유형의 Convolution 소개 \n                  \n                \n\n\nAn Introduction to different Types of Convolutions in Deep Learning 을 번역한 글입니다.\nzzsza.github.io/data/2018/02/23/introduction-convolution/\n\n\n\n\n\n                  \n                  14.10. Transposed Convolution - Dive into Deep Learning 1.0.0-beta0 documentation \n                  \n                \n\n\nThe CNN layers we have seen so far, such as convolutional layers () and pooling layers (), typically reduce (downsample) the spatial dimensions (height and width) of the input, or keep them unchanged.\nd2l.ai/chapter_computer-vision/transposed-conv.html\n\n\n\n\n\n                  \n                  What are deconvolutional layers? \n                  \n                \n\n\nI recently read Fully Convolutional Networks for Semantic Segmentation by Jonathan Long, Evan Shelhamer, Trevor Darrell.\ndatascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers\n\n\n\n\n2x2 Transposed Convolution (stride = 1)\n\n2x2 Transposed Convolution (stride = 2)\n피처맵의 공간 차원을 증가(Up Sampling)을 위해 사용하는 연산\nDeconvolution이라는 용어와 혼용하지만, 사실 구분해야 한다\n\n왜 ‘Transposed’라고 표현할까? 연산 방식과 깊은 관련이 있다\n\n\n일반적인 Convolution\n\n\n\nTransposed Convolution\n\n\n\n\n\nFCN(Fully Convolutional networks for semantic segmentation)\n\n\n                  \n                  Fully Convolutional Networks for Semantic Segmentation \n                  \n                \n\n\nConvolutional networks are powerful visual models that yield hierarchies of features.\narxiv.org/abs/1411.4038\n\n\n\n\n\n\n기존의 VGG에서 FC Layer들을 1x1 Conv로 변경\n\n\n마지막에 Transposed Convolution을 사용하여 Upacaling 수행, 입력 크기만큼 복원\n\n\nFCN의 성능을 향상시키기 위한 방법\nSkip Connection 사용\n\n\nMax Pooling에 의해서 잃어버린 정보를 복원해주는 작업을 진행\n\n\nUpsampled Size를 줄여주기에 좀 더 효율적인 이미지 복원 가능\n\n\nSkip Connection을 사용해 Pooling 전 Feature와 합성후 다시 Upsacaling 수행\n\nFCN-8s Network(www.researchgate.net/figure/Fully-convolutional-neural-network-architecture-FCN-8_fig1_327521314)\n\n\n\nFCN의 한계와 극복한 모델들 1\nFCN의 한계점\n객체의 크기가 크거나 작은 경우 예측을 잘 하지 못하는 문제\n\n\n큰 Object의 경우 지역적인 정보만으로 예측\n\n버스의 앞 부분은 버스로 예측하지만, 버스 유리창에 미친 자전거를 보고 자전거로 인식하는 문제도 발생\nConvolution Layer의 Receptive Field의 문제로 볼 수 있음\n\n\n\n작은 Object가 무시되는 경향 존재\n\n\nObject의 디테일한 모습이 사라지는 문제\n\n\nUpscaling 방식이 너무 간단해 경계를 학습하기 어려움\n\n\nDecoder를 개선한 Model들\nDeconvNet\n\n\n                  \n                  Learning Deconvolution Network for Semantic Segmentation \n                  \n                \n\n\nWe propose a novel semantic segmentation algorithm by learning a deconvolution network.\narxiv.org/abs/1505.04366\n\n\n\n\nConv Network는 VGG16 사용\n\n\n13개의 층\n\n\nReLU와 Pooling이 Convolution 사이에서 이루어짐\n\n\n7x7 Conv와 1x1 Conv 활용\n\n\nDeconvolution Network\n\n\nUnpooling\n\n디테일한 경계를 포착\nPooling의 경우, 노이즈를 제거하지만 정보가 손실되는 문제가 존재\nUnpooling을 통해 Pooling 시 지워진 경계의 정보를 기록했다 복원\n학습의 영역이 아니기 때문에 빠른 속도\n그러나 Sparse한 Activation Map을 가짐\n\nTransposed Convolution이 이를 보완\n\n\n\n\nDeconvolution(Transposed Convolution)\n\n전반적인 모습을 포착\n\n\nReLU\n\n\n깊은 계층에 대한 deconvolution일수록 디테일한 형상을 갖고 있음\n\n기존 FCN에 비해 더 디테일한 Predict 가능\n\n\nSegNet\n\n\n                  \n                  SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation \n                  \n                \n\n\nWe present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet.\narxiv.org/abs/1511.00561\n\n\n\nRoad Scene Understanding applications 분야에서 Semantic Segmentation을 수행하기 위해 개발\n\n\n차량, 도로, 차선, 건물, 보도, 하늘, 사람 들의 Class를 빠르게 구분할 수 있어야 함\n\n\n기존 DeconvNet의 경량화 버전\n\n\n\nDeconvNet vs SegNet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeconvNetSegNetEncoder / Decoder Network가 대칭인 구조Encoder / Decoder Network가 대칭인 구조Encoder Network  → VGG 16으로 Encoding  → 13개의 층  → Conv / BN / ReLU / Pooling  →  FC Layer로 7x7 Conv 및 1x1 ConvEncoder Network  → VGG 16으로 Encoding  → 13개의 층  → Conv / BN / ReLU / Pooling  →  중간의 1x1 Conv 제거Decoder Network  → Unpooling +  Deconvolution + ReLUDecoder Network  → Unpooling +  Convolution + ReLU\nSkip Connection을 적용한 Model들\nFC DenseNet\n\n\n                  \n                  Densely Connected Convolutional Networks \n                  \n                \n\n\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output.\narxiv.org/abs/1608.06993\n\n\n\n\n\n이전 Layer의 정보를 Concatenation해서 다음 Layer로 전달\nUnet\n\n\n                  \n                  U-Net: Convolutional Networks for Biomedical Image Segmentation \n                  \n                \n\n\nThere is large consent that successful training of deep networks requires many thousand annotated training samples.\narxiv.org/abs/1505.04597\n\n\n\n\nEncoder의 정보를 Skip connection을 통해 decoder로 전달, Concatenation 후 다음 Decoder Layer로 전달\nReceptive Field를 확장시킨 Model들\nDeepLab v1\n\n\n                  \n                  Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs \n                  \n                \n\n\nDeep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection.\narxiv.org/abs/1412.7062\n\n\n\nReceptive Field : 뉴런이 바라보고 있는 입력 영역의 크기\n\n\nReceptive Field가 클 수록 전체적인 정보를 통해 추론하므로, 더 잘 검출할 수 있을 것이라고 기대 할 수 있음\n\n\n\nConv → Maxpooling → Conv → …\n\n\n메모리 저감\n\n\n노이즈 저감\n\n\n효율적인 Receptive Field 확장\n\n\n하지만 Segmantation에서는 Low Feature resolution을 가지는 문제 발생\n\n\n\n\nDilated Convolution을 사용, Receptive Field 확장\n\nDilated Convolution[link]\n\n\nDense CRF를 사용해 Predict Image Upscaling\nDense CRF\n\n\n\n이미지 분할 문제를 해결하기 위해 사용되는 확률 그래픽 모델의 일종\n\n\nCRF를 사용하면 이미지의 색상과 텍스처 등의 특징을 기반으로 이미지의 각 픽셀을 특정 클래스에 할당\n\n\nCRF 모델은 픽셀 간의 관계를 고려하여 이미지의 더 정확한 분할이 가능\n\n\n이미지의 먼 부분 간의 관계를 잡아내기 어려운 전통적인 CRF 모델의 개선 버전으로 제안\n\n\n이미지 내의 모든 픽셀 간의 관계를 모델링하기 위해 완전히 연결된 그래프를 사용\n\n\n이웃 픽셀만 모델링, 픽셀 간의 관계를 더 자세하게 분석할 수 있게 해 세분화 결과의 정확도를 향상\n\n\nDilatedNet\n\n\n                  \n                  Multi-Scale Context Aggregation by Dilated Convolutions \n                  \n                \n\n\nState-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification.\narxiv.org/abs/1511.07122\n\n\n\nDeeplab v1의 개량형 모델\n\nFCN의 한계와 극복한 모델들 2\nReceptive Field를 확장시킨 Model들\nDeepLab v2\n\n\n                  \n                  DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs \n                  \n                \n\n\nIn this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit.\narxiv.org/abs/1606.00915\n\n\n\nDilatedNet, DeepLab v1 모두 Dilated Convolution을 사용해 Receptive Field를 키웠음\nDeepLab v1의 단점\n\n\n모든 풀링 계층에 대해 단일 속도만 사용하여 Receptive Field의 크기가 제한됨\n\n\nUp Sampling은 간단히 특징 맵을 반복하여 단순하게 수행됨\n\n\n본 문제들을 해결하기 위해, DeepLab v2는 multi-scale dilated convolution을 사용하는 것을 제안\n\n\n\n하나의 비율 대신, DeepLab v2는 각 레이어마다 비율이 증가하는 multiple rates를 사용\n더 큰 Receptive Field를 갖게 하여 더 나은 segmentation 성능 달성 가능\n단순한 Up Sampling 대신 Up Sampling에 bilinear interpolation을 사용\n\nPSPNet\n\n\n                  \n                  Pyramid Scene Parsing Network \n                  \n                \n\n\nScene parsing is challenging for unrestricted open vocabulary and diverse scenes.\narxiv.org/abs/1612.01105\n\n\n\n\n\n                  \n                  Object Detectors Emerge in Deep Scene CNNs \n                  \n                \n\n\nWith the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.\narxiv.org/abs/1412.6856\n\n\n\n도입 배경\n\n\nMsimatched relationship\n\n객체들간의 특징을 catch하지 못함\nex : 호수 주변에 boat가 있는데, 기존 모델(FCN)은 Car로 예측\nidea : 주변의 특징을 고려\n\n\nConfusion Categories\nInconspicuous Classes\n\n\n작아서 예측을 하지 못함\n\n\nex : 배개와 이불이 같은 무늬일 때, 기존 모델(FCN)은 배개를 이불로 예측\n\n\nidea : 작은 객체들도 global contextual information 사용\n\n\n\n\nReceptive Filed가 큰데, 왜 이런 문제가 발생할까?\n\nOBJECT DETECTORS EMERGE IN DEEP SCENE CNNS의 저자는 이론적인 Receptive Field의 크기와 실제 Receptive Field의 크기가 다르다고 주장\n\n실험결과, Pooling이 진행될수록 오차가 커짐\n\n\n\n\nPSP Net의 Idea\n\n\n\nGlobal Average Pooling을 사용\n\n\nGloabl Average Pooling\n\n\n다양한 양의 비율의 풀링을 연결하여 발생된 각 Feature Map을 객체 간의 상관관계를 잡아낼 수 있게 함\n\n\n따라서 객체간의 관계를 잡아낼 수 있게되고, 또한 작은 객체들도 고려할 수 있게 됨\n\n\n이를 통해 이미지 픽셀 분할을 모델링하기 위해 완전히 연결된 그래프를 사용하는 CRF 모델의 단점을 극복할 수 있게 됨\n\n\nDeepLab v3\n\n\n                  \n                  Rethinking Atrous Convolution for Semantic Image Segmentation \n                  \n                \n\n\nIn this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter’s field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation.\narxiv.org/abs/1706.05587\n\n\n\n\n\nDeepLabV2 모델의 개선 버전\n\n기존 DeepLab v2 버전에서 Atrous Spatial Pyramid pooling을 수정, Global Average Pooling이 추가\n\n\n확장된 Receptive Field를 사용하고 이미지 분할의 정확도를 향상시키기 위해 Dilated Convolution을 사용\nEncoder-Decoder 구조\nAtrous Convolutional Neural Network (CNN)을 사용해 결과물의 해상도를 높임\nMulti-Scale Feature Learning과 간단하면서도 효과적인 Decoder Module을 소개\nFully Connected Conditional Random Field (CRF)을 사용하여 분할 결과를 더욱 개선\n\nDeepLab v3+\n\n\n                  \n                  Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation \n                  \n                \n\n\nSpatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task.\narxiv.org/abs/1802.02611\n\n\n\n\n\nEncoder에서 Spatial diemsion의 축소로 인해 손실된 정모를 Decoder에서 점진적으로 복원\nEncoder\n\n\n수정된 Xception을 backbone으로 사용\n\n\n\nAtrous separable convolution을 적용한 ASPP 사용\n\n\nBackbone 내 low-level feature와 ASPP 모듈 출력을 모두 decoder에 전달\n\n\n\nDecoder\n\nASPP 모듈의 출력을 Up Sampling(bilinear)해서 Low-level feature와 결합\n결합된 정보는 Conv 연산 후 Up Sampling되어 최종 결과 도출\n기존의 단순한 Up Sampling 연산을 개선시켜 detail을 유지하도록 함\n\n\n\nHigh Performance를 자랑하는 UNet 계열의 모델들\nUNet\n\n\n                  \n                  U-Net: Convolutional Networks for Biomedical Image Segmentation \n                  \n                \n\n\nThere is large consent that successful training of deep networks requires many thousand annotated training samples.\narxiv.org/abs/1505.04597\n\n\n\nUNet 개요\n의료계열은 데이터를 확보하기 어려움\n기본적인 DL 모델들은 파라미터 수가 많고 네트워크가 깊어 많은 data 필요\nCell segmentation의 경우, 같은 클래스가 인점해 있는 셀 사이의 경계 구분 필요\nUNet Architecture\n\nData Augmentation\nRandom Elastic deformation을 통해 augmentation 수행\n한계점\n기본적으로 깊이가 4로 고정되어 있음\n\n데이터셋마다 최고의 성능을 보장하지 못함\n최적깊이 탐색비용 상승\n\n단순한 Skip Connection\n\n동일 깊이를 가지는 Encoder와 Decoder만 연결되는 제한적인 구조\n\nUNet++\n\n\n                  \n                  UNet++: A Nested U-Net Architecture for Medical Image Segmentation \n                  \n                \n\n\nIn this paper, we present UNet++, a new, more powerful architecture for medical image segmentation.\narxiv.org/abs/1807.10165\n\n\n\nUNet++ 개요\n\nUNet의 한계점을 극복하기 위해 새로운 아키텍처 제시\n\nEncoder를 공유하는 다양한 깊이의 UNet 생성\n동일한 깊이에서의 Feature Map들이 모두 결합되도록 Skip Connection 수행\n\nEnsemble 효과\n\n\nDeep Supervision Loss 사용\n\nX^(0,1) &amp; X^(0,2) &amp; X^(0,3) &amp; X^(0,4)에 대하여 각각 Loss 계산\n최종 Loss는 위 4개의 Loss의 평균\n\n\n\n한계점\n\n복잡한 Connection으로 인한 Parameter 증가\n많은 Connection으로 인한 메모리 증가\nEncoder-Decoder 사이에서의 Connection이 동일한 크기를 갖는 Feature map에서만 진행됨\n\nFull scale에서 충춘한 정보를 탐색하지 못해 위치와 경계를 명시적으로 학습하지 못함\n\n\n\nUNet3+\n\n\n                  \n                  UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation \n                  \n                \n\n\nRecently, a growing interest has been seen in deep learning-based semantic segmentation.\narxiv.org/abs/2004.08790\n\n\n\nUNet, UNet++의 한계점\n\nUNet\n\nDecoder를 구성하는 방법이 같은 level의 encoder layer로 부터 feature map을 받는 simple skip connection을 사용\n\n\nUNet++\n\nNested and dense skip connection을 사용해서 encoder-decoder 사이의 semantic gap을 줄임\n\n파라미터 증가, 메모리 증가, full scale에서의 충분한 정보 탐색을 잘 못함(예측위치와 경계를 명시적으로 잘 학습하지 못함)\n\n\n\n\n\nUNet3+에 적용된 Techniques\n\n\nFull-Scale Connections : [Conventional + Inter + Intra] skip connection\n\nDecoder Feature Map\n\n(Conventional)encoder layer로부터 same-scale의 feature maps를 받음\n(Inter)encoder layer로부터 smaller-scale의 low-level feature maps를 받음\n\n풍부한 공간 정보를 통해 경계 강조\n\n\n(Intra)decoder layer로부터 lager-scale의 high-level feature maps를 받음\n\n\n어디 위치하는지의 위치 정보 구현\n\n\n\n\n\n\nClassification-guided Module(CGM)\n\n\nlow-level layer에 남아있는 background의 noise로 인해 많은 False-Positive 문제 발생\n\n정확도를 높이고자 exrtra classfication task 즌행\nhigh-level feature maps인 X^{5}_{de} 를 활용\n\n\nDropout, 1x1 Conv, Adaptive max pool, sigmoid 통과\n\n\nargmax를 통해 organ이 없으면 0, 있으면 1로 출력\n\n\n위에서 얻은 결과의 각 low-layer마다 나온 결과를 곱\n\n\n\n\n\n\nFull-scale Deep Supervision\n\n경계부분을 잘 학습하기 위해서 Loss 여러가지 결합\n\nFocal Loss : class 불균형 해소\nms-ssim Loss : Boundary 인식 강화\niou : 픽셀의 분류 정확도 상승\n\n\n\nAnother Version of the UNet\n\n\nResidual-UNet\n\n\n                  \n                  Road Extraction by Deep Residual U-Net \nRoad extraction from aerial images has been a hot research topic in the field of remote sensing image analysis.\narxiv.org/abs/1711.10684\n                  \n                \n\n\nEncoder 및 Decoder 부분의 Block마다 residual unit with identity mapping 적용\n\n\n\nMobile-UNet\n\n\nMobileNet 을 backbone으로 사용\n\n\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.researchgate.net/publication/341749157_Mobile-Unet_An_efficient_convolutional_neural_network_for_fabric_defect_detection\n\n\n\n\n\nEff-UNet\n\n\nEfficientNet을 backbone으로 사용\n\n\n\n\n                  \n                  Info\n                  \n                \n\n\nopenaccess.thecvf.com/content_CVPRW_2020/papers/w22/Baheti_Eff-UNet_A_Novel_Architecture_for_Semantic_Segmentation_in_Unstructured_Environment_CVPRW_2020_paper.pdf\n\n\n\n\n\nSemantic Segmentation 대회에서 사용하는 방법들\nEfficientUNet Baseline\nModel 불러오기\nSegmentation Models[link]\n\nHigh Level API\nArchitectures\n\nUNet, UNet++, Manet, Linknet, FPN, PSPNet, PAN, DeepLabV3, DeepLabV3+\n\n\nEncoders\n\nResNet, ResNeXt, ResNeSt, Res2Ne(X)t, RegNet(x,y) SE-Ne(X)t, DenseNet, Inception, EfficientNet, MobileNet, VGG\n\n\n\nLatest version from source:\n$ pip install git+github.com/qubvel/segmentation_models.pytorch\nBaseline 이후에 실험해봐야할 사항들\nAugmentation\nCutout : 이미지를 일부 가려서 일반화 성능의 향상 → Object와 상관 없는 영역을 가릴 수 있음\nGridDropout : 전체적인 Grid를 가림\n\nCutmix : object 일부를 다른 object 이미지로 대치\n\nSnapmix : CAM을 이용해 이미지 및 라벨을 Mixing하는 방법\n\nCropNonEmptyMaskIfExist : Object가 존재하는 부분을 중심으로 Crop 할 수 있다면 model의 학습을 효율적으로 할 수 있음\nSOTA Model\nHRNet\n\n\n                  \n                  Deep High-Resolution Representation Learning for Visual Recognition \n                  \n                \n\n\nHigh-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection.\narxiv.org/abs/1908.07919\n\n\n\nScheduler\n\nCosineAnnealingLR\n\nLR의 최대-최소 값을 정해, 그 범위의 학습율을 Cosine 함수를 이용해 스케줄링 하는 방법\nSaddle Point를 빠르게 벗어나게 함\n\n\nReduceLROnPlateau\n\nMetric의 성능이 향상되지 않을 때 LR을 조절\nLocal Minima에 빠졌을 때, LR를 조절하여 효과적으로 빠져나옴\n\n\nGradual Warmup\n\n학습 시작 시 매우 작은 LR로 시작해서 특정 값에 도달할 때 까지 LR을 서서히 증가시키는 방법\n이 방식을 사용하면 Weight가 불한정한 초반에도 비교적 안정적으로 학습 수행 가능\nBackbone 네트워크 사용 시, Weight가 망가지는 것을 방지\n\n\n\nOptimizer / Loss\n\n\n                  \n                  AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights \n                  \n                \n\n\nNormalization techniques are a boon for modern deep learning.\narxiv.org/abs/2006.08217\n\n\n\n\n\nAdam\nAdamW\nAdamP\nRadam\nLookahead Optimizer\n\n\nAdam이나 SGD를 사용해 K번 업데이트 후, 처음 시작했던 point 방향으로 1 step back 후, 그 지점에서 다시 시작하는 방법\n\n\nAdam이나 SGD로 빠져나오기 힘든 Local Minima를 빠져나올 수 있음\n\n\n\n\nEnsemble\n\n5 Fold Ensemble\nEpoch Ensemble\nSWA(Stochastic Weight Averaging)\nSeed Ensemble\nResize Ensemble\nTTA(Test time Augmentation) Ensemble\n\nTTA Library : ttach\n\n\n\nPseudo Labeling\nlabel이 없는 데이터에 대해서 labeling 후, confidence가 높은 데이터를 학습에 참여시키는 방법\n그 외\n학습 이미지가 많고 큰 경우에는 네트워크를 한 번 학습하는 데 시간이 오래 걸림, 충분한 실험이 어려움\n\n\nAMP\n\n\n가벼운 상황으로 실험\n\n\nParams가 적은 모델로 실험, 최종은 성능이 잘 나오는 모델로 실험\n\n\nSliding Window를 사용\nLabel Smoothing\n\nHard target → Soft target\nCrossEntropyLoss / BCE Loss → SoftCrossEntropyLoss / SoftBCELoss\n\nSemantic Segmentation 연구 동향\nHRNet\n\n\n                  \n                  Deep High-Resolution Representation Learning for Visual Recognition \n                  \n                \n\n\nHigh-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection.\narxiv.org/abs/1908.07919\n\n\n\nImage Classification 모델이 해상도를 줄여나가는 이유\n\n특정 물체 분류시 많은 경우, 이미지 내 모든 특징점이 필요하지 않음\n해상도가 줄어들어 효율적인 연산이 가능, 넓은 Receptive Field를 갖게 됨\n주요 특징만을 추출하여 과적합 방지\n\n이미지 분류 모델을 사용하여 얻은 저해상도 특징은 모든 픽셀에 대해 정확한 분류를 수행하기에는 부족한 정보를 가짐\n따라서 segmantation 모델들은 해상도를 적게줄이면서 receptive field를 키울 필요가 있음\nHRNet의 구성 요소\n\n저해상도 → 고해상도 복원이 아닌, 고해상도 정보를 계속 유지\n\n\n입력 이미지의 해상도를 strided convolution Article 을 이용해 해상도를 1/4로 줄임\n\n\n전체 구조에서 해당 해상도 유지\n\n\n중간중간 분기하여 고해상도부터 저해상도까지 다양한 해상도의 feature를 계산\n\n\n중간중간 서로의 feature를 서로에게 전달\n\n\n\nhigh → low에서 strided conv를 사용한 이유 : 정보 손실 최소화\nlow → high에서 bilinear upsampling 사용한 이유 : time complexity\nWSSS\nSegmentation Labeling은 시간이 오래 걸림\nWeak Supervision\n\n\nTest시 요구하는 output보다 학습 시에 더 간단한 annotation을 이용하여 학습\n\n\nimage level label을 활용하기 위해 Classification Model 학습\n\n\n학습한 Classification model을 통해 CAM, Grad-CAM 혹은 attention 추출, label로 사용\n\n\nCAM &amp; Grad-CAM 문제점\n\n\npseudo-mask의 결과가 좋지 않음\n\n\nCAM의 문제점\n\n마지막 레이어는 꼭 GAP를 가져야 함\n마지막 레이어에서만 CAM을 만들 수 있음\n\n\n\n결과가 Sharp하지 않음\n\n\nSharp하게 만들기 위한 접근\n\nCRF를 사용\nObject-ness(saliency)를 학습한 모델을 전이학습\nSelf-supervised Learning 이용\n\n\n동일 Image를 input size와 관계 없이 CAM 모양이 같아지도록 L1 Loss 사용\n\n\n\n\nCAM Output의 특징\n\n\n분류 시, 다른 Class임을 확실하게 알 수 있도록 특징적인 부분에 집중(타조의 몸통보다 얼굴에 집중)\n\n\n같은 Class여도 서로 다른 모습을 보이므로 공통적으로 보이는 특징에 의존\n\n\nCAM 영역 확장을 위한 접근\n\nCAM 영역을 지워가며 반복 학습\n\n입력 이미지를 Classification Network1를 사용해 CAM 추출\nClassification Network1에서 만들 결과 영역 제거\n제거한 이미지로 Classification Network2 학습\n동일하게 Classification Network3 학습\n합치면, object 영역을 얻을 수 있음\nOutput별로 다른 모델을 학습해야 하는 번거로움 존재\n반복하다 보면 object 영역이 아닌 부분에 집중해버리는 Over-erasing 현상 존재\n\n\n입력 이미지에서 random하게 patch를 지워 최대한 다양한 영역에서 feature를 추출해야 하도록 강제\n\nCAM 영역이 확장됨\nOver-erasing 해결됨\n\n\nCAM 영역을 지워가며 반복학습 하는 것을 네트워크 하나로 수행\n\nAdversarial Complementary Learning for Weakly supervised Object localization, cvpr 2018\n\n\n다양한 receptive field 사용\n\nRevisiting Dilated Convolution: A Simple Approch for weakly-and semi-supervised semantic segmentation, cvpr 2018\n\n\nMixup\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-17":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-17","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 17.md","title":"Week 17","links":[],"tags":[],"content":"\nCloud\n\nCloud 서비스를 사용하는 이유\nCloud 서비스의 다양한 제품\n\n\nCI/CD\n\n개발 환경\nCI(Continuous Integration, 지속적 통합)\nDelivery, 지속적 배포)\n\n\nGithub Action\nMask Classification Streamlit\nBackend Programing\n\nServer 구성, Use Case\nServer의 형태\nREST API\nURI, URL\nHTTP Method\nHeader와 Body\nStatus Code\n동기와 비동기\nIP(Internet Protocol)\nPort\n\n\nFast API\n\nFastAPI 소개 &amp; 특징\nFastAPI vs Flask\n자주 사용하는 Fast API Project 구조\nSwagger\nFastAPI 기본지식\nRequest Body &amp; Response Body\nPydantic\nFastAPI 익숙해지기\n프로젝트 구조 - Cookecutter\n\n\n\n\nCloud\nCloud 서비스를 사용하는 이유\n\n웹, 앱 서비스를 만드는 경우\n\n자신의 PC로도 서비스를 만들 수 있음\n그러나 내 IP를 누구나 접근할 수 있게 하고 공유해야 함\n내 PC를 종료할 시, 웹과 앱 서비스로 종료됨\n\n\nLocal\n\nLocal에서 서비스 구축 시 전통적인 접근 방법\n\n물리적 공간, 확장성을 고려한 서비실을 만들고 운영\nIDC(Internet Data Center) 공간 개설 및 운영\n서버를 넣을 공간 + 추후 서버를 추가할 떄 즉각적으로 확장할 수 있는지\n서버가 갑자기 종료되지 않도록 준비 필요\n\n\nLocal의 문제\n\n갑작스러운 트래픽 증가시 대응의 어려움\n너무 적은 트래픽일 때, 서버를 놀게 놔둬야 함\n\n\n\n\nCloud\n\n개방자가 직접 설정해야 했던 작업 등을 클라우드에서 쉽게 할 수 있는 방향으로 발전\nApache Spark를 쉽게 운영할 수 있도록 AWS EMR, GCP Dataproc 등을 활용\n\n직접 하둡을 설치할 필요 없이 이미 설치되어 있음\n\n\nJupyter Notebook 환경도 미리 설치해두고 사용할 수 있음\n\n\n\nCloud 서비스의 다양한 제품\n\n\n다양한 Service\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.whatap.io/ko/blog/9/\n\n\n\n\nIAAS(for IT administrators)\n\nAmazon Web Service(AWS), Microsoft Azure, DigitalOcean, Google Compute Engine(GCE)\n\n\nPAAS(for SW Developers)\n\nAWS Elastic Beanstalk, Windows Azure, Heroku, Google App Engine\n\n\nSAAS(for End Users)\n\nGoogle Apps, Dropbox, Salesforce, WhaTap\n\n\n\n\n\nComputing Service\n\n연산을 수행하는 Computiong Service, 가장 많이 활용할 제품\nServer Instance 생성 후 Instance에 들어가서 사용\n\n\n\nServerless Computing\n\nComputing service와 유사, 그러나 서버 관리를 클라우드 쪽에서 진행\nCode를 Cloud에 제출하면 그 코드를 가지고 서버를 실행해주는 형태\n요청 부하에 따라 자동으로 확정(Auto Scaling)\nMicro Service로 많이 활용(ex: AWS Lambda)\n\n\n\nStateless Container\n\nDocker를 사용한 Container 기반으로 서버 실행\nDocker Image를 업로드하면 해당 이미지 기반으로 서버를 실행해주는 형태\n요청 부하에 따라 자동으로 확정(Auto Scaling)\n\n\n\nObject Storage\n\n다양한 Object를 저장할 수 있는 저장소\n다양한 형태의 데이터를 저장할 수 있으며, API를 사용해 데이터에 접근할 수 있음\n점점 데이터 저장 비용이 저렴해지고 있음\nML 모델의 Pkl 파일, CSV파일, Log등을 저장\n\n\n\nDatabase(RDB)\n\nDB가 필요한 경우, 클라우드에서 제공하는 DB 활용 가능\n웹, 앱 서비스와 DB가 연결된 경우가 많으며 대표적으로 My SQL, PosgreSQL등을 사용 가능\n\n\n\nData Warehouse\n\n서비스에서 활용할 DB\n일반 DB에 있는 데이터와, Object Storage에 있는 데이터 등을 모두 모아서 Data Warehouse 형태로 저장, Data 분석에 특화됨(쿼리가 더 빠름)\n\n\n\nAI Platform\n\nAI Research, AI Develop 과정을 더 편리하게 해주는 제품\nML Ops 관련 서비스 제공\nGoogle clout platform TPU\n\n\n\nGoogle Cloud Platform\n\n\n첫 가입시 300 크레딧 제공, 학습시 Cloud 비용의 부담이 적음\n\n\n하나의 Cloud에 익숙해지면, 다른 Cloud에서의 학습도 수월\n\n\n\n\nCI/CD\n개발 환경\n\nLocal\n\n각자의 컴퓨터에서 개발, 환경 통일을 위해 Docker 등을 사용\n\n\nDev\n\nLocal에서 개발한 기능을 테스트할 수 있는 환경, Test 서버\n\n\nStaging\n\nProduction 환경에 배포하기 전에 운영하거나 보안, 성능 등을 측정하는 환경\n\n\nProduction\n\n실제 서비스를 운영하는 환경 환경\n\n\n환경을 나누는 이유\n\n운영중인 서비스에 장애가 발생하면 안되기 때문\n\n\n\nCI(Continuous Integration, 지속적 통합)\n\n\n빌드, 테스트 자동화\n\n\n새롭게 작성한 코드 변경 사항이 Build, Test 진행 후 Test Case에 통과했는지 확인\n\n\n지속적으로 코드 품질 관리\n\n\n10명의 개발자가 코드를 수정했다면, 모두 CI 프로세스 진행\n\n\nCD(Continuous Deploy | Delivery, 지속적 배포)\n\n\n배포 자동화\n\n\n작성한 코드가 항상 신뢰 가능한 상태가 되면 자동으로 배포될 수 있도록 하는 과정\n\n\nCI 이후 CD를 진행\n\n\ndev, staging, main 브랜치에 Merge가 될 경우 코드가 자동으로 서버에 배포\n\n\n→ 비용 문제로, 개인 프로젝트에서는 서버를 많이 두지 않는 경우도 존재\nGithub Action\n\n\nWorkflow\n\n\n                  \n                  Actions · sdras/awesome-actions \nYou can&#039;t perform that action at this time.\ngithub.com/sdras/awesome-actions/actions+Awesome+List\n                  \n                \n\n\n\n                  \n                  GitHub - 404Vector/Study.GithubAction: Repos for studying github-action \nYou can&#039;t perform that action at this time.\ngithub.com/404Vector/Study.GithubAction\n                  \n                \n\n\nGithub에서 출시한 기능, SW Workflow 자동화를 도와주는 도구\n여러 Job으로 구성되고, Event로 Trigger되는 자동화된 Process\nFile\n\nYAML로 작성되며, Github Repos의 .github/workflows 폴더에 저장됨\n[name] : workflow 이름\n[on] : workflow 발생 조건\n[jobs] : Runner에서 실행되는 Steps의 조합\n\n여러 Job이 있는 경우 병렬로 실행\n순차 실행도 가능, ex: A Job Success 후 B Jos 실행\n\n\n[steps] : Job에서 실행되는 개별 작업\n\nAction을 실행하거나 쉘 커맨드 실행 가능\n하나의 Job에서는 데이터를 공유할 수 있음\n\n\n[actions] : workflow의 제일 작은 단위\n\nJob을 생성하기 위해 여러 Step을 묶은 개념\n재사용이 가능한 component\n개인적으로 Action을 만들 수 있고, Marketplace의 Action을 사용하는 것도 가능\n\n\n[runner]\n\nworkflow가 실행될 서버\nvCPU 2, 7GB Memory, 14GB Storage\n\n\n\n\nTest Code\n\n특정 함수의 Return 값이 어떻게 나오는지 확인하는 Test Code\n\nex) 특정 변수의 타입이 int가 맞는가\n\n\nUnit Test, End to End Test\n\n\n배포\n\nProd, Staging, Dev 서버에 코드 배포\nFTP로 파일 전송 or Docker Image를 Push 등\nNode.js 배포도 지원\n\n\nPython, Shell Script 실행 가능\n\nGithub Repo에 저장된 스크립트를 일정 주기를 가지고 실행 가능\n\n\nGithub Tag, Release 자동으로 설정 가능\n\n\n\n제약조건\n\nRepos당 Workflow는 최대 20개까지 등록 가능\nWorkflow 내 Job은 최대 6시간 실행 가능, 초과시 자동 중지\n\n\n\nMask Classification Streamlit\n\n\nCompute Engine에서 Streamlit 실행하기\n\n\n                  \n                  부스트캠프 AI Tech - Product Serving 자료 \nMachine Learning Engineer Basic Guide] 부스트캠프 AI Tech - Product Serving 자료 - GitHub - 404Vector/Study.\ngithub.com/404Vector/Study.BoostcampAITechProductServing\n                  \n                \n\n\n\nSSH Key 생성\ncd ~/.ssh/\nssh-keygen -t rsa -b 4096 -C &quot;{My-EMAIL}&quot;\ncat id_rsa.pub &gt;&gt; authorized_keys\ncat id_rsa.pub # Copy keys\n\n\nSSH Key 등록\n\nGCP/compute engine/metadata 접속, SSH Key(Public) 추가\n\n\n\nGithub Repos Actions secrets 설정\n\n외부로 나가면 안되는 정보 저장에 사용\n[Name : HOST, Value : Instance IP]로 추가\n[Name : USERNAME, Value : Instance UserName]로 추가\n[Name : SSH_KEY, Value : Instance SSH PrivateKey]로 추가\n\n\n\nInstance에서 Github action 추가 인증 없이 사용하도록 설정\n\ngit config --global credential.helper store\n\n\n\nclone git repos\n\n내 service repos를 clone, personal access token 등으로 로그인 되어있어야 함\n\n\n\ninstall requrements\ncd Study.BoostcampAITechProductServing/part2/04-cicd/\nsudo apt-get update\nsudo apt-get install python3.8-venv -y\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n\nstreamlit 실행\nnohup streamlit run app.py --server.runOnSave true &amp;\n\nnohup : background에서 실행\n\n\n\nInstance 방화벽 설정\n\nGCP 접속, 방화벽 규칙 설정\n방화벽 규칙 만들기\n태그 이름: streamlit\n대상: 지정된 대상 태그, 태그 이름은 streamlit, tcp port : 8501\n\n\n\nInstance 방화벽 적용\n\nCompute Engine / Instance 세부정보 / 수정 → 네트워크 태그 지정(위 방화벽 설정 이름)\n\n\n\n\n\nBackend Programing\nServer 구성, Use Case\n\n실제 회사의 Use Case\n\nApp, Web Service Server가 존재\nML Service의 Server가 존재\nService Server에서 ML Server에 Inference를 요청 및 통신(or Service Server의 한 Process로 실행)\n\n\n\nServer의 형태\n\nMonolithic Architecture : 하나의 서버에서 모든 일을 처리\nMicroservice Architecture(=MSA) : 개별 서버를 구성하고 서로 통신\n\nREST API\nRepresentation State Transfer\n\nResource, Method, Representaton of resource로 구성\nCreate, Read, Update, Delete에 사용\n\nURI, URL\n\nURI(Uniform Resource Locator)\n\n인터넷 상 자원의 위치\n\n\nURL(Uniform Resource Identifier)\n\n인터넷 상 자원을 식별하기 위한 문자열의 구성\nURL을 포함함\n\n\n\nHTTP Method\n\n\n종류\n\nGET(read) : 정보를 조회하기 위해 사용\nPOST(create) : 정보를 입력하기 위해 사용\nPUT(update) : 정보를 갱신하기 위해 사용\nPATCH(update) : 정보를 갱신하기 위해 사용\nDELETE(delete) : 정보를 삭제하기 위해 사용\n\n\n\nGET vs POST\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n처리방식GETPOSTURL Data 노출 여부OXURL 예시localhost:8080/login?id=kylelocalhost:8080/login데이터 위치HeaderBody캐싱 가능 여부OX\n\n\nHeader와 Body\n\nhttp 통신은 Request하고 Response를 받을 때 정보를 패킷에 저장\nPacket 구조 : Header / Body\nHeader : 보내는 주소, 받는 주소, 시간\nBody : 실제 전달하려는 내용\n\nStatus Code\n\n클라이언트 요청에 따라 서버가 어떻게 반응하는지를 알려주는 code\n1xx(정보) : 요청을 받았고, 프로세스를 계속 진행함\n2xx(성공) : 요청을 성공적으로 받았고, 실행함\n3xx(리다이렉션) : 요청 완료를 위한 추가 작업이 필요\n4xx(클라이언트 오류) : 요청 문법이 잘못되었거나 요청을 처리할 수 없음\n5xx(서버 오류) : 서버가 요청에 대해 실패함\n\n동기와 비동기\n\n동기 : 서버에 요청을 보내고, 응답이 돌아와야 다음 동작을 수행\n비동기 : 서버에 요청을 보낼 때 응답 상태와 상관없이 다음 동작을 수행함\n\nIP(Internet Protocol)\n\n네트워크에 연결된 특정 device의 주소를 나타내는 체계\n미리 약속된 IP\n\nlocalhost or 127.0.0.1 : 현재 사용 중인 local pc\n0.0.0.0, 255.255.255.255 : broadcast address, 네트워크에 접속된 모든 장치와 소통하는 주소\n\n\n\nPort\n\n해당 IP에 접속할 수 있는 통로\n0~65535\n0~1024는 표준 통신을 위해 미리 약속됨\n\nssh : 22\nhttp : 80\nhttps : 443\n\n\n\nFast API\nFastAPI 소개 &amp; 특징\n\n최근 떠오르는 python web framework\nhigh performance : nodejs, go와 대등한 성능\neasy : Flask와 비슷하며 micro service에 적합\nproductivity : swagger(문서) 자동 생성, pydantic(serialization) 사용 가능\n\nFastAPI vs Flask\n\n장점\n\nFlask보다 문법이 간결\n비동기 지원\nBuilt-in API(Swagger) 지원\nPydantic을 이용한 serialization 및 validation\n\n\n단점\n\n아직 Flask의 유저가 많음\nORM 등 DB와 관련된 라이브러리가 적음\n\n\n\n자주 사용하는 Fast API Project 구조\napp\n- __main__.py # 간단하게 app을 실행할 수 있는 진입점\n  main.py # 혹은 app.py, FastAPI의 APP과 Router 설정\n  model.py # model.py는 ML Model에 대한 클래스와 함수 정의\nSwagger\n\n\nREST API 설계 및 문서화할 때 사용\n\n\n다른 개발팀과 협업하는 경우\n\n\n구축된 프로젝트를 유지보수하는 경우\n\n\n기능\n\n\nAPI 디자인\n\n\nAPI 빌드\n\n\nAPI 문서화\n\n\nAPI 테스팅\n\n\n\n\nFastAPI 기본지식\n\nPath parameter\n\n서버에 path 형식으로 값 전달\n\n/users/402\n데이터가 존재하지 않는 경우 : 404Error\n\n\n\n\nQuery parameter\n\n서버에 query string 형식으로 값 전달\napi 뒤에 입력 데이터를 함께 제동하는 방식으로 사용\nkey-value 쌍으로 이루어지며 &amp;로 연결해 여러 데이터를 전달할 수 있음\n\n/users?id=402\n데이터가 존재하지 않는 경우 : empty list ⇒ 추가로 error handling 필요\n\n\n\n\n\nRequest Body &amp; Response Body\n\n\nRequest body : send data client to server\n\n항상 데이터가 포함되는 것은 아님\n\nGET의 경우 Request Header로 데이터 전달\n\n\n데이터를 보내고 싶다면 POST Method 사용\n\n\n\nResponse body : send data server to client\n\n\nPydantic\n\n\nData Validation / Settings Management 라이브러리\n\n\nType Hint를 런타임에서 강제해 안전하게 데이터 핸들링\n\n\n파이썬 기본 타입(str, int 등) + List, Dict, Typle에 대한 validation 지원\n\n\n기존 validation 라이브러리보다 빠름\n\n\nconfig를 효과적으로 관리할 수 있게 도와줌\n\n\nML Feature Data Validation으로도 활용 가능\n\n\nValidation Check Logic에 사용할 수 있는 방법\n\n일반 python class를 활용한 input definition and validation\n\n복잡한 검증로직이 필요한 경우 Class method가 복잡해지기 쉬움\nException handling을 자유롭게 할 수 있지만 그만큼 손이 많이 감\n\n\nDataclass를(python ≥ 3.7) 활용한 input definition and validation\n\ndataclass decorator 사용\n\ninit method가 필요하지 않음\n\n\n__post_init__과 같은 편의 method 존재\n\nvalidation 검증로직 작성용 method\n여전히 직접 작성해야 함\n\n\n\n\npydantic을 활용한 input definition and validation\n\n미리 정의된 validation class 존재\n\nHttpUrl : 올바른 Url인지 검증\nField(ge=x, le=y) : 값이 x이상, y 이하인지 검증\nDirectoryPath : 존재하는 디렉토리인지 검증\n\n\n어디서 에러가 발생했는지 location, type, message 출력\n\n\n\n\n\nPydantic Config\n\n\nConfig를 체계적으로 관리할 수 있는 방법을 제공\n\n\n설정 값을 상수로 코드에 저장 ⇒ Twelve-Factor 위반\n\nTwelve-Factor : SaaS(Software as a service)를 만들기 위한 방법론을 정리한 규칙\n\n\n\nvalidation처럼 BaseSetting을 상속한 클래스에서 Type hint로 주입된 설정 데이터를 검증할 수 있음\n\n\nField 클래스의 env 인자로 환경변수로 부터 해당 필드를 오버라이딩 할 수 있음\n\n\n\n\nFastAPI 익숙해지기\n\nEvent Handler\n\n\n이벤트 발생 시 그 처리를 담당\n\n\n데코레이터로 손쉽게 subscribe 가능\n@app.on_event(&quot;startup&quot;)\ndef startup_event():\n\tpass\n \n@app.on_event(&quot;shutdown&quot;)\ndef shutdown_event():\n\tpass\n\n\n\nError Handling\n\nFastAPI의 HTTP Exception : Error Response를 더 쉽게 보낼 수 있도록 하는 Class, client에게 더 자세한 메시지를 보내는 코드 작성 가능\n\n\nBackground Task\n\n\nFastAPI는 Starlett라는 Async 프래임워크를 래핑해서 사용\n\n\n오래 걸리는 작업들을 background task에서 실행\n\n\nonline serving에서 cpu 사용이 많은 작업들을 background task로 사용하면 client는 작업 완료를 기다리지 않고 즉시 response를 받을 수 있음\n# 비동기 작업이 등록됬을 때, 202(acceptied)를 리턴\n@app.post(&quot;/task&quot;, status_code=202)\nasync def create_task_in_background(task_input: TaskInput, background_tasks: BackgroundTasks):\n\tbackground_tasks.add_task(cpu_bound_task, task_input.wait_time)\n\treturn &quot;ok&quot;\n\n\n\n\n프로젝트 구조 - Cookecutter\n\n쿠키를 만들 때 사용한든 Cookiecutter\n많은 사람들이 프로젝트 구조에 대한 고민이 많아 템플릿을 서로 공유\nCLI 형태로 프로젝트 생성을 도와줌\ngithub.com/cookiecutter/cookiecutter\ngithub.com/drivendata/cookiecutter-data-science\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-18/Week-18":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-18/Week-18","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 18/Week 18.md","title":"Week 18","links":[],"tags":[],"content":"\nDocker\n\nDocker 소개\nDocker Image 저장 &amp; 배포\nDocker Compose\n\n\nLogging\n\nLogging Basics\nLogging in python\nOnline Serving Logging(Big Query)\n\n\nBentoML\n\nIntro\nBentoML 소개\nBentoML 특징\nBentoML 설치\nBentoML 사용 Flow\nBentoML Artifact Metadata 접근 방법\nBentoML API Functions, Adapters\nRetrieving BentoService\n\n\nAirflow\n\nApach Airflow 소개\nApach Airflow 설치\nApach Airflow 사용\nDAG와 Task\nApach Airflow 아키텍처와 활용방안\n\n\nML Design Pattern\n\nServing 패턴\n\n\n\n\nDocker\nDocker 소개\n\n\nSW 가상화\n\n서버 환경까지 소프트웨어화를 할 수 없을까에 대한 고민\n\n관리해야 하는 환경이 늘어날 수록 고민은 커짐\n\n\n특정 SW 환경을 만들고 Local, Production Server에서 그대로 활용\n\n\n\nVM\n\nOS를 포함한 가상화 SW → OS 위에 OS… 많은 리소스 사용\n\n\n\nContainer\n\nOS가 아닌 그 위에 SW 환경만 가상화, VM보다 훨신 가볍다\nContainer 기술을 쉽게 사용할 수 있도록 나온 도구가 바로 Docker\n\n\n\nVM vs Contrainers\n\n\n                  \n                  Containers vs Virtual Machines | Atlassian \nWhich option is better for you?\nwww.atlassian.com/microservices/cloud-computing/containers-vs-vms\n                  \n                \n\n\n\n\nDocker Image(Read-only)\n\n컨테이너 생성에 사용하는 템플릿\n\n\n\nDocker Container\n\nDocker Image를 사용해 생성한 인스턴스\n\n\n\n다른 사람이 만든 SW를 바로 가져와서 사용할 수 있음\n\nex: MySQL, Jupyter Notebook\n\n\n\nDocker Image 저장 &amp; 배포\n\nDocker Image 저장소\n\nDocker Hub\nGCR\nECR\n\n\nDocker Image 배포\n\n이미지를 서버에 배포하는 가장 간단한 방법 : Cloud 서비스 활용\nGCP : Cloud Run\n\nGCR에 올린 url을 넘겨주면 간단히 생성 가능\n\n\nAWS : ECS\n\n\nGCP Compute Engine에 Docker 이미지 배포하기 &amp; Github action 연동\n\nCompute Engine 인스턴스 실행 시 Docker Image를 가지고 실행하도록 설정 가능\nGithub Action을 사용해 Docker Image Push 자동화 가능\nIAM(Identity Access Management) 설정 필요\n\n클라우스 서비스 접근 권한을 관리하는 서비스\n서비스 계정 생성\n서비스 계정→키 생성 가능(노출주의)\n\n\n내 project의 github repos 접속\n\nSecret에 GCP Project ID추가(GCP 대시보드 참조)\nSecret에 GCP Instance 이름 추가\nSecret에 GCP Instance의 Zone 추가\n\n\nGCP 접속, Compute Engine 생성\n\n이름과 Zone은 secret과 일치시켜 입력\nDeploy Container 클릭\nGCR에 Push한 Container Image 선택\n네트워크 태그 추가 : streamlit\n\n\n외부 ip로 접속 시, streamlit page를 볼 수 있음\ngithub action으로 이미지 배포 자동화\n\n\ndeploy_docker.yml\nname: Build and Deploy to Google Compute Engine\n \non:\n  push:\n      branches: [ main ]\n      paths:\n        - &#039;part2/04-cicd/**&#039;\n \nenv:\n  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}\n  DOCKER_IMAGE_NAME: streamlit\n  GCE_INSTANCE: ${{ secrets.GCE_INSTANCE }}\n  GCE_INSTANCE_ZONE: ${{ secrets.GCE_INSTANCE_ZONE }}\n \njobs:\n  setup-build-publish-deploy:\n    name: Setup, Build, Publish, and Deploy\n    runs-on: ubuntu-latest\n \n    steps:\n    - name: Checkout\n      uses: actions/checkout@v2\n \n    # gcloud CLI 설정\n    - uses: google-github-actions/setup-gcloud@master\n      with:\n        version: &#039;290.0.1&#039;\n        service_account_key: ${{ secrets.SERVICE_ACCOUNT_KEY }}\n        project_id: ${{ secrets.GCP_PROJECT_ID }}\n \n    # gcloud에서 Docker 사용할 수 있도록 설정\n    - run: |-\n        gcloud --quiet auth configure-docker\n \n    # Model 파일 복사\n    - name: Copy model file\n      run: |\n        cd part2/04-cicd\n        sh copy_asset.sh\n \n    # Docker Image Build\n    - name: Build\n      run: |-\n        docker build part2/04-cicd --tag &quot;gcr.io/$PROJECT_ID/$DOCKER_IMAGE_NAME:$GITHUB_SHA&quot;\n \n    # Docker Image Push\n    - name: Publish\n      run: |-\n        docker push &quot;gcr.io/$PROJECT_ID/$DOCKER_IMAGE_NAME:$GITHUB_SHA&quot;\n \n    # Deploy : update-container로 업데이트 요청\n    - name: Deploy\n      run: |-\n        gcloud compute instances update-container &quot;$GCE_INSTANCE&quot; \\\n          --zone &quot;$GCE_INSTANCE_ZONE&quot; \\\n          --container-image &quot;gcr.io/$PROJECT_ID/$DOCKER_IMAGE_NAME:$GITHUB_SHA&quot;\n\n\n\n\n\n\nDocker Compose\n\n여러 Docker Image를 동시에 실행해야 한다면?\n\nex: DB Container + Web Service 동시 실행\n\n\ndocker compose를 사용해 여러 컨테이너를 동시에 실행할 수 있음\n실행하는 컨테이너들의 실행 순서, 의존도를 관리할 수 있음\ndocker-compose.yml 파일에 작성\n\nLogging\nLogging Basics\n\n데이터의 종류\n\nDB Data : DB에 저장\n\n서비스 로그\n서비스 운영을 위해 필요한 데이터\n고객 가입일, 물건 구매 기록 등\n\n\n사용자 행동 Data: 주로 Object Storage, Data Warehouse에 저장\n\n유저 행동 로그\n서비스에 반드시 필요한 내용은 아님\n그러나 더 좋은 제품을 만들기 위해 or 데이터 분석시 필요\n앱 or 웹에서 유저가 어떤 행동을 하는지를 나타내는 데이터\nClick, View, Swipe 등\n\n\n인프라 Data(metric)\n\n백엔드 웹 서버가 제대로 동작하고 있는지 확인하는 데이터\nRequest 수, Response 수, DB 부하 등\n\n\nMetric\n\n값을 측정할 때 사용\nCPU, Memory 등\n\n\nLog\n\n운영 관점에서 알아야 하는 데이터를 남길 때 사용\n함수 호출 기록, 예외처리 기록 등\n\n\nTrace\n\n개발 관적에서 알아야 하는 것\nException Trace\n\n\n데이터 적재 방식\n\nDB(RDB)\n\n실제 서비스용 DB, 비지니스와 연관된 중요 정보\n\n고객 정보, 주문 요청\n\n\n행렬로 구성\nMy SQL, PostgreSQL 등\n\n\nDB(NoSQL, Not Only SQL)\n\nElasticsearch, LogStach or Fluent, Kibana에서 활용하는 경우(스키마에 덜 민감)\n데이터가 많아지며 RDBMS로 트래픽 감당이 어려워서 개발됨\nKey-Value Store, Document, Column Family, Graph 등\nJSON과 유사한 형태, XML 등도 활용 됨\nMongoDB 등\n\n\nObjectStorage\n\nS3, Cloud Storage에 파일 형태로 저장, 어떤 형태의 파일이라도 저장할 수 있는 저장소\ncsv, parquent, json, 이미지, 음성 등\n별도 DB나 Data Warehouse에 옮기는 작업 필요\n\n\nData Warehouse\n\n데이터 분석시 활용, 여러 공간에 저장된 데이터를 한 곳으로 저장(데이터 창고)\nRDBMS와 같은 SQL을 사용하나 성능이 더 좋은 편\nAWS Redshift, GCP BigQuery, Snowflake 등\nRDB, NoSQL,Object Storage 등에 저장된 데이터를 한 곳으로 옮겨서 처리\n\n\n\n\n\n\n\nLogging in python\n\n\nlogging\n\n파이썬 기본 모듈\n\n\n\nlog level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevelValueRemarkDEBUG10문제 해결에 필요한 자세한 정보를 공유INFO20작업이 정상적으로 작동하고 있는 경우WARNING30예상하지 못한 일이거나 발생 가능한 문제일 경우ERROR40프로그램이 함수를 실행할 수 없는 심각한 상황CRITICAL50프로그램이 동작할 수 없는 심각한 문\n\n\nPython Logging Component\n\nLoggers\n\nLog를 생성하는 method 제공(logger.info() 등)\n\n\nHandlers\n\nLogger에서 만들어진 Log를 적절한 위치로 전송(파일 저장 또는 Console 출력 등)\n\n\nFilters\nFormatters\n\n최종적으로 Log에 출력될 Format 설정\n\n\n\n\n\nOnline Serving Logging(Big Query)\n\nBig Query\n\nGCP의 Data Warehouse\n데이터 분석을 위한 도구로 Apache Spark의 대용으로 활용 가능\nFirebase, Google Analytics4와 연동되어 많은 회사에서 사용 중\n데이터를 조회할 때 마다 비용 발생\n\n파티션 설정으로 모든 데이터가 아닌 일부 데이터만을 조회할 수 있음(비용 절감)\n\n\n\n\n\nBentoML\nIntro\nFastAPI로 직접 ML 서버 개발\n→ 1~2개의 모델을 만들 때는 직접 개발 가능\n그러나 30~50개의 모델을 만들어야 한다면?\n많은 모델을 만들다 보니 반복되는 작업이 존재(Config, FastAPI 설정 등)\n→ 추상화가 불가능할까?\n더 쉬운 개발을 위해 Serving에 특화된 라이브러리를 원하게 됨\n→ BentoML\nBentoML 소개\n\nArtifact, Asset 등 사이즈가 큰 파일을 패키징, Cloud Service에 지속적인 배포를 위한 많은 작업 등을 CLI 명령어로 진행할 수 있게 하여 문제의 복잡도를 낮춤\nPython Logging Module로 Access Log, Prediction Log를 기본적으로 기록, Config로 Logging도 수정할 수 있으며 Prometheus 같은 Metric 수집 서버에 전송할 수 있음\nAdaptive Micro Batching 방식을 사용하여 동시에 많은 요처이 들어와도 높은 처리량을 보여줌\n\nBentoML 특징\n\n쉬운 사용성\nOnline &amp; Ofline Serving 지원\nTensorflow, PyTorch, Keras, XGBoost 등 Major 프레임워크 지원\nDocker, Kubernetes, AWS, Azure 등의 비포 환경 지원 및 가이드 제공\nFlask 서버 대비 100배 처리량\n모델 저장소(yatai) 웹 대시보드 제공\n데이터 사이언스와 DevOps 사이의 간격을 이어주며 높은 성능의 Serving이 가능\n\nBentoML 설치\n\nPython 3.6 이상 지원\n\npip install bentoml\nBentoML 사용 Flow\n\n모델 학습 코드 생성\nPrediction Service Class 생성\nPrediction Service 모델 저장(pack)\n(Local)Serving\nDocker Image Build(컨테이너화)\nServing 배포\n\nBentoML Artifact Metadata 접근 방법\n\n\nby CLI\nbentoml get model:version\n\n\nby REST API\nserverip:serverport/metadata\n\n\nby Python\nfrom bentoml import load\nsvc = load(&#039;path_to_bento_service&#039;)\nprint(svc.artifacts[&#039;model&#039;].metadata)\n\n\nBentoML API Functions, Adapters\n\nBentoService API : Client가 예측 서비스에 접근하기 위한 end point를 생성\nAdapter : input/output을 추상화해서 중간 부분을 연경하는 layer\n\ncsv 파일 형식으로 예측 요청한 경우 ⇒ DataFrame Input을 사용하고 있으면 내부적으로 pandas의 DataFrame객체로 변환하고 API함수에 전달\n\n\n\nfrom my_lib import preprocessing, postprocessing, fetch_user_profile_fome_db\n \nclass ExamplePredictionService(bentoml.BentoService):\n\t@bentoml.api(input=DataframeInput(), batch=True)\n\tdef predict(self, df):\n\t\tuser_profile_column = fetch_user_profile_fome_db(df[&#039;user_id&#039;])\n\t\tdf[&#039;user_profile&#039;] = user_profile_column\n\t\tmodel_input = preprocessing(df)\n\t\tmodel_output = self.artifacts.model.predict(model_input)\n\t\treturn postprocessing(model_output)\nRetrieving BentoService\n학습모델 저장 후, artifact bundle을 찾을 수 있음\nbentoml retrieve ModelServe --target_dir=~/bentoml_bundle/\nAirflow\n\n\n                  \n                  버킷플레이스 Airflow 도입기 - 오늘의집 블로그 \n                  \n                \n\n\n탁월한 데이터플랫폼을 위한 Airflow 도입기\nwww.bucketplace.com/post/2021-04-13-%EB%B2%84%ED%82%B7%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-airflow-%EB%8F%84%EC%9E%85%EA%B8%B0/\n\n\n\n\n\n                  \n                  Kubernetes를 이용한 효율적인 데이터 엔지니어링(Airflow on Kubernetes VS Airflow Kubernetes Executor) - 1 \n                  \n                \n\n\n들어가며 안녕하세요.\nengineering.linecorp.com/ko/blog/data-engineering-with-airflow-k8s-1/\n\n\n\n\n\n                  \n                  쏘카 데이터 그룹 - Airflow와 함께한 데이터 환경 구축기(feat. Airflow on Kubernetes) \n                  \n                \n\n\n지난 3년간 Airflow 구축 및 운영기록\ntech.socarcorp.kr/data/2021/06/01/data-engineering-with-airflow.html\n\n\n\n\n\n                  \n                  Airflow Executors | Astronomer Documentation \n                  \n                \n\n\nAn introduction to the Apache Airflow Executors: Celery, Local, and Kubernetes.\ndocs.astronomer.io/learn/airflow-executors-explained\n\n\n\nApach Airflow 소개\n\nBatch Process : 예약된 시간에 실행되는 프로세스\n\n모델을 주기적으로 학습시키는 경우(Continuous Training) 필요\n모델을 주기적으로 Serving하는 경우 필요\n\n\n기존 대표적인 Batch Process : Linux Crontab\n\n아직도 간단하게 사용하기에 좋은 선택임\n실행중 오류가 발생한 경우, 별도의 처리를 해주지 않음\n과거 실행 이력 및 로그를 보기 어려움\n여러 파일 실행이나 복잡한 파이프라인을 만들기 어려움\n\n\nAirflow\n\n현재 스케줄링, 워크플로우 도구의 표준\nAirbnb에서 개발\n파이썬으로 스케줄링 및 파이프라인 작성\n실패 시 알람, 재실행 시도\n동시 실행 워커 수 지정\n설정 및 변수 값 분리\n\n\n\nApach Airflow 설치\n\nAirflow 설치\n\npip install apache-airflow\nApach Airflow 사용\n\nAirflow 기본 디렉토리 설정\n\nexport AIRFLOW_HOME=.\n\nAirflow DB 초기화\n\nairflow db init\n\nAirflow 어드민 계정 생성\n\nairflow user create\n\nAirflow 웹서버 실행\n\nairflow webserver\n\nAirflow 스케줄러 실행\n\nairflow scheduler\nDAG와 Task\n\nDAG : 1개의 파이프라인\n\nVariable : Airflow console에서 저장, DAG에서 활용\nConnection &amp; Hook : 연결하기 위한 설정(My SQL, GCP)\nSensor : 외부 이벤트를 기다리며 특정 조건이 만족하면 실행\nMarker\nXComs : Task끼리 결과를 주고받기를 원하는 경우 사용\n\n\nTask : DAG 내에서 실행할 작업\n\nApach Airflow 아키텍처와 활용방안\n\n실제 회사에서의 구축 방법 사례\n\nManaged Airflow(GCP COmposer, AWS MWAA)\nVM + Docker Compose\nKubernetes + Helm\n\n\nAirflow는 데이터 엔지니어링에서 많이 사용하지만, ML Ops에서도 활용 가능\n\n주기적인 실행이 필요한 경우\nBatch Training\nBatch Serving\nMove MySQL to Data Warehous\nBatch ETL for feature store\n\n\n\nML Design Pattern\n\nML의 특수성으로 별도의 디자인 패턴이 생김\n\nData, Model, Code\n\n\n큰 분류\n\nServing 패턴 : 모델을 Production 환경에 서빙하는 패턴\nTraining 패턴 : 모델을 학습하는 패턴\nQA 패턴 : 모델의 성능을 Production 환경에서 평가하기 위한 패턴\nOperation 패턴 : 모델을 운영하기 위한 패턴\n\n\n\nServing 패턴\nML Model을 Production 환경에서 어떻게 사용 할 것인가?\n\n\nWeb Single 패턴\n\nFastAPI, Flask 등으로 단일 REST API 생성\n\n\n\nSynchronuous 패턴\n\n동기로 실행\n예측 결과에 따라 로직이 달라지는 경우\n예측이 끝날 때 까지 프로세스를 block\n예측 속도가 병목이 됨\n\n\n\nAsynchronous 패턴\n\n비동기로 실행, 응답을 바로 받을 필요가 없는 경우\n클라이언트와 서버 사이에 메세지 큐를 추가\n실시간 예측에는 적절하지 않음\n\n\n\nMicroservice Vertical 패턴\n\n여러 모델이 순차적으로 연결되는 경우\n각각 모델을 별도 서버로 배포, 동기적으로 순서대로 예측\n\n\n\nMicroservice Horizontal 패턴\n\n하나의 request에 여러 모델을 병렬로 실행하고 싶은 경우\n\n\n\nPrediction Cache 패턴\n\n\nRequest할 때 데이터를 저장하고 예측 결과도 별도로 저장해야 하는 경우\n\n\n예측 결과가 자주 변경되지 않는 경우\n\n\n주기적 삭제 로직 필요\n\n\n\n\nTraining 패턴\n\n학습 파이프라인 구성을 위한 패턴\nBatch Training 패턴\n\n주기적으로 학습해야 하는 경우\n스케줄링 서버 필요\n\n\nPipeline Training 패턴\n\n\n학습 파이프라인 단계를 분리해 각각을 선택하고 재사용할 수 있도록 만드는 경우\n\n\nBatch Training 패턴의 응용\n\n\n각 작업을 개별 리소스로 분할\n\n\n시간이 많이 걸리는 작업은 자주 실행, 다른 작업은 적게 실행\n\n\n이전 작업의 실행 결과가 후속 작업의 input\n\n\n중간중간 결과를 data warehouse에 저장\n\n\n\n\n\n\nQA 패턴\n\n예측 서버와 모델의 성능 평가를 위한 패턴\nShadow AB Test 패턴\n\n새로운 예측 모델이 Production 환경에서 잘 동작하는지 확인하고 싶은 경우\n새로운 예측 서버가 Production 환경의 부하를 잘 견디는지 확인하고 싶은 경우\nrequest가 들어온 경우 기존 모델과 새로운 모델 모두에게 전달, 기존 모델 서버만 response\n새로운 예측 서버에 대한 비용 발생\n\n\nOnline AB Test 패턴\n\n새로운 예측 모델이 Production 환경에서 잘 동작하는지 확인하고 싶은 경우\n새로운 예측 서버가 Production 환경의 부하를 잘 견디는지 확인하고 싶은 경우\nShadow AB Test 패턴과 비슷한 방식\nRequest가 들어오면 지정된 비율(ex 1:1)로 트래픽을 나누어 절반은 기존, 절만은 새로운 모델로 예측(초반엔 9:1이 적당)\n새로운 예측 서버에 대한 비용 발생\n\n\n\n\n\nOperation 패턴\n\nModel in image 패턴\n\n서비스 환경과 모델을 통합해서 관리하고 싶은 경우\nDocker Image 안에 모델이 저장되어 있는 경우\n모델 수정이 빈번한 경우, Docker Image Build를 계속 수행해야 함\n\n\nModel Load 패턴\n\nDocker Image와 모델 파일을 분리하고 싶은 경우\n모델 업데이트가 빈번한 경우\n동일 서버 이미지, 다수 모델을 사용하는 경우\n모델파일은 Object Storage에 업로드하고 프로세스를 시작할 때 모델을 다운\n서비스 시작이 오래걸릴 수 있음, 서버 이미지와 모델관리 필요\n\n\n\n\n\nPrediction Log 패턴\n\n서비스 개선을 위해 예측, 지연시간 로그를 사용하려고 할 경우\n프로세스에서 로그를 저장하지 않고, 메세지 시스템으로 넘겨서 프로세스가 저장에 신경쓰는 시간을 줄임\n로그가 많아지면 저장 비용이 발생\n\n\n\nCondition Based Serving 패턴\n\n상황에 따라 예측해야 하는 대상이 다양한 경우(ex 국가에 따른 언어모델)\n룰베이스 방식으로 상황에 따라 모델을 선택해야 하는 경우\n모델 수에 따라 운영 비용 증가\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 2.md","title":"Week 2","links":[],"tags":[],"content":"\nComputational Graph\nComputational Graph::Define and Run\nComputational Graph::Define by Run(Dynamic Computational Graph, DCG)\n자동 미분(Autograd)\nnumpy.ndarray.view vs numpy.ndarray.reshape\nTorch.mm vs Torch.matmul 차이점\ntorch.tensor의 requires_grad param의 기능\ntorch.nn.Module\ntorch.nn.Parameter\noptimizer.zero_grad()\nPyTorch 딥러닝 학습의 기본 순서\ncollate_fn(parameter of torch.utils.data.DataLoader)\nTop 10 Performance Tuning practices\nTransfer Learning\nMetric(in machine learning)\nCross Entropy Loss\nBCELoss(Binary Cross Entroby Loss)\nML Monitoring tools for pytorch\npin_memory = True(parameter of torch.data.DataLoader)\ntorch.no_gard()\n\n\nComputational Graph\n\n연산의 과정을 그래프로 표현\n\n\n\n모든 신경망은 계산 그래프이다\n\n\nComputational Graph::Define and Run\n\n그래프를 먼저 정의, 실행 시점에 데이터를 feed\n\nComputational Graph::Define by Run(Dynamic Computational Graph, DCG)\n\n실행하면서 그래프 생성\n\n자동 미분(Autograd)\n\n\n                  \n                  점프 투 파이썬 \n                  \n                \n\n\n점프 투 파이썬 오프라인 책(개정판) 출간 !\nwikidocs.net/60754\n\n\n\n\n\n                  \n                  A Gentle Introduction to torch.autograd - PyTorch Tutorials 1.12.1+cu102 documentation \n                  \n                \n\n\ntorch.\npytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n\n\n\n\n역전파를 위한 자동 미분기능\n\nnumpy.ndarray.view vs numpy.ndarray.reshape\n\n\n                  \n                  What&#039;s the difference between reshape and view in pytorch? \n                  \n                \n\n\nview() will try to change the shape of the tensor while keeping the underlying data allocation the same, thus data will be shared between the two tensors.\nstackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n\n\n\n\n\nview는 항상 같은 메모리임을 보장, reshape의 경우 항상 같은 메모리임을 보장하지 않음\n\n\nview는 contiguity를 보장하지만 reshape는 보장하지 않음\n\n\n\nReturns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\n\n\n\nTorch.mm vs Torch.matmul 차이점\n\n\n                  \n                  Torch.mm과 Torch.matmul 차이점 \n                  \n                \n\n\nvector 및 matrix 간의 다양한 곱을 수행한다.\nneos518.tistory.com/178\n\n\n\ntorch.tensor의 requires_grad param의 기능\n\n\nrequires_grad = True일 경우, 해당 tensor는 model이 backward 될 때 자동미분(auto grad)된다.\n\n\ntorch.nn.Module\n\n\nai 모델을 구성하는 layer의 base class\n\n\ninput, output, forward, backward를 정의해야 함\n\n\n학습 대상이 되는 parameter(tensor)를 정의해야 함\n\n\ntorch.nn.Parameter\n\nTensor를 상속받는 Class\nnn.Moudule 내에 Attribute가 될 때, required_grad=True로 지정되어 자동으로 학습대상이 됨\n\n\n= AutoGrad의 대상이 됨\n\n\n\n\noptimizer.zero_grad()\n\n\n                  \n                  torch.optim.Optimizer.zero_grad - PyTorch 1.12 documentation \n                  \n                \n\n\nSets the gradients of all optimized s to zero.\npytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html%20zero_grad#torch.optim.Optimizer.zero_grad\n\n\n\noptimizer.zero_grad()\n\n\n호출 시 parameter(학습해야 할 tensor)의 grad 변수를 초기화\n\n\nbackward()를 호출할 시, 내부 Parameter Tensor의 grad에는 편미분한 결과 값이 저장됨\n\n\n그러나 역전파가 끝난 후에도 유지되므로, 다음 훈련 전에 초기화 필요\n\n\n\n\nPyTorch 딥러닝 학습의 기본 순서\n# Optimize 대상인 각 parameter들의 gradient 값 초기화\noptimizer.zero_grad()\n \n# 예측값 계산\noutput = model(inputs) # \n \n# Ground_truth와 예측 값 사이의 loss 계산\nloss = loss_function(output, ground_truth)\n \n# loss 값으로 각 parameter의 gradient 값 계산\nloss.backward()\n \n# gradient 값으로 각 parameter 갱신\noptimizer.step()\ncollate_fn(parameter of torch.utils.data.DataLoader)\n\n\n                  \n                  torch.utils.data - PyTorch 1.12 documentation \n                  \n                \n\n\nThe most important argument of constructor is , which indicates a dataset object to load data from.\npytorch.org/docs/stable/data.html#dataloader-collate-fn\n\n\n\nTop 10 Performance Tuning practices\n\n\n                  \n                  Top 10 Performance Tuning Practices for Pytorch \n                  \n                \n\n\nPytorch 모델의 학습 및 추론을 가속화 할 수 있는 10가지 팁을 공유드립니다.\nmedium.com/naver-shopping-dev/top-10-performance-tuning-practices-for-pytorch-e6c510152f76\n\n\n\nTransfer Learning\n\n\n                  \n                  Transfer learning - Wikipedia \n                  \n                \n\n\nTransfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.\nen.wikipedia.org/wiki/Transfer_learning\n\n\n\n\n\n                  \n                  전이학습 \n                  \n                \n\n\n전이학습은 하나의 작업을 위해 훈련된 모델을 유사 작업 수행 모델의 시작점으로 활용하는 딥러닝 접근법입니다.\nkr.mathworks.com/discovery/transfer-learning.html\n\n\n\n\n\n하나의 문제를 풀 때 얻은 지식을 관련있는 다른 문제를 풀 때 적용하는 것\n\n\n이미 훈련된 모델을 유사 작업 수행 모델의 시작점으로 활용하는 딥러닝 접근법\n\n\nMetric(in machine learning)\n\n\n                  \n                  Metrics in Machine Learning \n                  \n                \n\n\nfalse positives in a classification model.\nmachine-learning.paperspace.com/wiki/metrics-in-machine-learning\n\n\n\n\n\n                  \n                  Metric - Wikipedia \n                  \n                \n\n\nMetric or metrical may refer to: Metric system, an internationally adopted decimal system of measurement In mathematics, metric may refer to one of two related, but distinct concepts: A function which measures distance between two points in a metric space A metric tensor, in differential geometry, which allows defining lengths\nen.wikipedia.org/wiki/Metric\n\n\n\n\n\n머신러닝에서의 메트릭(metric)은 머신러닝 시스템의 최적화를 위해 우리가 관심을 갖는 숫자를 의미\n\n\nAccuracy, Loss, Confusion Matrix, AUC, MAE, RMSE, … etc\n\n\n\n\nCross Entropy Loss\n\n\n                  \n                  크로스 엔트로피 손실: 개요 \n                  \n                \n\n\n파이토치 및 텐서플로의 코드와 대화형 시각화를 포함한 크로스 엔트로피 손실을 다루는 튜토리얼.\nwandb.ai/wandb_fc/korean/reports/---VmlldzoxNDI4NDUx\n\n\n\n\n\n주로 분류 모델이 얼마나 잘 분류했는가를 측정하는데 사용하는 metric\n\n\nloss(error)는 0에서 1 사이로 측정됨( 0일 시, 완벽한 모델)\nL=l_1(f(x_1),y_1)+...+l_n(f(x_n),y_n) \\\\ l_c=-(y_c\\log(p_c) + (1-y_c)log(1-p_c)) \\\\ =-{\\sum_{c=1}^{n}}{y_c\\log(p_c)}\n\n\nBCELoss(Binary Cross Entroby Loss)\n\n\n                  \n                  BCELoss - PyTorch 1.12 documentation \n                  \n                \n\n\nJoin the PyTorch developer community to contribute, learn, and get your questions answered.\npytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n\n\n\n\n결과 값이 두개뿐인 시나리오에서 사용됨\n\nL=l_1(f(x_1),y_1)+l_2(f(x_2),y_2)\n\n\np : 예측 확률\n\n\ny : 지표(BCE의 경우 0 또는 1)\n\n\nt : n개의 class중 임의의 class의 index\n\n\nML Monitoring tools for pytorch\n\nTensorboard\n\n\n\n                  \n                  TensorBoard | TensorFlow \n                  \n                \n\n\n텐서보드는 머신러닝 실험에 필요한 시각화 및 도구를 제공합니다.\nwww.tensorflow.org/tensorboard\n\n\n\n\nWandB\n\n\n\n                  \n                  Machine Learning Articles, Tutorials &amp; Reports by Weights &amp; Biases \n                  \n                \n\n\nFully Connected: Where leading machine learning practitioners discover and share news, papers, findings and reports.\nwandb.ai/fully-connected\n\n\n\npin_memory = True(parameter of torch.data.DataLoader)\n\n\n                  \n                  When to set pin_memory to true? \n                  \n                \n\n\nFrom the imagenet example: train_loader = torch.\ndiscuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723\n\n\n\n\n\n                  \n                  How to Optimize Data Transfers in CUDA C/C++ | NVIDIA Technical Blog \n                  \n                \n\n\nIn the previous three posts of this CUDA C &amp; C++ series we laid the groundwork for the major thrust of the series: how to optimize CUDA C/C++ code.\ndeveloper.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n\n\n\n\n\nCPU로 dataset을 로드하고, 학습 시간에 이를 GPU로 push 한다면 위 옵션을 통해 host 에서 device(GPU)로 전송하는 속도를 향상시킬 수 있음\n\n\ntorch.no_gard()\n\n\n                  \n                  no_grad - PyTorch 1.12 documentation \n                  \n                \n\n\nJoin the PyTorch developer community to contribute, learn, and get your questions answered.\npytorch.org/docs/stable/generated/torch.no_grad.html\n\n\n\n\nbackward를 사용하지 않을 때(inference 시점) 메모리 소비를 줄임\n\n모델 내 tensor 중 requred_grad = True인 경우에도 required_grad=False처리하여 계산\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-3":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-3","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 3.md","title":"Week 3","links":[],"tags":[],"content":"\nLoss Function for task\nDeepLearning에서 가장 중요한 아이디어들(denny britz, 2020-07-29)\nOptimization에서 중요한 것들\nRegularization\n자가회귀모형(Autoregressive model, AR)\n사후 분포(Posterior distribution)\nVariational inference(VI)\n\n\nLoss Function for task\n\n\n손실 함수는 수행하고자 하는 작업 종류에 따라 변화\n\n\nRegression Task\nMSE = {1 \\over N}{\\sum_{i=1}^{N}}{\\sum_{d=1}^{D}}{(y_{i}^{d} - \\hat y_{i}^{d})}^2\n\n\nClassification Task\n\n\nCE = -{1 \\over N}{\\sum_{i=1}^{N}}{\\sum_{d=1}^{D}}{(y_{i}^{d} \\log \\hat y_{i}^{d})}\n\nProbabilistic Task\n\nMLE = {1 \\over N}{\\sum_{i=1}^{N}}{\\sum_{d=1}^{D}}{\\log N( y_{i}^{d} ; \\hat y_{i}^{d},1)} = MSE\nDeepLearning에서 가장 중요한 아이디어들(denny britz, 2020-07-29)\n\n\nAlexNet(2012)\n\n고전적인 ML이 아닌, Deep Learning을 이용해서 처음으로 ImageNet 대회에서 우승\nML → DL로의 Paradigm Shift\n\n\n\nDQN(2013)\n\n딥마인드가 아타리를 DQN으로 Play\n후에 AlphaGo 탄생\n\n\n\nEncoder / Decoder(2014)\n\n기계어 번역의 Trend가 바뀜\n\n\n\nAdam Optimizer(2014)\n\nHyperparameter에 Tune의 부담이 크게 감소\nAdam을 쓰면 왠만하면 잘 되기 때문에 연구자들이 본인의 연구에 집중 할 수 있게 됨\n\n\n\nGAN(2015)\n\n\nResidual Networks(2015)\n\n기존보다 Network를 더 깊이 쌓을 수 있게 됨\n\n\n\nTransformer(2017)\n\n\nBERT(2018, Bidirectional Encoder Representations from Transformers)\n\n\nBIG Language Models(2019, GPT-X)\n\n\nSelf Supervised Learning(2020)\n\n\nOptimization에서 중요한 것들\n\n\nGeneralization\n\nModel이 훈련에서 사용하지 않은 Data에 대해서도 성능을 발휘할수 있어야 함\n\n\n\nUnder-fitting vs Over-fitting\n\nUnderfitting\n\n네트워크가 특성을 나타내기 너무 간단한 경우\n모델의 훈련이 부족한 경우\n\n\nOverfitting\n\n네트워크를 너무 복잡한 경우\n훈련을 너무 과하게 한 경우\n데이터의 양이 충분하지 못한 경우\n\n\n\n\n\nCross validation\n\n\nTrain set을 train set과 validatin set으로 분리한 뒤, validation set을 사용해 검증하는 방식\n\n\nTest set은 그 어떤 방식으로든 훈련에 사용해서는 안되기 때문에 위와 같이 분리\n\n\nk-겹 교차 검증\n\n\n                  \n                  Cross-validation (statistics) - Wikipedia \nCross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\nen.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\n                  \n                \n\n\n가장 일반적인 교차검증 방법\nk개의 sub sample중 하나를 validation으로 사용하고 나머지 k-1개의 sub sample은 훈련에 사용하는 방식\n이후 교차검증을 정확히 k번 반복하여 교차검증을 수행\n\n\n\n\nHyper parameter를 찾기 위해 사용\n\n\n\n\nBias-variance tradeoff\n\n\nBootstrapping(in machine learning)\n\n\n                  \n                  Bootstrapping (statistics) - Wikipedia \nBootstrapping is any test or metric that uses random sampling with replacement (e.\nen.wikipedia.org/wiki/Bootstrapping_(statistics)\n                  \n                \n\n\n중복을 허용하는 Random sampling을 하는 실험, 또는 metric을 의미한다\n\n\n\nBagging(Bootstrap aggregating)\n\n\n                  \n                  배깅 - 위키백과, 우리 모두의 백과사전 \n배깅(bagging)은 bootstrap aggregating의 줄임말로 통계적 분류와 회귀 분석에서 사용되는 기계 학습 알고리즘의 안정성과 정확도를 향상시키기 위해 고안된 일종의 앙상블 학습법의 메타 알고리즘이다.\nko.wikipedia.org/wiki/%EB%B0%B0%EA%B9%85\n                  \n                \n\n\n\n                  \n                  What is Bagging? \nLearn how bootstrap aggregating, or bagging, can improve the accuracy of your machine learning models, enabling you to develop better insights.\nwww.ibm.com/cloud/learn/bagging\n                  \n                \n\n\n\n통계적 분류 및 회귀에 사용되는 기계 학습 알고리즘의 안정성과 정확성을 개선하기 위해 설계된 앙상블 메타 알고리즘(ensemble meta-algorithm)\n잡음이 많은 데이터 세트 내에서 분산을 줄이고 과적합을 방지\nn개의 전체 데이터 중, 복원추출(Sampling with replacement)방식으로 m개 새로운 훈련 세트를 생성\n\n이러한 샘플을 Bootstrap sample이라고 부름)\n\n\n회귀의 경우 Soft voting으로 최종 결과 값 산출\n분류의 경우 Hard voting으로 최종 결과 값 산출\n\n\n\nBoosting\n\n\n                  \n                  Boosting (machine learning) - Wikipedia \nIn machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.\nen.wikipedia.org/wiki/Boosting_(machine_learning)\n                  \n                \n\n\n\n                  \n                  What is Boosting? \nLearn about boosting algorithms and how they can improve the predictive power of your data mining initiatives.\nwww.ibm.com/cloud/learn/boosting\n                  \n                \n\n\n\n지도학습의 편향과 분산을 주로 줄이기 위한 앙상블 메타 알고리즘(ensemble meta-algorithm)\n약한 학습자 세트를 강한 학습자 세트로 결합하는 앙상블 학습 방법\n종류\n\n\nAdaBoost(or Adaptive boosting)\n\n\nGradient Boosting\n\n\nXGBoost(or Extreme Gradient Boosting)\n\n\n\n\n\n\nRegularization\n\n\nEarly Stopping\n\n\n                  \n                  Early stopping - Wikipedia \nIn machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent.\nen.wikipedia.org/wiki/Early_stopping\n                  \n                \n\n\n과적합을 피하기 위해 사용하는 정규화 방법의 일종\n훈련을 반복할 때 마다 어느정도까지는 훈련 세트의 외부의 데이터에 대한 학습자의 성능이 향상되나 그 지점을 지나면 over-fit되어 generalization error 증가\n\n\n\nParameter Norm Penalty\n\n(마음에 드는 자료 못찾음)\n\n\n\nData Augmentation\n\n\n                  \n                  Data augmentation - Wikipedia \nData augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\nen.wikipedia.org/wiki/Data_augmentation\n                  \n                \n\n\n약간의 수정이 가해진 복제본을 추가하여 데이터의 수를 늘리는 기법\n\n\n\nNoise Robustness\n\n(마음에 드는 자료 못찾음)\n\n\n\nLabel Smoothing\n\n\n                  \n                  Info\n                  \n                \n\n\ntowardsdatascience.com/what-is-label-smoothing-108debd7ef06\n\n\n\n\n\nDropout\n\n\n                  \n                  0042 Dropout - Deepest Documentation \nDropout [1]은 간단하지만 아주 강력한 정규화(regularization) 방법입니다.\ndeepestdocs.readthedocs.io/en/latest/004_deep_learning_part_2/0042/\n                  \n                \n\n\n매 훈련 과정에서 랜덤으로 일부 퍼셉트론을 없애버림\n\n\n\nBatch normalization(Batch Norm)\n\n\n                  \n                  Batch normalization - Wikipedia \nBatch normalization (also known as batch norm) is a method used to make training of artificial neural networks faster and more stable through normalization of the layers&#039; inputs by re-centering and re-scaling.\nen.wikipedia.org/wiki/Batch_normalization\n                  \n                \n\n\n\nLayer의 Input(batch)을 Normalization(Re-centering, Re-Scaling)을 통해 모델 훈련 속도를 가속시키고 더 안정화 시키는 것\n\n\n\n\n자가회귀모형(Autoregressive model, AR)\n\n\n                  \n                  자기회귀모형 - 위키백과, 우리 모두의 백과사전 \n                  \n                \n\n\n통계, 계량 경제학 및 신호 처리에서 자기회귀 모형(自己回歸模型, autoregressive model, AR)은 임의 프로세스 유형을 나타낸다.\nko.wikipedia.org/wiki/%EC%9E%90%EA%B8%B0%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95\n\n\n\n\n\n                  \n                  8.3 자기회귀 모델 | Forecasting: Principles and Practice \n                  \n                \n\n\n다중 회귀 모델에서, 목표 예상 변수(forecast variable)의 선형 조합을 이용하여 관심 있는 변수를 예측했습니다.\notexts.com/fppkr/AR.html\n\n\n\n\n\n출력변수가 자신의 이전 값과 확률적 항(불완전하게 예측 가능한 항)에 선형적으로 의존함을 지정\n\n\n사후 분포(Posterior distribution)\n\n\n                  \n                  Posterior Probability &amp; the Posterior Distribution \n                  \n                \n\n\nProbability &gt; Posterior Probability &amp; the Posterior Distribution Posterior probability is the probability an event will happen after all evidence or background information has been taken into account.\nwww.statisticshowto.com/posterior-distribution-probability/\n\n\n\n\n\n베이지안 분석에서 불확실한 양에 대해 우리가 알고 있는 것을 요약하는 방법\n\n\n사후분포 = 사전 분포 + likelihood(가능도, 우도) 함수\n\n\nVariational inference(VI)\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf\n\n\n\n\n\nVariational distribution\n\nVariational inference시 Posterior distribution에 근사하기 위해 만드는 분포\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 4/Week 4.md","title":"Week 4","links":[],"tags":[],"content":"\nClass of visual perception\nModern augmentation techniques\nTransfer learning\nKnowledge distillation\nHard Label vs Soft Label\nGoogLeNet\nReceptive Field(수용필드, 수용장)\nResNet\nTwo-stage detector vs One-stage detector\nAnalysis of model behaviors\n\n\nClass of visual perception\n\n\nColor perception\n\n\nMotion perception\n\n\n3D perception\n\n\nSemantic-level perception\n\n\nSocial perception (emotion perception)\n\n\nVisuomotor perception, etc.\n\n\nModern augmentation techniques\n\n\nidentity\n\n\nauto contrast\n\n\nequalize\n\n\nrotate\n\n\nsolarize\n\n\ncolor\n\n\nposterize\n\n\ncontrast\n\n\nbrightness\n\n\nsharpness\n\n\nshear-x, y\n\n\ntranslate-x, y\n\n\nTransfer learning\n\n현실에서는 고품질 혹은 대량의 dataset을 얻기 힘듬\n이미 잘 만들어진 dataset으로 잘 만든 model(pre-trained model)을 가져와 활용하는 기술\n\n\nKnowledge distillation\n\n\n                  \n                  Knowledge distillation - Wikipedia \n                  \n                \n\n\nIn machine learning, knowledge distillation is the process of transferring knowledge from a large model to a smaller one.\nen.wikipedia.org/wiki/Knowledge_distillation\n\n\n\n\n\n기계학습에서 Knowlkege distillation은 큰 모델에서 작은 모델(일반적으로) 지식을 이전하는 프로세스\n모델압축, unlabeled dataset에 대한 pseudo-label(가짜 라벨) 생성에 사용\n\n\n\n\nStudent Model이 Teacher Model의 결과를 흉내내게(mimic) 함\n\n\nHard Label vs Soft Label\n\n\nHard Label : one-hot vector 형태\n\nex [bear, cat, dog] = [0, 1, 0]\n\n\n\nSoft Label : softmax 형태\n\n\nex [bear, cat, dog] = [0.14, 0.8, 0.06]\n\n\n\n\nGoogLeNet\n\n\nStacked inception modules\n\n\n                  \n                  Inception Module \nLog In Sign Up Inception Modules are used in Convolutional Neural Networks to allow for more efficient computation and deeper Networks through a dimensionality reduction with stacked 1×1 convolutions.\ndeepai.org/machine-learning-glossary-and-terms/inception-module\n                  \n                \n\n\n\n                  \n                  Inception v1 - 2014 | DataCrew \n니가 뭘 좋아할지 몰라서 다 준비해봤어.\ndatacrew.tech/inception-v1-2014/\n                  \n                \n\n\n\n다양한 conv 필터와 pooling을 수행\n\n\n\n하지만 파라미터 수가 너무 많이 필요한 단점이 있음, 이를 1x1 Covolution을 통해 Channel 수를 줄여 해결\n\n\n\nAuxiliary Classifier\n\n\nVanishing gradient를 해결하기 위해 도중에 값을 injection\n\n\n낮은 단계에 해당하는 layer에서도 backpropagation되는 gradient signal을 증폭시킴\n\n\n\n\nReceptive Field(수용필드, 수용장)\n\n\n                  \n                  What is a receptive field? | CNNs #2 \n                  \n                \n\n\nwww.youtube.com/watch\n\n\n\n\n\n                  \n                  A guide to receptive field arithmetic for Convolutional Neural Networks \n                  \n                \n\n\nThe receptive field is perhaps one of the most important concepts in Convolutional Neural Networks (CNNs) that deserves more attention from the literature.\nblog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807\n\n\n\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.baeldung.com/cs/cnn-receptive-field-size\n\n\n\n\n\n특정 CNN Feature에 대해서 입력공간이 영향을 받는 크기\n\n\nex ) 첫 번째 CNN 계층의 Feature의 입력 공간에 대한 Receptive Field는 3x3이다.\n\n\nex ) 두 번째 CNN 계층의 Feature의 입력 공간에 대한 Receptive Field는 5x5이다.\n\n\n\n\nResNet\n\n\n                  \n                  Residual neural network - Wikipedia \n                  \n                \n\n\nA residual neural network ( ResNet) is an artificial neural network (ANN).\nen.wikipedia.org/wiki/Residual_neural_network\n\n\n\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n\n\n\n\n\nShortcut connection을 도입하여 기존보다 더 깊게 층을 쌓을 수 있게 됨\n직접 구현하기\n\n\nConvBlock\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super().__init__()\n \n        #\\#fill it##\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding) # kernel size = ...\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n \n    def forward(self, x):\n        \n        #\\#fill it##\n        x = self.conv(x)\n        x = self.batchnorm(x)\n        return x\n\n\nResBlock\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, pool_stride = 1):\n        super().__init__()\n \n        self.kernel_size = 3\n        self.padding = 1\n        self.stride = 1\n        self.relu = nn.ReLU()\n        self.pool_stride = pool_stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        if(in_channels == out_channels):\n          self.skip = torch.nn.Identity()\n        else:\n          self.skip = torch.nn.Conv2d(in_channels=in_channels, \n                               out_channels=out_channels, \n                               kernel_size=1, \n                               stride=self.pool_stride)\n          \n        self.conv1 = ConvBlock(in_channels=in_channels, \n                               out_channels = out_channels, \n                               kernel_size=self.kernel_size, \n                               stride=self.stride, \n                               padding=self.padding)\n        \n        self.conv2 = ConvBlock(in_channels=out_channels, \n                               out_channels = out_channels, \n                               kernel_size=self.kernel_size, \n                               stride=self.pool_stride, \n                               padding=self.padding)\n \n    def forward(self, x):\n        \n        #\\#fill##\n        y = self.conv1(x)\n        y = self.relu(y)\n        y = self.conv2(y)\n        return  self.relu(y + self.skip(x))\n\n\nResnet Model\nclass ResNet(nn.Module):\n    def __init__(self, in_channels, out_channels, nker=64, nblk=[3,4,6,3]):\n        super(ResNet, self).__init__()\n \n        self.enc = ConvBlock(in_channels, nker, kernel_size=7, stride=2, padding=1)\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.average_pool = nn.AvgPool2d(kernel_size=5, stride=1)\n \n \n        #\\#fill##\n        self.relu = nn.ReLU()\n        layers = []\n        for j, b in enumerate(nblk):\n          __out_chennel = 64 * (j+1)\n          for i in range(b):\n            if(j != 0 and i == 0):\n              __pool_stride = 2\n              __in_chennel = 64 * j\n \n            else:\n              __pool_stride = 1\n              __in_chennel = __out_chennel\n            print(__in_chennel, __out_chennel, __pool_stride)\n            layers.append(ResBlock(__in_chennel, __out_chennel, __pool_stride))\n        print(&#039;complete auto layer making&#039;)\n        self.conv = nn.Sequential(*layers)\n \n        self.fc = nn.Linear(nker*2*2, 10)\n \n    def forward(self, x):\n        x = self.enc(x)\n        x = self.max_pool(x)\n \n        #\\#fill##\n        x = self.conv(x)\n        x = self.average_pool(x)\n        x = x.view(x.shape[0], -1)\n        out = self.fc(x)\n \n        return out\n\n\n\n\nTwo-stage detector vs One-stage detector\n\n\n                  \n                  One stage vs two stage object detection \n                  \n                \n\n\nInstead of “region detection + object classification”, its “(1)region proposal + (2)classification and localization in two stage detectors.\nstackoverflow.com/questions/65942471/one-stage-vs-two-stage-object-detection\n\n\n\n\n\nObject detection 시, ROI Search와 ROI 내 Image Classification을 별도로 수행하면 Two-stage detector, 한번에 수행한다면 One-stage detector\n\n속도 : Two Stage detector &lt; One-stage detector(better)\n정확도 : Two Stage detector(better) &lt; One-stage detector\n\n\nTraditional Methods\n\nRaw Image Segmentation → 유사 영역 Merge → 후보 영역(Candidate box) 추출\n\n\nTwo-stage detector\n\n\nR-CNN\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1311.2524.pdf\n\n\n\n\n\nFast RCNN\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\n\n\n\n\n\nFaster RCNN\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1506.01497.pdf\n\n\n\n\n\n\nOne-stage detector\n\n\nYOLO\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\n\n\n\n\n\nSSD\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1512.02325.pdf\n\n\n\n\n\nRetinaNet\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1708.02002.pdf\n\n\n\n\n\n\netc. Detection with Transformer\n\n\nDETR\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/2005.12872.pdf\n\n\n\n\n\n\n\nAnalysis of model behaviors\n\n\nEmbedding feature analysis\n\n\nDataset들에 대한 Model의 고차원 high level feature vector를 수집(모델의 뒷부분, High level feature)\n알고싶은 이미지도 마찬가지로 inferrence 시켜서 high level feature vector를 수집\nfeature vector들끼리의 유사성으로 해당 이미지와 유사한 high level feature vector를 Dataset 내의 이미지들을 얻을 수 있음\n\n\n\n\nt-SNE(t-distributed stochastic neighbor embedding)\n\n고차원 백터를 저차원으로 표현할 수 있는 방법\n\n\n\n\nCAM(Class activation mapping)\n\n\nConv블록의 뒤에 3ch conv블록 생성, class에 대한 weighted sum\n\n\nhitmap 느낌으로 모델이 인지한 부분을 볼 수 있음\n\n\n\n모델이 처음부터 저 모양이라면 바로 쓸 수 있지만, 그렇지 않은 경우(FC layer 등이 있는 경우) 해당 부분을 제거하고 새롭게 추가한 신경망을 (GAP Layer + FC Layer)을 다시 훈련시켜야 함\n\n\n\n\nGrad-CAM\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1610.02391.pdf\n\n\n\n\n\n                  \n                  Grad-CAM - 새내기 코드 여행 \n인공지능은 이미 거의 모든 분야에서 다양한 용도로 사용되고 있습니다.\njoungheekim.github.io/2020/10/14/paper-review/\n                  \n                \n\n\n\nCAM을 일반화 한 방법\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-5/Week-5":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-5/Week-5","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 5/Week 5.md","title":"Week 5","links":[],"tags":[],"content":"\nConditional generative model\nSuper resolution GAN\nPix2Pix(Image-to-Image Translation with cGAN)\nCycleGAN\n\n\nConditional generative model\n\n\n                  \n                  Conditional Generative Adversarial Nets \n                  \n                \n\n\nGenerative Adversarial Nets [8] were recently introduced as a novel way to train generative models.\narxiv.org/abs/1411.1784\n\n\n\n\n기존 GAN에 추가적으로 Conditional Vector를 추가해서 원하는 결과를 생성할 수 있게 한다.\n\nSuper resolution GAN\n\n\n                  \n                  Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network \n                  \n                \n\n\nDespite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors?\narxiv.org/abs/1609.04802\n\n\n\n\n\nGan을 이용해서 Super resolution을 수행\nPix2Pix(Image-to-Image Translation with cGAN)\n\n\n                  \n                  Image-to-Image Translation with Conditional Adversarial Networks \n                  \n                \n\n\nWe investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems.\narxiv.org/abs/1611.07004\n\n\n\n\nL_{total}={arg_{{min(G)},{max(D)}}}{L_{cGAN}}+{\\lambda}{L_{1}}{(G)} \\\\ {L_1}{(G)}={E_{x,y,z}}{[{||y-G(x,z)||}_1]}\n기존의 GAN과 다른점은 Loss에 Ground-truth와의 차이도 반영한다는 것이다.\nL1을 사용하는 이유는 실험적으로 기존의 다른 Loss(ex: L2)보다 덜 blurry한 이미지를 얻었기 때문이다.\nCycleGAN\n\n\n                  \n                  Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \n                  \n                \n\n\nImage-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs.\narxiv.org/abs/1703.10593\n\n\n\n\n\nA → B, B → A로 양방향 mapping 방법을 학습하는 모델\n따라서 두 개의 Generator, 두 개의 Discriminator가 존재\n종래의 GAN 모델과 다른 점은 Paired Training Data를 사용하지 않고 학습\n종래의 GAN을 토대로 Loss함수를 작성한다면 아래와 같은 식이 된다.\n{L_{X \\to Y}}(G,D_Y,X,Y)=\\Epsilon_{y\\sim p(y)}{\\log D_Y(y)}+\\Epsilon_{x\\sim p(x)}{\\log (1-D_Y(G(x)))} \\\\ {L_{Y \\to X}}(F,D_X,X,Y)=\\Epsilon_{x\\sim p(x)}{\\log D_X(x)}+\\Epsilon_{y\\sim p(y)}{\\log (1-D_X(F(y)))}\n모델의 네트워크 용량이 크다면, 우리가 원하는 시각적 결과를 얻었건 얻지 않았건 상관 없이 Dy 혹은 Dx의 결과 값이 우리가 원하는 확률 분포를 출력하도록 모델이 학습될 수 있다.\n그러므로 단순 adversarial loss만으로는 x→y로의 translation을 보장 할 수 없다.\n여기서 Circle Constant(순환 일관성)이라는 개념이 등장한다. 만약 G와 F가 잘 동작한다면, X→Y로의 전환해서 나온 값 y’을 다시 Y→로의 전환을 했을 때 얻은 x’은 유사해야 한다.\nG(x)=\\hat y \\\\ F(\\hat y)=\\hat x\\\\ x \\approx \\hat x\n마찬가지로 y의 경우도 아래와 같다.\nF(y)=\\hat x \\\\ G(\\hat x)=\\hat y\\\\ y \\approx \\hat y\n이를 Forward cycle consistency라고 저자는 표현한다.\n모델이 이 결과를 낼 수 있도록 저자는 Cycle consistency loss를 사용한다.\nL_{cycle}(G,F,X,Y)=\\Epsilon_{x\\sim p(x)}||F(G(x))-x||_1 +\\Epsilon_{y\\sim p(y)}||G(F(y))-y||_1\nL1 Norm을 사용하는 이유는 F와 G의 adversarial loss를 사용했을 때, 성능 향상이 없었기 때문이다.\n최종목적함수는 아래와 같아같다.\nL(G,F,D_x,D_y)=L_{X \\to Y}(G,D_y,X,Y)+L_{Y \\to X}(F,D_x,Y,X)+\\lambda L_{cycle}(G,F,X,Y)\nLambda는 앞의 두 Loss와 Lcycle간의 중요도를 조절한다.\n\n이 방법은 다른 Loss들과 비교 했을 때, 좋은 성능을 보여주었다."},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-6/Week-6":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-6/Week-6","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 6/Week 6.md","title":"Week 6","links":[],"tags":[],"content":"\nProblem Definition\nSimple ML Flow\nCompetition Flow\nEDA(Exploratory Data Analysis, 탐색적 데이터 분석)\nAI Stages VS Code에서 원격으로 작업하기\n\nWindow 10 OpenSSH 서버 활성화 및 실행하기\nAI Stages - 마스크 착용 상태 분류 대회\n\n연령별 남녀 성비 시각화\n얼굴 검출 시도\n이미지 RGB 분포\n훈련을 위한 전처리\n작업 분담\nGit Repos 구성\nGender prediction\npipenv + pytorch 사용(Windows)(X)\nanaconda\n\n\n\n\n\n\nProblem Definition\n\n내가 지금 풀어야 할 문제가 무엇인가?\n이 문제의 Inpit과 Output은 무엇인가?\n이 솔루션은 어디서 어떻게 사용되어지는가?\n\n내가 풀어야 할 문제에 대해서 Define을 해야 본질에 집중할 수 있음.\n\nSimple ML Flow\nDomain understanding\n→ Data mining → Data analysis → Data processing\n→ Modeling → Training → Deploy\n\nCompetition Flow\nDomain understanding\n→ Data analysis → Data processing\n→ Modeling → Training\n\nEDA(Exploratory Data Analysis, 탐색적 데이터 분석)\n\n\n                  \n                  Exploratory data analysis - Wikipedia \n                  \n                \n\n\nIn statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods.\nen.wikipedia.org/wiki/Exploratory_data_analysis\n\n\n\n통계학에서 EDA는 통계 그래픽 및 기타 데이터 시각화 방법을 사용하여 Dataset을 분석하여 주요 특성을 요약하는 접근 방식.\n\nAI Stages VS Code에서 원격으로 작업하기\n웹기반으로 주피터 노트북을 사용하는 것은 개인적으로 불편함을 많이 느껴서 VS Code로 작업할 수 있는 방법을 찾아보았고, 요약한 내용을 정리\n\n\nSSH로 접속\n\nstages.ai/ 접속, 자신의 대회에서 서버탭 클릭\n자신의 서버 정보칸을 보면 SSH로 접근 시 필요한 명령어가 적혀 있음, 복사\n\nex)  ssh -i ./key root@101.101.217.87 -p 2231\n\n\nAI Stages 경진대회 참여 가이드를 참조하여 key파일 설정\n\nCLI로 진행해야하며, chmode 0600 [key path] 를 입력해야 함\nex) chmod 0600 ./key\n\n\n노트북 생성 시 받은 key 파일을 .ssh 폴더에 넣기\n\nwindows : c:/users/[user-name]/.ssh/\nmac : /Users/[user-name]/.ssh/\n\n\nVS Code 실행\n좌측에 원격 탐색기 아이콘 클릭\n상단에 콤보박스를 ssh 대상 으로 변경\n\n\n버튼 클릭\n\n\n상단에 나타난 TextBox에 ssh값 입력\n\nex) ssh -i ./key root@101.101.217.87 -p 2231\n\n\n접속 완료\n\n\n\nPassowrd로 접속\n\n\nstages.ai/ 접속, 자신의 대회에서 서버탭 클릭\n\n\n노트북 접속 아이콘 클릭, 첫 화면에서 “Terminal” 클릭\n\n\npasswd root입력, 이후 본인이 사용하고 싶은 password 입력\n\n\n자신의 서버 정보칸을 보면 SSH로 접근 시 필요한 명령어가 적혀 있음, 복사\n\nex)  ssh -i ./key root@101.101.217.87 -p 2231\n\n\n\nVS Code 실행\n\n\n좌측에 원격 탐색기 아이콘 클릭\n\n\n상단에 콤보박스를 ssh 대상 으로 변경\n\n\n\n버튼 클릭\n\n\n\n상단에 나타난 TextBox에 ssh값 입력\n\nex) ssh -i ./key root@101.101.217.87 -p 2231\n\n\n\npassword 입력\n\n\n접속 완료\n\n\n\n\n\nWindow 10 OpenSSH 서버 활성화 및 실행하기\n\n\n                  \n                  OpenSSH 서버 실행하고 SSH로 접속하는 방법 \n                  \n                \n\n\nLinux 서버는 원격에서 SSH 프로토콜로 사용하는 게 일반적입니다.\nwww.lainyzine.com/ko/article/how-to-run-openssh-server-and-connect-with-ssh-on-windows-10/\n\n\n\nOpenSSH Server 활성화\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\n\n\n출력\nPath :\nOnline : True\nRestartNeeded : False\n\n\nOpenSSH Server 실행\nStart-Service sshd\n부팅 시점에 자동으로 서비스가 실행되도록 설정\nSet-Service -Name sshd -StartupType &#039;Automatic&#039;\n동작 확인\nGet-NetFirewallRule -Name OpenSSH-Server-In-TCP\n\n\n출력\nName : OpenSSH-Server-In-TCP\nDisplayName : OpenSSH SSH Server (sshd)\nDescription : Inbound rule for OpenSSH SSH Server (sshd)\nDisplayGroup : OpenSSH Server\nGroup : OpenSSH Server\n==Enabled : True ← True일 경우 정상==\nProfile : Any\nPlatform : {}\nDirection : Inbound\nAction : Allow\nEdgeTraversalPolicy : Block\nLooseSourceMapping : False\nLocalOnlyMapping : False\nOwner :\nPrimaryStatus : OK\nStatus : 저장소에서 규칙을 구문 분석했습니다. (65536)\nEnforcementStatus : NotApplicable\nPolicyStoreSource : PersistentStore\nPolicyStoreSourceType : Local\n\n\n\nAI Stages - 마스크 착용 상태 분류 대회\n연령별 남녀 성비 시각화\n\nDataset - 연령별 남녀 성비 시각화\nDaycon에서 이미지 분류 대회에 참가했을 때, 특정 라벨에 대한 데이터가 적은 경우, 그 라벨은 성능을 얻기 힘들었습니다. 따라서 데이터 분포를 알아보기 위해 연령별 남녀비율을 시각화 했습니다. 21~49세의 데이터는 적어보입니다.\n\n연령 구간별 남녀 성비 시각화\n대회에서 원하는 실제 분류는 [30세 미만 / 30세 이상 ~ 60세 미만 / 60세 이상]입니다. 분류에 따라 다시 시각화 했습니다. 분류해야하는 구간에 맞춰 다시 시각각화를 해보니 오히려 60세 이상의 데이터가 상대적으로 부족합니다.\n시각화 결과, 알게된 것은 아래와 같습니다.\n\n\n전반적으로 여성에 대한 데이터가 많다.\n\n\n60세 이상에 대한 데이터가 적다.\n\n\n얼굴 검출 시도\n필요한 정보만 제공할 수 있으면 AI도 당연히 성능이 올라갈 수 밖에 없습니다.\nDataset의 이미지에서 얼굴 영역(ROI)만 따로 검출해서 넘겨줄 수 있다면 훨신 의미있는 결과를 얻을 수 있을 것입니다.\n하지만 HarrCascade로 검출해본 결과, 100개의 샘플링 데이터에서 50%의 검출률을 보였습니다.\n(데이터셋 이미지의 공개는 저작권에 접촉되므로 결과이미지 공개 불가)\n경향을 보니 우선 마스크의 색이 제각각이고, 영상 내 얼굴크기의 편차가 심했습니다.\n전통적인 방법으로 얼굴 영역를 검출하는 것은 어렵다고 판단했습니다.\n이미지 RGB 분포\n\nImage를 R, G, B 채널별로 나누어 각각 mean과 std를 구한 뒤, Mask착용 유무로 색상을 달리하여 각각 표현해본 plot\n마스크 착용 유무에 따라서 분포가 다르기를 기대했지만, 아쉽게도 크게 차이가 없었다.\n훈련을 위한 전처리\n훈련을 위해 데이터 전 처리를 수행해야 했다.\ncsv파일에 있는 [path] column의 경로는 이미지 경로가 아니라 폴더의 경로였다.\n폴더 내부에는 한 사람이 여러 종류의 마스크를 끼거나, 아에 안끼는 등 다양한 Case로 7장의 이미지가 존재했다.\n\n\n데이터 포맷\nid,gender,race,age,path\n000001,female,Asian,45,000001_female_Asian_45\n000002,female,Asian,52,000002_female_Asian_52\n000004,male,Asian,54,000004_male_Asian_54\n\n\n이것을 더 편하게 사용하기 위해 아래와 같이 노트북을 작성했다.\n\n\nCode\n# DataMaker\n이 노트북은 train.csv를 읽어서 훈련에 유용한 형태로 변환하여 저장합니다.\n## Import\nimport os\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport sys\nfrom tqdm import tqdm\nsys.path.append(&#039;../&#039;) # import를 위해 경로추가\nfrom utils import Utility as U\n## Args\nparser = argparse.ArgumentParser()\nparser.add_argument(&#039;--path_train&#039;, type=str, default=&quot;./data/train/&quot;)\nparser.add_argument(&#039;--path_eval&#039;, type=str, default=&quot;./data/eval/&quot;)\nparser.add_argument(&#039;--path_train_output&#039;, type=str, default=&quot;train_i.csv&quot;)\nparser.add_argument(&#039;--path_eval_output&#039;, type=str, default=&quot;eval_i.csv&quot;)\nargs = parser.parse_args(&#039;&#039;)\n## Load Data\ndf_train = pd.read_csv(os.path.join(args.path_train, &#039;train.csv&#039;))\nimages = []\nfor path in df_train[&#039;path&#039;]:\n    __path_folder = os.path.join(*[args.path_train, &#039;images&#039;, path])\n    __path_image = [os.path.join(*[__path_folder, p])  for p in os.listdir(__path_folder) if p[0] != &#039;.&#039; ]\n    images.append(__path_image)\n    \ndf_train[&#039;images&#039;] = images\n \ndf_train.head()\n \ndf_eval = pd.read_csv(os.path.join(args.path_eval, &#039;info.csv&#039;))\nimages = [os.path.join(*[args.path_eval, &#039;images&#039;, p])\n          for p in df_eval[&#039;ImageID&#039;]]\ndf_eval[&#039;images&#039;] = images\n \ndf_eval.head()\n \n## Make Data\nimage_df_labels = [&#039;id&#039;, &#039;gender&#039;, &#039;age&#039;, &#039;mask&#039;, &#039;path&#039;]\nimage_df_rows = []\nfor _id, (_gender, _age, _images) in enumerate(zip(df_train[&#039;gender&#039;], df_train[&#039;age&#039;], df_train[&#039;images&#039;])):\n    for  _path in _images:\n        _mask = U.convertImagePathToMaskStatus(_path)\n        image_df_rows.append(\n            [_id, _gender, _age, _mask, _path])\nimage_df = pd.DataFrame(image_df_rows, columns=image_df_labels)\nimage_df[&#039;gender_class&#039;] = [U.encodeGender(g.capitalize()) for g in image_df[&#039;gender&#039;]]\nimage_df[&#039;age_class&#039;] = [U.encodeAge(a) for a in image_df[&#039;age&#039;]]\nimage_df[&#039;mask_class&#039;] = [U.encodeMask(m) for m in image_df[&#039;mask&#039;]]\nprint(&#039;total number of images :&#039;, image_df.size / image_df.columns.size)\nimage_df.sample(5)\nimage_df.to_csv(args.path_train_output, index=False)\ndf_eval.to_csv(args.path_eval_output, index=False)\n\n\n변환결과\nid\tgender\tage\tmask\tpath\tgender_class\tage_class\tmask_class\n15649\t2235\tfemale\t40\tIncorrect\t./data/train/images/005534_female_Asian_40/inc...\t1\t1\t1\n1060\t151\tfemale\t58\tWear\t./data/train/images/000527_female_Asian_58/mas...\t1\t1\t0\n12251\t1750\tmale\t60\tNotWear\t./data/train/images/004096_male_Asian_60/norma...\t0\t2\t2\n5282\t754\tfemale\t58\tIncorrect\t./data/train/images/001492_female_Asian_58/inc...\t1\t1\t1\n\n\n작업 분담\n작업분담에 대하여 피어 세션에서 팀원들과 의논한 결과는 아래와 같았다.\n\n분류해야 하는 3개의 항목에 대하여 각각 모델을 나누어 작성\n\n\nGender 예측 모델 : 1명 (나)\n\n\nAge 예측 모델 : 1명\n\n\nMask 예측 모델 : 1명\n\n\n일반 예측 모델 : 2명\n\n\n\n\nGit Repos 구성\n최종 결과물은 git repos로 해야 한다. local로 작업하다 나중에 합치려고 하면 매우 골치아프고, 애초에 그냥 git기반으로 개발하는 것이 편하기도 해서 바로 구성했다.\nGender prediction\nGender 예측을 위해 notebook을 생성했다\nBackbone은 ResNext와 Resnet152 Resnet101을 사용해서 각각 성능을 평가했다.\nParameter\nInput image size : 244x244\nOutput image size : 1(Sigmoid)\nValidation ratio of train dataset : 0.2\nEpoch : 20\nBatch size : 16\nLearning rate : 4e-3\nTrue - False threashold value : 0.5\n결과, ResNext가 Accuracy가 가장 높았다.\nThreashold value에 따른 Accuracy 확인\n\nthreashold 값에 따른 모델의 정확도를 보기 위해 각 값마다 inferrence된 값을 평가했다.\nthreashold를 0.82로 했을 때 최고 정확도를 얻는 수 있다는 것을 알 수 있었다.\nValidation Dataset 평가\n3개 항목(Mask Status, Age, Gender)를 한번에 예측한다면 제출해서 결과를 볼 수 있겠지만, 각 항목별로 나눠서 inferrence 하기로 했기 때문에 평가 방법이 없었다.\n때문에 훈련 모델을 가져와서 inferrence 한 뒤, 그 결과를 원본이미지를 가지고 시각화해서 보는 방법을 취했다. 모든 이미지를 확인 할 수 없지만 100개정도의 이미지는 확인할 수 있으므로 모델이 잘 동작 하는지, 대략적인 정확도는 어떻게 되는지 추측할 수 있을 것이라고 판단했다.\n결과, 어느정도 준수한 성능을 보여주고 있다는 것을 알 수 있었다.\n\npipenv + pytorch 사용(Windows)(X)\n\n\n                  \n                  Pipenv + PyTorch 환경설정 \n                  \n                \n\n\n간단하게 pytorch를 쓸 가상환경을 pipenv로 만들기 위한 작업입니다.\nvelog.io/@sihyeong671/Pipenv-PyTorch-환경설정\n\n\n\n팀원분이 anaconda도 좋지만 pipenv도 편리하다고 추천해주셔서 pipenv로 구성을 시작했다.\n프로젝트별로 환경이 나눠지기 때문에 패키지가 꼬일 일이 없어 편리하다.\nai stages에서 제공한 서버는 구성에 문제가 없었다.\n==그러나 윈도우 환경에서는 정상적으로 환경구성이 되지 않았다. pipenv install ~로 설치가 안된다.\nWindow에서 뭔가를 하려고 하면 불편함이 너무 많다.\n결론적으로, 2022년 10월 기준으로는 Windows + pipenv + pytorch는 구성에 실패했다.\nWindows + WSL2 + docker + pipenv|anaconda\n\n\n                  \n                  Docker: Accelerated, Containerized Application Development \n                  \n                \n\n\nDocker is a platform designed to help developers build, share, and run modern applications.\nwww.docker.com\n\n\n\n\n\n                  \n                  Info\n                  \n                \n\n\nnamu.wiki/w/Docker\n\n\n\nwindows 기본 환경에서 pipenv와 pytorch를 사용하려던 계획이 무너졌고, 다른 방법을 모색했다.\n이 과정을 통해 내 마음속에 발현한 한가지 생각은 환경 구성에 실패했을 시 손쉽게 버리고 다시 구축할 수 있다면 좋겠다는 생각을 하게 되었다.\n시행착오를 거치며 만들어진 로컬 환경의 여러 pytorch, anaconda 찌꺼기들은 볼 때 마다 안타까움이..\n그렇다면 방법은 당연히 docker 밖에 없었다. docker는 LXC(리눅스 컨테이너스)라는 커널 컨테이너 기술을 이용하여 만든 컨테이너 기술이다. VM처럼 OS까지 가상화하지 않기 때문에 가볍다.\n작성일 기준 가장 높은 cuda stable 버전은 11.8이지만, docker hub에 있는 공식 PyTorch 이미지는 11.6이 최신이었다. 따라서 11.6버전으로 설치했다.(local cuda version이 11.8이어도 무방)\n그 후 아래와 같은 명령어로 container를 생성했다.\ndocker pull pytorch/pytorch:1.13.0-cuda11.6-cudnn8-runtime\n \ndocker run -it --gpus all -p 8080:8080 --name mytorch -v repos-vol:/repos 1c9fb6f6f844\n-it\ni = 컨테이너의 표준 입력(stdin)을 활성화\nt = 리눅스에 키보드를 통해 표준 입력(stdin)을 전달할 수 있게한다\n—gpus all\n가상환경에서 모든 gpu의 사용을 가능하게 함\n만약 특정 gpu만 사용할 수 있게 하고싶으면, 해당 gpu의 번호나 uuid를 all대신에 넘겨주면 된다\n-p 8080:22\n로컬의 port를 가상환경의 port로 mapping\nssh로 접근하기 위해 밖으로 22번 포트를 노출해줬다\n—name mytorch\n생성하려는 컨테이너의 이름\n지정해주지 않으면 무작위로 생성된다\n-v repos-vol:/repos\n따로 만들어 놓은 volume을 가상환경에 마운트해준다\nrepository 설정 시 git 정보 초기화(name, mail) 및 clone과 token 발생 등을 해야하기 때문에\n따로 volume을 만들어서 설정해 놓으면 편리하다. 나중에 가상환경 구성에 실패해서 처음부터 다시 하더라도 ‘가상 환경 설정’만 다시할 수 있게 된다\n1c9fb6f6f844\ncontrainer 생성에 사용할 image의 id\n내 경우, [pytorch/pytorch:1.13.0-cuda11.6-cudnn8-runtime]의 id를 의미한다\n우선 Docker로 환경을 구성하고 나면 그 이후에는 anaconda를 사용하던, pipenv를 사용하던 상관 없다.\n단, PyTorch Docker 이미지는 기본적으로 anaconda의 base에 환경이 구성되어 있다."},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 7/Week 7.md","title":"Week 7","links":[],"tags":[],"content":"AI Stages - 마스크 착용 상태 분류 대회\nAge-Gender-MaskStatus 한번에 예측하기 #1\n기존에 Age, Gender, Mask Train을 위해 만든 자원을 활용, 다시 코드를 짜서 train을 시켜보려고 했다.\nBasemodel은 다른 대외에서 준수한 성능을 보여주었단 EfficientNet B4를 사용했다.\n\n\nOutput\nNamespace(batch_size=32, csv_path=&#039;../train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=380, lr=0.0004, save_name=&#039;weights_all_efb4_v0.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_gamma=1.0, step_size=10, target_model=&#039;EfficientnetB4()&#039;, validation_ratio=0.2)\n100%|███████████████████████████████████████████| 473/473 [08:20&lt;00:00,  1.06s/it]\n100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.08it/s]\nEpoch [1] / Train Loss : [0.51088] / Val Loss : [4.52802] / F1 : [0.16130]\n * New Best Model -&gt; Epoch [1] / best_score : [0.16130]\n -&gt; The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar\n100%|███████████████████████████████████████████| 473/473 [08:17&lt;00:00,  1.05s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.14it/s]\nEpoch [2] / Train Loss : [0.23640] / Val Loss : [4.87189] / F1 : [0.16493]\n * New Best Model -&gt; Epoch [2] / best_score : [0.16493]\n -&gt; The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar\n100%|███████████████████████████████████████████| 473/473 [08:11&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.16it/s]\nEpoch [3] / Train Loss : [0.15332] / Val Loss : [5.03418] / F1 : [0.17144]\n * New Best Model -&gt; Epoch [3] / best_score : [0.17144]\n -&gt; The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar\n100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.05it/s]\nEpoch [4] / Train Loss : [0.12163] / Val Loss : [5.47039] / F1 : [0.16459]\n100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]\nEpoch [5] / Train Loss : [0.09997] / Val Loss : [6.38236] / F1 : [0.16500]\n100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.19it/s]\nEpoch [6] / Train Loss : [0.09601] / Val Loss : [6.03461] / F1 : [0.17007]\n100%|███████████████████████████████████████████| 473/473 [08:09&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.13it/s]\nEpoch [7] / Train Loss : [0.08823] / Val Loss : [6.99676] / F1 : [0.16531]\n100%|███████████████████████████████████████████| 473/473 [08:13&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]\nEpoch [8] / Train Loss : [0.06689] / Val Loss : [7.06806] / F1 : [0.17427]\n * New Best Model -&gt; Epoch [8] / best_score : [0.17427]\n -&gt; The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar\n100%|███████████████████████████████████████████| 473/473 [08:14&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.06it/s]\nEpoch [9] / Train Loss : [0.06878] / Val Loss : [7.18693] / F1 : [0.17101]\n100%|███████████████████████████████████████████| 473/473 [08:11&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.11it/s]\nEpoch [10] / Train Loss : [0.06150] / Val Loss : [7.87434] / F1 : [0.16583]\n100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.11it/s]\nEpoch [11] / Train Loss : [0.06174] / Val Loss : [7.73951] / F1 : [0.17413]\n100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.12it/s]\nEpoch [12] / Train Loss : [0.05296] / Val Loss : [7.74434] / F1 : [0.17096]\n100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.14it/s]\nEpoch [13] / Train Loss : [0.05569] / Val Loss : [7.90913] / F1 : [0.17755]\n * New Best Model -&gt; Epoch [13] / best_score : [0.17755]\n -&gt; The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar\n100%|███████████████████████████████████████████| 473/473 [08:13&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.13it/s]\nEpoch [14] / Train Loss : [0.05386] / Val Loss : [7.90242] / F1 : [0.16470]\n100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.20it/s]\nEpoch [15] / Train Loss : [0.05038] / Val Loss : [8.74257] / F1 : [0.17196]\n100%|███████████████████████████████████████████| 473/473 [08:09&lt;00:00,  1.03s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.16it/s]\nEpoch [16] / Train Loss : [0.05482] / Val Loss : [8.96333] / F1 : [0.16495]\n100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]\nEpoch [17] / Train Loss : [0.04385] / Val Loss : [8.56864] / F1 : [0.17062]\n100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]\nEpoch [18] / Train Loss : [0.04033] / Val Loss : [8.51767] / F1 : [0.16381]\n100%|███████████████████████████████████████████| 473/473 [08:11&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.08it/s]\nEpoch [19] / Train Loss : [0.03861] / Val Loss : [9.03492] / F1 : [0.16623]\n100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]\n100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]\nEpoch [20] / Train Loss : [0.04452] / Val Loss : [9.04841] / F1 : [0.16245]\n\n\n결과, 성능이 나오질 않았다.\nTrain Loss는 지속적으로 감소했지만, Validation Loss는 증가했다.\nLearning Rate의 문제라고 생각하여 1e-2 ~1e-5까지 0.1배로 테스트를 진행했으나 달라진 점이 보이지 않았다.\n원인 분석\n\n\nLearning Rate를 잘못 설정해서 훈련 자체가 안되는 것일까?\n\n우선, TrainLoss는 감소하므로 훈련이 아에 안되는 상황은 아닐 것이다.\n\n\n\nData Argmentation을 너무 과하게 한 것인가?\n\n\n결과\ntrain_transform = A.Compose([\n                                # A.Resize(args.img_size, args.img_size),\n                                A.RandomResizedCrop(\n                                    args.img_size, args.img_size, scale=(0.8, 1.0)),\n                                A.RandomBrightnessContrast(p=0.3),\n                                A.RandomGamma(p=0.3),\n                                A.RandomFog(),\n                                A.RandomToneCurve(),\n                                A.HorizontalFlip(p=0.5),\n                                A.Normalize(mean=(0.485, 0.456, 0.406), std=(\n                                    0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n                                ToTensorV2()\n                                ])\n \n    test_transform = A.Compose([\n        A.Resize(args.img_size, args.img_size),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),\n                    max_pixel_value=255.0, always_apply=False, p=1.0),\n        ToTensorV2()\n    ])\n\n\nGender 분류 모델에 넣은 Argmentation을 그대로 사용했기 때문에 가능성이 있다.\n\n\n원복 및 재실행 결과\nNamespace(batch_size=32, csv_path=&#039;../train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=380, lr=0.0004, save_name=&#039;MGAWeight_EfficientnetB4.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_gamma=1.0, step_size=10, target_model=&#039;EfficientnetB4()&#039;, validation_ratio=0.2)\n100%|███████████████████████████████████| 473/473 [06:37&lt;00:00,  1.19it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  2.99it/s]\nEpoch [1] / Train Loss : [0.41027] / Val Loss : [4.48446] / F1 : [0.16489]\n * New Best Model -&gt; Epoch [1] / best_score : [0.16489]\n -&gt; The model has been saved at ../models/checkpoint/MGAWeight_EfficientnetB4.tar\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:34&lt;00:00,  1.20it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.04it/s]\nEpoch [2] / Train Loss : [0.12687] / Val Loss : [5.51019] / F1 : [0.17274]\n * New Best Model -&gt; Epoch [2] / best_score : [0.17274]\n -&gt; The model has been saved at ../models/checkpoint/MGAWeight_EfficientnetB4.tar\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:33&lt;00:00,  1.20it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.05it/s]\nEpoch [3] / Train Loss : [0.10025] / Val Loss : [5.66333] / F1 : [0.17255]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:35&lt;00:00,  1.20it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.04it/s]\nEpoch [4] / Train Loss : [0.06157] / Val Loss : [6.57190] / F1 : [0.16740]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:34&lt;00:00,  1.20it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.03it/s]\nEpoch [5] / Train Loss : [0.07978] / Val Loss : [6.40279] / F1 : [0.17934]\n * New Best Model -&gt; Epoch [5] / best_score : [0.17934]\n -&gt; The model has been saved at ../models/checkpoint/MGAWeight_EfficientnetB4.tar\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:35&lt;00:00,  1.20it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.04it/s]\nEpoch [6] / Train Loss : [0.04555] / Val Loss : [6.41356] / F1 : [0.17498]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:33&lt;00:00,  1.20it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.09it/s]\nEpoch [7] / Train Loss : [0.03668] / Val Loss : [7.69871] / F1 : [0.16752]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:36&lt;00:00,  1.19it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.07it/s]\nEpoch [8] / Train Loss : [0.03775] / Val Loss : [7.32922] / F1 : [0.16793]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:35&lt;00:00,  1.20it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.04it/s]\nEpoch [9] / Train Loss : [0.05067] / Val Loss : [7.42311] / F1 : [0.16720]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:32&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.08it/s]\nEpoch [10] / Train Loss : [0.05210] / Val Loss : [7.89588] / F1 : [0.17300]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.13it/s]\nEpoch [11] / Train Loss : [0.02483] / Val Loss : [7.87729] / F1 : [0.16588]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.09it/s]\nEpoch [12] / Train Loss : [0.04351] / Val Loss : [8.21418] / F1 : [0.16275]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.12it/s]\nEpoch [13] / Train Loss : [0.02155] / Val Loss : [8.29829] / F1 : [0.16369]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.19it/s]\nEpoch [14] / Train Loss : [0.02002] / Val Loss : [8.14832] / F1 : [0.15482]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.09it/s]\nEpoch [15] / Train Loss : [0.03869] / Val Loss : [9.19703] / F1 : [0.16264]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:29&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.09it/s]\nEpoch [16] / Train Loss : [0.02364] / Val Loss : [8.64021] / F1 : [0.16931]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.08it/s]\nEpoch [17] / Train Loss : [0.02183] / Val Loss : [9.18431] / F1 : [0.17084]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:40&lt;00:00,  2.95it/s]\nEpoch [18] / Train Loss : [0.03817] / Val Loss : [8.90696] / F1 : [0.16667]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.10it/s]\nEpoch [19] / Train Loss : [0.03505] / Val Loss : [9.06653] / F1 : [0.16512]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.14it/s]\nEpoch [20] / Train Loss : [0.01673] / Val Loss : [8.40525] / F1 : [0.16786]\n\n\n\n\n팀원들과 논의\n방향을 잡기 어려워 팀원들에게 도움을 요청했다. 리더보드에서 가장 높은 성적은 거둔 팀원은 timm의 vit를 사용했다고 한다.\n==ViT로 Classification 하기==\nViT를 사용해서 Classification을 하려면 어떻게 해야 할까?\n인터넷의 블로그에서 예제코드를 찾았다.\nimport timm\n \nnum_classes = 120\nmodel = timm.create_model(&#039;vit_base_patch16_224&#039;, pretrained=True, num_classes=num_classes)\ntimm이라는 library를 사용해서 모델을 로드하고 파라미터로 num_classes를 넘겨준다\n하지만 이래서는 blackbox여서 내부 구조를 알 수 없다\nprint를 사용해 내부를 확인해보았다.\n\n\n기본 구조(for 1000 classes)\nVisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (norm_pre): Identity()\n  (blocks): Sequential(\n    (0): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (1): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (2): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (3): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (4): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (5): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (6): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (7): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (8): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (9): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (10): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (11): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (fc_norm): Identity()\n  (head): Linear(in_features=768, out_features=1000, bias=True)\n)\n\n\n변경 구조(for 18 classes)\nVisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (norm_pre): Identity()\n  (blocks): Sequential(\n    (0): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (1): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (2): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (3): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (4): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (5): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (6): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (7): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (8): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (9): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (10): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (11): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate=none)\n        (drop1): Dropout(p=0.0, inplace=False)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (fc_norm): Identity()\n  (head): Linear(in_features=768, out_features=18, bias=True)\n)\n\n\n기본적으로 patch_embed → encoder block → head 구조로 되어있다\nclass parameter 변경 시, head 부분의 out_features가 변경된다\ntorchvision.models에도 vit가 존재한다\n\n\ntorchvision.models\nVisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate=none)\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Sequential(\n    (head): Linear(in_features=768, out_features=1000, bias=True)\n  )\n)\n\n\nMultiheadAttention Block 내부가 print로 노출되지 않는 점을 제외하고는 동일한 구조를 가지고 있으므로 TorchVision의 ViT를 사용해서 모델을 구성했다.\n\n\nModel Code\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.models as models\n \n \nclass VIT_V0_KHS(nn.Module):\n    def __init__(self, is_freeze:bool = True):\n        super(VIT_V0_KHS, self).__init__()\n        self.backborn = models.vit_b_16(weights = models.ViT_B_16_Weights.IMAGENET1K_V1)\n        if(is_freeze == True):\n            for p in self.backborn.parameters():\n                p.requires_grad = False\n        self.backborn.heads = nn.Sequential(nn.Linear(768, 18))\n \n    def forward(self, x):\n        x = self.backborn(x)\n        return x\n\n\n결과, 마찬가지로 훈련이 되지 않았다\n\n\n출력(도중에 취소)\n(lv1_imageclassification_cv02) (base) root@536ffaebb1ec:~/repos/lv1_imageclassification_cv02/train# python trainALL.py \nNamespace(batch_size=32, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V0_KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V0_KHS()&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [01:57&lt;00:00,  4.04it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:28&lt;00:00,  4.22it/s]\nEpoch [1] / Train Loss : [0.90020] / Val Loss : [2.79555] / F1 : [0.16190]\n * New Best Model -&gt; Epoch [1] / best_score : [0.16190]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [01:53&lt;00:00,  4.19it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:26&lt;00:00,  4.48it/s]\nEpoch [2] / Train Loss : [0.54853] / Val Loss : [3.16076] / F1 : [0.16314]\n * New Best Model -&gt; Epoch [2] / best_score : [0.16314]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [01:54&lt;00:00,  4.12it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:26&lt;00:00,  4.45it/s]\nEpoch [3] / Train Loss : [0.46461] / Val Loss : [3.36511] / F1 : [0.16870]\n * New Best Model -&gt; Epoch [3] / best_score : [0.16870]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [01:55&lt;00:00,  4.08it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:27&lt;00:00,  4.39it/s]\nEpoch [4] / Train Loss : [0.41091] / Val Loss : [3.66533] / F1 : [0.16694]\n\n\n내가 직접 짠 코드의 코드의 문제일까?(train, validation 오류 or loss의 잘못된 사용, … etc)\nbaseline code를 가져와 다시 세팅을 시작했다\nbaselinecode에 vit를 돌려본 결과, 된다…\n\n\n출력(required_grad = true)\nNamespace(augmentation=&#039;BaseAugmentation&#039;, batch_size=64, criterion=&#039;cross_entropy&#039;, data_dir=&#039;/opt/ml/repos/lv1_imageclassification_cv02/data/train/images&#039;, dataset=&#039;MaskBaseDataset&#039;, epochs=10, log_interval=20, lr=0.001, lr_decay_step=20, model=&#039;MyModel&#039;, model_dir=&#039;/opt/ml/repos/lv1_imageclassification_cv02/local/v2/model&#039;, name=&#039;exp&#039;, optimizer=&#039;SGD&#039;, resize=[224, 224], seed=42, val_ratio=0.2, valid_batch_size=1000)\nsave_dir is  /opt/ml/repos/lv1_imageclassification_cv02/local/v2/model/exp\n/opt/ml/.local/share/virtualenvs/lv1_imageclassification_cv02-bp8_CroY/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument &#039;interpolation&#039; of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n  warnings.warn(\nEpoch[0/10](20/236) || training loss 2.663 || training accuracy 16.64% || lr 0.001\nEpoch[0/10](40/236) || training loss 2.311 || training accuracy 28.20% || lr 0.001\nEpoch[0/10](60/236) || training loss 2.185 || training accuracy 33.75% || lr 0.001\nEpoch[0/10](80/236) || training loss 2.066 || training accuracy 34.45% || lr 0.001\nEpoch[0/10](100/236) || training loss 1.981 || training accuracy 37.73% || lr 0.001\nEpoch[0/10](120/236) || training loss 1.88 || training accuracy 42.34% || lr 0.001\nEpoch[0/10](140/236) || training loss 1.826 || training accuracy 45.47% || lr 0.001\nEpoch[0/10](160/236) || training loss 1.77 || training accuracy 48.91% || lr 0.001\nEpoch[0/10](180/236) || training loss 1.716 || training accuracy 50.00% || lr 0.001\nEpoch[0/10](200/236) || training loss 1.68 || training accuracy 50.62% || lr 0.001\nEpoch[0/10](220/236) || training loss 1.571 || training accuracy 54.53% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 46.69%! saving the best model..\n[Val] acc : 46.69%, loss:  1.5 || best acc : 46.69%, best loss:  1.5\n \nEpoch[1/10](20/236) || training loss 1.464 || training accuracy 59.14% || lr 0.001\nEpoch[1/10](40/236) || training loss 1.425 || training accuracy 58.52% || lr 0.001\nEpoch[1/10](60/236) || training loss 1.434 || training accuracy 58.75% || lr 0.001\nEpoch[1/10](80/236) || training loss 1.39 || training accuracy 59.84% || lr 0.001\nEpoch[1/10](100/236) || training loss 1.33 || training accuracy 63.44% || lr 0.001\nEpoch[1/10](120/236) || training loss 1.243 || training accuracy 65.47% || lr 0.001\nEpoch[1/10](140/236) || training loss 1.243 || training accuracy 65.47% || lr 0.001\nEpoch[1/10](160/236) || training loss 1.204 || training accuracy 66.41% || lr 0.001\nEpoch[1/10](180/236) || training loss 1.173 || training accuracy 67.03% || lr 0.001\nEpoch[1/10](200/236) || training loss 1.165 || training accuracy 68.59% || lr 0.001\nEpoch[1/10](220/236) || training loss 1.145 || training accuracy 68.05% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 54.92%! saving the best model..\n[Val] acc : 54.92%, loss:  1.1 || best acc : 54.92%, best loss:  1.1\n \nEpoch[2/10](20/236) || training loss 1.083 || training accuracy 69.69% || lr 0.001\nEpoch[2/10](40/236) || training loss 0.9979 || training accuracy 72.42% || lr 0.001\nEpoch[2/10](60/236) || training loss 1.025 || training accuracy 71.17% || lr 0.001\nEpoch[2/10](80/236) || training loss 0.9999 || training accuracy 72.42% || lr 0.001\nEpoch[2/10](100/236) || training loss 0.9508 || training accuracy 73.59% || lr 0.001\nEpoch[2/10](120/236) || training loss 0.9475 || training accuracy 74.38% || lr 0.001\nEpoch[2/10](140/236) || training loss 0.858 || training accuracy 77.81% || lr 0.001\nEpoch[2/10](160/236) || training loss 0.8918 || training accuracy 75.78% || lr 0.001\nEpoch[2/10](180/236) || training loss 0.8423 || training accuracy 77.03% || lr 0.001\nEpoch[2/10](200/236) || training loss 0.8633 || training accuracy 75.08% || lr 0.001\nEpoch[2/10](220/236) || training loss 0.7923 || training accuracy 78.98% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 61.35%! saving the best model..\n[Val] acc : 61.35%, loss: 0.83 || best acc : 61.35%, best loss: 0.83\n \nEpoch[3/10](20/236) || training loss 0.7954 || training accuracy 78.12% || lr 0.001\nEpoch[3/10](40/236) || training loss 0.7668 || training accuracy 80.16% || lr 0.001\nEpoch[3/10](60/236) || training loss 0.7573 || training accuracy 79.84% || lr 0.001\nEpoch[3/10](80/236) || training loss 0.7483 || training accuracy 79.84% || lr 0.001\nEpoch[3/10](100/236) || training loss 0.7233 || training accuracy 80.94% || lr 0.001\nEpoch[3/10](120/236) || training loss 0.7215 || training accuracy 80.55% || lr 0.001\nEpoch[3/10](140/236) || training loss 0.6908 || training accuracy 80.78% || lr 0.001\nEpoch[3/10](160/236) || training loss 0.6719 || training accuracy 81.25% || lr 0.001\nEpoch[3/10](180/236) || training loss 0.6898 || training accuracy 82.66% || lr 0.001\nEpoch[3/10](200/236) || training loss 0.6436 || training accuracy 82.73% || lr 0.001\nEpoch[3/10](220/236) || training loss 0.6389 || training accuracy 84.22% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 64.71%! saving the best model..\n[Val] acc : 64.71%, loss: 0.67 || best acc : 64.71%, best loss: 0.67\n \nEpoch[4/10](20/236) || training loss 0.583 || training accuracy 86.02% || lr 0.001\nEpoch[4/10](40/236) || training loss 0.5989 || training accuracy 84.06% || lr 0.001\nEpoch[4/10](60/236) || training loss 0.5982 || training accuracy 84.30% || lr 0.001\nEpoch[4/10](80/236) || training loss 0.5952 || training accuracy 83.44% || lr 0.001\nEpoch[4/10](100/236) || training loss 0.594 || training accuracy 84.69% || lr 0.001\nEpoch[4/10](120/236) || training loss 0.5895 || training accuracy 83.91% || lr 0.001\nEpoch[4/10](140/236) || training loss 0.5601 || training accuracy 84.92% || lr 0.001\nEpoch[4/10](160/236) || training loss 0.5484 || training accuracy 84.69% || lr 0.001\nEpoch[4/10](180/236) || training loss 0.5619 || training accuracy 84.06% || lr 0.001\nEpoch[4/10](200/236) || training loss 0.5694 || training accuracy 84.14% || lr 0.001\nEpoch[4/10](220/236) || training loss 0.5262 || training accuracy 85.70% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 66.30%! saving the best model..\n[Val] acc : 66.30%, loss: 0.57 || best acc : 66.30%, best loss: 0.57\n \nEpoch[5/10](20/236) || training loss 0.4774 || training accuracy 87.50% || lr 0.001\nEpoch[5/10](40/236) || training loss 0.534 || training accuracy 84.38% || lr 0.001\nEpoch[5/10](60/236) || training loss 0.4892 || training accuracy 86.64% || lr 0.001\nEpoch[5/10](80/236) || training loss 0.5108 || training accuracy 86.33% || lr 0.001\nEpoch[5/10](100/236) || training loss 0.4871 || training accuracy 86.09% || lr 0.001\nEpoch[5/10](120/236) || training loss 0.5371 || training accuracy 84.61% || lr 0.001\nEpoch[5/10](140/236) || training loss 0.501 || training accuracy 86.48% || lr 0.001\nEpoch[5/10](160/236) || training loss 0.464 || training accuracy 87.66% || lr 0.001\nEpoch[5/10](180/236) || training loss 0.4492 || training accuracy 87.89% || lr 0.001\nEpoch[5/10](200/236) || training loss 0.4843 || training accuracy 87.42% || lr 0.001\nEpoch[5/10](220/236) || training loss 0.4314 || training accuracy 87.58% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 68.20%! saving the best model..\n[Val] acc : 68.20%, loss:  0.5 || best acc : 68.20%, best loss:  0.5\n \nEpoch[6/10](20/236) || training loss 0.4387 || training accuracy 87.81% || lr 0.001\nEpoch[6/10](40/236) || training loss 0.4259 || training accuracy 88.52% || lr 0.001\nEpoch[6/10](60/236) || training loss 0.4104 || training accuracy 89.53% || lr 0.001\nEpoch[6/10](80/236) || training loss 0.4057 || training accuracy 87.73% || lr 0.001\nEpoch[6/10](100/236) || training loss 0.4395 || training accuracy 87.42% || lr 0.001\nEpoch[6/10](120/236) || training loss 0.4188 || training accuracy 88.20% || lr 0.001\nEpoch[6/10](140/236) || training loss 0.3998 || training accuracy 89.22% || lr 0.001\nEpoch[6/10](160/236) || training loss 0.4294 || training accuracy 88.12% || lr 0.001\nEpoch[6/10](180/236) || training loss 0.3973 || training accuracy 90.08% || lr 0.001\nEpoch[6/10](200/236) || training loss 0.379 || training accuracy 90.23% || lr 0.001\nEpoch[6/10](220/236) || training loss 0.4163 || training accuracy 87.27% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 68.84%! saving the best model..\n[Val] acc : 68.84%, loss: 0.45 || best acc : 68.84%, best loss: 0.45\n \nEpoch[7/10](20/236) || training loss 0.343 || training accuracy 91.17% || lr 0.001\nEpoch[7/10](40/236) || training loss 0.3674 || training accuracy 90.00% || lr 0.001\nEpoch[7/10](60/236) || training loss 0.3756 || training accuracy 90.39% || lr 0.001\nEpoch[7/10](80/236) || training loss 0.3744 || training accuracy 88.91% || lr 0.001\nEpoch[7/10](100/236) || training loss 0.3935 || training accuracy 89.14% || lr 0.001\nEpoch[7/10](120/236) || training loss 0.3587 || training accuracy 90.62% || lr 0.001\nEpoch[7/10](140/236) || training loss 0.3971 || training accuracy 88.52% || lr 0.001\nEpoch[7/10](160/236) || training loss 0.3804 || training accuracy 90.00% || lr 0.001\nEpoch[7/10](180/236) || training loss 0.3774 || training accuracy 89.06% || lr 0.001\nEpoch[7/10](200/236) || training loss 0.3152 || training accuracy 91.09% || lr 0.001\nEpoch[7/10](220/236) || training loss 0.3563 || training accuracy 89.61% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 69.05%! saving the best model..\n[Val] acc : 69.05%, loss: 0.42 || best acc : 69.05%, best loss: 0.42\n \nEpoch[8/10](20/236) || training loss 0.3367 || training accuracy 90.55% || lr 0.001\nEpoch[8/10](40/236) || training loss 0.3338 || training accuracy 91.09% || lr 0.001\nEpoch[8/10](60/236) || training loss 0.3573 || training accuracy 89.38% || lr 0.001\nEpoch[8/10](80/236) || training loss 0.3324 || training accuracy 90.62% || lr 0.001\nEpoch[8/10](100/236) || training loss 0.33 || training accuracy 90.78% || lr 0.001\nEpoch[8/10](120/236) || training loss 0.3303 || training accuracy 91.09% || lr 0.001\nEpoch[8/10](140/236) || training loss 0.294 || training accuracy 92.34% || lr 0.001\nEpoch[8/10](160/236) || training loss 0.3216 || training accuracy 91.09% || lr 0.001\nEpoch[8/10](180/236) || training loss 0.3058 || training accuracy 91.80% || lr 0.001\nEpoch[8/10](200/236) || training loss 0.3355 || training accuracy 91.02% || lr 0.001\nEpoch[8/10](220/236) || training loss 0.3109 || training accuracy 90.94% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 69.44%! saving the best model..\n[Val] acc : 69.44%, loss: 0.38 || best acc : 69.44%, best loss: 0.38\n \nEpoch[9/10](20/236) || training loss 0.29 || training accuracy 92.66% || lr 0.001\nEpoch[9/10](40/236) || training loss 0.3052 || training accuracy 91.09% || lr 0.001\nEpoch[9/10](60/236) || training loss 0.2971 || training accuracy 91.02% || lr 0.001\nEpoch[9/10](80/236) || training loss 0.317 || training accuracy 91.02% || lr 0.001\nEpoch[9/10](100/236) || training loss 0.3074 || training accuracy 91.41% || lr 0.001\nEpoch[9/10](120/236) || training loss 0.2781 || training accuracy 91.72% || lr 0.001\nEpoch[9/10](140/236) || training loss 0.2847 || training accuracy 91.80% || lr 0.001\nEpoch[9/10](160/236) || training loss 0.2853 || training accuracy 92.19% || lr 0.001\nEpoch[9/10](180/236) || training loss 0.3097 || training accuracy 91.17% || lr 0.001\nEpoch[9/10](200/236) || training loss 0.2889 || training accuracy 91.95% || lr 0.001\nEpoch[9/10](220/236) || training loss 0.275 || training accuracy 93.59% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 69.63%! saving the best model..\n[Val] acc : 69.63%, loss: 0.37 || best acc : 69.63%, best loss: 0.37\n\n\n출력(required_grad = false)\nNamespace(augmentation=&#039;BaseAugmentation&#039;, batch_size=64, criterion=&#039;cross_entropy&#039;, data_dir=&#039;/opt/ml/repos/lv1_imageclassification_cv02/data/train/images&#039;, dataset=&#039;MaskBaseDataset&#039;, epochs=10, log_interval=20, lr=0.001, lr_decay_step=20, model=&#039;MyModel&#039;, model_dir=&#039;/opt/ml/repos/lv1_imageclassification_cv02/local/v2/model&#039;, name=&#039;exp&#039;, optimizer=&#039;SGD&#039;, resize=[224, 224], seed=42, val_ratio=0.2, valid_batch_size=1000)\nsave_dir is  /opt/ml/repos/lv1_imageclassification_cv02/local/v2/model/exp2\n/opt/ml/.local/share/virtualenvs/lv1_imageclassification_cv02-bp8_CroY/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument &#039;interpolation&#039; of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n  warnings.warn(\nEpoch[0/10](20/236) || training loss 2.801 || training accuracy 10.16% || lr 0.001\nEpoch[0/10](40/236) || training loss 2.536 || training accuracy 20.62% || lr 0.001\nEpoch[0/10](60/236) || training loss 2.401 || training accuracy 28.12% || lr 0.001\nEpoch[0/10](80/236) || training loss 2.286 || training accuracy 29.53% || lr 0.001\nEpoch[0/10](100/236) || training loss 2.212 || training accuracy 30.94% || lr 0.001\nEpoch[0/10](120/236) || training loss 2.129 || training accuracy 35.94% || lr 0.001\nEpoch[0/10](140/236) || training loss 2.098 || training accuracy 36.02% || lr 0.001\nEpoch[0/10](160/236) || training loss 2.072 || training accuracy 37.19% || lr 0.001\nEpoch[0/10](180/236) || training loss 2.04 || training accuracy 37.66% || lr 0.001\nEpoch[0/10](200/236) || training loss 2.022 || training accuracy 37.66% || lr 0.001\nEpoch[0/10](220/236) || training loss 1.961 || training accuracy 41.09% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 34.97%! saving the best model..\n[Val] acc : 34.97%, loss:  1.9 || best acc : 34.97%, best loss:  1.9\n \nEpoch[1/10](20/236) || training loss 1.885 || training accuracy 43.75% || lr 0.001\nEpoch[1/10](40/236) || training loss 1.889 || training accuracy 43.05% || lr 0.001\nEpoch[1/10](60/236) || training loss 1.901 || training accuracy 42.73% || lr 0.001\nEpoch[1/10](80/236) || training loss 1.878 || training accuracy 43.59% || lr 0.001\nEpoch[1/10](100/236) || training loss 1.86 || training accuracy 45.16% || lr 0.001\nEpoch[1/10](120/236) || training loss 1.762 || training accuracy 47.66% || lr 0.001\nEpoch[1/10](140/236) || training loss 1.784 || training accuracy 48.12% || lr 0.001\nEpoch[1/10](160/236) || training loss 1.774 || training accuracy 48.05% || lr 0.001\nEpoch[1/10](180/236) || training loss 1.73 || training accuracy 49.77% || lr 0.001\nEpoch[1/10](200/236) || training loss 1.752 || training accuracy 49.84% || lr 0.001\nEpoch[1/10](220/236) || training loss 1.744 || training accuracy 49.92% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 40.48%! saving the best model..\n[Val] acc : 40.48%, loss:  1.7 || best acc : 40.48%, best loss:  1.7\n \nEpoch[2/10](20/236) || training loss 1.722 || training accuracy 50.31% || lr 0.001\nEpoch[2/10](40/236) || training loss 1.644 || training accuracy 54.14% || lr 0.001\nEpoch[2/10](60/236) || training loss 1.68 || training accuracy 52.03% || lr 0.001\nEpoch[2/10](80/236) || training loss 1.654 || training accuracy 52.03% || lr 0.001\nEpoch[2/10](100/236) || training loss 1.615 || training accuracy 52.81% || lr 0.001\nEpoch[2/10](120/236) || training loss 1.667 || training accuracy 52.50% || lr 0.001\nEpoch[2/10](140/236) || training loss 1.582 || training accuracy 54.77% || lr 0.001\nEpoch[2/10](160/236) || training loss 1.608 || training accuracy 54.61% || lr 0.001\nEpoch[2/10](180/236) || training loss 1.552 || training accuracy 54.77% || lr 0.001\nEpoch[2/10](200/236) || training loss 1.576 || training accuracy 53.83% || lr 0.001\nEpoch[2/10](220/236) || training loss 1.522 || training accuracy 57.19% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 44.60%! saving the best model..\n[Val] acc : 44.60%, loss:  1.5 || best acc : 44.60%, best loss:  1.5\n \nEpoch[3/10](20/236) || training loss 1.537 || training accuracy 55.31% || lr 0.001\nEpoch[3/10](40/236) || training loss 1.521 || training accuracy 55.86% || lr 0.001\nEpoch[3/10](60/236) || training loss 1.524 || training accuracy 55.31% || lr 0.001\nEpoch[3/10](80/236) || training loss 1.526 || training accuracy 54.77% || lr 0.001\nEpoch[3/10](100/236) || training loss 1.482 || training accuracy 55.55% || lr 0.001\nEpoch[3/10](120/236) || training loss 1.479 || training accuracy 58.44% || lr 0.001\nEpoch[3/10](140/236) || training loss 1.46 || training accuracy 59.14% || lr 0.001\nEpoch[3/10](160/236) || training loss 1.474 || training accuracy 58.20% || lr 0.001\nEpoch[3/10](180/236) || training loss 1.459 || training accuracy 58.67% || lr 0.001\nEpoch[3/10](200/236) || training loss 1.466 || training accuracy 58.05% || lr 0.001\nEpoch[3/10](220/236) || training loss 1.433 || training accuracy 58.98% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 47.17%! saving the best model..\n[Val] acc : 47.17%, loss:  1.4 || best acc : 47.17%, best loss:  1.4\n \nEpoch[4/10](20/236) || training loss 1.435 || training accuracy 57.58% || lr 0.001\nEpoch[4/10](40/236) || training loss 1.414 || training accuracy 58.36% || lr 0.001\nEpoch[4/10](60/236) || training loss 1.418 || training accuracy 57.89% || lr 0.001\nEpoch[4/10](80/236) || training loss 1.42 || training accuracy 59.22% || lr 0.001\nEpoch[4/10](100/236) || training loss 1.413 || training accuracy 60.16% || lr 0.001\nEpoch[4/10](120/236) || training loss 1.401 || training accuracy 58.20% || lr 0.001\nEpoch[4/10](140/236) || training loss 1.369 || training accuracy 62.11% || lr 0.001\nEpoch[4/10](160/236) || training loss 1.343 || training accuracy 62.03% || lr 0.001\nEpoch[4/10](180/236) || training loss 1.359 || training accuracy 60.94% || lr 0.001\nEpoch[4/10](200/236) || training loss 1.368 || training accuracy 60.94% || lr 0.001\nEpoch[4/10](220/236) || training loss 1.384 || training accuracy 59.77% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 48.94%! saving the best model..\n[Val] acc : 48.94%, loss:  1.3 || best acc : 48.94%, best loss:  1.3\n \nEpoch[5/10](20/236) || training loss 1.316 || training accuracy 61.48% || lr 0.001\nEpoch[5/10](40/236) || training loss 1.362 || training accuracy 60.62% || lr 0.001\nEpoch[5/10](60/236) || training loss 1.33 || training accuracy 63.28% || lr 0.001\nEpoch[5/10](80/236) || training loss 1.322 || training accuracy 61.64% || lr 0.001\nEpoch[5/10](100/236) || training loss 1.329 || training accuracy 61.17% || lr 0.001\nEpoch[5/10](120/236) || training loss 1.341 || training accuracy 61.09% || lr 0.001\nEpoch[5/10](140/236) || training loss 1.354 || training accuracy 59.61% || lr 0.001\nEpoch[5/10](160/236) || training loss 1.306 || training accuracy 62.03% || lr 0.001\nEpoch[5/10](180/236) || training loss 1.276 || training accuracy 64.61% || lr 0.001\nEpoch[5/10](200/236) || training loss 1.312 || training accuracy 62.19% || lr 0.001\nEpoch[5/10](220/236) || training loss 1.255 || training accuracy 62.81% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 50.45%! saving the best model..\n[Val] acc : 50.45%, loss:  1.3 || best acc : 50.45%, best loss:  1.3\n \nEpoch[6/10](20/236) || training loss 1.293 || training accuracy 62.42% || lr 0.001\nEpoch[6/10](40/236) || training loss 1.268 || training accuracy 64.22% || lr 0.001\nEpoch[6/10](60/236) || training loss 1.232 || training accuracy 65.00% || lr 0.001\nEpoch[6/10](80/236) || training loss 1.264 || training accuracy 63.36% || lr 0.001\nEpoch[6/10](100/236) || training loss 1.278 || training accuracy 62.97% || lr 0.001\nEpoch[6/10](120/236) || training loss 1.261 || training accuracy 63.20% || lr 0.001\nEpoch[6/10](140/236) || training loss 1.266 || training accuracy 61.88% || lr 0.001\nEpoch[6/10](160/236) || training loss 1.279 || training accuracy 62.50% || lr 0.001\nEpoch[6/10](180/236) || training loss 1.215 || training accuracy 63.52% || lr 0.001\nEpoch[6/10](200/236) || training loss 1.195 || training accuracy 66.80% || lr 0.001\nEpoch[6/10](220/236) || training loss 1.266 || training accuracy 62.27% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 51.48%! saving the best model..\n[Val] acc : 51.48%, loss:  1.2 || best acc : 51.48%, best loss:  1.2\n \nEpoch[7/10](20/236) || training loss 1.198 || training accuracy 65.70% || lr 0.001\nEpoch[7/10](40/236) || training loss 1.226 || training accuracy 63.98% || lr 0.001\nEpoch[7/10](60/236) || training loss 1.218 || training accuracy 64.22% || lr 0.001\nEpoch[7/10](80/236) || training loss 1.229 || training accuracy 65.31% || lr 0.001\nEpoch[7/10](100/236) || training loss 1.241 || training accuracy 62.19% || lr 0.001\nEpoch[7/10](120/236) || training loss 1.198 || training accuracy 64.77% || lr 0.001\nEpoch[7/10](140/236) || training loss 1.225 || training accuracy 63.12% || lr 0.001\nEpoch[7/10](160/236) || training loss 1.253 || training accuracy 62.81% || lr 0.001\nEpoch[7/10](180/236) || training loss 1.201 || training accuracy 65.23% || lr 0.001\nEpoch[7/10](200/236) || training loss 1.137 || training accuracy 67.58% || lr 0.001\nEpoch[7/10](220/236) || training loss 1.186 || training accuracy 64.22% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 52.38%! saving the best model..\n[Val] acc : 52.38%, loss:  1.2 || best acc : 52.38%, best loss:  1.2\n \nEpoch[8/10](20/236) || training loss 1.186 || training accuracy 65.94% || lr 0.001\nEpoch[8/10](40/236) || training loss 1.166 || training accuracy 66.17% || lr 0.001\nEpoch[8/10](60/236) || training loss 1.208 || training accuracy 64.61% || lr 0.001\nEpoch[8/10](80/236) || training loss 1.166 || training accuracy 65.47% || lr 0.001\nEpoch[8/10](100/236) || training loss 1.192 || training accuracy 64.14% || lr 0.001\nEpoch[8/10](120/236) || training loss 1.149 || training accuracy 66.17% || lr 0.001\nEpoch[8/10](140/236) || training loss 1.128 || training accuracy 68.12% || lr 0.001\nEpoch[8/10](160/236) || training loss 1.147 || training accuracy 64.92% || lr 0.001\nEpoch[8/10](180/236) || training loss 1.14 || training accuracy 67.97% || lr 0.001\nEpoch[8/10](200/236) || training loss 1.197 || training accuracy 63.59% || lr 0.001\nEpoch[8/10](220/236) || training loss 1.171 || training accuracy 64.69% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 52.75%! saving the best model..\n[Val] acc : 52.75%, loss:  1.2 || best acc : 52.75%, best loss:  1.2\n \nEpoch[9/10](20/236) || training loss 1.165 || training accuracy 65.55% || lr 0.001\nEpoch[9/10](40/236) || training loss 1.14 || training accuracy 66.80% || lr 0.001\nEpoch[9/10](60/236) || training loss 1.127 || training accuracy 65.86% || lr 0.001\nEpoch[9/10](80/236) || training loss 1.182 || training accuracy 64.30% || lr 0.001\nEpoch[9/10](100/236) || training loss 1.122 || training accuracy 67.58% || lr 0.001\nEpoch[9/10](120/236) || training loss 1.141 || training accuracy 67.11% || lr 0.001\nEpoch[9/10](140/236) || training loss 1.107 || training accuracy 67.73% || lr 0.001\nEpoch[9/10](160/236) || training loss 1.127 || training accuracy 66.25% || lr 0.001\nEpoch[9/10](180/236) || training loss 1.162 || training accuracy 64.06% || lr 0.001\nEpoch[9/10](200/236) || training loss 1.087 || training accuracy 69.45% || lr 0.001\nEpoch[9/10](220/236) || training loss 1.099 || training accuracy 68.05% || lr 0.001\nCalculating validation results...\nNew best model for val accuracy : 53.52%! saving the best model..\n[Val] acc : 53.52%, loss:  1.1 || best acc : 53.52%, best loss:  1.1\n\n\nTrain, Validation 메소드를 baseline code에서 가져와 원래 프로젝트에 맞게 변경\n→ 그래도 안된다…\n원인발견\nvailidation을 위한 image loader 시, val_df가 아닌 train_df을 넣어버렸다…\n\n\n문제의 원본\ntrain_img_paths = train_df[&#039;path&#039;].values\n    train_labels = [U.convertAgeGenderMaskToLabel(m, g, a) for m, g, a in zip(train_df[&#039;mask_class&#039;].values, train_df[&#039;gender_class&#039;].values, train_df[&#039;age_class&#039;].values)]\n    val_img_paths = val_df[&#039;path&#039;].values\n    val_labels = [U.convertAgeGenderMaskToLabel(m, g, a) for m, g, a in zip(train_df[&#039;mask_class&#039;].values, train_df[&#039;gender_class&#039;].values, train_df[&#039;age_class&#039;].values)]\n\n\n변경\ntrain_img_paths = train_df[&#039;path&#039;].values\n    train_labels = [U.convertAgeGenderMaskToLabel(m, g, a) for m, g, a in zip(train_df[&#039;mask_class&#039;].values, train_df[&#039;gender_class&#039;].values, train_df[&#039;age_class&#039;].values)]\n    val_img_paths = val_df[&#039;path&#039;].values\n    val_labels = [U.convertAgeGenderMaskToLabel(m, g, a) for m, g, a in zip(val_df[&#039;mask_class&#039;].values, val_df[&#039;gender_class&#039;].values, val_df[&#039;age_class&#039;].values)]\n\n\n원인 발견 후, 훈련 및 결과\n\n\n출력 backbone not freeze, lr 0.001, batch 64, image size 224\nBest : V.Acc[0.67969] / F1[0.55520]\npython trainALL.py --save_name=Weight_VIT_V1.KHS.tar --target_model=&quot;VIT_V0_KHS(False)&quot;\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V1.KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V0_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:43&lt;00:00,  2.21s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.41s/it]\nEp[1] / T.Loss[2.35598] / T.Acc[0.22917] / V.Loss[2.28715] / V.Acc[0.25859] / F1[0.03890]\n * New Best Model -&gt; Epoch [1] / best_score : [0.25859]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:06&lt;00:00,  2.56s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:28&lt;00:00,  1.47s/it]\nEp[2] / T.Loss[2.09842] / T.Acc[0.30505] / V.Loss[1.99231] / V.Acc[0.34063] / F1[0.10350]\n * New Best Model -&gt; Epoch [2] / best_score : [0.34063]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:04&lt;00:00,  2.55s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:16&lt;00:00,  1.27s/it]\nEp[3] / T.Loss[1.73236] / T.Acc[0.41851] / V.Loss[1.65402] / V.Acc[0.44896] / F1[0.19274]\n * New Best Model -&gt; Epoch [3] / best_score : [0.44896]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:12&lt;00:00,  2.58s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:20&lt;00:00,  1.35s/it]\nEp[4] / T.Loss[1.35878] / T.Acc[0.54252] / V.Loss[1.29845] / V.Acc[0.55651] / F1[0.36214]\n * New Best Model -&gt; Epoch [4] / best_score : [0.55651]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:02&lt;00:00,  2.54s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.33s/it]\nEp[5] / T.Loss[1.12199] / T.Acc[0.61617] / V.Loss[1.11763] / V.Acc[0.60781] / F1[0.39866]\n * New Best Model -&gt; Epoch [5] / best_score : [0.60781]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [09:59&lt;00:00,  2.53s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.32s/it]\nEp[6] / T.Loss[0.96528] / T.Acc[0.66667] / V.Loss[1.11791] / V.Acc[0.61146] / F1[0.38909]\n * New Best Model -&gt; Epoch [6] / best_score : [0.61146]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:08&lt;00:00,  2.57s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:20&lt;00:00,  1.34s/it]\nEp[7] / T.Loss[0.85278] / T.Acc[0.69818] / V.Loss[1.05862] / V.Acc[0.61797] / F1[0.46073]\n * New Best Model -&gt; Epoch [7] / best_score : [0.61797]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:11&lt;00:00,  2.58s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:18&lt;00:00,  1.31s/it]\nEp[8] / T.Loss[0.76820] / T.Acc[0.72752] / V.Loss[1.04059] / V.Acc[0.63333] / F1[0.48444]\n * New Best Model -&gt; Epoch [8] / best_score : [0.63333]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [09:57&lt;00:00,  2.52s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:25&lt;00:00,  1.43s/it]\nEp[9] / T.Loss[0.68204] / T.Acc[0.76035] / V.Loss[1.04405] / V.Acc[0.66224] / F1[0.49360]\n * New Best Model -&gt; Epoch [9] / best_score : [0.66224]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:14&lt;00:00,  2.59s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.41s/it]\nEp[10] / T.Loss[0.62996] / T.Acc[0.77696] / V.Loss[1.05925] / V.Acc[0.65104] / F1[0.49769]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:09&lt;00:00,  2.57s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:22&lt;00:00,  1.37s/it]\nEp[11] / T.Loss[0.56258] / T.Acc[0.79971] / V.Loss[1.01786] / V.Acc[0.65026] / F1[0.49967]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:08&lt;00:00,  2.57s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.32s/it]\nEp[12] / T.Loss[0.50302] / T.Acc[0.82021] / V.Loss[1.00199] / V.Acc[0.66615] / F1[0.55401]\n * New Best Model -&gt; Epoch [12] / best_score : [0.66615]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:59&lt;00:00,  2.02s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[13] / T.Loss[0.46993] / T.Acc[0.82964] / V.Loss[1.07661] / V.Acc[0.65964] / F1[0.49023]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:22&lt;00:00,  1.61s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[14] / T.Loss[0.42177] / T.Acc[0.84751] / V.Loss[1.18239] / V.Acc[0.66276] / F1[0.49722]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:29&lt;00:00,  1.64s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.21it/s]\nEp[15] / T.Loss[0.38900] / T.Acc[0.85687] / V.Loss[1.06578] / V.Acc[0.67760] / F1[0.52162]\n * New Best Model -&gt; Epoch [15] / best_score : [0.67760]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:25&lt;00:00,  1.63s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[16] / T.Loss[0.35508] / T.Acc[0.86841] / V.Loss[1.10693] / V.Acc[0.67969] / F1[0.55520]\n * New Best Model -&gt; Epoch [16] / best_score : [0.67969]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:27&lt;00:00,  1.64s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[17] / T.Loss[0.32741] / T.Acc[0.88205] / V.Loss[1.15547] / V.Acc[0.67161] / F1[0.53029]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:21&lt;00:00,  1.61s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[18] / T.Loss[0.30776] / T.Acc[0.88252] / V.Loss[1.12714] / V.Acc[0.67682] / F1[0.53797]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:27&lt;00:00,  1.64s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[19] / T.Loss[0.27441] / T.Acc[0.89511] / V.Loss[1.23778] / V.Acc[0.64922] / F1[0.48549]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:29&lt;00:00,  1.64s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[20] / T.Loss[0.25071] / T.Acc[0.90388] / V.Loss[1.17095] / V.Acc[0.67526] / F1[0.53311]\n\n\n출력 backbone freeze, lr 0.001, batch 64, image size 224\n==Best : V.Acc[0.80156] / F1[0.68465]==\npython trainALL.py --save_name=Weight_VIT_V0.KHS.tar --target_model=&quot;VIT_V0_KHS(True)&quot;\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V0.KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V0_KHS(True)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:42&lt;00:00,  1.44s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.35s/it]\nEp[1] / T.Loss[1.01287] / T.Acc[0.68176] / V.Loss[0.76897] / V.Acc[0.74687] / F1[0.59991]\n * New Best Model -&gt; Epoch [1] / best_score : [0.74687]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:35&lt;00:00,  1.42s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.44s/it]\nEp[2] / T.Loss[0.60664] / T.Acc[0.80729] / V.Loss[0.66516] / V.Acc[0.77005] / F1[0.63105]\n * New Best Model -&gt; Epoch [2] / best_score : [0.77005]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:39&lt;00:00,  1.43s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.36s/it]\nEp[3] / T.Loss[0.51408] / T.Acc[0.83511] / V.Loss[0.62337] / V.Acc[0.78281] / F1[0.65041]\n * New Best Model -&gt; Epoch [3] / best_score : [0.78281]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:38&lt;00:00,  1.43s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:25&lt;00:00,  1.42s/it]\nEp[4] / T.Loss[0.45651] / T.Acc[0.85476] / V.Loss[0.60633] / V.Acc[0.78281] / F1[0.64349]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:31&lt;00:00,  1.40s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.32s/it]\nEp[5] / T.Loss[0.41884] / T.Acc[0.86702] / V.Loss[0.58609] / V.Acc[0.79505] / F1[0.68581]\n * New Best Model -&gt; Epoch [5] / best_score : [0.79505]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:31&lt;00:00,  1.40s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.44s/it]\nEp[6] / T.Loss[0.38804] / T.Acc[0.88014] / V.Loss[0.58559] / V.Acc[0.79245] / F1[0.67278]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:39&lt;00:00,  1.43s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:28&lt;00:00,  1.48s/it]\nEp[7] / T.Loss[0.36484] / T.Acc[0.88443] / V.Loss[0.57582] / V.Acc[0.79375] / F1[0.67049]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:44&lt;00:00,  1.45s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.40s/it]\nEp[8] / T.Loss[0.34282] / T.Acc[0.89280] / V.Loss[0.58755] / V.Acc[0.79063] / F1[0.66752]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:31&lt;00:00,  1.40s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.44s/it]\nEp[9] / T.Loss[0.32651] / T.Acc[0.89695] / V.Loss[0.56441] / V.Acc[0.80156] / F1[0.68465]\n * New Best Model -&gt; Epoch [9] / best_score : [0.80156]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:35&lt;00:00,  1.42s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.37s/it]\nEp[10] / T.Loss[0.31110] / T.Acc[0.90157] / V.Loss[0.56519] / V.Acc[0.79688] / F1[0.67979]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:38&lt;00:00,  1.43s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.41s/it]\nEp[11] / T.Loss[0.29579] / T.Acc[0.90763] / V.Loss[0.57807] / V.Acc[0.79010] / F1[0.67123]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:37&lt;00:00,  1.43s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.40s/it]\nEp[12] / T.Loss[0.28497] / T.Acc[0.91390] / V.Loss[0.58366] / V.Acc[0.79036] / F1[0.67093]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:40&lt;00:00,  1.44s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.44s/it]\nEp[13] / T.Loss[0.27343] / T.Acc[0.91700] / V.Loss[0.56615] / V.Acc[0.79505] / F1[0.66275]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:35&lt;00:00,  1.42s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.45s/it]\nEp[14] / T.Loss[0.26345] / T.Acc[0.92102] / V.Loss[0.56966] / V.Acc[0.79661] / F1[0.67525]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:35&lt;00:00,  1.42s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:23&lt;00:00,  1.39s/it]\nEp[15] / T.Loss[0.25552] / T.Acc[0.92240] / V.Loss[0.57085] / V.Acc[0.79766] / F1[0.67867]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:40&lt;00:00,  1.44s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.32s/it]\nEp[16] / T.Loss[0.24716] / T.Acc[0.92491] / V.Loss[0.57247] / V.Acc[0.79609] / F1[0.67629]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:42&lt;00:00,  1.44s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.41s/it]\nEp[17] / T.Loss[0.23834] / T.Acc[0.92807] / V.Loss[0.57362] / V.Acc[0.79505] / F1[0.67354]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:37&lt;00:00,  1.43s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:20&lt;00:00,  1.35s/it]\nEp[18] / T.Loss[0.23141] / T.Acc[0.93163] / V.Loss[0.58015] / V.Acc[0.79505] / F1[0.67538]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:32&lt;00:00,  1.40s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.35s/it]\nEp[19] / T.Loss[0.22470] / T.Acc[0.93493] / V.Loss[0.59474] / V.Acc[0.78958] / F1[0.66586]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:40&lt;00:00,  1.44s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.35s/it]\nEp[20] / T.Loss[0.21834] / T.Acc[0.93440] / V.Loss[0.57225] / V.Acc[0.79818] / F1[0.68122]\n\n\n출력 backbone not freeze, lr 0.001, batch 32, image size 224\nBest : V.Acc[0.68803] / F1[0.53817]\npython trainALL.py --save_name=Weight_VIT_V3_KHS.tar --target_model=&quot;VIT_V0_KHS(False)&quot; --batch_size=32\nNamespace(batch_size=32, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V3_KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V0_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:18&lt;00:00,  1.31s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:20&lt;00:00,  1.48it/s]\nEp[1] / T.Loss[2.35882] / T.Acc[0.23077] / V.Loss[2.24234] / V.Acc[0.30935] / F1[0.06268]\n * New Best Model -&gt; Epoch [1] / best_score : [0.30935]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:26&lt;00:00,  1.32s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.47it/s]\nEp[2] / T.Loss[2.05232] / T.Acc[0.31917] / V.Loss[2.04367] / V.Acc[0.33456] / F1[0.08747]\n * New Best Model -&gt; Epoch [2] / best_score : [0.33456]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:22&lt;00:00,  1.32s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.46it/s]\nEp[3] / T.Loss[1.70743] / T.Acc[0.43036] / V.Loss[1.55862] / V.Acc[0.46507] / F1[0.24850]\n * New Best Model -&gt; Epoch [3] / best_score : [0.46507]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:16&lt;00:00,  1.30s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.46it/s]\nEp[4] / T.Loss[1.37479] / T.Acc[0.53621] / V.Loss[1.38156] / V.Acc[0.52048] / F1[0.35487]\n * New Best Model -&gt; Epoch [4] / best_score : [0.52048]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:26&lt;00:00,  1.33s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:17&lt;00:00,  1.53it/s]\nEp[5] / T.Loss[1.15002] / T.Acc[0.60610] / V.Loss[1.19543] / V.Acc[0.59611] / F1[0.40237]\n * New Best Model -&gt; Epoch [5] / best_score : [0.59611]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:28&lt;00:00,  1.33s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:19&lt;00:00,  1.50it/s]\nEp[6] / T.Loss[0.97343] / T.Acc[0.66246] / V.Loss[1.17950] / V.Acc[0.58613] / F1[0.40528]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:21&lt;00:00,  1.31s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:20&lt;00:00,  1.48it/s]\nEp[7] / T.Loss[0.86760] / T.Acc[0.70085] / V.Loss[1.09320] / V.Acc[0.63813] / F1[0.48825]\n * New Best Model -&gt; Epoch [7] / best_score : [0.63813]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:25&lt;00:00,  1.32s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:25&lt;00:00,  1.40it/s]\nEp[8] / T.Loss[0.78374] / T.Acc[0.72774] / V.Loss[0.99804] / V.Acc[0.65678] / F1[0.52388]\n * New Best Model -&gt; Epoch [8] / best_score : [0.65678]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:19&lt;00:00,  1.31s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:19&lt;00:00,  1.49it/s]\nEp[9] / T.Loss[0.70054] / T.Acc[0.75251] / V.Loss[1.04469] / V.Acc[0.65389] / F1[0.49099]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:22&lt;00:00,  1.32s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:22&lt;00:00,  1.45it/s]\nEp[10] / T.Loss[0.61854] / T.Acc[0.78165] / V.Loss[1.01717] / V.Acc[0.66518] / F1[0.53997]\n * New Best Model -&gt; Epoch [10] / best_score : [0.66518]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:27&lt;00:00,  1.33s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:20&lt;00:00,  1.47it/s]\nEp[11] / T.Loss[0.56697] / T.Acc[0.79982] / V.Loss[0.95836] / V.Acc[0.67516] / F1[0.53046]\n * New Best Model -&gt; Epoch [11] / best_score : [0.67516]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:18&lt;00:00,  1.31s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:00&lt;00:00,  1.97it/s]\nEp[12] / T.Loss[0.50977] / T.Acc[0.81488] / V.Loss[1.09090] / V.Acc[0.67201] / F1[0.53584]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:52&lt;00:00,  2.27it/s]\nEp[13] / T.Loss[0.46537] / T.Acc[0.83278] / V.Loss[1.00580] / V.Acc[0.68251] / F1[0.54040]\n * New Best Model -&gt; Epoch [13] / best_score : [0.68251]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:52&lt;00:00,  2.26it/s]\nEp[14] / T.Loss[0.39941] / T.Acc[0.85485] / V.Loss[1.15871] / V.Acc[0.67647] / F1[0.52933]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:25&lt;00:00,  1.23it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:51&lt;00:00,  2.31it/s]\nEp[15] / T.Loss[0.38542] / T.Acc[0.86000] / V.Loss[1.15737] / V.Acc[0.68251] / F1[0.54623]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:32&lt;00:00,  1.20it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:49&lt;00:00,  2.40it/s]\nEp[16] / T.Loss[0.35436] / T.Acc[0.87018] / V.Loss[1.22028] / V.Acc[0.65546] / F1[0.49839]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:32&lt;00:00,  1.21it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:51&lt;00:00,  2.31it/s]\nEp[17] / T.Loss[0.32017] / T.Acc[0.88293] / V.Loss[1.31025] / V.Acc[0.68803] / F1[0.53817]\n * New Best Model -&gt; Epoch [17] / best_score : [0.68803]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:29&lt;00:00,  1.22it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:52&lt;00:00,  2.26it/s]\nEp[18] / T.Loss[0.28085] / T.Acc[0.89984] / V.Loss[1.26609] / V.Acc[0.68277] / F1[0.56806]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:45&lt;00:00,  2.63it/s]\nEp[19] / T.Loss[0.25701] / T.Acc[0.90684] / V.Loss[1.28518] / V.Acc[0.68409] / F1[0.50869]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:06&lt;00:00,  1.54it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:28&lt;00:00,  4.12it/s]\nEp[20] / T.Loss[0.23210] / T.Acc[0.91788] / V.Loss[1.37192] / V.Acc[0.66833] / F1[0.52610]\n\n\n출력 backbone freeze, lr 0.001, batch 32, image size 224\nBest : V.Acc[0.80672] / F1[0.67891]\npython trainALL.py --save_name=Weight_VIT_V4_KHS.tar --target_model=&quot;VIT_V0_KHS(True)&quot; --batch_size=32\nNamespace(batch_size=32, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V4_KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V0_KHS(True)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:43&lt;00:00,  1.38it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:22&lt;00:00,  1.45it/s]\nEp[1] / T.Loss[0.90020] / T.Acc[0.71545] / V.Loss[0.70784] / V.Acc[0.76628] / F1[0.61103]\n * New Best Model -&gt; Epoch [1] / best_score : [0.76628]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:40&lt;00:00,  1.39it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.42it/s]\nEp[2] / T.Loss[0.54853] / T.Acc[0.82426] / V.Loss[0.63471] / V.Acc[0.78755] / F1[0.64693]\n * New Best Model -&gt; Epoch [2] / best_score : [0.78755]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:39&lt;00:00,  1.39it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.46it/s]\nEp[3] / T.Loss[0.46461] / T.Acc[0.85016] / V.Loss[0.60170] / V.Acc[0.79123] / F1[0.64586]\n * New Best Model -&gt; Epoch [3] / best_score : [0.79123]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:36&lt;00:00,  1.41it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.41it/s]\nEp[4] / T.Loss[0.41091] / T.Acc[0.86853] / V.Loss[0.60359] / V.Acc[0.78493] / F1[0.65325]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:38&lt;00:00,  1.40it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.47it/s]\nEp[5] / T.Loss[0.37491] / T.Acc[0.88062] / V.Loss[0.58578] / V.Acc[0.79412] / F1[0.66977]\n * New Best Model -&gt; Epoch [5] / best_score : [0.79412]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:34&lt;00:00,  1.41it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.40it/s]\nEp[6] / T.Loss[0.34709] / T.Acc[0.89072] / V.Loss[0.58770] / V.Acc[0.80042] / F1[0.67648]\n * New Best Model -&gt; Epoch [6] / best_score : [0.80042]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:31&lt;00:00,  1.42it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.40it/s]\nEp[7] / T.Loss[0.32333] / T.Acc[0.89859] / V.Loss[0.58049] / V.Acc[0.80173] / F1[0.67949]\n * New Best Model -&gt; Epoch [7] / best_score : [0.80173]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:39&lt;00:00,  1.39it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.45it/s]\nEp[8] / T.Loss[0.30371] / T.Acc[0.90817] / V.Loss[0.59278] / V.Acc[0.79779] / F1[0.66623]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:35&lt;00:00,  1.41it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.43it/s]\nEp[9] / T.Loss[0.28532] / T.Acc[0.91385] / V.Loss[0.58300] / V.Acc[0.80593] / F1[0.68009]\n * New Best Model -&gt; Epoch [9] / best_score : [0.80593]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:37&lt;00:00,  1.40it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.47it/s]\nEp[10] / T.Loss[0.27151] / T.Acc[0.91801] / V.Loss[0.57586] / V.Acc[0.80095] / F1[0.67977]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:43&lt;00:00,  1.38it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.43it/s]\nEp[11] / T.Loss[0.25854] / T.Acc[0.92065] / V.Loss[0.58382] / V.Acc[0.79701] / F1[0.67650]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:32&lt;00:00,  1.42it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.43it/s]\nEp[12] / T.Loss[0.24652] / T.Acc[0.92548] / V.Loss[0.60004] / V.Acc[0.79727] / F1[0.66746]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:40&lt;00:00,  1.39it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.41it/s]\nEp[13] / T.Loss[0.23740] / T.Acc[0.92845] / V.Loss[0.58483] / V.Acc[0.80042] / F1[0.67031]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:36&lt;00:00,  1.40it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:25&lt;00:00,  1.40it/s]\nEp[14] / T.Loss[0.22740] / T.Acc[0.93281] / V.Loss[0.59447] / V.Acc[0.79727] / F1[0.66578]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:34&lt;00:00,  1.41it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.41it/s]\nEp[15] / T.Loss[0.21846] / T.Acc[0.93605] / V.Loss[0.59572] / V.Acc[0.79727] / F1[0.67786]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:43&lt;00:00,  1.38it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:25&lt;00:00,  1.39it/s]\nEp[16] / T.Loss[0.20956] / T.Acc[0.93856] / V.Loss[0.58916] / V.Acc[0.80672] / F1[0.67891]\n * New Best Model -&gt; Epoch [16] / best_score : [0.80672]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:38&lt;00:00,  1.40it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.42it/s]\nEp[17] / T.Loss[0.20336] / T.Acc[0.94146] / V.Loss[0.61937] / V.Acc[0.79254] / F1[0.66284]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:40&lt;00:00,  1.39it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.43it/s]\nEp[18] / T.Loss[0.19630] / T.Acc[0.94279] / V.Loss[0.60255] / V.Acc[0.80226] / F1[0.68113]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:39&lt;00:00,  1.39it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.40it/s]\nEp[19] / T.Loss[0.19012] / T.Acc[0.94391] / V.Loss[0.62428] / V.Acc[0.79858] / F1[0.66664]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:35&lt;00:00,  1.41it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:17&lt;00:00,  1.53it/s]\nEp[20] / T.Loss[0.18292] / T.Acc[0.94642] / V.Loss[0.59904] / V.Acc[0.80252] / F1[0.68419]\n\n\n기본적으로 backbone을 freeze한 것이 성능이 더 좋다\n다음으로는 Best Case를 도출한 Hyper Parameter 기준으로 Argmentation 적용해보았다\n기존 : A.Resize -&gt; A.Normalize -&gt; ToTensorV2\n변경 : A.RandomResizedCrop -&gt; A.RandomBrightnessContrast(p=0.3)\n\t\t\t-&gt; A.RandomGamma(p=0.3) -&gt; A.RandomToneCurve()\n\t\t\t-&gt; A.HorizontalFlip(p=0.5) -&gt; A.Normalize\n\t\t\t-&gt; ToTensorV2\n\n\n출력 backbone freeze, lr 0.001, batch 64, image size 224 + Argmentation\nBest : V.Acc[0.79870] / F1[0.68489]\npython trainALL.py --save_name=Weight_VIT_V5.KHS.tar --target_model=&quot;VIT_V0_KHS(True)&quot;\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V5.KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V0_KHS(True)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [02:12&lt;00:00,  1.79it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]\nEp[1] / T.Loss[1.01179] / T.Acc[0.68348] / V.Loss[0.77707] / V.Acc[0.73932] / F1[0.59065]\n * New Best Model -&gt; Epoch [1] / best_score : [0.73932]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:10&lt;00:00,  1.25it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\nEp[2] / T.Loss[0.61161] / T.Acc[0.80406] / V.Loss[0.66892] / V.Acc[0.76745] / F1[0.63043]\n * New Best Model -&gt; Epoch [2] / best_score : [0.76745]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.20it/s]\nEp[3] / T.Loss[0.52576] / T.Acc[0.82555] / V.Loss[0.63488] / V.Acc[0.78151] / F1[0.64378]\n * New Best Model -&gt; Epoch [3] / best_score : [0.78151]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:40&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.20it/s]\nEp[4] / T.Loss[0.47068] / T.Acc[0.84909] / V.Loss[0.61181] / V.Acc[0.77656] / F1[0.64096]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:38&lt;00:00,  1.08it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.21it/s]\nEp[5] / T.Loss[0.43788] / T.Acc[0.85278] / V.Loss[0.58862] / V.Acc[0.79349] / F1[0.67686]\n * New Best Model -&gt; Epoch [5] / best_score : [0.79349]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[6] / T.Loss[0.41271] / T.Acc[0.86313] / V.Loss[0.58636] / V.Acc[0.79141] / F1[0.66837]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:44&lt;00:00,  1.05it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\nEp[7] / T.Loss[0.38895] / T.Acc[0.87118] / V.Loss[0.58156] / V.Acc[0.78906] / F1[0.66444]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:37&lt;00:00,  1.09it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[8] / T.Loss[0.36667] / T.Acc[0.87658] / V.Loss[0.57438] / V.Acc[0.79115] / F1[0.65924]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.20it/s]\nEp[9] / T.Loss[0.35887] / T.Acc[0.87836] / V.Loss[0.56909] / V.Acc[0.79844] / F1[0.68349]\n * New Best Model -&gt; Epoch [9] / best_score : [0.79844]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[10] / T.Loss[0.35002] / T.Acc[0.87915] / V.Loss[0.56809] / V.Acc[0.78984] / F1[0.67526]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[11] / T.Loss[0.33657] / T.Acc[0.88726] / V.Loss[0.56813] / V.Acc[0.78776] / F1[0.67163]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.22it/s]\nEp[12] / T.Loss[0.32434] / T.Acc[0.89016] / V.Loss[0.57840] / V.Acc[0.78594] / F1[0.67011]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:43&lt;00:00,  1.06it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[13] / T.Loss[0.31700] / T.Acc[0.89300] / V.Loss[0.56339] / V.Acc[0.79557] / F1[0.68140]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.12it/s]\nEp[14] / T.Loss[0.30919] / T.Acc[0.89623] / V.Loss[0.56173] / V.Acc[0.79870] / F1[0.68489]\n * New Best Model -&gt; Epoch [14] / best_score : [0.79870]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:44&lt;00:00,  1.06it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.20it/s]\nEp[15] / T.Loss[0.30300] / T.Acc[0.89768] / V.Loss[0.56118] / V.Acc[0.79297] / F1[0.67191]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:44&lt;00:00,  1.06it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[16] / T.Loss[0.29778] / T.Acc[0.89827] / V.Loss[0.55687] / V.Acc[0.79583] / F1[0.68419]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[17] / T.Loss[0.29262] / T.Acc[0.90045] / V.Loss[0.55521] / V.Acc[0.79818] / F1[0.68579]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.06it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[18] / T.Loss[0.28258] / T.Acc[0.90691] / V.Loss[0.55916] / V.Acc[0.79609] / F1[0.68550]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[19] / T.Loss[0.28178] / T.Acc[0.90493] / V.Loss[0.58251] / V.Acc[0.78750] / F1[0.67331]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:47&lt;00:00,  1.04it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.11it/s]\nEp[20] / T.Loss[0.27821] / T.Acc[0.90520] / V.Loss[0.55727] / V.Acc[0.79792] / F1[0.68210]\n\n\n생각보다 성능이 별로다 model의 complexity가 부족한걸까?\n더 큰 image size를 사용하는 weight를 적용해보았고, 매우 큰 성능 향상이 일어났다\n기존 : IMAGENET1K_V1 (input size : 224x224)\n변경 : IMAGENET1K_SWAG_E2E_V1 (input size : 384x384)\n\n\n출력 backbone freeze, lr 0.001, batch 64, image size 384 + Argmentation\nBest : V.Acc[0.84792] / F1[0.76518]\n==제출 : Finished f1 : 0.6365 / acc: 69.1746== \npython trainALL.py --save_name=Weight_VIT_V6.KHS.tar --target_model=&quot;VIT_V1_KHS(True)&quot; --img_size=384\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=384, lr=0.001, save_name=&#039;Weight_VIT_V6.KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V1_KHS(True)&#039;, validation_ratio=0.2)\nDownloading: &quot;download.pytorch.org/models/vit_b_16_swag-9ac1b537.pth&quot; to /opt/ml/.cache/torch/hub/checkpoints/vit_b_16_swag-9ac1b537.pth\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 331M/331M [00:02&lt;00:00, 132MB/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:55&lt;00:00,  1.75s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:36&lt;00:00,  1.60s/it]\nEp[1] / T.Loss[0.75290] / T.Acc[0.76490] / V.Loss[0.49472] / V.Acc[0.82266] / F1[0.71996]\n * New Best Model -&gt; Epoch [1] / best_score : [0.82266]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:55&lt;00:00,  1.75s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:37&lt;00:00,  1.62s/it]\nEp[2] / T.Loss[0.42801] / T.Acc[0.85581] / V.Loss[0.45157] / V.Acc[0.83151] / F1[0.72053]\n * New Best Model -&gt; Epoch [2] / best_score : [0.83151]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:59&lt;00:00,  1.77s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.64s/it]\nEp[3] / T.Loss[0.36666] / T.Acc[0.87355] / V.Loss[0.44778] / V.Acc[0.83255] / F1[0.70651]\n * New Best Model -&gt; Epoch [3] / best_score : [0.83255]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:55&lt;00:00,  1.75s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:37&lt;00:00,  1.63s/it]\nEp[4] / T.Loss[0.32939] / T.Acc[0.88641] / V.Loss[0.42136] / V.Acc[0.84219] / F1[0.75506]\n * New Best Model -&gt; Epoch [4] / best_score : [0.84219]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:54&lt;00:00,  1.75s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:36&lt;00:00,  1.61s/it]\nEp[5] / T.Loss[0.30715] / T.Acc[0.89089] / V.Loss[0.41402] / V.Acc[0.84115] / F1[0.72825]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:56&lt;00:00,  1.76s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:37&lt;00:00,  1.63s/it]\nEp[6] / T.Loss[0.28671] / T.Acc[0.90005] / V.Loss[0.42921] / V.Acc[0.83047] / F1[0.73401]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:57&lt;00:00,  1.76s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.65s/it]\nEp[7] / T.Loss[0.26758] / T.Acc[0.90447] / V.Loss[0.40914] / V.Acc[0.84792] / F1[0.76518]\n * New Best Model -&gt; Epoch [7] / best_score : [0.84792]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:59&lt;00:00,  1.77s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:36&lt;00:00,  1.61s/it]\nEp[8] / T.Loss[0.25894] / T.Acc[0.90665] / V.Loss[0.40105] / V.Acc[0.84427] / F1[0.74449]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:00&lt;00:00,  1.77s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:37&lt;00:00,  1.62s/it]\nEp[9] / T.Loss[0.25316] / T.Acc[0.90803] / V.Loss[0.39724] / V.Acc[0.84531] / F1[0.75330]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:58&lt;00:00,  1.76s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:25&lt;00:00,  1.43s/it]\nEp[10] / T.Loss[0.23876] / T.Acc[0.91568] / V.Loss[0.42472] / V.Acc[0.83359] / F1[0.73190]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:46&lt;00:00,  1.21s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:04&lt;00:00,  1.08s/it]\nEp[11] / T.Loss[0.23479] / T.Acc[0.91653] / V.Loss[0.42066] / V.Acc[0.83724] / F1[0.74352]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:46&lt;00:00,  1.21s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:06&lt;00:00,  1.11s/it]\nEp[12] / T.Loss[0.23051] / T.Acc[0.91634] / V.Loss[0.41568] / V.Acc[0.84401] / F1[0.76051]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:48&lt;00:00,  1.22s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:06&lt;00:00,  1.11s/it]\nEp[13] / T.Loss[0.22216] / T.Acc[0.91983] / V.Loss[0.41315] / V.Acc[0.84323] / F1[0.74415]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:46&lt;00:00,  1.21s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:06&lt;00:00,  1.10s/it]\nEp[14] / T.Loss[0.22216] / T.Acc[0.92023] / V.Loss[0.41681] / V.Acc[0.84167] / F1[0.74784]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:46&lt;00:00,  1.21s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:08&lt;00:00,  1.14s/it]\nEp[15] / T.Loss[0.21173] / T.Acc[0.92412] / V.Loss[0.41928] / V.Acc[0.84609] / F1[0.73804]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:07&lt;00:00,  1.80s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:46&lt;00:00,  1.77s/it]\nEp[16] / T.Loss[0.20711] / T.Acc[0.92616] / V.Loss[0.45866] / V.Acc[0.83203] / F1[0.72702]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:52&lt;00:00,  2.00s/it]\n 40%|█████████████████████████████████████████▌                                                                                     42%|███████████████████████████████████████████▎                                                                                   43%|█████████████████████████████████████████████                                                                                  45%|██████████████████████████████████████████████▊                                                         | 27/60 [00:48&lt;01:01,  47%|████████████████████████████████████████████████▌                                                       | 28/60 [00:51&lt;01:01,  48%|██████████████████████████████████████████████████▎                                                     | 29/60 [00:52&lt;00:57,  50%|████████████████████████████████████████████████████                                                    | 30/60 [00:54&lt;00:52,  52%|█████████████████████████████████████████████████████▋                                                  | 31/60 [00:56&lt;00:51,  53%|███████████████████████████████████████████████████████▍                                                | 32/60 [00:58&lt;00:52,  55%|█████████████████████████████████████████████████████████▏                                              | 33/60 [01:00&lt;00:54,  57%|██████████████████████████████████████████████████████████▉                                             | 34/60 [01:02&lt;00:53,  58%|████████████████████████████████████████████████████████████▋                                           | 35/60 [01:04&lt;00:48,  60%|██████████████████████████████████████████████████████████████▍                                         | 36/60 [01:05&lt;00:43,  62%|████████████████████████████████████████████████████████████████▏                                       | 37/60 [01:07&lt;00:40,  63%|█████████████████████████████████████████████████████████████████▊                                      | 38/60 [01:09&lt;00:39,  65%|███████████████████████████████████████████████████████████████████▌                                    | 39/60 [01:11&lt;00:41,  67%|█████████████████████████████████████████████████████████████████████▎                                  | 40/60 [01:13&lt;00:39,  68%|███████████████████████████████████████████████████████████████████████                                 | 41/60 [01:15&lt;00:37,  70%|████████████████████████████████████████████████████████████████████████▊                               | 42/60 [01:17&lt;00:33,  72%|██████████████████████████████████████████████████████████████████████████▌                             | 43/60 [01:18&lt;00:30,  73%|████████████████████████████████████████████████████████████████████████████▎                           | 44/60 [01:20&lt;00:28,  75%|██████████████████████████████████████████████████████████████████████████████                          | 45/60 [01:22&lt;00:25,  77%|███████████████████████████████████████████████████████████████████████████████▋                        | 46/60 [01:24&lt;00:25,  78%|█████████████████████████████████████████████████████████████████████████████████▍                      | 47/60 [01:26&lt;00:24,  80%|███████████████████████████████████████████████████████████████████████████████████▏                    | 48/60 [01:28&lt;00:24,  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 49/60 [01:30&lt;00:22,  83%|██████████████████████████████████████████████████████████████████████████████████████▋                 | 50/60 [01:32&lt;00:20,  85%|████████████████████████████████████████████████████████████████████████████████████████▍               | 51/60 [01:35&lt;00:18,  87%|██████████████████████████████████████████████████████████████████████████████████████████▏             | 52/60 [01:36&lt;00:15,  88%|███████████████████████████████████████████████████████████████████████████████████████████▊            | 53/60 [01:38&lt;00:12,  90%|█████████████████████████████████████████████████████████████████████████████████████████████▌          | 54/60 [01:39&lt;00:10,  92%|███████████████████████████████████████████████████████████████████████████████████████████████▎        | 55/60 [01:42&lt;00:09,  93%|█████████████████████████████████████████████████████████████████████████████████████████████████       | 56/60 [01:44&lt;00:08,  95%|██████████████████████████████████████████████████████████████████████████████████████████████████▊     | 57/60 [01:46&lt;00:06,  97%|████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 58/60 [01:48&lt;00:03,  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 59/60 [01:49&lt;00:01, 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:49&lt;00:00,  1.83s/it]\nEp[17] / T.Loss[0.20190] / T.Acc[0.92570] / V.Loss[0.40630] / V.Acc[0.84141] / F1[0.73352]\n100%|████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:50&lt;00:00,  1.99s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.92s/it]\nEp[18] / T.Loss[0.20074] / T.Acc[0.92675] / V.Loss[0.40801] / V.Acc[0.84583] / F1[0.75911]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:52&lt;00:00,  1.99s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:48&lt;00:00,  1.81s/it]\nEp[19] / T.Loss[0.19480] / T.Acc[0.92820] / V.Loss[0.42851] / V.Acc[0.84531] / F1[0.72476]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:56&lt;00:00,  2.01s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:48&lt;00:00,  1.81s/it]\nEp[20] / T.Loss[0.19190] / T.Acc[0.93183] / V.Loss[0.41594] / V.Acc[0.84557] / F1[0.76164]\n\n\n그렇다면 같은 작은 입력크기지만 parameter가 더 큰 타입을 써보면 어떨까?\n\n\n출력backbone freeze, lr 0.001, batch 64, image size 224 + Argmentation\nBest : V.Acc[0.85026] / F1[0.73974]\n==제출 : Finished f1 : 0.6365 / acc: 71.8889== \npython trainALL.py --save_name=Weight_VIT_V7.KHS.tar --target_model=&quot;VIT_V2_KHS(True)&quot; --img_size=224\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V7.KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V2_KHS(True)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:54&lt;00:00,  1.75s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:36&lt;00:00,  1.60s/it]\nEp[1] / T.Loss[0.77953] / T.Acc[0.75514] / V.Loss[0.57383] / V.Acc[0.79896] / F1[0.61356]\n * New Best Model -&gt; Epoch [1] / best_score : [0.79896]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:51&lt;00:00,  1.74s/it]\n 35%|████████████████████████████████████▍                                                                                          37%|██████████████████████████████████████▏                                                                                        38%|███████████████████████████████████████▊                                                                                       40%|█████████████████████████████████████████▌                                                              | 24/60 [00:38&lt;00:54,  42%|███████████████████████████████████████████▎                                                            | 25/60 [00:40&lt;00:56,  43%|█████████████████████████████████████████████                                                           | 26/60 [00:42&lt;00:57,  45%|██████████████████████████████████████████████▊                                                         | 27/60 [00:43&lt;00:52,  47%|████████████████████████████████████████████████▌                                                       | 28/60 [00:44&lt;00:49,  48%|██████████████████████████████████████████████████▎                                                     | 29/60 [00:46&lt;00:45,  50%|████████████████████████████████████████████████████                                                    | 30/60 [00:47&lt;00:43,  52%|█████████████████████████████████████████████████████▋                                                  | 31/60 [00:49&lt;00:46,  53%|███████████████████████████████████████████████████████▍                                                | 32/60 [00:51&lt;00:49,  55%|█████████████████████████████████████████████████████████▏                                              | 33/60 [00:53&lt;00:50,  57%|██████████████████████████████████████████████████████████▉                                             | 34/60 [00:55&lt;00:44,  58%|████████████████████████████████████████████████████████████▋                                           | 35/60 [00:56&lt;00:39,  60%|██████████████████████████████████████████████████████████████▍                                         | 36/60 [00:57&lt;00:35,  62%|████████████████████████████████████████████████████████████████▏                                       | 37/60 [00:59&lt;00:33,  63%|█████████████████████████████████████████████████████████████████▊                                      | 38/60 [01:00&lt;00:33,  65%|███████████████████████████████████████████████████████████████████▌                                    | 39/60 [01:02&lt;00:35,  67%|█████████████████████████████████████████████████████████████████████▎                                  | 40/60 [01:04&lt;00:34,  68%|███████████████████████████████████████████████████████████████████████                                 | 41/60 [01:06&lt;00:33,  70%|████████████████████████████████████████████████████████████████████████▊                               | 42/60 [01:07&lt;00:28,  72%|██████████████████████████████████████████████████████████████████████████▌                             | 43/60 [01:09&lt;00:25,  73%|████████████████████████████████████████████████████████████████████████████▎                           | 44/60 [01:10&lt;00:23,  75%|██████████████████████████████████████████████████████████████████████████████                          | 45/60 [01:12&lt;00:23,  77%|███████████████████████████████████████████████████████████████████████████████▋                        | 46/60 [01:13&lt;00:21,  78%|█████████████████████████████████████████████████████████████████████████████████▍                      | 47/60 [01:15&lt;00:21,  80%|███████████████████████████████████████████████████████████████████████████████████▏                    | 48/60 [01:17&lt;00:21,  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 49/60 [01:19&lt;00:21,  83%|██████████████████████████████████████████████████████████████████████████████████████▋                 | 50/60 [01:22&lt;00:19,  85%|████████████████████████████████████████████████████████████████████████████████████████▍               | 51/60 [01:23&lt;00:17,  87%|██████████████████████████████████████████████████████████████████████████████████████████▏             | 52/60 [01:25&lt;00:15,  88%|███████████████████████████████████████████████████████████████████████████████████████████▊            | 53/60 [01:27&lt;00:12,  90%|█████████████████████████████████████████████████████████████████████████████████████████████▌          | 54/60 [01:28&lt;00:10,  92%|███████████████████████████████████████████████████████████████████████████████████████████████▎        | 55/60 [01:30&lt;00:07,  93%|█████████████████████████████████████████████████████████████████████████████████████████████████       | 56/60 [01:31&lt;00:06,  95%|██████████████████████████████████████████████████████████████████████████████████████████████████▊     | 57/60 [01:33&lt;00:04,  97%|████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 58/60 [01:35&lt;00:03,  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 59/60 [01:37&lt;00:01, 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00, 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.63s/it]\nEp[2] / T.Loss[0.47517] / T.Acc[0.84171] / V.Loss[0.49324] / V.Acc[0.81979] / F1[0.65554]\n * New Best Model -&gt; Epoch [2] / best_score : [0.81979]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar\n100%|████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:54&lt;00:00,  1.75s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.64s/it]\nEp[3] / T.Loss[0.41909] / T.Acc[0.85522] / V.Loss[0.47600] / V.Acc[0.81953] / F1[0.68702]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:59&lt;00:00,  1.77s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.63s/it]\nEp[4] / T.Loss[0.36425] / T.Acc[0.87599] / V.Loss[0.44738] / V.Acc[0.83672] / F1[0.71814]\n * New Best Model -&gt; Epoch [4] / best_score : [0.83672]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:55&lt;00:00,  1.75s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:35&lt;00:00,  1.59s/it]\nEp[5] / T.Loss[0.34096] / T.Acc[0.87922] / V.Loss[0.45111] / V.Acc[0.83307] / F1[0.70400]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:25&lt;00:00,  1.37s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:56&lt;00:00,  1.06it/s]\nEp[6] / T.Loss[0.32696] / T.Acc[0.88423] / V.Loss[0.45945] / V.Acc[0.82734] / F1[0.69541]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:02&lt;00:00,  1.02s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:56&lt;00:00,  1.07it/s]\nEp[7] / T.Loss[0.30602] / T.Acc[0.89260] / V.Loss[0.45552] / V.Acc[0.83437] / F1[0.68316]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:05&lt;00:00,  1.04s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:54&lt;00:00,  1.10it/s]\nEp[8] / T.Loss[0.28945] / T.Acc[0.89913] / V.Loss[0.45325] / V.Acc[0.83698] / F1[0.70789]\n * New Best Model -&gt; Epoch [8] / best_score : [0.83698]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:53&lt;00:00,  1.24s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]\nEp[9] / T.Loss[0.28267] / T.Acc[0.90282] / V.Loss[0.42905] / V.Acc[0.84297] / F1[0.73575]\n * New Best Model -&gt; Epoch [9] / best_score : [0.84297]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:08&lt;00:00,  2.06s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]\nEp[10] / T.Loss[0.27318] / T.Acc[0.90381] / V.Loss[0.43089] / V.Acc[0.83802] / F1[0.71885]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:13&lt;00:00,  2.08s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]\nEp[11] / T.Loss[0.26326] / T.Acc[0.90454] / V.Loss[0.41700] / V.Acc[0.84870] / F1[0.73673]\n * New Best Model -&gt; Epoch [11] / best_score : [0.84870]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:05&lt;00:00,  2.05s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]\nEp[12] / T.Loss[0.25318] / T.Acc[0.91001] / V.Loss[0.44166] / V.Acc[0.83750] / F1[0.72551]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:06&lt;00:00,  2.05s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:54&lt;00:00,  1.90s/it]\nEp[13] / T.Loss[0.25543] / T.Acc[0.90816] / V.Loss[0.42322] / V.Acc[0.84844] / F1[0.73197]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:06&lt;00:00,  2.05s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:56&lt;00:00,  1.95s/it]\nEp[14] / T.Loss[0.24292] / T.Acc[0.91357] / V.Loss[0.42808] / V.Acc[0.84505] / F1[0.74298]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:11&lt;00:00,  2.07s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:56&lt;00:00,  1.94s/it]\nEp[15] / T.Loss[0.24305] / T.Acc[0.91304] / V.Loss[0.41805] / V.Acc[0.85026] / F1[0.73974]\n * New Best Model -&gt; Epoch [15] / best_score : [0.85026]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:13&lt;00:00,  2.08s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]\nEp[16] / T.Loss[0.23123] / T.Acc[0.91851] / V.Loss[0.43814] / V.Acc[0.84635] / F1[0.73798]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:10&lt;00:00,  2.07s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:56&lt;00:00,  1.93s/it]\nEp[17] / T.Loss[0.22616] / T.Acc[0.91957] / V.Loss[0.45303] / V.Acc[0.83307] / F1[0.71838]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:02&lt;00:00,  2.03s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.92s/it]\nEp[18] / T.Loss[0.22559] / T.Acc[0.91733] / V.Loss[0.44956] / V.Acc[0.84089] / F1[0.72816]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:08&lt;00:00,  2.06s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:57&lt;00:00,  1.95s/it]\nEp[19] / T.Loss[0.22509] / T.Acc[0.91805] / V.Loss[0.44156] / V.Acc[0.83698] / F1[0.72255]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:11&lt;00:00,  2.08s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:56&lt;00:00,  1.95s/it]\nEp[20] / T.Loss[0.21347] / T.Acc[0.92418] / V.Loss[0.42967] / V.Acc[0.84349] / F1[0.74568]\n\n\n정확도가 조금 더 올라갔지만, F1 Score는 떨어졌다\n모델의 Complexity가 단순히 크다고 성능이 올라가는 것이 아니었다\n또한 제출결과, 두 모델의 f1 score는 같았다\n그렇다면 더 큰 입력을 받는 vit 모델을 사용해보면 어떨까?\n\n\n출력backbone freeze, lr 0.001, batch 64, image size 512 + Argmentation\nBest : V.Acc[0.85625] / F1[0.77284] (시간이 너무 오래걸려 중도 취소)\n==제출 : Finished f1 : 0.6996 / acc: 77.2698== \npython trainALL.py --save_name=Weight_VIT_V8.KHS.tar --target_model=&quot;VIT_V3_KHS(True)&quot; --img_size=512\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=512, lr=0.001, save_name=&#039;Weight_VIT_V8.KHS.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V3_KHS(True)&#039;, validation_ratio=0.2)\nDownloading: &quot;download.pytorch.org/models/vit_l_16_swag-4f3808c9.pth&quot; to /opt/ml/.cache/torch/hub/checkpoints/vit_l_16_swag-4f3808c9.pth\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.14G/1.14G [01:02&lt;00:00, 19.6MB/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [35:14&lt;00:00,  8.92s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [08:13&lt;00:00,  8.23s/it]\nEp[1] / T.Loss[0.80442] / T.Acc[0.75738] / V.Loss[0.51355] / V.Acc[0.81380] / F1[0.60947]\n * New Best Model -&gt; Epoch [1] / best_score : [0.81380]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [35:16&lt;00:00,  8.93s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [08:15&lt;00:00,  8.26s/it]\nEp[2] / T.Loss[0.44198] / T.Acc[0.86208] / V.Loss[0.42709] / V.Acc[0.84115] / F1[0.72250]\n * New Best Model -&gt; Epoch [2] / best_score : [0.84115]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [32:11&lt;00:00,  8.15s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:49&lt;00:00,  4.83s/it]\nEp[3] / T.Loss[0.36728] / T.Acc[0.88001] / V.Loss[0.39352] / V.Acc[0.84896] / F1[0.72509]\n * New Best Model -&gt; Epoch [3] / best_score : [0.84896]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [26:13&lt;00:00,  6.64s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:52&lt;00:00,  4.87s/it]\nEp[4] / T.Loss[0.33172] / T.Acc[0.88957] / V.Loss[0.38783] / V.Acc[0.84974] / F1[0.74419]\n * New Best Model -&gt; Epoch [4] / best_score : [0.84974]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [23:24&lt;00:00,  5.92s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:52&lt;00:00,  4.87s/it]\nEp[5] / T.Loss[0.30828] / T.Acc[0.89616] / V.Loss[0.36456] / V.Acc[0.85625] / F1[0.77284]\n * New Best Model -&gt; Epoch [5] / best_score : [0.85625]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [20:40&lt;00:00,  5.23s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:51&lt;00:00,  4.85s/it]\nEp[6] / T.Loss[0.29063] / T.Acc[0.90289] / V.Loss[0.39297] / V.Acc[0.84297] / F1[0.72794]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [20:33&lt;00:00,  5.20s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:49&lt;00:00,  4.83s/it]\nEp[7] / T.Loss[0.27253] / T.Acc[0.90638] / V.Loss[0.37759] / V.Acc[0.84870] / F1[0.72398]\n 22%|██████████████████████▏\n\n\n피어세션 시간에 팀원들과 다시 상의해본 결과, 가장 결과가 잘 나온 팀원은 Baseline 코드를 사용했는데, 내 코드와의 차이점은 Adam대신 SGD를 사용하고, Scheduler를 사용했다는 점이다.\nOptimizer와 Scheduler를 동일하게 해서 다시 돌려보았다\n기존(V.Acc[0.67969] / F1[0.55520])대비 향상된 결과를 얻었다\n그러나 향상된 결과의 f1 score도 기존 case와 값과 크게 다르지 않다\n\n\n출력backbone not freeze, lr 0.001, batch 64, image size 224 + Argmentation\nBest : V.Acc[0.86016] / F1[0.74706]\n==제출 : Finished f1 : 0.6275 / acc: 72.0476==\npython trainALL.py \nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V0_KHS_SGD.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=True, step_gamma=1.0, step_size=10, target_model=&#039;VIT_V0_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:50&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.21it/s]\nEp[1] / T.Loss[1.93595] / T.Acc[0.41377] / V.Loss[1.52286] / V.Acc[0.56536] / F1[0.27258]\n * New Best Model -&gt; Epoch [1] / best_score : [0.27258]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:50&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29&lt;00:00,  2.06it/s]\nEp[2] / T.Loss[1.22550] / T.Acc[0.66172] / V.Loss[1.07307] / V.Acc[0.68802] / F1[0.40309]\n * New Best Model -&gt; Epoch [2] / best_score : [0.40309]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:47&lt;00:00,  1.04it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.20it/s]\nEp[3] / T.Loss[0.88808] / T.Acc[0.75461] / V.Loss[0.84355] / V.Acc[0.75365] / F1[0.49735]\n * New Best Model -&gt; Epoch [3] / best_score : [0.49735]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:48&lt;00:00,  1.04it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.15it/s]\nEp[4] / T.Loss[0.69466] / T.Acc[0.80861] / V.Loss[0.69515] / V.Acc[0.79453] / F1[0.54780]\n * New Best Model -&gt; Epoch [4] / best_score : [0.54780]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:53&lt;00:00,  1.01it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.10it/s]\nEp[5] / T.Loss[0.57230] / T.Acc[0.83966] / V.Loss[0.61124] / V.Acc[0.80859] / F1[0.56861]\n * New Best Model -&gt; Epoch [5] / best_score : [0.56861]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29&lt;00:00,  2.05it/s]\nEp[6] / T.Loss[0.49565] / T.Acc[0.85746] / V.Loss[0.55649] / V.Acc[0.81641] / F1[0.58890]\n * New Best Model -&gt; Epoch [6] / best_score : [0.58890]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]\nEp[7] / T.Loss[0.43692] / T.Acc[0.87144] / V.Loss[0.56076] / V.Acc[0.82005] / F1[0.60620]\n * New Best Model -&gt; Epoch [7] / best_score : [0.60620]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:48&lt;00:00,  1.04it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.09it/s]\nEp[8] / T.Loss[0.39887] / T.Acc[0.87816] / V.Loss[0.48795] / V.Acc[0.82786] / F1[0.61825]\n * New Best Model -&gt; Epoch [8] / best_score : [0.61825]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:51&lt;00:00,  1.02it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.08it/s]\nEp[9] / T.Loss[0.36532] / T.Acc[0.88832] / V.Loss[0.46459] / V.Acc[0.83542] / F1[0.63643]\n * New Best Model -&gt; Epoch [9] / best_score : [0.63643]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:51&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.18it/s]\nEp[10] / T.Loss[0.34100] / T.Acc[0.89326] / V.Loss[0.47769] / V.Acc[0.83307] / F1[0.66169]\n * New Best Model -&gt; Epoch [10] / best_score : [0.66169]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:50&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]\nEp[11] / T.Loss[0.31711] / T.Acc[0.90144] / V.Loss[0.44010] / V.Acc[0.83880] / F1[0.65670]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:51&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.19it/s]\nEp[12] / T.Loss[0.29949] / T.Acc[0.90447] / V.Loss[0.42319] / V.Acc[0.84688] / F1[0.68023]\n * New Best Model -&gt; Epoch [12] / best_score : [0.68023]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:52&lt;00:00,  1.02it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]\nEp[13] / T.Loss[0.28249] / T.Acc[0.90843] / V.Loss[0.43525] / V.Acc[0.84089] / F1[0.66333]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.17it/s]\nEp[14] / T.Loss[0.26582] / T.Acc[0.91535] / V.Loss[0.41765] / V.Acc[0.84583] / F1[0.68537]\n * New Best Model -&gt; Epoch [14] / best_score : [0.68537]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:52&lt;00:00,  1.02it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.11it/s]\nEp[15] / T.Loss[0.25331] / T.Acc[0.91924] / V.Loss[0.47107] / V.Acc[0.83203] / F1[0.69194]\n * New Best Model -&gt; Epoch [15] / best_score : [0.69194]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:50&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.12it/s]\nEp[16] / T.Loss[0.23658] / T.Acc[0.92346] / V.Loss[0.53544] / V.Acc[0.80990] / F1[0.70046]\n * New Best Model -&gt; Epoch [16] / best_score : [0.70046]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:48&lt;00:00,  1.04it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.14it/s]\nEp[17] / T.Loss[0.22975] / T.Acc[0.92517] / V.Loss[0.44945] / V.Acc[0.84036] / F1[0.73061]\n * New Best Model -&gt; Epoch [17] / best_score : [0.73061]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.19it/s]\nEp[18] / T.Loss[0.21803] / T.Acc[0.92985] / V.Loss[0.38858] / V.Acc[0.85495] / F1[0.74456]\n * New Best Model -&gt; Epoch [18] / best_score : [0.74456]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]\nEp[19] / T.Loss[0.20869] / T.Acc[0.93440] / V.Loss[0.39791] / V.Acc[0.85651] / F1[0.73926]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:51&lt;00:00,  1.02it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.12it/s]\nEp[20] / T.Loss[0.19833] / T.Acc[0.93625] / V.Loss[0.38119] / V.Acc[0.86016] / F1[0.74706]\n * New Best Model -&gt; Epoch [20] / best_score : [0.74706]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar\n\n\n고민해본 결과, 224 x 224로 이미지를 압축하는데, 이는 이미지의 feature를 담기에 충분하지 않은 것이 아닐까 추측됬다\n원본 이미지의 경우, 사람의 얼굴만 담겨있는 것이 아니라 상체, 배경 등이 모두 찍혀있다\n따라서 입력 이미지의 크기를 키워주거나 불필요한 배경 영역을 crop을 통해 줄여줄 수 있다면 정확도를 더 올릴 수 있을 것이라도 추측했다\n입력 이미지 크기를 키우고, SGD를 적용, 더 향상된 결과를 얻었다\n\n\n출력backbone not freeze, lr 0.001, batch 64, image size 384\nBest : V.Acc[0.90152] / F1[0.82868]\n==제출 : Finished f1 : 0.7156 / acc: 78.3333==\npython trainALL.py \nNamespace(batch_size=32, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=384, lr=0.001, save_name=&#039;Weight_VIT_V1_KHS_SGD.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=True, step_gamma=0.5, step_size=10, target_model=&#039;VIT_V1_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:06&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:02&lt;00:00,  1.89it/s]\nEp[1] / T.Loss[0.48437] / T.Acc[0.85267] / V.Loss[0.30058] / V.Acc[0.88944] / F1[0.77559]\n * New Best Model -&gt; Epoch [1] / best_score : [0.77559]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:01&lt;00:00,  1.27s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[2] / T.Loss[0.16489] / T.Acc[0.94364] / V.Loss[0.31616] / V.Acc[0.88997] / F1[0.75934]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:02&lt;00:00,  1.27s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.84it/s]\nEp[3] / T.Loss[0.08276] / T.Acc[0.97298] / V.Loss[0.32128] / V.Acc[0.88524] / F1[0.80838]\n * New Best Model -&gt; Epoch [3] / best_score : [0.80838]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:06&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[4] / T.Loss[0.03709] / T.Acc[0.99009] / V.Loss[0.36563] / V.Acc[0.90021] / F1[0.81112]\n * New Best Model -&gt; Epoch [4] / best_score : [0.81112]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[5] / T.Loss[0.01758] / T.Acc[0.99584] / V.Loss[0.37122] / V.Acc[0.89942] / F1[0.82090]\n * New Best Model -&gt; Epoch [5] / best_score : [0.82090]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]\nEp[6] / T.Loss[0.00972] / T.Acc[0.99782] / V.Loss[0.39228] / V.Acc[0.89233] / F1[0.80634]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:18&lt;00:00,  1.31s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:05&lt;00:00,  1.81it/s]\nEp[7] / T.Loss[0.00583] / T.Acc[0.99848] / V.Loss[0.40954] / V.Acc[0.90310] / F1[0.82603]\n * New Best Model -&gt; Epoch [7] / best_score : [0.82603]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:22&lt;00:00,  1.32s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.84it/s]\nEp[8] / T.Loss[0.00394] / T.Acc[0.99874] / V.Loss[0.41472] / V.Acc[0.90047] / F1[0.82537]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:14&lt;00:00,  1.30s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.89it/s]\nEp[9] / T.Loss[0.00314] / T.Acc[0.99868] / V.Loss[0.42522] / V.Acc[0.89916] / F1[0.82081]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]\nEp[10] / T.Loss[0.00240] / T.Acc[0.99894] / V.Loss[0.42924] / V.Acc[0.90152] / F1[0.82868]\n * New Best Model -&gt; Epoch [10] / best_score : [0.82868]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar\n\n\nClass 별 Image Analysis\n작은 input_size를 유지하면서 최대한의 feature를 전달하기 위해서는 최대한 얼굴 영역만 넣어주는 것이 필요하다\n버려도 되는 영역이 있는지 확인하기 위해 train image에 있는 모든 이미지를 불러와 평균 이미지를 만들었다\nClass별 분포\n\nClass별 평균 이미지\n\n전체 평균 이미지\n\nCrop Area 정의 및 시각화\n눈으로 보았을 때, y축의 경우 050, 400500 구간은 불필요해보였다\nx축의 경우 050, 334384 구간은 불필요해보였다\n\n\n기존 이미지의 크기(HxW)는 512x384 = 196608이다\nCrop 후 이미지의 크기(HxW)는 350x284=99400이다\ninput image는 보통 이보다 더 작기 때문에 down sizing이 필요하지만 feature의 정보를 잃게 한다\n그러나 Crop한 이미지는 기존 이미지보다 약 50% 감소한 넓이를 가지고 있기 때문에\ndown sizing 시 더 적게 feature 정보 잃어버릴 것이라고 기대할 수 있다\nAge-Gender-MaskStatus 한번에 예측하기 #2\nCrop 후 다시 Train 진행\n\n\n출력backbone not freeze, lr 0.001, batch 64, image size 384\nBest : V.Acc[0.90100] / F1[0.82616]\n==제출 : Finished f1 : 0.74706 / acc: 79.74706==\npython trainALL_temp.py \nNamespace(batch_size=32, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=384, lr=0.001, save_name=&#039;Weight_VIT_V1_KHS_SGD_C.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=True, step_gamma=0.5, step_size=5, target_model=&#039;VIT_V1_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:59&lt;00:00,  2.15s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:41&lt;00:00,  1.17it/s]\nEp[1] / T.Loss[0.47756] / T.Acc[0.85690] / V.Loss[0.32220] / V.Acc[0.87684] / F1[0.78062]\n * New Best Model -&gt; Epoch [1] / best_score : [0.78062]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [17:03&lt;00:00,  2.16s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:50&lt;00:00,  1.08it/s]\nEp[2] / T.Loss[0.16756] / T.Acc[0.94285] / V.Loss[0.31923] / V.Acc[0.88577] / F1[0.75986]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:58&lt;00:00,  2.15s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:49&lt;00:00,  1.08it/s]\nEp[3] / T.Loss[0.08937] / T.Acc[0.97139] / V.Loss[0.30343] / V.Acc[0.89601] / F1[0.81500]\n * New Best Model -&gt; Epoch [3] / best_score : [0.81500]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [17:01&lt;00:00,  2.16s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:42&lt;00:00,  1.17it/s]\nEp[4] / T.Loss[0.04302] / T.Acc[0.98837] / V.Loss[0.44436] / V.Acc[0.86843] / F1[0.79183]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [17:00&lt;00:00,  2.16s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:48&lt;00:00,  1.10it/s]\nEp[5] / T.Loss[0.02137] / T.Acc[0.99491] / V.Loss[0.35330] / V.Acc[0.90100] / F1[0.80714]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:53&lt;00:00,  2.14s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:50&lt;00:00,  1.08it/s]\nEp[6] / T.Loss[0.01106] / T.Acc[0.99769] / V.Loss[0.36034] / V.Acc[0.89653] / F1[0.81579]\n * New Best Model -&gt; Epoch [6] / best_score : [0.81579]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:58&lt;00:00,  2.15s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:43&lt;00:00,  1.15it/s]\nEp[7] / T.Loss[0.00811] / T.Acc[0.99835] / V.Loss[0.36717] / V.Acc[0.90047] / F1[0.82211]\n * New Best Model -&gt; Epoch [7] / best_score : [0.82211]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:55&lt;00:00,  2.15s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:49&lt;00:00,  1.09it/s]\nEp[8] / T.Loss[0.00657] / T.Acc[0.99855] / V.Loss[0.36998] / V.Acc[0.90126] / F1[0.82594]\n * New Best Model -&gt; Epoch [8] / best_score : [0.82594]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [12:22&lt;00:00,  1.57s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[9] / T.Loss[0.00542] / T.Acc[0.99855] / V.Loss[0.37530] / V.Acc[0.89995] / F1[0.82378]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[10] / T.Loss[0.00463] / T.Acc[0.99868] / V.Loss[0.38403] / V.Acc[0.89811] / F1[0.81544]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:03&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.88it/s]\nEp[11] / T.Loss[0.00397] / T.Acc[0.99881] / V.Loss[0.38562] / V.Acc[0.90100] / F1[0.82616]\n * New Best Model -&gt; Epoch [11] / best_score : [0.82616]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[12] / T.Loss[0.00373] / T.Acc[0.99881] / V.Loss[0.38903] / V.Acc[0.90021] / F1[0.82182]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:04&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:02&lt;00:00,  1.91it/s]\nEp[13] / T.Loss[0.00355] / T.Acc[0.99881] / V.Loss[0.39441] / V.Acc[0.89968] / F1[0.82154]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]\nEp[14] / T.Loss[0.00329] / T.Acc[0.99881] / V.Loss[0.39345] / V.Acc[0.89968] / F1[0.81809]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.88it/s]\nEp[15] / T.Loss[0.00311] / T.Acc[0.99881] / V.Loss[0.39927] / V.Acc[0.89942] / F1[0.82184]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:04&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]\nEp[16] / T.Loss[0.00299] / T.Acc[0.99881] / V.Loss[0.39715] / V.Acc[0.90074] / F1[0.82121]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[17] / T.Loss[0.00286] / T.Acc[0.99881] / V.Loss[0.39832] / V.Acc[0.89995] / F1[0.82018]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:02&lt;00:00,  1.90it/s]\nEp[18] / T.Loss[0.00281] / T.Acc[0.99881] / V.Loss[0.39851] / V.Acc[0.90021] / F1[0.82028]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:03&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.85it/s]\nEp[19] / T.Loss[0.00270] / T.Acc[0.99888] / V.Loss[0.40031] / V.Acc[0.89995] / F1[0.82018]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[20] / T.Loss[0.00267] / T.Acc[0.99888] / V.Loss[0.40088] / V.Acc[0.90021] / F1[0.82053]\n\n\n입력 크기가 384x384이므로 feature 손실이 거의 일어나지 않다고 생각해도 무방하다\n하지만 그럼에도 f1이 잘 올라가지 않는다\n이전 [Class 별 Image Analysis]에서 Class의 분포를 확인한 결과, 매우 불균형한 Dataset임을 알 수 있었다\n따라서 성능을 더 올리기 위해서는 이를 해소할 방법을 찾아야 한다\nFocal Loss\n\n\n                  \n                  Focal Loss for Dense Object Detection \n                  \n                \n\n\nThe highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations.\narxiv.org/abs/1708.02002\n\n\n\nCross Entropy는 class unbalance를 고려하지 않는다\nFocal Loss는 확률분포상에서 p 값이 낮을때에 비해 p 값이 높아질수록 loss값이 급격하게 낮게 설정된다는 것이다.\nFocal Loss 미적용 (224x224)\n\n\n출력backbone not freeze, lr 0.001, batch 64, image size 224\nBest : V.Acc[0.85547] / F1[0.75931]\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V0_KHS_SGD_C.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=True, step_gamma=0.5, step_size=20, target_model=&#039;VIT_V0_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.14it/s]\nEp[1] / T.Loss[1.88979] / T.Acc[0.43645] / V.Loss[1.43225] / V.Acc[0.60286] / F1[0.29889]\n * New Best Model -&gt; Epoch [1] / best_score : [0.29889]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:40&lt;00:00,  1.08it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.10it/s]\nEp[2] / T.Loss[1.13299] / T.Acc[0.70075] / V.Loss[0.96170] / V.Acc[0.73880] / F1[0.46717]\n * New Best Model -&gt; Epoch [2] / best_score : [0.46717]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.06it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.11it/s]\nEp[3] / T.Loss[0.79181] / T.Acc[0.79496] / V.Loss[0.73595] / V.Acc[0.79375] / F1[0.54226]\n * New Best Model -&gt; Epoch [3] / best_score : [0.54226]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:37&lt;00:00,  1.09it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.10it/s]\nEp[4] / T.Loss[0.60914] / T.Acc[0.83518] / V.Loss[0.62587] / V.Acc[0.80990] / F1[0.56784]\n * New Best Model -&gt; Epoch [4] / best_score : [0.56784]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:43&lt;00:00,  1.06it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:33&lt;00:00,  1.81it/s]\nEp[5] / T.Loss[0.49894] / T.Acc[0.86188] / V.Loss[0.54779] / V.Acc[0.82109] / F1[0.58347]\n * New Best Model -&gt; Epoch [5] / best_score : [0.58347]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:02&lt;00:00,  1.28s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[6] / T.Loss[0.42728] / T.Acc[0.87790] / V.Loss[0.49307] / V.Acc[0.83646] / F1[0.61411]\n * New Best Model -&gt; Epoch [6] / best_score : [0.61411]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:47&lt;00:00,  1.47s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[7] / T.Loss[0.37494] / T.Acc[0.89023] / V.Loss[0.52960] / V.Acc[0.82266] / F1[0.61055]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:23&lt;00:00,  1.62s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.13it/s]\nEp[8] / T.Loss[0.33490] / T.Acc[0.90157] / V.Loss[0.45111] / V.Acc[0.83698] / F1[0.63438]\n * New Best Model -&gt; Epoch [8] / best_score : [0.63438]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:48&lt;00:00,  1.47s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[9] / T.Loss[0.30143] / T.Acc[0.90941] / V.Loss[0.43849] / V.Acc[0.83932] / F1[0.63993]\n * New Best Model -&gt; Epoch [9] / best_score : [0.63993]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:55&lt;00:00,  1.50s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.11it/s]\nEp[10] / T.Loss[0.27647] / T.Acc[0.91759] / V.Loss[0.45354] / V.Acc[0.83906] / F1[0.66146]\n * New Best Model -&gt; Epoch [10] / best_score : [0.66146]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:02&lt;00:00,  1.53s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.13it/s]\nEp[11] / T.Loss[0.24905] / T.Acc[0.92590] / V.Loss[0.41210] / V.Acc[0.84844] / F1[0.67002]\n * New Best Model -&gt; Epoch [11] / best_score : [0.67002]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:59&lt;00:00,  1.52s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[12] / T.Loss[0.22626] / T.Acc[0.93447] / V.Loss[0.41260] / V.Acc[0.85391] / F1[0.69689]\n * New Best Model -&gt; Epoch [12] / best_score : [0.69689]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:47&lt;00:00,  1.47s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.12it/s]\nEp[13] / T.Loss[0.20725] / T.Acc[0.94014] / V.Loss[0.40826] / V.Acc[0.84922] / F1[0.68709]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:59&lt;00:00,  1.52s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[14] / T.Loss[0.18829] / T.Acc[0.94620] / V.Loss[0.43351] / V.Acc[0.84844] / F1[0.70059]\n * New Best Model -&gt; Epoch [14] / best_score : [0.70059]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:01&lt;00:00,  1.53s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.14it/s]\nEp[15] / T.Loss[0.17282] / T.Acc[0.95102] / V.Loss[0.40533] / V.Acc[0.85000] / F1[0.73521]\n * New Best Model -&gt; Epoch [15] / best_score : [0.73521]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:58&lt;00:00,  1.51s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[16] / T.Loss[0.15531] / T.Acc[0.95642] / V.Loss[0.45812] / V.Acc[0.82917] / F1[0.72246]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:51&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.20it/s]\nEp[17] / T.Loss[0.14054] / T.Acc[0.96301] / V.Loss[0.45406] / V.Acc[0.83594] / F1[0.74068]\n * New Best Model -&gt; Epoch [17] / best_score : [0.74068]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:07&lt;00:00,  1.55s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.12it/s]\nEp[18] / T.Loss[0.12696] / T.Acc[0.96770] / V.Loss[0.41726] / V.Acc[0.85208] / F1[0.74794]\n * New Best Model -&gt; Epoch [18] / best_score : [0.74794]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:57&lt;00:00,  1.51s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:54&lt;00:00,  1.10it/s]\nEp[19] / T.Loss[0.11474] / T.Acc[0.97112] / V.Loss[0.40845] / V.Acc[0.86276] / F1[0.75076]\n * New Best Model -&gt; Epoch [19] / best_score : [0.75076]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:02&lt;00:00,  1.53s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[20] / T.Loss[0.10637] / T.Acc[0.97350] / V.Loss[0.41985] / V.Acc[0.85547] / F1[0.75931]\n * New Best Model -&gt; Epoch [20] / best_score : [0.75931]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar\n\n\nFocal Loss 적용 (224x224)\nFocal Loss 적용 시 확실히 더 좋은 성능을 보여준다\n\n\n출력backbone not freeze, lr 0.001, batch 64, image size 224\nBest : V.Acc[0.85599] / F1[0.76157]\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.001, save_name=&#039;Weight_VIT_V0_KHS_SGD_C_FL.tar&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=True, step_gamma=0.5, step_size=20, target_model=&#039;VIT_V0_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:57&lt;00:00,  1.51s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[1] / T.Loss[1.34291] / T.Acc[0.47811] / V.Loss[0.88064] / V.Acc[0.64219] / F1[0.34061]\n * New Best Model -&gt; Epoch [1] / best_score : [0.34061]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:54&lt;00:00,  1.49s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.13it/s]\nEp[2] / T.Loss[0.65392] / T.Acc[0.73075] / V.Loss[0.53905] / V.Acc[0.76068] / F1[0.50438]\n * New Best Model -&gt; Epoch [2] / best_score : [0.50438]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:07&lt;00:00,  1.55s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.14it/s]\nEp[3] / T.Loss[0.41895] / T.Acc[0.81059] / V.Loss[0.39059] / V.Acc[0.79609] / F1[0.55500]\n * New Best Model -&gt; Epoch [3] / best_score : [0.55500]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[4] / T.Loss[0.30483] / T.Acc[0.84309] / V.Loss[0.32417] / V.Acc[0.81068] / F1[0.58714]\n * New Best Model -&gt; Epoch [4] / best_score : [0.58714]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:00&lt;00:00,  1.52s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[5] / T.Loss[0.24065] / T.Acc[0.86142] / V.Loss[0.27832] / V.Acc[0.82396] / F1[0.61083]\n * New Best Model -&gt; Epoch [5] / best_score : [0.61083]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:07&lt;00:00,  1.55s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.13it/s]\nEp[6] / T.Loss[0.20037] / T.Acc[0.87375] / V.Loss[0.24717] / V.Acc[0.83516] / F1[0.63559]\n * New Best Model -&gt; Epoch [6] / best_score : [0.63559]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:52&lt;00:00,  1.49s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[7] / T.Loss[0.17200] / T.Acc[0.88443] / V.Loss[0.24000] / V.Acc[0.83073] / F1[0.63941]\n * New Best Model -&gt; Epoch [7] / best_score : [0.63941]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:49&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[8] / T.Loss[0.15111] / T.Acc[0.89333] / V.Loss[0.21895] / V.Acc[0.83464] / F1[0.64770]\n * New Best Model -&gt; Epoch [8] / best_score : [0.64770]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:10&lt;00:00,  1.56s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[9] / T.Loss[0.13467] / T.Acc[0.90170] / V.Loss[0.20378] / V.Acc[0.83516] / F1[0.66274]\n * New Best Model -&gt; Epoch [9] / best_score : [0.66274]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.51s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[10] / T.Loss[0.12255] / T.Acc[0.90691] / V.Loss[0.19901] / V.Acc[0.84219] / F1[0.70703]\n * New Best Model -&gt; Epoch [10] / best_score : [0.70703]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:00&lt;00:00,  1.52s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[11] / T.Loss[0.11057] / T.Acc[0.90955] / V.Loss[0.19342] / V.Acc[0.84453] / F1[0.70841]\n * New Best Model -&gt; Epoch [11] / best_score : [0.70841]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:52&lt;00:00,  1.49s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[12] / T.Loss[0.10140] / T.Acc[0.91759] / V.Loss[0.18816] / V.Acc[0.84479] / F1[0.71606]\n * New Best Model -&gt; Epoch [12] / best_score : [0.71606]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:09&lt;00:00,  1.56s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.14it/s]\nEp[13] / T.Loss[0.09307] / T.Acc[0.92392] / V.Loss[0.18592] / V.Acc[0.84349] / F1[0.72371]\n * New Best Model -&gt; Epoch [13] / best_score : [0.72371]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:10&lt;00:00,  1.56s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[14] / T.Loss[0.08579] / T.Acc[0.92919] / V.Loss[0.18219] / V.Acc[0.84583] / F1[0.72470]\n * New Best Model -&gt; Epoch [14] / best_score : [0.72470]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:05&lt;00:00,  1.29s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.12it/s]\nEp[15] / T.Loss[0.08016] / T.Acc[0.93091] / V.Loss[0.17729] / V.Acc[0.85000] / F1[0.73899]\n * New Best Model -&gt; Epoch [15] / best_score : [0.73899]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29&lt;00:00,  2.02it/s]\nEp[16] / T.Loss[0.07410] / T.Acc[0.93506] / V.Loss[0.18023] / V.Acc[0.84714] / F1[0.74333]\n * New Best Model -&gt; Epoch [16] / best_score : [0.74333]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30&lt;00:00,  1.97it/s]\nEp[17] / T.Loss[0.06946] / T.Acc[0.93987] / V.Loss[0.17984] / V.Acc[0.84531] / F1[0.74026]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]\nEp[18] / T.Loss[0.06459] / T.Acc[0.94225] / V.Loss[0.17489] / V.Acc[0.85234] / F1[0.75522]\n * New Best Model -&gt; Epoch [18] / best_score : [0.75522]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29&lt;00:00,  2.05it/s]\nEp[19] / T.Loss[0.06011] / T.Acc[0.94746] / V.Loss[0.17372] / V.Acc[0.85495] / F1[0.75458]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.07it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.18it/s]\nEp[20] / T.Loss[0.05585] / T.Acc[0.94996] / V.Loss[0.17397] / V.Acc[0.85599] / F1[0.76157]\n * New Best Model -&gt; Epoch [20] / best_score : [0.76157]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar\n\n\nFocal Loss + Step10, gamma0.5\n\n\n출력backbone not freeze, lr 0.004, batch 64, image size 224\nBest : V.Acc[0.86224] / F1[0.78143]\npython trainALL.py --lr=4e-3 --step_size=10 --epochs=30 --save_name=Weight_VIT_V0_KHS_SGC_C_S10\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=20, img_size=224, lr=0.004, save_name=&#039;Weight_VIT_V0_KHS_SGC_C_S10&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=True, step_gamma=0.5, step_size=10, target_model=&#039;VIT_V0_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.50s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.12it/s]\nEp[1] / T.Loss[0.72190] / T.Acc[0.68836] / V.Loss[0.34943] / V.Acc[0.79557] / F1[0.58657]\n * New Best Model -&gt; Epoch [1] / best_score : [0.58657]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:06&lt;00:00,  1.55s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[2] / T.Loss[0.21428] / T.Acc[0.86603] / V.Loss[0.22265] / V.Acc[0.83802] / F1[0.63386]\n * New Best Model -&gt; Epoch [2] / best_score : [0.63386]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.50s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[3] / T.Loss[0.13595] / T.Acc[0.89735] / V.Loss[0.18575] / V.Acc[0.85052] / F1[0.71738]\n * New Best Model -&gt; Epoch [3] / best_score : [0.71738]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\nEp[4] / T.Loss[0.10158] / T.Acc[0.91232] / V.Loss[0.17468] / V.Acc[0.84792] / F1[0.73956]\n * New Best Model -&gt; Epoch [4] / best_score : [0.73956]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:03&lt;00:00,  1.53s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[5] / T.Loss[0.07939] / T.Acc[0.92998] / V.Loss[0.18733] / V.Acc[0.83464] / F1[0.73658]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:59&lt;00:00,  1.51s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[6] / T.Loss[0.06369] / T.Acc[0.94179] / V.Loss[0.18287] / V.Acc[0.83516] / F1[0.76561]\n * New Best Model -&gt; Epoch [6] / best_score : [0.76561]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:12&lt;00:00,  1.57s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.21it/s]\nEp[7] / T.Loss[0.05203] / T.Acc[0.95174] / V.Loss[0.17255] / V.Acc[0.85859] / F1[0.77612]\n * New Best Model -&gt; Epoch [7] / best_score : [0.77612]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:59&lt;00:00,  1.52s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[8] / T.Loss[0.04293] / T.Acc[0.96150] / V.Loss[0.18020] / V.Acc[0.85078] / F1[0.77197]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:06&lt;00:00,  1.55s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\nEp[9] / T.Loss[0.03466] / T.Acc[0.96783] / V.Loss[0.18515] / V.Acc[0.84427] / F1[0.75693]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:03&lt;00:00,  1.54s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[10] / T.Loss[0.02871] / T.Acc[0.97350] / V.Loss[0.17529] / V.Acc[0.85417] / F1[0.76863]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:05&lt;00:00,  1.54s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[11] / T.Loss[0.02154] / T.Acc[0.98233] / V.Loss[0.17757] / V.Acc[0.85990] / F1[0.77990]\n * New Best Model -&gt; Epoch [11] / best_score : [0.77990]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:54&lt;00:00,  1.49s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\nEp[12] / T.Loss[0.01896] / T.Acc[0.98536] / V.Loss[0.18180] / V.Acc[0.86224] / F1[0.77990]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:51&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[13] / T.Loss[0.01707] / T.Acc[0.98741] / V.Loss[0.18875] / V.Acc[0.86224] / F1[0.76702]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:19&lt;00:00,  1.60s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:48&lt;00:00,  1.24it/s]\nEp[14] / T.Loss[0.01554] / T.Acc[0.98813] / V.Loss[0.18762] / V.Acc[0.86302] / F1[0.77517]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:01&lt;00:00,  1.52s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[15] / T.Loss[0.01435] / T.Acc[0.98853] / V.Loss[0.19114] / V.Acc[0.86016] / F1[0.77536]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:55&lt;00:00,  1.50s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\nEp[16] / T.Loss[0.01275] / T.Acc[0.99057] / V.Loss[0.19873] / V.Acc[0.86224] / F1[0.78143]\n * New Best Model -&gt; Epoch [16] / best_score : [0.78143]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\nEp[17] / T.Loss[0.01172] / T.Acc[0.99156] / V.Loss[0.20302] / V.Acc[0.85964] / F1[0.77807]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:09&lt;00:00,  1.56s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[18] / T.Loss[0.01069] / T.Acc[0.99156] / V.Loss[0.20453] / V.Acc[0.85677] / F1[0.77985]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:53&lt;00:00,  1.49s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[19] / T.Loss[0.00964] / T.Acc[0.99262] / V.Loss[0.20540] / V.Acc[0.86198] / F1[0.77771]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:49&lt;00:00,  1.47s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.20it/s]\nEp[20] / T.Loss[0.00880] / T.Acc[0.99308] / V.Loss[0.20924] / V.Acc[0.85469] / F1[0.77885]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:01&lt;00:00,  1.53s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\n\n\nFocal Loss + Step5, gamma0.5\n\n\n출력backbone not freeze, lr 0.004, batch 64, image size 224\nBest : V.Acc[0.86068] / F1[0.78284]\n==제출 : Finished f1 : 0.6654 / acc: 73.1587==\npython trainALL.py --lr=4e-3 --step_size=\n5 --epochs=30 --save_name=Weight_VIT_V0_KHS_SGC_C_S5\nNamespace(batch_size=64, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=30, img_size=224, lr=0.004, save_name=&#039;Weight_VIT_V0_KHS_SGC_C_S5&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=True, step_gamma=0.5, step_size=5, target_model=&#039;VIT_V0_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:42&lt;00:00,  1.45s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[1] / T.Loss[0.72190] / T.Acc[0.68836] / V.Loss[0.34943] / V.Acc[0.79557] / F1[0.58657]\n * New Best Model -&gt; Epoch [1] / best_score : [0.58657]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:08&lt;00:00,  1.55s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[2] / T.Loss[0.21428] / T.Acc[0.86603] / V.Loss[0.22265] / V.Acc[0.83802] / F1[0.63386]\n * New Best Model -&gt; Epoch [2] / best_score : [0.63386]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:55&lt;00:00,  1.50s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[3] / T.Loss[0.13595] / T.Acc[0.89735] / V.Loss[0.18575] / V.Acc[0.85052] / F1[0.71738]\n * New Best Model -&gt; Epoch [3] / best_score : [0.71738]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:51&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[4] / T.Loss[0.10158] / T.Acc[0.91232] / V.Loss[0.17468] / V.Acc[0.84792] / F1[0.73956]\n * New Best Model -&gt; Epoch [4] / best_score : [0.73956]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:04&lt;00:00,  1.54s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[5] / T.Loss[0.07939] / T.Acc[0.92998] / V.Loss[0.18733] / V.Acc[0.83464] / F1[0.73658]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.51s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[6] / T.Loss[0.06154] / T.Acc[0.94436] / V.Loss[0.16302] / V.Acc[0.85547] / F1[0.76878]\n * New Best Model -&gt; Epoch [6] / best_score : [0.76878]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:11&lt;00:00,  1.57s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[7] / T.Loss[0.05433] / T.Acc[0.95121] / V.Loss[0.16818] / V.Acc[0.85677] / F1[0.76885]\n * New Best Model -&gt; Epoch [7] / best_score : [0.76885]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:58&lt;00:00,  1.51s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]\nEp[8] / T.Loss[0.04918] / T.Acc[0.95583] / V.Loss[0.16799] / V.Acc[0.85417] / F1[0.76732]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:10&lt;00:00,  1.56s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[9] / T.Loss[0.04386] / T.Acc[0.95879] / V.Loss[0.17267] / V.Acc[0.85130] / F1[0.76561]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:01&lt;00:00,  1.53s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.14it/s]\nEp[10] / T.Loss[0.03987] / T.Acc[0.96288] / V.Loss[0.16654] / V.Acc[0.85599] / F1[0.77333]\n * New Best Model -&gt; Epoch [10] / best_score : [0.77333]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:02&lt;00:00,  1.53s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.21it/s]\nEp[11] / T.Loss[0.03433] / T.Acc[0.96948] / V.Loss[0.16849] / V.Acc[0.85911] / F1[0.77092]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:51&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[12] / T.Loss[0.03225] / T.Acc[0.97178] / V.Loss[0.17023] / V.Acc[0.86068] / F1[0.77492]\n * New Best Model -&gt; Epoch [12] / best_score : [0.77492]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]\nEp[13] / T.Loss[0.03036] / T.Acc[0.97468] / V.Loss[0.17377] / V.Acc[0.85885] / F1[0.76709]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:20&lt;00:00,  1.60s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[14] / T.Loss[0.02878] / T.Acc[0.97521] / V.Loss[0.17221] / V.Acc[0.86120] / F1[0.77510]\n * New Best Model -&gt; Epoch [14] / best_score : [0.77510]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:00&lt;00:00,  1.52s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]\nEp[15] / T.Loss[0.02755] / T.Acc[0.97607] / V.Loss[0.17549] / V.Acc[0.85807] / F1[0.76886]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.50s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[16] / T.Loss[0.02538] / T.Acc[0.98009] / V.Loss[0.17614] / V.Acc[0.85990] / F1[0.77545]\n * New Best Model -&gt; Epoch [16] / best_score : [0.77545]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:48&lt;00:00,  1.47s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]\nEp[17] / T.Loss[0.02478] / T.Acc[0.97976] / V.Loss[0.17756] / V.Acc[0.85964] / F1[0.77784]\n * New Best Model -&gt; Epoch [17] / best_score : [0.77784]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:06&lt;00:00,  1.55s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[18] / T.Loss[0.02405] / T.Acc[0.98128] / V.Loss[0.17941] / V.Acc[0.85833] / F1[0.77824]\n * New Best Model -&gt; Epoch [18] / best_score : [0.77824]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:52&lt;00:00,  1.49s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[19] / T.Loss[0.02328] / T.Acc[0.98207] / V.Loss[0.17868] / V.Acc[0.86146] / F1[0.77820]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]\nEp[20] / T.Loss[0.02275] / T.Acc[0.98253] / V.Loss[0.18019] / V.Acc[0.86068] / F1[0.78284]\n * New Best Model -&gt; Epoch [20] / best_score : [0.78284]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5\n\n\nCrop을 했지만 여전히 Resize 이후, image feature가 부족한거같다\nSize를 키워서 다시 진행해보았다\n결과는 좋아졌지만 기존 최고 기록을 넘지는 못했다\nOverfitting이 예상된다\n\n\n출력backbone not freeze, lr 0.004, batch 64, image size 384\nBest : V.Acc[0.90257] / F1[0.82163]\n==제출 : Finished f1 : 0.7218 / acc: 78.4286==\npython trainALL.py --lr=4e-3 --step_size=5 --epochs=10 --save_name=Weight_VIT_V1_KHS_SGC_C_S5 --target_model=&quot;VIT_V1_KHS(False)&quot; --img_size=384 --batch_size=32\nNamespace(batch_size=32, csv_path=&#039;../data/train_i.csv&#039;, device=&#039;cuda&#039;, epochs=10, img_size=384, lr=0.004, save_name=&#039;Weight_VIT_V1_KHS_SGC_C_S5&#039;, save_path=&#039;../models/checkpoint/&#039;, seed=41, step_enable=True, step_gamma=0.5, step_size=5, target_model=&#039;VIT_V1_KHS(False)&#039;, validation_ratio=0.2)\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.85it/s]\nEp[1] / T.Loss[0.25867] / T.Acc[0.85181] / V.Loss[0.11856] / V.Acc[0.87710] / F1[0.77540]\n * New Best Model -&gt; Epoch [1] / best_score : [0.77540]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:09&lt;00:00,  1.29s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.88it/s]\nEp[2] / T.Loss[0.04351] / T.Acc[0.95587] / V.Loss[0.13384] / V.Acc[0.89837] / F1[0.79012]\n * New Best Model -&gt; Epoch [2] / best_score : [0.79012]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:09&lt;00:00,  1.29s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.84it/s]\nEp[3] / T.Loss[0.01495] / T.Acc[0.98447] / V.Loss[0.15800] / V.Acc[0.89207] / F1[0.80832]\n * New Best Model -&gt; Epoch [3] / best_score : [0.80832]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:11&lt;00:00,  1.29s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.83it/s]\nEp[4] / T.Loss[0.00487] / T.Acc[0.99538] / V.Loss[0.15480] / V.Acc[0.89811] / F1[0.80708]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:10&lt;00:00,  1.29s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:05&lt;00:00,  1.81it/s]\nEp[5] / T.Loss[0.00190] / T.Acc[0.99802] / V.Loss[0.16241] / V.Acc[0.90336] / F1[0.81799]\n * New Best Model -&gt; Epoch [5] / best_score : [0.81799]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5\n 23%|███████████████████████                                                     23%|███████████████████████▎                                                    23%|███████████████████████▌                                                    23%|███████████████████████▋                                                    23%|███████████████████████▉                                                    24%|████████████████████████▏                                                   24%|████████████████████████▎                                                   24%|████████████████████████▌                                                   24%|████████████████████████▊                                                   25%|█████████████████████████                                                   25%|█████████████████████████▏                                                  25%|█████████████████████████▍                                                  25%|█████████████████████████▋                                                  25%|█████████████████████████▉                                                  26%|██████████████████████████                                                  26%|██████████████████████████▎                                                 26%|██████████████████████████▌                                                 26%|██████████████████████████▋                                                 26%|██████████████████████████▉                                                                           | 125/473 [02:39&lt;07:21,  1.27s/it]source /opt/ml/.local/share/virtualenvs/lv1_imageclassification_cv02-bp8_CroY/bin/activate\n 27%|██████████████████████████▋                                                 27%|██████████████████████████▊                                                100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [11:59&lt;00:00,  1.52s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:06&lt;00:00,  1.80it/s]\nEp[6] / T.Loss[0.00090] / T.Acc[0.99874] / V.Loss[0.16468] / V.Acc[0.90389] / F1[0.82088]\n * New Best Model -&gt; Epoch [6] / best_score : [0.82088]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:13&lt;00:00,  1.30s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]\nEp[7] / T.Loss[0.00070] / T.Acc[0.99894] / V.Loss[0.16688] / V.Acc[0.90179] / F1[0.81186]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:11&lt;00:00,  1.29s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:06&lt;00:00,  1.80it/s]\nEp[8] / T.Loss[0.00063] / T.Acc[0.99881] / V.Loss[0.17004] / V.Acc[0.90152] / F1[0.81653]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:10&lt;00:00,  1.29s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]\nEp[9] / T.Loss[0.00050] / T.Acc[0.99894] / V.Loss[0.17162] / V.Acc[0.90205] / F1[0.81566]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.85it/s]\nEp[10] / T.Loss[0.00042] / T.Acc[0.99894] / V.Loss[0.17372] / V.Acc[0.90257] / F1[0.82163]\n * New Best Model -&gt; Epoch [10] / best_score : [0.82163]\n -&gt; The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5\n\n\n아쉽지만, 대회 기간이 끝나서 추가 작업을 해볼 수는 없었다.\n회고\n이번 프로젝트에서 나의 목표는 무엇이었는가?\n리더보드 상위권, 직접 코드짜기, 팀원들과 협업, 더 효율적인 훈련\n나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\nGitHub Repos 구성 및 팀원들과 공유\n지속적으로 Utility Function Update\nEDA 결과 지속적 공유\n훈련 결과 지속적 공유\nGender 단일 분류 모델 학습 및 결과 공유\nAge/Gender/Mask 전체 분류 모델 학습 및 결과 공유\nBaseline code 참조, Train 및 Inferrence용 python script 제작 및 공유\n나는 어떤 방식으로 모델을 개선했는가?\n전체 이미지에 대한 RGB 평균 및 표준편차 시각화\n전체 이미지에 대한 class별 분포 시각화\n전체 이미지에 대한 class별 평균 이미지 시각화 및 전체 이미지의 평균 이미지 시각화\n데이터 불균형을 해소하기 위해 FocalLoss추가\nSGD와 Adam optimizer의 성능 비교\nViT 사용, 여러 Case(input size, model size, argmentation, ..etc)들에 대하여 훈련 및 결과 비교\n내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?\nGender, Age, Mask를 각각 분류하는 모델을 만들고 결과를 합치는 방식이 좋은 결과를 도출할 것이라고 생각했다. 개별적으로 훈련을 진행한 뒤, 잘 예측하는 항목은 고정하고 잘 예측하지 못하는 항목에 집중하면 더 좋을 것이라고 생각했기 때문이다. 하지만 결과는 좋지 않았다. 한번에 예측하는 모델이라면 제출해서 정량적으로 판단할 수 있다. 그러나 개별 모델로 나누고 이를 합쳐서 제출하면 어떤 모델을 수정해야하는지 알 수 없게 되어버렸다.\n어떤 계획을 세웠을 때, 실제로 실현이 가능한지 면밀이 검토가 필요하다고 느꼈다.\n또한 개별 분류 방식에서 한번에 분류하는 방식으로 넘어갔을 때, 기록의 중요성을 느꼈다. 개별 분류 방식에서 했던 많은 시행착오를 기록해놓지 않았기 때문에, 0부터 다시 시작하는 기분이었다. 그래서 한번에 분류하는 방식으로 훈련할 때는 지속적으로 기록했다.\n전과 비교해서, 내가 새롭게 시도한 변화는 무엇이고, 어떤 효과가 있었는가?\n성별 분류 모델은 기본적으로 성능이 잘 나와서 크게 새로운 것은 없었다. 한번에 분류하는 모델의 경우, 처음으로 ViT를 사용해서 분류해보았다. ViT는 Res-net, Efficient-net등과는 구조가 조금 다르다. 그래서 처음에는 어떻게 끝단을 수정해야 할지 알 수 없어서 직접 모델 내부를 분석해 보았다. 결과, timm 라이브러리를 사용하지 않고 torchvision에 있는 ViT의 Head를 직접 수정해서 사용할 수 있었다.\nViT를 사용해 HyperParameter와 ViT Model Type 및 Argmentation을 변경해가며 실험했고, 이전과 같은 실수를 반복하지 않기 위하여 모두 기록으로 남겼다. 결과, 실험적으로 성능이 올라가는 조건들을 하나씩 찾을 수 있었고, 단일 제출로는 가장 높은 점수를 얻을 수 있었다.\n마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\nViT만 사용해서 실험을 진행한 점이 아쉬웠다. 앞서 성별 분류를 하면서 여러 모델을 사용해보았고, 시간을 많이 소모했다. 따라서 다른 모델을 사용해볼 여유가 없다고 판단했기 때문이다.\n모델의 훈련과정을 WandB등으로 시각화하고 기록하지 않은것이 아쉽다.\n데이터를 직접 확인하지 않은 것이 아쉽다. 데이터 항상 올바르게 라벨링 되어있을 것이라고 생각해서는 안된다.\nConfusion Matrix로 결과를 시각화해보지 못한 것이 아쉽다.\n한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n가장 기본이 되는 모델로 훈련하여 기본이 될 정보들을 만들어놓겠다. 그리고 그 정보들을 기반으로 여러가지 모델을 실험해보고 성능을 평가하겠다.\nWandB나 Tensorboard를 사용해 파라미터를 기록하고 훈련과정을 시각화해보고 싶다. 또한 여러 파라미터에 대해 자동으로 입력 및 훈련하고, 이를 기록하는 시퀀스를 만들어보겠다.\n데이터 분석을 더 면밀히 하겠다.\n\nTorchVision에서 Pre-Trained model을 불러왔을 때, 초기 required_grad 값은?\n\n\n                  \n                  Finetuning Torchvision Models - PyTorch Tutorials 1.2.0 documentation \n                  \n                \n\n\nAuthor: Nathan Inkawhich In this tutorial we will take a deeper look at how to finetune and feature extract the torchvision models, all of which have been pretrained on the 1000-class Imagenet dataset.\npytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#set-model-parameters-requires-grad-attribute\n\n\n\nTorchVision에서 미리 학습된 weight를 불러왔을 때, 모델 각 레이어의 required_grad 값은?\n공식 홈페이지 검색 → 기본적으로 True로 설정되어 있음\n따라서 기존의 feature를 유지하고 싶다면, required_grad = false를 사용해야 한다\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\n추가 메모\n저장 model이 여러 개가 되니 뭐가 뭔지 알 수가 없다. 좋은 방법이 없을까?\n그래프로 계속 경향을 저장해놓고싶다 어떻게 해야할까?\ncode는 cli에서 사용해도 전혀 불편함이 없을 수준으로 자동화해놔야 편리하다\n시간은 짧고 할 일은 많다. 빠르게 시도하고 결과를 확인할 수 있는게 좋다\n성실하게 훈련을 자동화 할 수 있는 코드를 만들어두면, 실패해도 쉽게 복구할 수 있다"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-8/Week-8":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-8/Week-8","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 8/Week 8.md","title":"Week 8","links":["vault/resources/kakaotalk_log.csv"],"tags":[],"content":"\nStreamlit을 사용한 Mask &amp; Gender &amp; Age Classifier Web Service(Special Mission 1)\nLinux &amp; Shell Command 정리 및 실습(Special Mission 2)\n\nShell 종류\nShell Command 정리\n실습 - 카카오톡 그룹 채팅방에서 옵션 - 대화 내보내기로 csv로 저장 후, 쉘 커맨드 1줄로 카카오톡 대화방에서 2021년(또는 2022년)에 제일 메세지를 많이 보낸 TOP 3명 추출하기!\n\n\nPoetry로 가상환경 + 패키지 관리\n\nNo module named ‘six’\nVS Code에서 env 인식 불가 현상\npoetry에서 requirements.txt 내보내기\nDocker file build error(failed to solve with frontend dockerfile.v0: failed to read dockerfile: open~)\n\n\nMac에서 pyenv로 여러 Python 버전 사용하기\n\npyenv 설치\npython 설치\nshell path 추가\npython 버전 설정\n현재 python 버전 확인\n\n\nMac에서 pyenv + poetry로 환경 설정하기\nMac + Pytorch + poetry 사용시 문제\nBentoML 공부(스크랩)\nStreamlit(Frontend) + BentoML(Backend)를 사용한 Mask &amp; Gender &amp; Age Classifier Web Service\nAirFlow\n\n공식문서\n참조\n추가로 읽어보기\nGit Repos\n\n\nML Flow\nMLOps\n\n\nStreamlit을 사용한 Mask &amp; Gender &amp; Age Classifier Web Service(Special Mission 1)\ngithub.com/404Vector/WebService.Classifier.Mask-Gender-Age\n\nLinux &amp; Shell Command 정리 및 실습(Special Mission 2)\nShell 종류\n\nsh : 최초의 쉘\nbash : Linux 표준 쉘\nzsh : Mac 카탈리나 OS 기본 쉘\n\nShell Command 정리\n\n\nman [command] : 쉘 커맨드의 매뉴얼 문서 출력, 종료는 ‘:q’ 입력\n\n\nmkdir [dir] : 폴더 생성\n\n\nls [-option] : List Segments, 현재 디렉토리의 폴더 및 파일 확인\n\n\npwd [dir] : Print Working Directory, 현재 디렉토리의 절대경로 출력\n\n\ncd [dir] : Change Directory, 디렉토리 이동\n\n\necho ”some text” : 텍스트 출력\n\n\necho &#039;some command&#039; : 해당 쉘 커맨드의 결과 출력\n\n\nvi : 파일 편집 및 생성기\n\n\ncp [source dir] [result dir] : 파일 또는 폴더 복사\n\n\nmv [source dir] [result dir] : 파일 또는 폴더 이동\n\n\ncat [file] : 특정 파일 내용 출력, 뒤에 더 file을 입력 시 합쳐서 출력\n\n\nclear : 터미널 창 clear\n\n\nhistory : 최근 입력 쉘 커맨드 내역 출력\n\n\nfind [search dir] [name] : 해당 디렉토리에서 name이라는 이름을 같는 파일 및 디렉토리 검색\n\n\nexport [var name]=[value] : 환경변수 설정, ex) export water=”물”\n\n\nalias [new command name]=[’some command set’] : 명령어 명명, ex) alias ll2=’ls -l’\n\n\nhead [file] : 파일의 앞 or 뒤 n행 출력\n\n\nsort [file] : 행 단위 정렬\n\n\nuniq [file] : 중복된 행 중복 제거\n\n\ngrep [file] : 파일에 주어진 패턴 목록과 매칭되는 라인 검색\n\n\n[some command] &gt; [some command] : overwite or save, ex) cat vi-test2.sh vi-test3.sh &gt; new_test\n\n\n[some command] &gt;&gt; [some command] : append, ex) cat vi-test2.sh vi-test3.sh &gt; new_test.sh\n\n\n[some command] | [some command] : pipe, 커맨드 출력을 다른 커맨드의 입력으로 전달 ex) ls | grep “vi”\n\n\n실습 - 카카오톡 그룹 채팅방에서 옵션 - 대화 내보내기로 csv로 저장 후, 쉘 커맨드 1줄\n로 카카오톡 대화방에서 2021년(또는 2022년)에 제일 메세지를 많이 보낸 TOP 3명 추출하기!\nTransclude of kakaotalk_log.csv\ncat kakaotalk_log.csv | cut -d , -f 2 | uniq -c | sort -r | head -n 3\n \n&gt;&gt; 10 &quot;치맥하는 제이지/cv&quot;\n    7 &quot;ㅇㅇ/cv&quot;\n    6 &quot;하하/recsys&quot;\n\nPoetry로 가상환경 + 패키지 관리\n\n\n                  \n                  poetry 로 파이썬 패키지 관리하기 \n                  \n                \n\n\n파이썬으로 개발하다보면 여러가지 가상환경 매니징 패키지를 마주하게 된다.\nwww.woolog.dev/backend/python/poetry/#poetry를-사용하자\n\n\n\npoetry를 사용하면 가상환경 구성뿐만 아니라 패키지 관리도 가능\nNo module named ‘six’\n버그 발생\npoetry init\n \n&gt;&gt; No module named &#039;six&#039;\nbrew install poetry 로 설치 시, 정상 동작하지 않았다. pip3로 설치하니 동작.\nVS Code에서 env 인식 불가 현상\n\n\n                  \n                  poetry를 사용하는 프로젝트를 vscode에서 개발할 때 interpreter를 잡는 방법 \n                  \n                \n\n\npoetry를 사용하는 경우에 vscode에서 python interpreter를 잘 잡지 못한다.\namazingguni.medium.com/python-poetry를-사용하는-프로젝트를-vscode에서-개발할-때-interpreter를-잡는-방법-e1806f093e6d\n\n\n\n\n아래 코드 입력\npoetry config virtualenvs.in-project true\npoetry config virtualenvs.path &quot;./.venv&quot;\n \npoetry install &amp;&amp; poetry update\npoetry에서 requirements.txt 내보내기\npoetry export --without-hashes --format=requirements.txt &gt; requirements.txt\nDocker file build error(failed to solve with frontend dockerfile.v0: failed to read dockerfile: open~)\ndocker file로 build하는 도중 오류가 발생했다.\ndocker build . -t &quot;my-first-fastapi-app&quot;\n \n&gt;&gt; failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount2664817077/Dockerfile: no such file or directory\n이유 : docker file의 이름이 정확해야 한다…\n(기존)DockerFile → (변경) Dockerfile\n\nMac에서 pyenv로 여러 Python 버전 사용하기\n\n\n                  \n                  Pyenv로 MacOS에서 파이썬 버전 관리하기 \n                  \n                \n\n\n프로젝트 혹은 가상환경별로 서로 다른 파이썬 버전을 활용하고자 한다면 어떻게 해야 할까요?\nblog.eunsukim.me/posts/managing-python-versions-on-your-mac\n\n\n\npyenv 설치\nbrew install pyenv\npython 설치\npyenv install {원하는 버전}\n \n# 예시\npyenv install 3.7.6\nshell path 추가\necho -e &#039;if command -v pyenv 1&gt;/dev/null 2&gt;&amp;1; then\\n  eval &quot;$(pyenv init -)&quot;\\nfi&#039; &gt;&gt; ~/.zshrc\nsource ~/.zshrc\npython 버전 설정\npyenv global {원하는 버전}\n \n# 예시\npyenv global 3.7.3\n현재 python 버전 확인\npyenv versions\n\nMac에서 pyenv + poetry로 환경 설정하기\n\n\n                  \n                  pyenv + poetry 조합으로 다양한 python 버전 개발환경 구성 \n                  \n                \n\n\n오랜만에 글을 쓰게 됐네요.\nmattpy.tistory.com/entry/pyenv-poetry-조합으로-다양한-python-버전-개발환경-구성\n\n\n\n\n\npyenv를 통해 원하는 필요한 버전의 python interpreter로 변경한다\n\n\n아래 코드를 입력한다\npoetry env use python # 자동\npoetry env use python {version} # 버전 지정\n \npoetry env use python 3.7 # 예시\n\n\n\nMac + Pytorch + poetry 사용시 문제\nMac에서 poetry로 가상환경을 구성하고, pytorch를 설치하려고 했다.\n그러나\npoetry add pytorch 를 입력하면 nvidia 기반 gpu가속을 하는 버전을 설치한다.\n따라서 torch를 사용해서 가상환경을 구성할 수 없다.\nmac의 mps가속을 지원하는 pytorch버전도 아직 정식 버전으로 출시되지 못하고 있다.\n그러므로 2022년 11월 기준, 아직까지는 pipenv를 사용하는 것이 현명한거같다.\n또한, mac으로 pytorch를 사용하지 않는것이 정신건강에 이로운것같다.\n\nBentoML 공부(스크랩)\n\n\n                  \n                  Machine Learning Serving - BentoML 사용법 \n                  \n                \n\n\nMachien Learning Serving 라이브러리인 BentoML 사용법에 대해 정리한 글입니다 키워드 : BentoML Serving, Bentoml Tutorial, Bentoml ai, bentoml artifacts, bentoml github, bentoml serve, AI model serving, MLOps Serving 데이터 사이언스팀이 만든 모델을 쉽게 테스트, 배포, 통합할 수 있어야 함 이를 위해 데이터 과학자는 서버에 모델 파일 또는 Protobuf 파일을 업로드하는게 아닌, 예측 서비스를 구축하는데 도움이 되는 도구가 필요함 데이터 과학자 입장에서 정말 적은 코드로 프러덕션 서비스까지 가능한 BentoML 들어가기 전에 사용되는 용어 간단 정리(글 읽으신 후, 다시 보셔도 좋아요) Bento : 일본의 도시락 요리.\nzzsza.github.io/mlops/2021/04/18/bentoml-basic/\n\n\n\n\n\nbentoml.BentoService\n\nbentoml.BentoService는 예측 서비스를 만들기 위한 베이스 클래스\n\n@bentoml.artifacts 데코레이터를 통해 여러 머신러닝 모델을 포함할 수 있음\n@bentoml.api의 인자인 input에 DataframeInput, JsonInput, ImageInput, TfTensorInput 등을 넣을 수 있으며, output도 JsonOutput 등을 사용할 수 있음\n\nAPI 함수 코드에서 self.artifacts.ARTIFACT_NAME으로 접근할 수 있음. 위에서 실행한 코드는 self.artifacts.model로 접근했음\n\n\nBentoService는 __main__ 모듈에서 정의할 수 없고 항상 파일로 저장해야 함. 노트북에서 사용하려면 %writefile을 사용\n\n\nBentoService를 저장하려면 save 메소드 사용\n\n머신러닝 프레임워크, Artifact를 기반으로 모델을 저장\nBentoService 클래스에 필요한 pip 종속성을 자동으로 추출하고 requirements.txt에 저장함\n모든 파이썬 코드 종속성 저장\n생성된 파일을 특정 디렉토리에 저장\nsave 함수는 내부적으로 save_to_dir를 호출함\n\n\nBentoML bundle은 예측 서비스에서 실행될 모든 코드, 파일, 설정이 저장된 파일 디렉토리\n\nBentoML bundle은 도커 컨테이너 이미지 또는 바이너리로 생각할 수 있음. Train 과정에서 Bundle이 생성됨\n\n\n\n\n\nService Environment\n\n\nPyPI Packages\n\n@betoml.env(infer_pip_packages=True)를 사용하면 자동으로 필요한 라이브러리를 추론함\nrequirements_txt_file을 지정할 수도 있음\n\n@bentoml.env( requirements_txt_file=&quot;./requirements.txt&quot; ) class ExamplePredictionService(bentoml.BentoService): @bentoml.api(input=DataframeInput(), batch=True) def predict(self, df): return self.artifacts.model.predict(df)\n\n@bentoml.env(pip_packages=[])를 사용하면 버전을 지정해서 저장함\n\n@bentoml.env( pip_packages=[ &#039;scikit-learn==0.24.1&#039;, &#039;pandas @github.com/pypa/pip/archive/1.3.1.zip&#039;, ] ) class ExamplePredictionService(bentoml.BentoService): @bentoml.api(input=DataframeInput(), batch=True) def predict(self, df): return self.artifacts.model.predict(df)\n\n@bentoml.env(coda_channels=[], conda_dependencies=[])로 Conda 패키지 의존성도 처리할 수 있음\n\n단, Conda 패키지는 AWS Lambda에서 작동하지 않음(플랫폼의 제한)\n\n\n\n\n\nCustom Docker Image\n\n@bentoml.env(Docker_base_image=&quot;image_:v1&quot;)로 사용할 수 있음\nBentoML Slim 기본 이미지는 90MB라 유용할 수 있음(bentoml/model-server:0.12.0-slim-py37)\n\n\n\nInit Bash Script\n\nDocker 컨테이너 셋팅하는 스크립트를 인자로 주입할 수 있음\n@bentoml.env(setup_sh=&quot;init_script.sh&quot;)\n\n\n\n@bentoml.ver를 사용해 버전을 지정할 수 있음\n\nmajor, minor\nDocument\n\nfrom bentoml import ver, artifacts from bentoml.service.artifacts.common import PickleArtifact @ver(major=1, minor=4) @artifacts([PickleArtifact(&#039;model&#039;)]) class MyMLService(BentoService): pass svc = MyMLService() svc.pack(&quot;model&quot;, trained_classifier) svc.set_version(&quot;2019-08.iteration20&quot;) svc.save() # The final produced BentoService bundle will have version: # &quot;1.4.2019-08.iteration20&quot;\n\n\n\n\nModel Artifact 패키징\n\nArtifact API(@artifacts)를 사용하면 모델을 지정할 수 있음\n\n모델을 Load할 때 모델 Serilization, deserialization를 자동으로 처리함\n여러 아티팩트를 지정할 수 있음\n\n\n\nimport bentoml\nfrom bentoml.adapters import DataframeInput\nfrom bentoml.frameworks.sklearn import SklearnModelArtifact\nfrom bentoml.frameworks.xgboost import XgboostModelArtifact\n\t\n@bentoml.env(infer_pip_packages=True)\n@bentoml.artifacts([\n    SklearnModelArtifact(&quot;model_a&quot;),\n    XgboostModelArtifact(&quot;model_b&quot;)\n])\nclass MyPredictionService(bentoml.BentoService):\n\t\n    @bentoml.api(input=DataframeInput(), batch=True)\n    def predict(self, df):\n        # assume the output of model_a will be the input of model_b in this example:\n        df = self.artifacts.model_a.predict(df)\n\t\n        return self.artifacts.model_b.predict(df)\n\n모델 a의 output이 b 모델의 input이 되는 경우를 구현한 코드\n위 코드에서 @bentoml.artifacts([ ])을 사용해서 sklearn, XGBoost 모델 2개를 사용함(각각의 이름은 model_a, model_b)\n\nsvc = MyPredictionService()\nsvc.pack(&#039;model_a&#039;, my_sklearn_model_object)\nsvc.pack(&#039;model_b&#039;, my_xgboost_model_object)\nsvc.save()\n\n보통 예측 서비스당 하나의 모델을 권장하며 관련 없는 모델은 별도로 분리함\n\n위 예시처럼 여러 모델이 의존하는 경우에만 이렇게 사용\n\n\n\n\n\nModel Management &amp; Yatai\n\n\nBentoService의 save 메소드는 번들 파일을 ~/bentoml/repository/{서비스 이름}/{서비스 버전}에 저장함\n\n메타 데이터는 로컬 SQLite에 저장됨(~/bentoml/storage.db)\n\n\n\n모델 리스트 확인\nbentoml list\n\n\n특정 모델 정보 가져오기\nbentoml get IrisClassifier\n\n\nYatai\n\nBentoML의 Model Management Component\n\n일본식 포장마차를 뜻하는 단어\nCLI, Web UI, BentoML 번들을 생성하기 위한 Python API 제공\n팀 전용 Yatai 서버를 구축해서 팀의 모든 모델을 관리하고 CI/CD를 구축할 수 있음\n\n\n\n\n\nYataiService\n\n모델 저장소나 배포를 관리하는 컴포넌트\n기본적으로 local YataiService를 사용\n커스텀해서 Model Repository를 수정할 수 있음\nYataiService의 host server를 설정할 수 있음\n추천 방식\n\nPostgreSQL DB와 S3 Bucket으로 저장\n\n\n\n&gt; docker run -p 3000:3000 -p 50051:50051 \\ -e AWS_SECRET_ACCESS_KEY=... -e AWS_ACCESS_KEY_ID=... \\ bentoml/yatai-service \\ --db-url postgresql://scott:tiger@localhost:5432/bentomldb \\ --repo-base-url s3://my-bentoml-repo/ * Starting BentoML YataiService gRPC Server * Debug mode: off * Web UI: running on http://127.0.0.1:3000 * Running on 127.0.0.1:50051 (Press CTRL+C to quit) * Usage: `bentoml config set yatai_service.url=127.0.0.1:50051` * Help and instructions: docs.bentoml.org/en/latest/guides/yatai_service.html * Web server log can be found here: /Users/chaoyu/bentoml/logs/yatai_web_server.log\n\n참고로 YataiService는 인증을 제공하지 않으므로 같은 VPC에서 접근하도록 하는게 좋음\n\n\n\n\n\nModel Artifact Metadata\n\nAccuracy, 사용한 데이터셋, static 정보 등 사용자에게 의미있는 정보를 저장할 수 있음\n메타데이터에 정보를 추가하고 싶으면 pack할 때 metadata 인자로 넘겨주면 됨\n\n# Using the example above. svc = MyPredictionService() svc.pack( &#039;model_a&#039;, my_sklearn_model_object, metadata={ &#039;precision_score&#039;: 0.876, &#039;created_by&#039;: &#039;joe&#039; } ) svc.pack( &#039;model_b&#039;, my_xgboost_model_object, metadata={ &#039;precision_score&#039;: 0.792, &#039;mean_absolute_error&#039;: 0.88 } ) svc.save()\n\n\n참고로 Model Arficat Metadadata는 immutable함(변하지 않음)\n\n\n메타 데이터 접근하는 방법\n\n\n\nCLI\n\n\n\nbentoml get MyPredictionService:latest\n\n\n\nREST API\n\n\n\nbentoml serve MyPredictionService:latest # or bentoml serve-gunicorn MYPredictionService:latest\n- 그 후 URL path/metada로 접근\n\n\n\n파이썬으로 접근\n\n\n\nfrom bentoml import load svc = load(&#039;path_to_bento_service&#039;) print(svc.artifacts[&#039;model&#039;].metadata)\n\n\n\n\nAPI Function and Adapters\n\n\nBentoService API는 클라이언트가 예측 서비스에 접근하기 위한 End Point\n\n\nAdapter는 API callback 함수를 정의하고 다양한 형태로 예측을 요청하는 추상화 레이어\n\nAdapters\nAPI 핸들링 함수로 정의됨\n\n\n\n@bentoml.api를 사용해 InputAdapter 인스턴스에 넘김\nclass ExamplePredictionService(bentoml.BentoService): @bentoml.api(input=DataframeInput(), batch=True) def predict(self, df): assert type(df) == pandas.core.frame.DataFrame return postprocessing(model_output)\n\n\nAPI의 함수에서 데이터 전처리 등으로 활용할 수 있음\nfrom my_lib import preprocessing, postprocessing, fetch_user_profile_from_database class ExamplePredictionService(bentoml.BentoService): @bentoml.api(input=DataframeInput(), batch=True) def predict(self, df): user_profile_column = fetch_user_profile_from_database(df[&#039;user_id&#039;]) df[&#039;user_profile&#039;] = user_profile_column model_input = preprocessing(df) model_output = self.artifacts.model.predict(model_input) return postprocessing(model_output)\n\n사용자가 정의한 API 함수에 전달된 입력 변수는 인퍼런스 input list임. 입력 데이터의 배치를 처리함. Micro Batching을 수행함\n\n\n\nBatch API 설정하기\n\nAPI에 batch=True를 지정하면 List로 Input을 넣어줘야 함\nbatch=False를 지정하면 한번에 하나씩 Input\n\n\n\nBatch Input 처리하는 동안 Data Validation도 가능\n\nInput data에서 특정한 경우 에러를 발생시킬 수 있음\n\n즉, 데이터가 invalid, malformatted한 경우\n\n\ndiscard API를 사용해 에러를 발생할 수 있음\n\nfrom typings import List from bentoml import env, artifacts, api, BentoService from bentoml.adapters import JsonInput from bentoml.types import JsonSerializable, InferenceTask # type annotations are optional @env(infer_pip_packages=True) @artifacts([SklearnModelArtifact(&#039;classifier&#039;)]) class MyPredictionService(BentoService): @api(input=JsonInput(), batch=True) def predict_batch(self, parsed_json_list: List[JsonSerializable], tasks: List[InferenceTask]): model_input = [] for json, task in zip(parsed_json_list, tasks): if &quot;text&quot; in json: model_input.append(json[&#039;text&#039;]) else: task.discard(http_status=400, err_msg=&quot;input json must contain `text` field&quot;) results = self.artifacts.classifier(model_input) return results\n\nHTTP 응답, CLI 추론 작업 출력 등을 디테일하게 작성할 수 있음\n\nfrom bentoml.types import JsonSerializable, InferenceTask, InferenceError # type annotations are optional class MyService(bentoml.BentoService): @bentoml.api(input=JsonInput(), batch=False) def predict(self, parsed_json: JsonSerializable, task: InferenceTask) -&gt; InferenceResult: if task.http_headers[&#039;Accept&#039;] == &quot;application/json&quot;: predictions = self.artifact.model.predict([parsed_json]) return InferenceResult( data=predictions[0], http_status=200, http_headers={&quot;Content-Type&quot;: &quot;application/json&quot;}, ) else: return InferenceError(err_msg=&quot;application/json output only&quot;, http_status=400)\n\nhttp_status를 200, 400 등으로 정의하거나 data를 예측의 첫 값만 취한다거나\nBatch가 True인 경우\n\n예측 결과값을 for loop\n\n\n\nimport bentoml from bentoml.types import JsonSerializable, InferenceTask, InferenceError # type annotations are optional class MyService(bentoml.BentoService): @bentoml.api(input=JsonInput(), batch=True) def predict(self, parsed_json_list: List[JsonSerializable], tasks: List[InferenceTask]) -&gt; List[InferenceResult]: rv = [] predictions = self.artifact.model.predict(parsed_json_list) for task, prediction in zip(tasks, predictions): if task.http_headers[&#039;Accept&#039;] == &quot;application/json&quot;: rv.append( InferenceResult( data=prediction, http_status=200, http_headers={&quot;Content-Type&quot;: &quot;application/json&quot;}, )) else: rv.append(InferenceError(err_msg=&quot;application/json output only&quot;, http_status=400)) # or task.discard(err_msg=&quot;application/json output only&quot;, http_status=400) return rv\n\n\n여러 API 사용하기\n\ninput으로 dataframe인 경우 predict, input으로 json인 경우 predict_json을 사용하도록 두 API를 생성할 수 있음\n\nfrom my_lib import process_custom_json_format class ExamplePredictionService(bentoml.BentoService): @bentoml.api(input=DataframeInput(), batch=True) def predict(self, df: pandas.Dataframe): return self.artifacts.model.predict(df) @bentoml.api(input=JsonInput(), batch=True) def predict_json(self, json_arr): df = process_custom_json_format(json-arr) return self.artifacts.model.predict(df)\n\n\nOperational API\n\n추론 요청을 처리하는 대신 예측 서비스 config 업데이트 요청을 처리하거나, 새로 도착한 데이터로 모델을 재학습시키는 API를 만들 수 있음\n다만 아직 Beta라 공개되진 않고 이메일로 연락달라고 함\n\n\n\n\n\nModel Serving\n\n\nBentoService가 Bento로 저장되면 다양한 방법으로 배포할 수 있음\n\n\n3가지 방식\n\nOnline Serving : API endpoint를 통해 실시간 예측\nOffline Batch Serving : 배치로 처리한 후, 결과를 스토리지에 저장함\nEdge Serving : 모바일, IoT 기기에 모델 배포\n\n\n\nOnline API Serving\n\nBentoService를 저장하기만 하면 REST API 서버를 쉽게 만들 수 있음\n\nbentoml serve IrisClassifier:latest\n\nAPI Server Dockerization\n\nBento를 저장하면 Dockerfile이 생성됨\ndocker build 가능\n\n\n\nsaved_path=$(bentoml get IrisClassifier:latest --print-location --quiet) # Build docker image using saved_path directory as the build context, replace the # {username} below to your docker hub account name docker build -t {username}/iris_classifier_bento_service $saved_path # Run a container with the docker image built and expose port 5000 docker run -p 5000:5000 {username}/iris_classifier_bento_service # Push the docker image to docker hub for deployment docker push {username}/iris_classifier_bento_service\n\n\nAdaptive Micro-Batching\n\n0.12.0부터 Default 설정\n마이크로 배치는 예측 요청을 작은 배치로 그룹화해 모델 추론 작업에서 배처 처리의 성능 이점을 발휘하는 기술\nBentoML은 Clipper에서 영감을 받아 마이크로 배치 레이어를 구현함\nBentoML API는 사용자의 코드 변경없이 마이크로 배치와 함께 작동하도록 설계됨\n자세한 내용은 공식 문서의 Micro Batching 참고\n\n\n\n\nPython API(Programmatic Access)\n\n\n\n저장된 Bento를 load\n\n\n\nimport bentoml bento_service = bentoml.load(saved_path) result = bento_service.predict(input_data)\n\n\n\nPyPI 패키지로 설치\n\n\n\nsaved_path=$(bentoml get IrisClassifier:latest --print-location --quiet) pip install $saved_path\n\n\n\nCommand Line에서 사용\n\n\n\n# With BentoService name and version pair bentoml run IrisClassifier:latest predict --input &#039;[[5.1, 3.5, 1.4, 0.2]]&#039; bentoml run IrisClassifier:latest predict --input-file &#039;./iris_test_data.csv&#039; # With BentoService&#039;s saved path bentoml run $saved_path predict --input &#039;[[5.1, 3.5, 1.4, 0.2]]&#039; bentoml run $saved_path predict --input-file &#039;./iris_test_data.csv&#039;\n\n만약 이미 설치되어 있다면 특정해서 사용할 수 있음(BentoService Class name)\n\nIrisClassifier run predict --input &#039;[[5.1, 3.5, 1.4, 0.2]]&#039; IrisClassifier run predict --input-file &#039;./iris_test_data.csv&#039;\n\n\n\n\nLabels\n\n\n최대 63글자, dash(-), underscore(_), dot(.), 숫자, 알파벳 사용 가능\n\n\n예시\n“cicd-status”: “success” “data-cohort”: “2020.9.10-2020.9.11” “created_by”: “Tim_Apple”\n\n\nBento Bundle로 저장할 경우에도 Label 지정\n\n\nsvc = MyBentosService() svc.pack(&#039;model&#039;, model) svc.save(labels={&quot;framework&quot;: &quot;xgboost&quot;})\n\n\n배포를 위한 Label 지정\n\n현재(21년 4월 기준) CLI로만 설정 가능\n\n$ # In any of the deploy command, you can add labels via --label option $ bentoml azure-functions deploy my_deployment --bento service:name \\ --labels key1:value1,key2:value2\n\n\nLabel selector\n\nLabel selector를 제공함. equality-based와 set-based 2가지로 찾을 수 있음\nEquality-based requirements\n\n= 또는 != 사용\n\n\nSet-based requirements\n\nIn, NotIn, Exists, DoesNotExist\n\n\n\nbentoml get bento_name --labels &quot;key1=value1, key2 In (value2, value3)&quot;\n\n\n\n\nRetrieving BentoServices\n\n\n학습한 모델을 저장한 후, Artifact bundle을 찾을 수 있음\n\n\n-target_dir flag를 사용\nbentoml retrieve ModelServe --target_dir=~/bentoml_bundle/\n\n\n\n\nWeb UI 커스텀\n\n\n@bentoml.web_static_content를 사용하면 웹 프론트엔드에 추가할 수 있음\n\n예시 Github\n\n@env(auto_pip_dependencies=True) @artifacts([SklearnModelArtifact(&#039;model&#039;)]) @web_static_content(&#039;./static&#039;) class IrisClassifier(BentoService): @api(input=DataframeInput(), batch=True) def predict(self, df): return self.artifacts.model.predict(df)\n\n\n\n\n\nStreamlit(Frontend) + BentoML(Backend)를 사용한 Mask &amp; Gender &amp; Age Classifier Web Service\ngithub.com/404Vector/Study.Bentoml-Streamlit.Classifier.Mask-Gender-Age\n예제를 받았지만, 직접 해보고 싶은 마음이 있어 변성윤 마스터님의 블로그 글을 참조하여 하나씩 붙여보았다.\nmodel은 artifact라는 betoml의 구성요소로 wrapping되어 service에 packing 된다.\nstreamlit과는 **==REST API==**로 통신하며, streamlit에서 post한 url의 메소드명이 service 단에서 호출된다.\nresponse = requests.post(&quot;``[http://localhost:5000/predict](http://localhost:5000/predict)``&quot;, files=files)\n→ SomeServiceServiceInstance의 predict method 호출\npacking된 model(artifact)에서 inference를 하면, 기존 model의 결과 값이 아닌 serialize된 결과 값이 나온다. 따라서 별도의 serialize를 할 필요가 없어서 매우 편리하게 느껴졌다.\n하지만, 그렇기때문에 전처리는 가능하지만, 후처리를 service의 predict단 inferrence를 받은 후에 하기는 번거롭다. 그러므로 model단에서 별도의 후처리가 필요 없도록 잘 가공해서 던져주자.\n또한, artifact는 기본적으로 beotoml이 지원하는 걸 사용하는 것이 당연히 편리하고, 원하는 artifact가 없을 경우에는 BentoServiceArtifact를 상속해서 새롭게 정의해야 한다. (내부에 model을 save / load 할 수 있게 serializer / desirializer를 구현해야 한다)\n\nAirFlow\n공식문서\n\n\n                  \n                  What is Airflow? - Airflow Documentation \n                  \n                \n\n\nApache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows.\nairflow.apache.org/docs/apache-airflow/stable/index.html\n\n\n\n참조\n\n\n                  \n                  Apache Airflow - Workflow 관리 도구(1) \n                  \n                \n\n\n오늘은 Workflow Management Tool인 Apache Airflow 관련 포스팅을 하려고 합니다.\nzzsza.github.io/data/2018/01/04/airflow-1/\n\n\n\n\n\n                  \n                  Apache Airflow Tutorials for Beginner \n                  \n                \n\n\nDocs for beginner who want to use Apache Airflow\nheumsi.github.io/apache-airflow-tutorials-for-beginner/\n\n\n\n추가로 읽어보기\n\n\n                  \n                  Info\n                  \n                \n\n\nmk.kakaocdn.net/dn/if-kakao/conf2019/발표자료_2019/T03-S04.pdf\n\n\n\nGit Repos\ngithub.com/404Vector/Study.AirFlow\npoetry같은 가상환경과 같이 사용하기에는 경로 오류 등으로 불편한 점이 있다고 느껴졌다.\ndocker로 올리는 것이 깔끔해보였다.\n\nML Flow\n\n\n                  \n                  MLflow 소개 및 Tutorial \n                  \n                \n\n\n머신러닝 라이프 사이클을 관리할 수 있는 오픈소스인 MLflow에 대한 소개 및 간단한 Tutorial에 대한 글입니다\nzzsza.github.io/mlops/2019/01/16/mlflow-basic/\n\n\n\n==(공부하려고 자료를 찾다보면 결국 변성윤님 블로그로 가게된다.. 뭐지?)==\ngithub.com/404Vector/Docker.MLFlow\nAirFlow보다는 간편해보이는 느낌이었다. logging은 아무래도 wandb가 시각화 / 연동 면에서 더 편하게 느껴졌다.\n\nMLOps\n\n\n                  \n                  What is MLOps? \n                  \n                \n\n\nNote: This article was updated March 23, 2022 to include information about the AI Infrastructure Association and additional resources on MLOps.\nblogs.nvidia.com/blog/2020/09/03/what-is-mlops/\n\n\n\n\n\n                  \n                  MLOps - Wikipedia \n                  \n                \n\n\nMLOps or ML Ops is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently.\nen.wikipedia.org/wiki/MLOps\n\n\n\n\n\n                  \n                  머신러닝 오퍼레이션 자동화, MLOps \n                  \n                \n\n\nMLOps 춘추 전국 시대 정리 자료를 정리한 글입니다 최초 작성했던 글을 2021년 6월에 모두 수정했습니다 키워드 : MLOps, MLOps란, MLOps 정의, MLOps 플랫폼, MLOps 엔지니어, MLOps 뜻, MLOps pipeline, MLOps framework 머신러닝 모델을 실제 서비스화하는 과정을 Production으로 정의함 실제 환경 : Production, Real World 실제 환경의 예시 : 모바일 앱\nzzsza.github.io/mlops/2018/12/28/mlops/\n\n\n\n\n\n                  \n                  Hidden Technical Debt in Machine Learning Systems 리뷰 \n                  \n                \n\n\nHidden Technical Debt in Machine Learning Systems 논문을 읽고 정리한 포스팅입니다.\nzzsza.github.io/data/2018/01/28/hidden-technical-debt-in-maching-learning-systems/\n\n\n\n\nMLOps는 프로덕션에서 기계 학습 모델을 안정적이고 효율적으로 배포하고 유지 관리하는 것을 목표로 하는 일련의 사례이며, 이 단어는 “기계 학습”과 DevOps의 합성어\nMLOps를 통해 기업이 달성하고자 하는 목표\n\n\n배포 및 자동화\n\n\n모델 및 예측의 재현성\n\n\n진단\n\n\n거버넌스 및 규정 준수\n\n\n확장성\n\n\n협업\n\n\n상업적 사용\n\n\n모니터링 및 관리\n\n\nML Ops Component\n\n\n                  \n                  What is MLOps? - Databricks \n                  \n                \n\n\nBack to glossary MLOps stands for Machine Learning Operations.\nwww.databricks.com/glossary/mlops\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-9/Week-9":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-9/Week-9","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/Week 9/Week 9.md","title":"Week 9","links":[],"tags":[],"content":"\nObject Detection Overview\n\nObject Detection-History\nCNN을 이용한 Image Task 예시\nEvaluation Metric\n\nConfusion matrix\nPrecision\nRecall\nmAP(mean average precision)\nIOU(Intersection Over Union)\nFLOPs(Floating Point Operations)\nObject Detection Library\n\n\n\n\n2 Stage Detectors\n\nOverview\n\nBasic Sequence\nSliding Window\nSelective Search\n\n\nR-CNN\n\nPipeline\nHard negative minig\nR-CNN의 한계\n\n\nSPPNet\n\nPipeline\nSPPNet의 한계\nSpatial Pyramid Pooling\n\n\nFast R-CNN\n\nPipeline\nROI Pooling\n\n\nFaster R-CNN\n\nPipeline\nRPN(Region Proposal Network)\nNon-maximum suppression\nClassification\nTrain : 4 steps alternative training\n\n\n\n\n\n\nObject Detection Overview\nObject Detection-History\n\n2013.11 R-CNN\n2015.ICCV Fast R-CNN\n2015.NIPS Faster R-CNN\n2016.CVPR YOLO v1\n2016.ECCV SSD\n2017.CVPR YOLO v2\n2017.CVPR FPN\n2017.ICCV RetinaNet\n2018.arXiv YOLO v3\n2018.CVPR PANet\n2020.CVPR EfficientNet\n2021.CVPR Swin-T\n\nCNN을 이용한 Image Task 예시\nClassification\nObject Detection\nSemantic Segmentation\nInstance Segmentation\nEvaluation Metric\n정확도 : mAP(mean average precision)\n속도 : fps, flops\nConfusion matrix\nTP : True Positive → 검출되야하는 것이 검출 됨\nFN : False Negative → 검출되야하는 것이 검출 안됨\nFP : False Positive → 검출되면 안되는 것이 검출 됨\nTN : True Negative → 검출되면 안되는 것이 검출 안됨\nPrecision\n{Precision}={TP \\over TP+FP}\nRecall\nRecall={TP \\over TP+FN}\nmAP(mean average precision)\nmAP = {1 \\over n}{\\sum_{k=1}^{k=n}}AP_k \\\\ AP_k : the \\ AP\\ of\\ class\\ k \\\\ n:the\\ number\\ of\\ classes\nmAP50 : IOU 0.5이상을 True로, 이하를 False로\nmAP60 : IOU 0.6이상을 True로, 이하를 False로\nmAP70 : IOU 0.7이상을 True로, 이하를 False로\nmAP80 : IOU 0.8이상을 True로, 이하를 False로\nmAP90 : IOU 0.9이상을 True로, 이하를 False로(가장 엄격)\nIOU(Intersection Over Union)\nIOU={overlapping\\ region \\over combined\\ region}\nFLOPs(Floating Point Operations)\nmodel이 얼마나 빠르게 동작하는지 측정하는 metric→연산량 횟수\nchannel별 계산 : Cin x Cout\nkernel 계산 : Kheight x Kwidth\n높이 너비 별 계산 : Hout x Wout (stride, padding을 고려한 결과여야 하기 때문에)\nConvolution layer Flops\nFLOPS_{conv}=(C_{in}*K_{hegit}*K_{width})*(Out_{width}*Out_{height}*Out_{Channel})\nObject Detection Library\nMMDetection : pytorch 기반 object detection Lib\nDetectron2 : pytorch 기반 object detection, segmentation Lib\nYOLOv5\nEfficientNet\n\n2 Stage Detectors\nOverview\nBasic Sequence\nInput Image → Region Proposals → Warpping each ROI → Classification each ROI\nSliding Window\n어떤 Window를 Sliding해가며 후보 영역을 추출(무수히 많은 후보 영역이 나옴) → 비효율저거\nSelective Search\n색감, 질감, shape 등을 사용해서 initial segmentation 수행\n초기에는 마찬가지로 매우 많은 segmentation영역 존재 알고리즘을 통해 점차 segmentation 영역을 통합하거나 버리고 줄여나감\nR-CNN\nPipeline\n\nROI 생성 : Selective Search를 통해 후보군 생성, 알고리즘을 통해 후보군을 2000개 까지 줄여나감\nWarping : 후에 Feature 추출을 위해 CNN 모델을 통과하는데, CNN 모델 마지막에 고정된 FC Layer가 있기 때문에 정해진 input size를 입력해야 함\nFeature 추출 : Pretrained CNN을 통해 Feature Vector를 추출\nBounding box regression : Feature map과 ROI를 가지고 GT의 Bounding box와의 차이를 통해 다시 더 정밀하게 ROI를 예측(SVM 등을 사용)\n\n훈련 시 Hard negative mining을 수행\nHard negative minig\n배경으로 식별하기 어려운 샘플을 강제로 다음 배치의 negative sample로 추가하는 방법\n일반적으로, 이미지에서 배경(negative region)이 대부분을 차지하고 물체(positivie resion)이 차지하는 비중은 낮다. 따라서 더 퀄리티 있는 negative region을 주기 위해 사용한다.\nR-CNN의 한계\n2000개나 되는 region이 CNN을 통과해야 함\nwarping을 통해 성능이 하락할 가능성이 있음\nConv Network의 입력이미지가 고정되어 있음\nCNN, SVM, Bounding box regressor를 각각 학습해하므로 end-to-end가 아님\nSPPNet\nPipeline\nimage → CNN(get feature map) → spatial pyramid pooling → fc → SVM, Bounding box regressor\nSPPNet의 한계\n2000개나 되는 region이 CNN을 통과해야 함 해결!\nwarping을 통해 성능이 하락할 가능성이 있음 해결!\nConv Network의 입력이미지가 고정되어 있음\nCNN, SVM, Bounding box regressor를 각각 학습해하므로 end-to-end가 아님\nSpatial Pyramid Pooling\n\n\n                  \n                  Papers with Code - Spatial Pyramid Pooling Explained \n                  \n                \n\n\nSpatial Pyramid Pooling (SPP) is a pooling layer that removes the fixed-size constraint of the network, i.\npaperswithcode.com/method/spatial-pyramid-pooling\n\n\n\nbinning(bin 공간에가 가장 큰 값 출력)을 통해 image pooling 가능\n→ 고정된 크기의 feature vector로 변경 가능\n→ Warpig을 통한 성능 하락 해결\nFast R-CNN\nPipeline\nimage → CNN(get feature map, VGG) → roi pooling→ fc → Softmax Classifier, Bounding box regressor\nROI Pooling\n\n\n                  \n                  RoI Pooling은 무엇이고 왜 하는가? \n                  \n                \n\n\nFast R-CNN에서 등장하는 RoI Pooling은 여러 computer vision 딥러닝 분야에 사용된다.\nfrogbam07.tistory.com/28\n\n\n\nSpatial Pyramid Pooling과 같음. 단, Pyramid level 1, Target grid size 7x7만을 사용\nFaster R-CNN\nPipeline\n\n\n                  \n                  Faster R-CNN Explained for Object Detection Tasks | Paperspace Blog \n                  \n                \n\n\nThis article gives a review of the Faster R-CNN model developed by a group of researchers at Microsoft.\nblog.paperspace.com/faster-r-cnn-explained-object-detection/\n\n\n\n\nimage → CNN(get feature map) → RPN(Region proposal network) → NMS(Non-maximum suppression)\nRPN(Region Proposal Network)\n\n\n                  \n                  Info\n                  \n                \n\n\ntowardsdatascience.com/region-proposal-network-a-detailed-view-1305c7875853\n\n\n\n\n\n                  \n                  RPN(Region Proposal Network) 정리 \n                  \n                \n\n\nObject Detection에서 핵심역할을 하고 있는 부분은 RPN(Region Proposal Network)이다.\nvelog.io/@suminwooo/RPNRegion-Proposal-Network-%EC%A0%95%EB%A6%AC\n\n\n\n\n\n                  \n                  Faster R-CNN Explained for Object Detection Tasks | Paperspace Blog \n                  \n                \n\n\nThis article gives a review of the Faster R-CNN model developed by a group of researchers at Microsoft.\nblog.paperspace.com/faster-r-cnn-explained-object-detection/\n\n\n\n\n\n\nRPN의 입력 : feature map\n\nRegion proposal을 생성하기 위해 feature map위에 nxn window를 sliding\n\n\n\nAnchor Box\n\nFaster R-CNN에서는 3개의 Scale(128128, 256256, 512*512), 3 개의 비율(1:1, 1:2, 2:1)을 이용해 k=9개의 Anchor Box(미리 정의된 형태를 가진 박스)를 만든다.\n\n\n\nDelta : Anchor의 크기와 위치를 조정하기 위한 값, 모델을 통해 학습됨\n\nAnchor 하나 당 Delta 하나가 대응됨\nAhchor 구성: (Y1,X1,Y2,X2)\nDelta :구성 (deltaCenterY, deltaCenterX, deltaHeight, deltaWidth)\n\n\n\nProbability\n\nAnchor 내부에 Object가 있을 확률\n\n\n\nInput : H x W x C의 Feature Map\n\n\nOutput(각 anchor box 별 픽셀 별 객체 존재 확률) : H x W x 2k\n\nH x W : Feature Map의 공간 크기와 동일\n2k : 2(Object 존재 여부에 대한 확률) x k(anchor box 수 만큼의 channel)\n\n\n존재 여부라면, 0과 1로 표현할 수 있으므로(Sigmoid) k개로도 가능\n\n\n그렇다면 왜 2k(2-class softmax)를 사용했는가?[논문 link]\n→ 간단함 때문이라고 한다.\n\n3.1.1 Anchors\nAt each sliding-window location, we simultaneously\npredict multiple region proposals, where the number\nof maximum possible proposals for each location is\ndenoted as k.\nSo the reg layer has 4k outputs encoding\nthe coordinates of k boxes, and the cls layer outputs\n2k scores that estimate probability of object or not\nobject for each proposal.*\n\nFor simplicity we implement the cls layer as a two-class\nsoftmax layer. Alternatively, one may use logistic regression to\nproduce k scores.\n\n\n\n\n\n\n\n\nOutput(Bounding Box Regression, 각 anchor box 별 픽셀 별 delta ) : H x W x 4k\n\nH x W : Feature Map의 공간 크기와 동일\n4k : 각 anchor box의 delta(dx, dy ,dh, dw)와 anchor box 의 총 수 k\n이해한 것\n\nImage Space 위에서, 어떤 Anchor Box를 생성할 때, Anchor Box의 중점이 위치할 수 있는 경우의 수는 이미지 공간의 총 픽셀 수(H x W)와 같다.\n그렇다면 임의의 위치(x,y)를 중심으로 하는 Anchor box가 어떤 Object 위에 있다고 가정했을 때, 해당 Anchor box가 그 Object를 온전히 감싸려면 얼마나 이동하고(dx,dy) 얼마나 크기를 조정해야(dh, dw) Object를 온전히 감쌀 수 있는 Bounding Box가 되는가에 대한 추측 값\n\n\n\n그러므로 만들어질 수 있는 Anchor Box 위치의 모든 경우의 수는 H x W x k\n그리고 각 경우마다 가지고 있는 정보의 수는 2(object probability) + 4(delta)\n따라서 RPN의 출력 차원은 H x W x k x 6 → 무수히 많은 bbox가 만들어짐.\n\n\nBounding box select\n\nH x W x k x 6개의 box는 너무 많은 양이기 때문에 줄이는 것이 필요하다.(후보군 선택)\n따라서 object 존재 확률이 가장 높은 anchor box position부터 내림차순으로 n개를 선택한다. 논문에서는 2000개 정도를 선택했다고 한다.\n하지만 그럼에도 불구하고, 너무 많은 box이며 나아가 겹치는 box들 또한 존재한다.\n그러므로 NMS(Non-maximum suppression)을 통해 불필요한 box를 한번 더 제거한다.\n\n\n\nTrain\n\nIoU &gt; 0.7인 anchor box를 positive\nIoU &lt; 0.3인 anchor box를 negative\n다른 경우 사용 안함\n\n\n\nLoss\nL({\\{p_i\\}},{\\{t_i\\}})={1 \\over N_{cls}}{\\sum_i}L_{cls}(p_i, p_i^*) \\\\+\\lambda{1 \\over N_{reg}}{\\sum_i}{p_i^*}{L_{reg}}(t_i, t_i^*)\n\n\nNon-maximum suppression\n\n\n                  \n                  점프 투 파이썬 \n                  \n                \n\n\n점프 투 파이썬 오프라인 책(개정판) 출간 !\nwikidocs.net/142645\n\n\n\n\n\n모든 Bounding box는 자신이 해당 객체를 얼마나 잘 잡아내지 나타내는 confidence score를 가짐\nThreshold 이하의 confidence score를 가지는 Bounding Box는 제거\n\n\n남은 Bounding Box들을 Confidence score 기준 모두 내림차순 정렬\n\n\n맨 앞에 있는 Bounding box 하나를 기준으로 잡고, 다른 bounding box와 IoU 값을 계산\nIoU가 threshold 이상인 Bounding box들은 제거(같은 물체를 검출하고 있다고 판단)\n\n\n해당 과정을 순차적으로 시행, 모든 Bounding box를 비교 및 제거\n\n\nClassification\nRPN + NMS를 통해 얻은 BBox들로 Featuremap의 ROI를 ROI Pooling 후 Classification\nTrain : 4 steps alternative training\n\n\nstep 1) imagenet pretarined backbone load → Train RPN\n\n\nstep 2) imagenet pretarined backbone load + Trained step 1 RPN → Train Fast RCNN\n\n\nstep 3) step 2 backbone load(Freeze) → Train RPN\n\n\nstep 4) step 2 backbone load(Freeze) + Trained step 3 RPN → Train Fast RCNN\n→ 너무 복잡하기 때문에 최근에는 Approximate Joint Training 활용\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/데이터-제작-프로젝트---Wrapup-Report/데이터-제작-프로젝트---Wrapup-Report":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/데이터-제작-프로젝트---Wrapup-Report/데이터-제작-프로젝트---Wrapup-Report","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/데이터 제작 프로젝트 - Wrapup Report/데이터 제작 프로젝트 - Wrapup Report.md","title":"데이터 제작 프로젝트 - Wrapup Report","links":["대회-환경-세팅","캠퍼-제작-데이터셋-EDA","외부-데이터셋-분석","외부-데이터셋-변환","Augmentation-실험","외부-데이터셋-모델-사전-학습","모델-Fine-Tuning","모델-앙상블","대회-결과-분석-및-회고"],"tags":[],"content":"\n프로젝트 개요\n\n설명\n평가\n\n\n프로젝트 팀 구성 및 역할\n프로젝트 수행 절차 및 방법\n\n1. 협업 환경\n2. 대회 접근 전략\n3. 수행 과정\n\n\n프로젝트 수행 결과\n\n1. 데이터 셋 분석\n2. 데이터 셋 적용\n3. N times Pre-Trained\n4. Augmentation 실험\n5. Model Ensemble\n6. Model Training Overview\n7. 리더보드 제출 평가 (최종)\n\n\n자체 평가 의견\n\n1. 결과 분석\n2. 회고 및 향후 개선점\n\n\n\n\n프로젝트 개요\n설명\n\n스마트폰으로 카드를 결제하거나, 카메라로 카드를 인식할 경우 자동으로 카드 번호가 입력되는 경우가 있습니다. 또 주차장에 들어가면 차량 번호가 자동으로 인식되는 경우도 흔히 있습니다. 이처럼 OCR (Optimal Character Recognition) 기술은 사람이 직접 쓰거나 이미지 속에 있는 문자를 얻은 다음 이를 컴퓨터가 인식할 수 있도록 하는 기술로, 컴퓨터 비전 분야에서 현재 널리 쓰이는 대표적인 기술 중 하나입니다.\nOCR task는 글자 검출 (text detection), 글자 인식 (text recognition), 정렬기 (Serializer) 등의 모듈로 이루어져 있습니다. 본 대회에서는 ‘글자 검출’ task 만을 해결하게 됩니다.\n\n평가\n\nDetEval 방식\n\n이미지 레벨에서 정답 박스가 여러개 존재하고, 예측한 박스가 여러개가 있을 경우, 박스끼리의 다중 매칭을 허용하여 점수를 주는 평가방법 중 하나\n평가 방법\n\n\n모든 정답/예측박스들에 대해서 Area Recall, Area Precision을 미리 계산해냅니다.\n\n\n\n모든 정답 박스와 예측 박스를 순회하면서, 매칭이 되었는지 판단하여 박스 레벨로 정답 여부를 측정합니다.\n\n박스들이 매칭이 되는 조건은 박스들을 순회하면서, 위에서 계산한 Area Recall, Area Precision이 0 이상일 경우 매칭 여부를 판단하게 되며, 박스의 정답 여부는 Area Recall 0.8 이상, Area Precision 0.4 이상을 기준으로 하고 있습니다.\n매칭이 되었는가 대한 조건은 크게 3가지 조건이 있습니다.\n\none-to-one match: 정답 박스 1개와 예측 박스 1개가 매칭 &amp;&amp; 기본 조건 성립\none-to-many match: 정답 박스 1개와 예측 박스 여러 개가 매칭되는 경우\nmany-to-one match: 정답 박스 여러 개와 예측 박스 1개가 매칭되는 경우\n\n\n여기서, one-to-many match 경우에 한해서, 박스 recall / precision 에 0.8 로 penalty가 적용됩니다.\n\n\n\n모든 이미지에 대하여 Recall, Precision을 구한 이후, 최종 F1-Score은 모든 이미지 레벨에서 측정 값의 평균으로 측정됩니다.\n\n\n\n\n\n\n\n프로젝트 팀 구성 및 역할\n\nT4063 김형석\n\nDataset Polygon to rotated rect 변환 기능 개발, Geometric Augmentation 기능 개발, Raw Image Caching 최적화, Geometric Augmentation 실험, Input Size 실험\n\n\nT4148 이동훈\n\nEAST모델 분석, optimizer, scheduler 설정, Noise Augmentation 실험, validation loader와 datasets class 구축\n\n\nT4190 전지용\n\nDataset 분석 및 변환, Dataset Merge, Augmentation 실험, Ensemble, Input/Output 시각화 및 분석\n\n\nT4199 정원국\n\nDataset 분석 및 변환, Noise Augmentation 실험, Geometric Augmentation 실험, Color Augmentation 실험\n\n\nT4226 한상준\n\n환경 구축 표준화 및 Utility Tool 개발, Pretrained model weight 학습 및 공유\n\n\n\n\n프로젝트 수행 절차 및 방법\n1. 협업 환경\n\n\nGithub 활용 (Custom gitlab-flow 적용)\n\nMaster 브랜치 / Develop 브랜치를 기본으로 하고, 각자 실험이 필요한 경우는 Feat/실험명 또는 EDA/분석명 등과 같은 서브 브랜치를 생성하는 방식으로 Git 충돌을 회피하고자 하였다.\n\n\n\n\n\nCode Sharing\n\n\nGit과 Symbolic link를 활용한 환경 구성\n\n\n\n\n\nWandB &amp; Notion실험 관리\n\n\nWandB를 통한 Model 성능 비교 결과를 서로 공유\n\n\n\nNotion을 이용한 협업\n\n\nTo Do → Request Review → Complete의 흐름으로 서로의 작업 내용을 지속적으로 공유\n\n\n\n개별 실험 기록과 실험 결과에 대한 정리를 한 곳에서 볼 수 있도록 함 / 실험 제출 후보를 관리하며 중요도를 판단하여 선택함\n\n\n\n\n\n\n\n\n2. 대회 접근 전략\n\nHuman Error를 미연에 방지하기 위한 Tool\n\n누구나 헷갈릴 수 있는 Symbolic link를 생성하고 변경해주는 Shell script를 개발\n리더보드 제출 Open 하기 전에 모든 환경 구성을 마무리\n\n\n일일 제출 횟수 제한에 대한 고찰\n\n대회 첫날과 대회 마지막 날의 동일한 하루 10번의 제출 기회는 같은 가치를 가지는가?\n모델 성능을 필요로 하지 않는 실험은 대회 첫날 하자!\n\n\nPretrained model weight 의 중요성\n\n이번 대회의 경우 Backbone model의 VGG weight만 제공됨\n대량의 학습 데이터를 통해 우리의 Pretrained model weight를 만들자\n\nPretrained model weight를 Load 하여 본 대회 데이터로 Fine-tuning 하자\n다양한 실험을 빠르게 할 수 있다\n\n\n\n\n다양한 외부 데이터 셋 선정\n\nCamper Dataset, ICDAR19(MLT, LSVT, ArT), COCO-Text, HierText 등 많은 외부 데이터 셋 탐색 및 분석\n\n\n과정\n\nEDA → 대회 환경 구축 및 대회 진행 분석 → 내/외부 데이터 셋 분석 → Pretraind weights 생성 → 학습 실험 → 앙상블 전략\n\n\n\n3. 수행 과정\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이름날짜태그대회 환경 세팅2022년 12월 5일 → 2022년 12월 7일캠퍼 제작 데이터셋 EDA2022년 12월 8일 → 2022년 12월 9일외부 데이터셋 분석2022년 12월 8일 → 2022년 12월 11일외부 데이터셋 변환2022년 12월 8일 → 2022년 12월 12일Augmentation 실험2022년 12월 9일 → 2022년 12월 14일외부 데이터셋 모델 사전 학습2022년 12월 11일 → 2022년 12월 13일모델 Fine Tuning2022년 12월 13일 → 2022년 12월 15일모델 앙상블2022년 12월 14일 → 2022년 12월 15일대회 결과 분석 및 회고2022년 12월 16일 → 2022년 12월 18일\n\n프로젝트 수행 결과\n1. 데이터 셋 분석\n\nCamper Dataset\n\nAnnotation이 깔끔한 좋은 데이터가 있는 반면 어노테이션이 불안정한 데이터도 포함\nAnnotation 보정 없이 그대로 전체 사용\n\n\n일부 실험에서 제외하였을 경우도 평가함\n\n\n본 대회의 annotation 기준과 가장 유사하다고 판단\n\n\n\n\n\n\n\n\n\n\nICDAR19(MLT, LSVT, ArT)\n\n곡선과 원형 패턴의 글자 이미지들이 많이 들어가 있음\nImage size (2000, 2000) ~ (4000, 4000) 사이의 다양한 사이즈로 구성\nICDAR19 - MLT\n\n학습 시간을 고려하여 Korean only 최종 채택 (약 1,000장)\n\n\nICDAR19 - LSVT\n\n학습 시간을 고려하여 사용하지 않음\n\n\nICDAR19 - ArT\n\n\n캠퍼 제작 데이터셋과 가장 유사한 Annotation rule\n\n\n학습 시간을 고려하여 사용하지 않음\n\n\n\n\n\n\n\n\n\nHierText\n\nCVPR 2022 / Towards End-to-End Unified Scene Text Detection and Layout Analysis @Google Research 에서 발표\nScene Text Detection Task 와 Layout Analysis 을 통합하기 위한 모델과 관련한 논문과 데이터셋을 공개\n특징\n\n\n이미지당 평균 100단어 이상\n\n\n현재 가장 밀도가 높은 TextOCR(2021년)보다 두 배 더 밀도가 높음\n\n\n\n\n\n\n\n2. 데이터 셋 적용\n\n\nJPEG 이미지 메타정보(exif) Orientation 변환\n\n\n\n다각형 Annotation → Rect 형식으로 변환\n\n\n\n3. N times Pre-Trained\n\n\n기존의 Vgg16 weights와 구축한 데이터 셋을 활용하여 더 좋은 Pretrained weights 생성\nPretrained weights 부터 다시 Fine-Tuning\n\n빠르고 다양한 실험의 기반\n\n\n\n4. Augmentation 실험\n\n다양한 실험을 기반으로 학습 시 최적의 Augmentation을 찾음\nGeometric Augmentation\n\nRotate : ±30º\nRandom Crop : 20% ~ 100%\n\n\nChannel Augmentation\n\nColor Jitter\n성능 하락으로 인해 사용하지 않음\n\nGauss Noise, ISO, Channel Shuffle, CHALE, ImageCompression 등\n\n\n\n\nInput Augmentation Visualization\n\n\nInput 이미지를 시각화 함으로써 각 Augmentation 적용 시 어떤 효과가 있는지 눈으로 확인\n\n\n\n\n\n5. Model Ensemble\n\nEnsemble Method\n\n\nNMS(None Maximum Suppression) 사용\n\n\n\n각 모델에서 나온 추론 결과를 모아 NMS를 진행하여 최종 Output 결정\n\n\n\n\nOutput Visualization\n\n\n모델 추론 결과를 시각화하여 모델이 어떤 것들을 잘 예측하고 어떤 문제가 있는지 파악\n\n\n이를 활용하여 다음 스텝을 계획하거나 앙상블에 필요한 모델의 장점을 파악\n\n\n\n\nMulti Scale Inference\n\n입력 이미지 크기에 따라 성능이 달라짐\n장단점을 확인하고 입력 이미지의 크기를 다르게 하여 Multi Scale Inference 진행\n\n\nModel Ensemble\n\n여러 실험 결과의 장점을 살리고자 앙상블 진행\n\n\n\n6. Model Training Overview\n\n\nMiro diagram\n\n\n\n7. 리더보드 제출 평가 (최종)\n\n\n자체 평가 의견\n1. 결과 분석\n\nAI 모델 학습에서 데이터가 얼만큼 중요한지 경험을 통해 알게됨\n강건한 모델은 좋은 데이터로부터 만들 수 있음\n\nHierText dataset을 통해 일반화 성능을 높인것이 Public rank 6위에서 Private rank 1위를 할 수 있게 만들어 주었다고 판단\n\n\n적은 데이터로 높은 성능을 냄\n\n잘 구성된 웰메이든 데이터셋의 중요성\n\n\n\n2. 회고 및 향후 개선점\n\n성장 포인트 회고\n\n데이터의 중요성! 협업의 중요성!\n대회는 대회일뿐! 차근차근 정한 방향대로 나아가면 결과는 따라올 것\n포기하지 않고 도전하니, 어제의 나보다 성장해있었다.\n이번 대회를 통해 여러 사람과 함께 협업할 수 있는 AI Engineer로 성장해가고 있음을 느낌\n\n\n좋았던 점\n\n다양한 실험을 통한 학습을 시도하였음\n\nAugmentation, 데이터 변환툴 개발 등 각자의 역할 모두가 큰 도움이 되었음\n\n\n근거에 기반한 실험 설계로 분석 가능하였음\n\nWandB와 Notion을 통해 실험을 기록하여 우수한 Pretrained 모델을 선정할 수 있었음\n\n\n빠른 의사결정과 협업의 가치를 살렸음\n\nTo Do 관리와 데일리 스크럼을 통하여 실험과 코드 개발에 효율성을 높일 수 있었음\n\n\n\n\n아쉬운 점\n\n건강과 체력 관리가 중요하다는 것을 다시 느낌\n\n건강상 이유로 인한 우수 인력의 부재\n\n\n오픈소스 및 공개 데이터셋을 다소 늦게 발견하여, 더 많은 실험 기회를 얻지 못하였음\n\n오픈소스를 사용함에 있어 코드 분석이 부족함\n\n\n\n\n향후 개선점\n\n실험 기록을 시각화하거나 직관적으로 이해할 수 있는 기록방법 찾기\n\n체계적인 실험의 중요성을 깨달았고 결과와 무관한 모든 실험에서 Insight를 얻을 수 있었음\n\n\nData의 효율적인 관리를 위한 Data관리 툴 학습\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/데이터-제작-프로젝트---발표자료/데이터-제작-프로젝트---발표자료":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/데이터-제작-프로젝트---발표자료/데이터-제작-프로젝트---발표자료","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/데이터 제작 프로젝트 - 발표자료/데이터 제작 프로젝트 - 발표자료.md","title":"데이터 제작 프로젝트 - 발표자료","links":[],"tags":[],"content":""},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---WrapUp-Report/마스크-착용-상태-분류---WrapUp-Report":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---WrapUp-Report/마스크-착용-상태-분류---WrapUp-Report","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/마스크 착용 상태 분류 - WrapUp Report/마스크 착용 상태 분류 - WrapUp Report.md","title":"마스크 착용 상태 분류 - WrapUp Report","links":[],"tags":[],"content":""},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---개인-회고":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---개인-회고","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/마스크 착용 상태 분류 - 개인 회고.md","title":"마스크 착용 상태 분류 - 개인 회고","links":[],"tags":[],"content":"이번 프로젝트에서나의 목표는 무엇이었는가?\n리더보드 상위권, 직접 코드짜기, 팀원들과 협업, 더 효율적인 훈련\n나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\n\nGitHub Repos 구성 및 팀원들과 공유\n지속적으로 Utility Function Update\nEDA 결과 지속적 공유\n훈련 결과 지속적 공유\nGender 단일 분류 모델 학습 및 결과 공유\nAge/Gender/Mask 전체 분류 모델 학습 및 결과 공유\nBaseline code 참조, Train 및 Inferrence용 python script 제작 및 공유\n\n나는 어떤 방식으로 모델을 개선했는가?\n\n\n전체 이미지에 대한 RGB 평균 및 표준편차 시각화\n\n\n전체 이미지에 대한 class별 분포 시각화\n\n\n전체 이미지에 대한 class별 평균 이미지 시각화 및 전체 이미지의 평균 이미지 시각화\n\n\n데이터불균형을 해소하기 위해 FocalLoss추가\n\n\nSGD와 Adam optimizer의 성능비교\n\n\nViT 사용, 여러 Case(input size, model size, argmentation, ..etc)들에 대하여 훈련 및 결과 비교\n\n\n내가한행동의결과로어떤지점을달성하고, 어떠한깨달음을 얻었는가?\n\nGender, Age, Mask를 각각 분류하는 모델을 만들고 결과를 합치는 방식이 좋은 결과 를도출할 것이라고 생각했다. 개별적으로 훈련을 진행한 뒤, 잘 예측하는 항목은 고정 하고 잘 예측하지 못하는 항목에 집중하면 더 좋을 것이라고 생각했기 때문이다. 하지만 결과는 좋지않았다. 한번에 예측하는 모델이라면 제출해서 정량적으로 판단할 수 있지만, 여러 모델을 훈련시켜 제출하면 Test dataset에 대한 성능 판단이 어렵다.\n그러나 개별 모델로나누고 이를 합쳐서 제출하니 어떤 모델을 수정해야하는지 알 수 없게 되어버렸다. 어떤 계획을 세웠을 때, 실제로 실현이 가능한지 면밀이 검토가 필요하다고 느꼈다.\n또한 개별분류 방식에서 한번에 분류하는 방식으로 넘어갔을 때, 기록의 중요성을 느꼈다. 개별 분류방식에서 했던 많은 시행착오를 기록해놓지 않았기 때문에, 0부터 다시 시 작하는 기분이었다. 그래서 한번에 분류하는 방식으로 훈련할 때는 지속적으로 기록했다.\n\n전과비교해서, 내가새롭게시도한변화는무엇이고, 어떤 효과가 있었는가?\n\n성별 분류 모델은 기본적으로 성능이 잘 나와서 크게 새로운 것은 없었다.\n한번에 분류 하는모델의 경우, 처음으로 ViT를 사용해서 분류해보았다. ViT는 Res-net, Efficient- net등과는구조가 조금 다르다. 그래서 처음에는 어떻게 끝단을 수정해야 할지 알 수 없어서 직접 모델 내부를 분석해 보았다. 결과, timm 라이브러리를 사용하지 않고torchvision에 있는 ViT의 Head를 직접 수정해서 사용할 수 있었다.\nViT를 사용해 HyperParameter와 ViT Model Type 및 Argmentation을 변경해가며 실험 했고,이전과 같은 실수를 반복하지 않기 위하여 모두 기록으로 남겼다. 결과, 실험적 으로 성능이올라가는 조건들을 하나씩 찾을 수 있었고, 단일 제출로는 가장 높은 점수 를 얻을 수있었다.\n\n마주한한계는무엇이며, 아쉬웠던점은무엇인가?\n\nViT만 사용해서 실험을 진행한 점이 아쉬웠다. 앞서 성별 분류를 하면서 여러 모델을 사용해보았고, 시간을 많이 소모했다. 따라서 다른 모델을 사용해볼 여유가 없다고 판단했기 때문이다.\n모델의 훈련과정을 WandB등으로 시각화하고 기록하지 않은것이 아쉽다.데이터를 직접 확인하지 않은 것이 아쉽다.\n데이터 항상 올바르게 라벨링 되어있을 것이 라고생각해서는 안된다.\nConfusion Matrix로 결과를 시각화해보지 못한 것이 아쉽다.\n\n한계/교훈을 바탕으로다음 프로젝트에서스스로 새롭게시도해 볼 것은무엇일까?\n\n가장 기본이 되는 모델로 훈련하여 기본이 될 정보들을 만들어놓겠다.\n그리고 그 정보들을기반으로 여러가지 모델을 실험해보고 성능을 평가하겠다.\nWandB나 Tensorboard를 사용해 파라미터를 기록하고 훈련과정을 시각화해보고 싶다.\n또한 여러 파라미터에 대해 자동으로 입력 및 훈련하고, 이를 기록하는 시퀀스를 만들어보겠다.\n데이터 분석을 더 면밀히 하겠다.\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---멘토-피드백":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/마스크-착용-상태-분류---멘토-피드백","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/마스크 착용 상태 분류 - 멘토 피드백.md","title":"마스크 착용 상태 분류 - 멘토 피드백","links":[],"tags":[],"content":"전체 피드백\n전체 피드백전반적으로 개요, 문제 정의, 데이터 포맷 및 설명 부분을 명확하게 잘\n적어주셨습니다.문제 정의 부분 중 문제를 푼 방식에서는 Multi-Class or Multi-   Label Classification 으로 작성했다면 더 좋았을 것 같네요.팀원 역할 부분에서도\n접근 방식 별로 균형있게 팀원 분배를 잘하신 것 같습니다. 추가적으로 역할 부분은\n접근 방식 관점으로 적기 보다는 베이스라인 작성, 백본 작성, 실험 관리, 시각화 및\nEDA 담당 등과 같이 태스크 별로 적었으면 더 좋겠습니다.EDA 부분은 각 클래스 별\n분포와 이미지 자체의 픽셀 값에 대한 시각화 등이 좋았습니다. 특히 전체 평균\n이미지 시각화가 눈에 띄네요.실제로 팀 내에서 시도해본 전처리, 모델링, 파라미터\n튜닝 방법 들에 대해 상세히 적어준 부분이 좋았습니다. 사용하신 백본, 로스,\n이미지 사이즈 등의 조합에 따라 성능을 한눈에 확인할 수 있어서\n좋았습니다.하지만, 적용해본 기법에서 단순히 성능 저하와 같이 기재하신 부분은\n조금 아쉽습니다. 명확한 원인을 파악하기는 어렵지만, 대략적으로 유추해보거나\n사후 분석을 통해 이러이러해서 잘 안됐던 것 같다와 같은 의견을 적어주면\n좋겠습니다. 자체적으로 아쉽다고 남겨주신 부분은 제가 레포트를 읽으면서\n잘했다고 생각한 부분도 많았지만, 아쉽다고 생각했던 부분도 많이\n겹쳐있습니다.하지만 아쉬운게 많을 수록 좋습니다. 캠퍼님들이 개선하고 더 나아갈\n수 있는 피드백이 될 수 있는 것들이니 너무 마음쓰지 마시고 다음 대회에서 잘\n개선하시면 좋겠습니다.마지막으로 포트 폴리오와 레포트의 기본은 디테일 입니다.\n제출해주신 문서에서 포맷이 일관성 없다거나(영어 대문자 소문자), 폰트가\n다르다거나, 폰트의 크기가 맞지 않는다거나, 오타, 띄어쓰기 등 부분이 매우\n아쉽습니다. 저희에게 제출하는 보고서는 상관이 없지만, 추후에 사용하실\n포트폴리오는 깔끔하게 잘 정리하시면 좋겠습니다.\n\n김형석\n김형석 캠퍼님 첫 대회이긴 하지만 목표가 상당히 도전적인 부분이 좋습니다.목표를 달성하기\n위해 실제로 노력하신 부분이 인상 깊습니다. 팀원들과 더 많은 정보를 공유하고, 한 두번으로\n끝나는 것이 아니라 지속적으로 하려고 하셨던 부분이 눈에 띄네요.레포트에서 제가 맘에 들었던\n부분 중 하나가 시각화 부분이었는데, 형석 캠퍼님께서 작업하신 부분이셨군요. 다른 캠퍼님께도\n피드백 드렸지만, 데이터 EDA 시각화는 우리가 자주 중요성을 잊게되는 부분입니다. 특히\n이미지와 같은 데이터는 어떻게 해야할지 더 막막한 부분이 있죠.대회를 진행함에 있어 가설을\n세우고 실제로 접근하셨다는 부분은 아주 잘하셨습니다. 비록 결과는 좋지 않을 수 있지만,\n이런식으로 파고들고 계속 도전해서 나만의 노하우를 쌓는 기회라고 생각하시면 좋겠습니다.직접\n모델을 뜯어고쳐보며 사용해보신 부분 좋았습니다. 주어진 모델을 그대로 사용하는 것도 좋지만\n상황에 맞게 커스터마이징 할 수 있는 능력도 AI 엔지니어에게 필요한 역량 중 하나입니다.실험\n기록 부분에서 아쉬움을 많이 느끼신 것 같습니다. 사후 분석을 위해서라도 실험 기록과 관리는\n매우 중요합니다.이번 대회에서 아쉽다고 생각하셨던 부분은 반드시 다음 대회나 다음\n프로젝트에서는 개선해보셨으면 좋겠습니다. 아쉽지 않은 대회나 프로젝트는 없을겁니다. 하지만\n그런 아쉬움 속에서 계속 개선할 부분을 찾아내고 셀프 피드백을 통해 더 나은 AI 엔지니어가\n되셨으면 좋겠습니다.\n\n정혁기\n정혁기 캠퍼님스스로 본인의 베이스라인 코드를 만드셨다니\n대단하십니다. 저도 대회 경험이 얼마 없을 때 어떻게 시작해야할지\n막막한 경험이 있다보니 공개된 베이스라인을 뜯어보고 파악해보며\n저만의 베이스라인을 만드는 방법을 연습했던 기억이 있습니다.모델 개선\n부분에서도 스스로 가설을 세우고 그 가설을 검증하는 방식이\n좋았습니다.Git 사용법이 두렵지 않아졌다는 부분은 개인적으로 뿌듯함을\n느끼는 부분이네요. 아주 중요한 기본기 중 하나이니 다른 멘토님과 다른\n캠퍼님을 만나시더라도 꼭 전파하셔서 유지하시길 바라겠습니다.항상\n저도 대회를 하다보면 데이터 EDA 부분에서 아쉬움을 느낍니다. 우리는\n데이터를 만지고 데이터에서 인사이트를 뽑는 사람들이니 EDA를 가볍게\n여기지 않도록 습관을 들이시면 좋겠습니다.이번 대회에서 아쉽다고\n생각하셨던 부분은 반드시 다음 대회나 다음 프로젝트에서는\n개선해보셨으면 좋겠습니다. 아쉽지 않은 대회나 프로젝트는 없을겁니다.\n하지만 그런 아쉬움 속에서 계속 개선할 부분을 찾아내고 셀프 피드백을\n통해 더 나은 AI 엔지니어가 되셨으면 좋겠습니다.\n\n노순빈\n노순빈 캠퍼님 이전에 직접 모델 구현을 해보았던 경험을 토대로 이어서 도전해보신 것은 아주\n좋습니다. 그리고 성능이 좋지 못하다고 하셨는데, 이러한 부분도 추가로 분석해서 왜 좋지 않았을지\n고민해보면 더더욱 좋겠습니다.모델 튜닝은 말씀해주신 작은 파라미터를 건들여보는 것에서부터\n시작됩니다. 다음 대회나 추후 프로젝트에서는 모델 파라미터 뿐만 아니라 서로 다른 모델의\n아이디어를 차용해서 모델링에 직접 적용해보시는 것은 어떨까요?이번 이미지 분류 대회와 같이 이런\n단순한 태스크는 백본 모델을 선정하고 그 이후에는 Augmentation 싸움 입니다. 어떻게 더 많고\n다양한 데이터를 모델에 학습시킬지 고민해보시는것이 좋습니다.대회를 하시면서 조금씩 전체적인\n그림에 대해 이해하고 있는 과정에 있으신 것 같네요. 이렇게 조금씩 조금씩 꾸준하게 나아가시면\n좋겠습니다.이번 대회에서 아쉽다고 생각하셨던 부분은 반드시 다음 대회나 다음 프로젝트에서는\n개선해보셨으면 좋겠습니다. 아쉽지 않은 대회나 프로젝트는 없을겁니다. 하지만 그런 아쉬움 속에서\n계속 개선할 부분을 찾아내고 셀프 피드백을 통해 더 나은 AI 엔지니어가 되셨으면 좋겠습니다.\n\n박시형\n박시형 캠퍼님 관심 있는 논문을 직접 구현해서 접근해보신 부분이 좋았습니다. 실제로 대회를\n진행하다보면 새로운 아이디어가 필요할 때 논문 서치를 많이 하는 편입니다. 접근 방식이\n좋았습니다.정하신 학습 목표를 달성하기 위해 시도해보신 부분, 시도했지만 성능이 좋지 않았거나 잘\n되지 않았던 부분에 대한 의견도 좋았습니다.사실 대회를 진행하면서 내가 시도한 것 중에 1/3 정도라도\n구현했다면 저는 잘했다고 생각합니다. 물론 그 중에 절반도 성능을 향상 시키는 부분에 도움이 되지\n않을 수도 있지만요. 그럼에도 이러한 작은 시도와 도전들이 성장을 이끄는 발판이라고\n생각합니다.이번 대회를 하시면서 작게작게 배웠던 부분들이 쌓이면 이게 나만의 노하우가 될것이고\n이후 대회나 프로젝트에서 좋은 성적을 거둘 수 있게 할 경험들이 될겁니다.대부분의 캠퍼님들이\n그러듯이 시형 캠퍼님께서도 실험 관리나 모델 관리에 대해 아쉬움을 많이 느끼시고 계시네요.\n이후에는 팀원들과 잘 협업할 수 있는 방법이 무엇이 있을지 고민해보시면 좋겠습니다.이번 대회에서\n아쉽다고 생각하셨던 부분은 반드시 다음 대회나 다음 프로젝트에서는 개선해보셨으면 좋겠습니다.\n아쉽지 않은 대회나 프로젝트는 없을겁니다. 하지만 그런 아쉬움 속에서 계속 개선할 부분을 찾아내고\n셀프 피드백을 통해 더 나은 AI 엔지니어가 되셨으면 좋겠습니다.\n\n장국빈\n장국빈 캠퍼님 강의를 토대로 전체적인 흐름과 주어진 베이스라인을 잘 분석해보셨다는 부분이\n좋았습니다. 물론 본인이 모델링 역량이 좋아서 직접 본인의 베이스라인을 만드는 것도 좋겠지만,\n다른 사람의 코드를 공부하고 뜯어보며 내것으로 만든다면 그것도 좋은 공부 방법이라고\n생각합니다.사용해보신 부분을 보니 확실히 강의에서 제안해주었던 아이디어들을 많이 차용하셨네요.\n어떻게보면 다른 캠퍼님들과 다르게 강의를 아주 잘 흡수하셨을 것 같습니다.EDA 부분은 항상\n중요합니다. 데이터 속에서 인사이트를 모델이 잘 찾아주는 것도 맞지만, 잘 찾을 수 있도록 우리가\n좋은 정보를 제공하는 것도 잘 해주어야 합니다. 그 부분은 사람의 손이 어느정도는 필요한\n영역이에요.실험 설계 부분과 관리 부분은 아쉬움이 많으셨을 것 같습니다. 첫 대회이기도 하고, 아직\nAI 프로젝트를 하는 방식이 익숙하시지 않다 보니 더욱 그랬을 것 같네요.이번 대회에서 아쉽다고\n생각하셨던 부분은 반드시 다음 대회나 다음 프로젝트에서는 개선해보셨으면 좋겠습니다. 아쉽지\n않은 대회나 프로젝트는 없을겁니다. 하지만 그런 아쉬움 속에서 계속 개선할 부분을 찾아내고 셀프\n피드백을 통해 더 나은 AI 엔지니어가 되셨으면 좋겠습니다."},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - Wrapup Report/재활용 품목 분류를 위한 Object Detection - Wrapup Report.md","title":"재활용 품목 분류를 위한 Object Detection - Wrapup Report","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/Object-Detection-개인-학습","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/서버-설정","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/WandB-설정","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/EDA-및-시각화","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/모델-탐색","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/개별-실험-진행","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/앙상블"],"tags":[],"content":"\n프로젝트 개요\n\n설명\n평가\n\n\n프로젝트 팀 구성 및 역할\n프로젝트 수행 절차 및 방법\n\n1. 협업 방식\n2. 접근 전략\n3. 수행 과정\n\n\n프로젝트 수행 결과\n\n1. 데이터 분석\n2. 모델 탐색\n3. Image Size\n4. Overlapping Object Detection Strategy\n5. Augmentation\n6. Model Ensemble\n7. 리더보드 제출 평가 (최종)\n\n\n자체 평가 의견\n\n1. 결과 분석\n2. 회고 및 향후 개선점\n\n\n\n\n프로젝트 개요\n설명\n\n바야흐로 대량 생산, 대량 소비의 시대. 우리는 많은 물건이 대량으로 생산되고, 소비되는 시대를 살고 있습니다. 하지만 이러한 문화는 ‘쓰레기 대란’, ‘매립지 부족’과 같은 여러 사회 문제를 낳고 있습니다.\n분리수거는 이러한 환경 부담을 줄일 수 있는 방법 중 하나입니다. 잘 분리배출 된 쓰레기는 자원으로서 가치를 인정받아 재활용되지만, 잘못 분리배출 되면 그대로 폐기물로 분류되어 매립 또는 소각되기 때문입니다.\n따라서 우리는 사진에서 쓰레기를 Detection 하는 모델을 만들어 이러한 문제점을 해결해보고자 합니다. 문제 해결을 위한 데이터셋으로는 일반 쓰레기, 플라스틱, 종이, 유리 등 10 종류의 쓰레기가 찍힌 사진 데이터셋이 제공됩니다.\n여러분에 의해 만들어진 우수한 성능의 모델은 쓰레기장에 설치되어 정확한 분리수거를 돕거나, 어린아이들의 분리수거 교육 등에 사용될 수 있을 것입니다. 부디 지구를 위기로부터 구해주세요! 🌎\n\n평가\n\nTest set의 mAP50으로 평가\nmAP\n\n\n\n\n\n프로젝트 팀 구성 및 역할\n\nT4063 김형석\n\nEnsemble, EDA, SSD 훈련 및 평가, 조건부 Annoation 삭제 기능, Submission 생성 자동화, Confusion Matrix 생성 자동화\n\n\nT4148 이동훈\n\nEDA, class relabeling, retina model 다양하게 실험.\n\n\nT4190 전지용\n\nEDA, Model 탐색, Small Object 탐지 전략, Overlapping Objcet 탐지 전략\n\n\nT4199 정원국\n\nEDA, Loss, Faster RCNN, WandB, Data split, Heavy Augmentation, Mosiac Augmentation, K-fold\n\n\nT4226 한상준\n\nTrain 셋 분석 툴 개발, Model 탐색 (Deformable DETR with R50, Faster-RCNN with Swin-S, RetinaNet with Swin-T)\n\n\n\n\n프로젝트 수행 절차 및 방법\n1. 협업 방식\n\n\nGithub 활용\n\n\nCustom gitlab-flow 적용\n\nMaster 브랜치 / Develop 브랜치를 기본으로 하고, 각자 실험이 필요한 경우는 Feat/실험명 또는 EDA/분석명 등과 같은 서브 브랜치를 생성하는 방식으로 Git 충돌을 회피하고자 하였다. 실제 협업중에 Merge 에 필요한 Overhead 는 발생하지 않았고, 팀원 각자의 Learning curve 도 높지 않아 빠르게 적응할 수 있었다.\n\n\n\n\n\n\n\nWandB 실험 관리\n\n\nMMDetection을 이용한 Object detection 결과를 한 곳에서 취합하여 성능을 비교하기 위하여 WandB 를 사용하였고, 모델간의 특성을 비교하거나 성능을 비교하는데 큰 도움이 되었다.\n\n\n\n\n\n\n2. 접근 전략\nEDA 전략 → 모델 탐색 전략 → 앙상블 전략\n3. 수행 과정\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이름날짜태그Object Detection 개인 학습2022년 11월 14일 → 2022년 11월 21일서버 설정2022년 11월 17일 → 2022년 11월 21일WandB 설정2022년 11월 20일 → 2022년 11월 22일EDA 및 시각화2022년 11월 21일 → 2022년 11월 28일모델 탐색2022년 11월 22일 → 2022년 11월 30일개별 실험 진행2022년 11월 23일 → 2022년 12월 1일앙상블2022년 11월 30일 → 2022년 12월 1일\n\n프로젝트 수행 결과\n1. 데이터 분석\n\n\nRaw Image Size Analysis\n\n\nTrain Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwidthheightlicenseidwidth_realheight_realcount4883.04883.04883.04883.0000004883.04883.0mean1024.01024.00.02441.0000001024.01024.0std0.00.00.01409.7450120.00.0min1024.01024.00.00.0000001024.01024.025%1024.01024.00.01220.5000001024.01024.050%1024.01024.00.02441.0000001024.01024.075%1024.01024.00.03661.5000001024.01024.0max1024.01024.00.04882.0000001024.01024.0\n\n\nTest Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwidthheightlicenseidwidth_realheight_realcount4871.04871.04871.04871.0000004871.04871.0mean1024.01024.00.02435.0000001024.01024.0std0.00.00.01406.2809110.00.0min1024.01024.00.00.0000001024.01024.025%1024.01024.00.01217.5000001024.01024.050%1024.01024.00.02435.0000001024.01024.075%1024.01024.00.03652.5000001024.01024.0max1024.01024.00.04870.0000001024.01024.0\n\n\n\n\nCategory Histogram Analysis\n\n\n\nRGB Component Analysis by Category\n\n\nAverage, Std by Channel for red\n\n\n\n\n\n\nAverage, Std by Channel for green\n\n\n\n\n\n\nAverage, Std by Channel for blue\n\n\n\n\n\n\nRGB Average\n\n\n\nRGB Mean\n\n\n\nRGB Std.\n\n\n\n\n\nAnnotation per Image Analysis\n\n\n\n\n\n\nSize Analysis by Category\nWidth &amp; Height\n\nCategory ID &amp; W/H Ratio\n\n\n\n2. 모델 탐색\n\n개별 최적 모델 탐색\n\nBackbone\n\nSwin 계열, Resnet50, Resnext, Darknet, …\n\n\nModel\n\nRetinaNet, YOLOX, FPN, R-CNN, Cascade R-CNN, Deformable, DETR, SSD…\n\n\n\n\n단일 모델 Top-2\n\nDeformable DETR (Backbone : Resnet 50)\n\n\n모델 구조\n\n\n\n학습 결과\n\n\n\n모델에 대한 고찰\n\n백본으로 Resnet-50을 사용하고, Neck / Head 구조는 Deformable DETR 을 채택하였고, Two-stage 와 One-stage가 모두 가능한 구조여서 본 실험에서는 Two-stage로 실험을 하게 되었다.\nMMDetection에서 Pre-trained model weight 를 제공하고 있어서, weight 를 불러오지 않았을 때에 비하여 모델의 빠른 수렴을 얻을 수 있었다.\nSoft NMS 를 적용함에 따라 최종 추론하는 Boundary Box를 유효하게 늘릴 수 있었고, mAP의 향상이 있었다.\n\n\n\n\nFaster RCNN (Backbone : Swin-S)\n\n\n모델 구조\n\n\n\n학습 결과\n\n\n\n모델에 대한 고찰\n\n가장 좋은 결과를 내고 있었던 Faster-RCNN 모델에 백본을 Swin Transformer S 로 변경하여 학습을 진행하였고, Resnet-50 과 비교하여 Overfitting이 지연되고, 학습 셋의 특징을 더 잘 수렴할 수 있었다.\nMMDetection에서 Pre-trained model weight 를 제공하고 있어서, weight 를 불러오지 않았을 때에 비하여 모델의 빠른 수렴을 얻을 수 있었다.\nSoft NMS 를 적용함에 따라 최종 추론하는 Boundary Box를 유효하게 늘릴 수 있었고, mAP의 향상이 있었다.\n\n\n\n\n\n\n\n3. Image Size\n\nTrain Image Size\n\nEDA를 통해 작은 BBox가 많은 것을 확인\n원본 데이터 정보 손실을 최대한 줄이기 위해 (512, 512) 보다 더 큰 사이즈로 학습 및 추론\n\n\nImage slice\n\n(1024, 1024) 원본의 정보를 그대로 활용할 수 있도록 이미지를 분할하여 학습\nTest image 또한 분할하여 추론하고 추출된 결과를 합병\n학습 과정에서는 성능이 좋아지는 듯 보였으나 합병하는 과정에서 Overlapping Object의 정보의 손실이 발생하여 오히려 낮은 결과를 보임\n\n\n\n4. Overlapping Object Detection Strategy\n\nSoft NMS\n\n기존 NMS 방식을 Soft NMS 방식으로 바꿔 Overlapping Object를 탐지 할 수 있도록 변경\n\n\nConfidence Score\n\nConfidence thresold를 낮춰 많은 BBox를 검출하여 Overlapping Object를 검사할 수 있게 추론\n\n\nNMS IoU thresold\n\nIoU thresold를 낮춰 가장 점수가 높은 Bbox와 근처의 Bbox까지 더 찾아질 수 있도록 변경\n\n\n\n5. Augmentation\n\n\n모델이 Overfitting 되는 경향을 파악, 추가되는 Augmentation의 장점과 모델에 가해지는 단점을 전부 생각한 후 적용 유무를 결정\n\nShiftScaleRotate → Overfitting을 막기위해 데이터 증강의 방법으로서 진행\nRandomBrightnessContrast → 이미지 EDA결과 밝기와 대조가 올라가면 이미지 구분이 더 잘 될 것이라는 판단 하에 진행.\nRGBShift → RGB 값을 조정하여 원하는 Object가 더 눈에 띄고 모델이 잘 찾아낼 수 있게 함\n\n다만 값을 크게 줄 경우 오히려 방해가 되므로 적정 한 값을 테스트를 통해 찾아냄\n\nHueSaturationValue → HSV 값을 조정하여 원하는 Object가 더 눈에 띄고 모델이 잘 찾아낼 수 있게 함\n\nRGBShift와 마찬가지로 적정 한 값을 테스트를 통해 찾아냄\n\nJpegCompression → 이미지 품질을 낮춰서 강선성을 높이고자 함, 품질이 낮아지는 면이 걱정되었지만 단일 이미지에 적용 후 확인해보니 괜찮다고 판단\nChannelShuffle → 무작위로 채널을 섞어줘서 강건성을 높이고자 함\nBlur → Drop out 과 같은 효과를 주어 모델의 강건성을 높이고자 함.\nMedianBlur → Noise를 어느정도 제거하고자 하였음, 다만 화질이 뭉쳐 있는 부작용이 일어날 수 있으나 Task를 수행하는 데는 큰 영향이 없다고 판단\n\n\n\n6. Model Ensemble\n\n\nEnsemble Method\n\nNMS(None Maximum Suppression) 사용\n\n\n\n\nEnsemble Parameter : IOU Threshold\n\nIOU가 해당 Threshold이상인 것만 Ensemble 수행\nParameter Search : 이진 탐색 기법으로 접근, Submission을 통해 최적 Threahold 추정\n\nIOU 0.5 → IOU 0.9 → IOU 0.7 → IOU 0.6 → IOU 0.55(Best)\n\n\n\n\n\n7. 리더보드 제출 평가 (최종)\n\n\nDeformable DETR (R50) 과 Faster-RCNN (Swin-S), Faster-RCNN (800x600, soft_nms,treshold 0.1 ) 모델을 IoU Threshold 0.55 로 NMS 하여 앙상블한 결과를 제출한 것이 최종 제출에서 가장 좋은 결과를 얻게 되었다.\n이는, Neck이 트랜스포머인 경우, 백본이 트랜스포머인 경우 등 서로 다른 특성을 가지는 모델을 앙상블 하였기에 다양성을 이끌어 낼 수 있었으며, mAP 를 계산하는데 있어 Precision을 높을 수 있었다고 평가하고 있다.\n\n\n자체 평가 의견\n1. 결과 분석\n\nBackbone 만 Pretrained 모델을 사용한 것 보다 전체 Architecture의 Pretrained 모델을 가져오는 것이 더 학습에 효과적이였다.\n\nBackbone 만이 아닌 Neck, head 부분도 Task에 적합한 Pretrained 모델을 사용했다면 더 좋은 결과를 얻을 수 있을 것이라 생각됨\n\n\n추론된 Bbox가 많을 수록 더 높은 mAP 성능을 보임\n\nmAP가 높아도 결과 이미지 시각화 시 원하는 모습을 보이지 않았다.\n실생활에 사용할 모델을 학습할 때는 사용 목적과 의미에 따라 평가 지표를 더 정확하게 사용해야 함을 알게됨\n\n\n\n2. 회고 및 향후 개선점\n\n회고 의견\n\n늦은 출발인 만큼 팀원 모두가 적극적으로 팀에 기여할 부분을 찾아내 주도적으로 학습하고, 실험하여 우리의 최선을 다한 대회였다고 생각한다.\n상호간에 의견을 항상 존중하는 자세는 트러블을 최소화하고 한 마음으로 대회를 하는데 큰 도움이 되어주었다.\nEDA 를 통한 통찰과, 학습 이미지를 눈으로 보며 얻은 사람의 직관을 딥러닝 모델에 전달하고자 했던 것들이 생각만큼 의미있는 결과로 이어지지 못하였다.\n협업 툴에 익숙해지자는 프로젝트 첫 목표를 잘 이루어 내었다.\n\n\n아쉬운 점\n\n외부 경진대회 참가로 인하여 일주일 정도 늦은 출발을 하게되었는데, 실험 횟수의 부족이 최종 결과 도출에서도 큰 악영향으로 다가왔다.\n대회 후반부에 들어선 후, 오르지 않는 성능과 타 팀과의 리더보드 랭킹 격차, 부족한 시간으로 체계적인 실험을 진행하는데 큰 부담이 되었다.\n실패한 실험을 적극적으로 공유하여 이유를 찾고 더 좋은 가설이나 실험을 찾아내는 데 기여할 수 있었지만 이를 제대로 하지 못하였다.\n\n\n향후 개선점\n\n하루도 허투루 쓰지 않고 최대한 많은 실험을 진행하여 온전히 대회에 집중하는것이 필요하다고 느꼈다.\n본 대회처럼 실험 관리를 위하여 WandB 등 을 적극적으로 활용해야 할 것이라고 생각하며, 제공되는 기본 기능 이외에도 우리가 수집해야 할 Metric 이 있다면 추가하거나 이미지도 수집하는 등 커스텀도 필요하다고 느꼈다.\nNotion이나 다른 협업 툴을 사용하여 실험 설계 및 결과, TODO 등을 더 적극적으로 문서화하는 과정이 필요하다.\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/EDA-및-시각화":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/EDA-및-시각화","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - Wrapup Report/제목 없음/EDA 및 시각화.md","title":"EDA 및 시각화","links":[],"tags":[],"content":""},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/Object-Detection-개인-학습":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/Object-Detection-개인-학습","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - Wrapup Report/제목 없음/Object Detection 개인 학습.md","title":"Object Detection 개인 학습","links":[],"tags":[],"content":"외부 대회 참여"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/WandB-설정":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/WandB-설정","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - Wrapup Report/제목 없음/WandB 설정.md","title":"WandB 설정","links":[],"tags":[],"content":""},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/개별-실험-진행":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/개별-실험-진행","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - Wrapup Report/제목 없음/개별 실험 진행.md","title":"개별 실험 진행","links":[],"tags":[],"content":""},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/모델-탐색":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/모델-탐색","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - Wrapup Report/제목 없음/모델 탐색.md","title":"모델 탐색","links":[],"tags":[],"content":""},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/서버-설정":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/서버-설정","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - Wrapup Report/제목 없음/서버 설정.md","title":"서버 설정","links":[],"tags":[],"content":""},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/앙상블":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---Wrapup-Report/제목-없음/앙상블","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - Wrapup Report/제목 없음/앙상블.md","title":"앙상블","links":[],"tags":[],"content":""},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---개인-실험-일지/재활용-품목-분류를-위한-Object-Detection---개인-실험-일지":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---개인-실험-일지/재활용-품목-분류를-위한-Object-Detection---개인-실험-일지","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - 개인 실험 일지/재활용 품목 분류를 위한 Object Detection - 개인 실험 일지.md","title":"재활용 품목 분류를 위한 Object Detection - 개인 실험 일지","links":[],"tags":[],"content":"\n대회개요\n\n소개\n평가방법\n세부 일정\n대회 룰\n학습데이터\n평가데이터\n베이스라인 코드 및 제출방법\n\n\nEDA\n\nRaw Image Size Analysis\nCategory Histogram Analysis\nRGB Component Analysis by Category\nAnnotation per Image Analysis\nSize Analysis by Category\n\n\nModel 탐색\nData Refine\n\nDrop small annotations\n\n\n\n\n대회개요\n소개\n바야흐로 대량 생산, 대량 소비의 시대. 우리는 많은 물건이 대량으로 생산되고, 소비되는 시대를 살고 있습니다. 하지만 이러한 문화는 ‘쓰레기 대란’, ‘매립지 부족’과 같은 여러 사회 문제를 낳고 있습니다.\n\n분리수거는 이러한 환경 부담을 줄일 수 있는 방법 중 하나입니다. 잘 분리배출 된 쓰레기는 자원으로서 가치를 인정받아 재활용되지만, 잘못 분리배출 되면 그대로 폐기물로 분류되어 매립 또는 소각되기 때문입니다.\n따라서 우리는 사진에서 쓰레기를 Detection 하는 모델을 만들어 이러한 문제점을 해결해보고자 합니다. 문제 해결을 위한 데이터셋으로는 일반 쓰레기, 플라스틱, 종이, 유리 등 10 종류의 쓰레기가 찍힌 사진 데이터셋이 제공됩니다.\n여러분에 의해 만들어진 우수한 성능의 모델은 쓰레기장에 설치되어 정확한 분리수거를 돕거나, 어린아이들의 분리수거 교육 등에 사용될 수 있을 것입니다. 부디 지구를 위기로부터 구해주세요! 🌎\n\nInput : 쓰레기 객체가 담긴 이미지가 모델의 인풋으로 사용됩니다. 또한 bbox 정보(좌표, 카테고리)는 model 학습 시 사용이 됩니다. bbox annotation은 COCO format으로 제공됩니다. (COCO format에 대한 설명은 학습 데이터 개요를 참고해주세요.)\nOutput : 모델은 bbox 좌표, 카테고리, score 값을 리턴합니다. 이를 submission 양식에 맞게 csv 파일을 만들어 제출합니다. (submission format에 대한 설명은 평가방법을 참고해주세요.)\n\n평가방법\n\n\n모델 제출은 하루 10회로 제한됩니다.\n\n\nTest set의 mAP50(Mean Average Precision)로 평가\n\n\nObject Detection에서 사용하는 대표적인 성능 측정 방법\n\n\nGround Truth 박스와 Prediction 박스간 IoU(Intersection Over Union, Detector의 정확도를 평가하는 지표)가 50이 넘는 예측에 대해 True라고 판단합니다.\n\n\nExample of IoU\n\n\n\nmetric\n\n\n\n\n\n\n\nExample of mAP50\n\n\nOrange\n\nTP = 1, FP = 1, FN = 0\n총 2개의 Orange 박스 중 하단의 박스 1개는 객체를 잘 detection하였습니다.(TP) 상단의 박스 1개는 Blue category에 해당하는 객체를 Orange category로 예측하였기 때문에 잘못된 detection입니다. (FP)\nPrecision_Orange = 1 / (1 + 1) = 0.5\nRecall_Orange = 1 / (1 + 0) = 1\n\n\nBlue\n\nTP = 2, FP = 1, FN = 3\n총 3개의 Blue 박스 중 두 개의 박스는 객체를 잘 detection하였습니다.(TP) 우측 하단의 박스는 객체 위치를 정확히 detection하지 못했습니다. (FP)\nPrecision_Blue = 2 / (2 + 1) = 0.66\nRecall_Blue = 2 / (2 + 3) = 0.4\n\n\n\n\n\n이후 각 클래스별(Orange, Blue) AP는 PR curve를 통해서 계산됩니다.\n모든 이미지의 각 클래스별 AP 계산 후, 평균내어 최종 점수가 구해집니다.\n\n\n제출 방법\n\n\n베이스라인 코드 실행\n\n\nsubmission.csv 제출\nPredictionString = (label, score, x_min, y_min, x_max, y_max), …\n\n\n\nCOCO가 아닌 Pascal VOC 포맷입니다.\n\n\n\n\n\n\n\n\n세부 일정\n\nV100 GPU 서버 제공 (1인 1GPU) : 11/14 (월) 10:00\n팀 병합 기간 : 11/14 (월) 10:00 ~ 11/15 (화) 16:00\n\n팀명 컨벤션 : 도메인_팀번호(2자리)조 / ex) CV__03조, NLP__02조, RecSys__08조\n유의사항 : 한 번 결성한 팀은 다시 해체할 수 없기 때문에 신중하게 초대 부탁드립니다.\n\n\n대회 기간 : 11/14 (월) 10:00 ~ 12/1 (목) 19:00\n\n리더보드 제출 오픈 : 11/16 (수) 10:00\n최종 리더보드 (Private) 공개 : 12/1 (목) 19:00\n\n\nV100 GPU 서버 회수 : 12/2 (금) 16:00\n대회 솔루션 발표 : 마스터클래스 일정 참조\n\n대회 룰\n\n[대회 참여 제한] CV 도메인을 수강하고 있는 캠퍼에 한하여 리더보드 제출이 가능합니다.\n[팀 결성 기간] 팀 결성은 대회 시작 2일차 화요일 오후 4시까지 필수로 진행합니다. 팀이 완전히 결성되기 전까지는 리더보드 제출이 불가합니다.\n[일일 제출횟수] 일일 제출횟수는 팀 단위 10회로 제한합니다. (일일횟수 초기화는 자정에 진행)\n[외부 데이터셋 규정] 본 대회에서는 외부 데이터셋 사용을 금지합니다. (외부 데이터에 의존하는 것이 아니라, 주어진 데이터셋을 기반으로 모델링 성능 개선에 집중해 보시길 바랍니다)\n[사전학습 가중치 사용 규정] 아래 데이터셋으로 학습된 가중치만 허용됩니다. (pretrained weight)\n\n이미지넷 (www.image-net.org/)\n코코 데이터셋 (cocodataset.org/)\n파스칼 VOC 데이터셋 (host.robots.ox.ac.uk/pascal/VOC/index.html)\n\n\n[테스트셋 활용 허용 여부] 대회 테스트셋은 자유롭게 활용 가능하나 (EDA, pseudo-labeling 등), 정답을 매뉴얼하게 파악 후 코드로 심는 행위는 허용하지 않습니다. 즉, 눈으로 직접 판별 후 라벨링 하는 행위는 금지합니다.\n[데이터셋 저작권] 대회 데이터셋은 ‘캠프 교육용 라이선스’ 아래 사용 가능합니다. 저작권 관련 세부 내용은 부스트코스 공지사항을 반드시 참고 해주세요.\n\n\nAI Stages 대회 공통사항\n\n[Private Sharing 금지] 비공개적으로 다른 팀과 코드 혹은 데이터를 공유하는 것은 허용하지 않습니다.코드 공유는 반드시 대회 게시판을 통해 공개적으로 진행되어야 합니다.\n[최종 결과 검증 절차] 리더보드 상위권의 경우 추후 최종 코드 검수가 진행됩니다. 반드시 결과가 재현될 수 있도록 최종 코드를 정리해 주세요! 부정행위가 의심될 경우에는 결과 재현을 요구할 수 있으며, 재현이 어려울 경우 리더보드 순위표에서 제외될 수 있습니다.\n[공유 문화] 공개적으로 토론 게시판을 통해 모델링에 대한 아이디어 혹은 작성한 코드를 공유하실 것을 권장 드립니다. 공유 문화를 통해서 더욱 뛰어난 모델을 대회 참가자 분들과 같이 개발해 보시길 바랍니다.\n[대회 참가 기본 매너] 좋은 대회 문화 정착을 위해 아래 명시된 행위는 지양합니다.\n\n대회 종료를 앞두고 (3일 전) 높은 점수를 얻을 수 있는 전체 코드를 공유하는 행위\n타 참가자와 토론이 아닌 단순 솔루션을 캐내는 행위\n\n\n\n학습데이터\n우리는 수많은 쓰레기를 배출하면서 지구의 환경파괴, 야생동물의 생계 위협 등 여러 문제를 겪고 있습니다. 이러한 문제는 쓰레기를 줍는 드론, 쓰레기 배출 방지 비디오 감시, 인간의 쓰레기 분류를 돕는 AR 기술과 같은 여러 기술을 통해서 조금이나마 개선이 가능합니다.\n제공되는 이 데이터셋은 위의 기술을 뒷받침하는 쓰레기를 판별하는 모델을 학습할 수 있게 해줍니다.\n데이터셋 통계\n\n\n전체 이미지 개수 : 9754장\n10 class : General trash, Paper, Paper pack, Metal, Glass, Plastic, Styrofoam, Plastic bag, Battery, Clothing\n이미지 크기 : (1024, 1024)\n\n예제) image, target 시각화\n\n\nannotation file\n\nannotation file은 coco format 으로 이루어져 있습니다.\ncoco format은 크게 2가지 (images, annotations)의 정보를 가지고 있습니다.\n\nimages:\n\nid: 파일 안에서 image 고유 id, ex) 1\nheight: 1024\nwidth: 1024\nfile_name: ex) train_/002.jpg\n\n\nannotations: (참고 : detection 대회에서는 아래의 모든 정보를 활용 가능합니다.)\n\nid: 파일 안에 annotation 고유 id, ex) 1\nbbox: 객체가 존재하는 박스의 좌표 (x_min, y_min, w, h)\narea: 객체가 존재하는 박스의 크기\ncategory_id: 객체가 해당하는 class의 id\nimage_id: annotation이 표시된 이미지 고유 id\n\n\n\ncompetition에서 사용되는 데이터&amp;코드의 전체 구성\n\n├── detectron2\n│   ├── faster_rcnn_train.ipynb\n│   ├── faster_rcnn_inference.ipynb\n│   └── etc\n├── faster_rcnn\n│   ├── faster_rcnn_torcchvision_train.ipynb\n│   ├── faster_rcnn_torchvision_inference.ipynb\n├── mmdetection\n│   ├── faster_rcnn_train.ipynb\n│   ├── faster_rcnn_inference.ipynb\n│   └── etc\n└── dataset\n    ├── train.json\n    ├── test.json\n    ├── train\n    └── test\n\ndetectron2\n\n해당 폴더에는 detectron2 library file들이 존재\nfaster_rcnn_train.ipynb 노트북 파일로 train data 학습 및 model weight를 저장\nfaster_rcnn_inference.ipynb 노트북 파일로 test data inference 및 submission file 생성\n\n\nmmdetection\n\n해당 폴더에는 mmdetection library file들이 존재\nfaster__rcnn_train_.ipynb 노트북 파일로 train data 학습 및 model weight를 저장\nfaster_rcnn_inference.ipynb 노트북 파일로 test data inference 및 submission file 생성\n\n\nfaster_rcnn\n\nfaster__rcnn_torchvision_train.ipynb: torchvision library를 활용하여 faster rcnn 학습 및 model weight 저장_\nfaster_rcnn_torchvision_inference.ipynb: torchvision libarary를 활용하여 faster rcnn inference 및 submission file 생성\n\n\ndataset\n\ntrain: 4883장의 train image 존재\ntest: 4871장의 test image 존재\ntrain.json: train image에 대한 annotation file (coco format)\ntest.json: test image에 대한 annotation file (coco format)\n\n\n\n평가데이터\n학습 데이터와 평가 데이터는 각각 이미지 4883장, 4871장입니다.\n\npublic (대회 진행중)\n\ntest.json으로 만든 submission csv 중에 public 데이터만 평가됩니다. 이 때 public 데이터는 평가 데이터의 약 50% 입니다.\n\n\nprivate (대회 종료후)\n\ntest.json으로 만든 submission csv 파일을 통해 모든 평가 데이터가 평가됩니다. 이에 따라 최종 순위가 결정됩니다.\n\n\n\n평가 데이터는 test 폴더안의 이미지들과 test.json 으로 구성되어 있으며, 학습 데이터와는 다르게 annotation 정보가 포함되어 있지 않고 이미지 정보만 가지고 있습니다.\n베이스라인 코드 및 제출방법\nFile Structure\n├── baseline\n│   ├── detectron2\n│   ├── faster_rcnn\n│   ├── mmdetection\n│   └── requirements.txt\n├── dataset\n│   ├── test\n│   ├── test.json\n│   ├── train\n│   └── train.json\n├── mission\n│   └── ...\n└── sample_submission\n    ├── faster_rcnn_mmdetection_submission.csv\n    ├── faster_rcnn_torchvision_submission.csv\n    ├── submission_ensemble.csv\n    └── train_sample.csv\n위와 같은 file structure를 가지고 있습니다.\nConda 가상환경\ndetection: mmdetection, detectron2를 실행시키기 위한 가상환경\nSubmission\n\n\n제출 방법 1 - faster rcnn torchvision (pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)torchvision에서 제공되는 faster rcnn object detection model을 활용하여 학습합니다.\n\nfaster__rcnn_torchvision__train.ipynb를 실행합니다.\nfaster__rcnn_torchvision__inference.ipynb를 실행합니다.\n생성된 faster_rcnn_torchvision_submission.csv을 제출합니다.\n\n\n\n제출 방법 2 - mmdetection(github.com/open-mmlab/mmdetection)\n└── mmdetection\n    ├── mmdet\n    ├── config\n    ├── tools\n    ├── work_dirs\n    ├── faster_rcnn_train.ipynb\n    ├── faster_rcnn_inference.ipynb\n    └── etc\n\n\nmmdet : 실제 모델들이 작성되어 있는 곳. faster rcnn 등 강의에서 다루는 여러 모델들이 구현되어 있음\n\n\nconfig : config를 통해 mmdet 모델들을 조합\n\n\ntools: train.py, test.py 등 라이브러리를 이용할 때 참고할 수 있는 베이스라인 코드가 구현되어 있음+ train, inference 코드, 라이브러리에 대한 상세 설명은 3강 강의 영상 참고 바랍니다.\n\n\n\nmmdetection folder의 faster__rcnn_train.ipynb를 실행합니다._\nwork_dirs 폴더에 생성된 faster_rcnn weight를 이용해 faster_rcnn_inference.ipynb 실행합니다.\nwork__dirs 폴더에 생성된_ submission csv file을 제출합니다.\n\n\n\n제출 방법 3 - Detectron2(github.com/facebookresearch/detectron2)\n└── detectron2\n    ├── detectron2\n    ├── config\n    ├── tools\n    ├── faster_rcnn_train.ipynb\n    ├── faster_rcnn_inference.ipynb\n    └── etc\n\n\ndetectron2 : 실제 모델들이 작성되어 있는 곳. faster rcnn 등 강의에서 다루는 여러 모델들이 구현되어 있음\n\n\nconfig : config를 통해 detectron2 모델들을 조합\n\n\ntools: train_net.py, visualize_.py 등 라이브러리를 이용할 때 참고할 수 있는 베이스라인 코드가 구현되어 있음+ train, inference 코드, 라이브러리에 대한 상세 설명은 3강 강의 영상 참고 바랍니다.\n\n\n\ndetectron2 폴더 내의 faster__rcnn_train.ipynb 파일을 실행합니다._\noutput 폴더에 생성된 faster__rcnn weight를 이용해 faster_rcnn_inference.ipynb 파일을 실행합니다._\noutput 폴더에 생성된 submission csv file을 제출합니다.\n\n\n\nHow to install?\n\n데이터셋 다운로드\n\n\ndata 구성\n\n\n\ntrain, test folder 안에 이미지 존재\n\n\ntrain.json은 이미지의 annotation 정보를 포함하며 train에 사용\n\n\ntest.json은 이미지의 annotation 정보를 포함하지 않으며, submission file을 만드는데 사용\n\n\n\n라이브러리 설치 (기존 할당 받은 서버에는 자동으로 설치)\n\n\nfaster rcnn naive version 설치\npip install -r requirements.txt\n\n\nmmdetection library 설치\nconda install pytorch=1.7.1 cudatoolkit=11.0 torchvision -c pytorch\n \npip install openmim\n \nmim install mmdet\n\n\ndetectron2 설치\npython -m pip install -e detectron2\n\n\n\n\n\nEDA\nRaw Image Size Analysis\n목적\nTrain/Test에 사용하는 모든 Raw Image의 size를 확인\nJson에도 size가 적혀 있지만, 확실히 확인하기 위해 직접 load 및 확인을 진행\n결과 - Train Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwidthheightlicenseidwidth_realheight_realcount4883.04883.04883.04883.0000004883.04883.0mean1024.01024.00.02441.0000001024.01024.0std0.00.00.01409.7450120.00.0min1024.01024.00.00.0000001024.01024.025%1024.01024.00.01220.5000001024.01024.050%1024.01024.00.02441.0000001024.01024.075%1024.01024.00.03661.5000001024.01024.0max1024.01024.00.04882.0000001024.01024.0\n결과 - Test Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwidthheightlicenseidwidth_realheight_realcount4871.04871.04871.04871.0000004871.04871.0mean1024.01024.00.02435.0000001024.01024.0std0.00.00.01406.2809110.00.0min1024.01024.00.00.0000001024.01024.025%1024.01024.00.01217.5000001024.01024.050%1024.01024.00.02435.0000001024.01024.075%1024.01024.00.03652.5000001024.01024.0max1024.01024.00.04870.0000001024.01024.0\n결론\nTrain/Test 이미지는 모두 동일한 크기로 되어있다.\nCategory Histogram Analysis\n목적\nTrainSet Image 내에서 Category 분포를 확인, Detection해야 하는 Class가 얼마나 고르게 분포되어 있는지 확인한다.\n결과\n\n결론\n심한 Class Imbalance를 가진 Dataset이다.\nRGB Component Analysis by Category\n목적\nTrainSet Image 내에서 각 ROI별 RGB 분포를 확인, Category별로 라벨링 하어 경향성이 존재하는지 확인한다.\n결과 - Average, Std by Channel for red\n\n\n결과 - Average, Std by Channel for green\n\n\n결과 - Average, Std by Channel for blue\n\n\n결과 - RGB Average\n\n결과 - RGB Mean\n\n결과 - RGB Std.\n\n결론\nROI로 Crop해서 분석했지만, 여전히 배경성분이 포함되어 있어서인지 Class별 차이가 크게 없었다.\n하지만 그럼에도, 카테고리별 분포의 차이는 보였다.\nAnnotation per Image Analysis\n목적\n이미지 하나에 얼마나 많은 bbox가 존재하는지, 그리고 얼마나 많은 종류의 bbox가 이미지 하나에 존재하는지 확인해본다.\n결과 - Category Count / Count\n\n\n결론\n예측해야 하는 Class는 총 10개(배경 포함)인데, 대부분 1~3개의 Category, 10개 이하의 bbox를 가지고 있었다.\n하지만, 그와 반대로 매우 많은 bbox를 가지고 있는 경우도 존재했다.\n또한 가장 우측 상단에 있는 case(bbox70 &amp; category 7)의 경우 과연 제대로 된 라벨링을 한 것인지 의문이 든다.\n데이터를 다시 정제해야 하는 것인지 고민된다.\nSize Analysis by Category\n목적\nTrain Dataset의 모든 Annotation을 조사해서, 각 Annotation의 크기를 확인해본다.\n결과 - Width &amp; Height\n\n결과 - Category ID &amp; W/H ratio\n\n결론\n너무 다양한 분포의 bbox가 존재하고 있다.\n또한 0에 가까운 bbox나 50pixel도 되지 않는 bbox, 이미지 크기를 다 채우는 bbox도 존재했다.\n\nModel 탐색\n\n\nfaster_rcnn | submission mAp : 0.4188\n대회측에서 제공한 base line code\n\n\nSSD300 AdamW | submission mAp : 0.2077\nAdamW를 사용해서 훈련한 결과\n생각보다 너무 성능이 낮아서 놀랐다\nLr Scheduler의 문제인가 의심되었다\n\n\nSSD300 SGD | submission mAp : 0.3946\nSGD를 사용하여 다시 훈련한 결과\n드리어 baseline 결과에 근접했다\n이미지의 원본 입력 Size는 1024x1024이며, SSD300의 입력사이즈는 300x300이므로, 약 1/3의 Down Sizing이 발생한다.\nSSD의 Extra Feature Layers를 통해 다양한 크기의 Classify를 수행할 수 있어도, 3x3 크기 이하의 object는 크게 의미가 없다고 생각된다.\n따라서 원본 기준, 그 3배인 10x10 미만의 bbox는 잡지 못할 것이며, 작은 영역의 bbox는 당연히 검출이 어려울 것이다.\n그렇다면, input size를 키우는 것이 유리하다.\n\n\nSSD512 SGD | submission mAp : 0.4521\ninput size를 512로 증가시켜 다시 훈련한 결과\n많은 map 상승이 있었다\n다른 모델들의 성능은 어떨까?\n\n\nretinanet_r50_fpn_1x AdamW | submission mAp : 0.3375\nSGD를 사용하면 훈련이 제대로 되지 않아 AdamW를 사용했다\n오랜시간 훈련했음에도, 좋은 결과를 얻지 못했다\n네트워크 복잡도가 부족한 것일까?\n\n\nretinanet_r101_fpn_1x AdamW | submission mAp : 0.3736\n마찬가지로 SGD를 사용했을 때, 훈련이 잘 되지 않아 AdamW로 훈련했다는\n기존 r50 backbone 대비, 0.05의 map의 상승이 있었다.\n아직 다양한 Backbone을 사용해볼 수 있고, WandB로 본 Loss의 경향으로는 더 성능개선이 가능할 것이라고 생각된다\n하지만, SSD보다 너무 많은 시간이 필요해서 비효율적이라도 생각됬다\n==빠른 실험을 위해 가장 좋은 성능을 얻은 SSD로 모델을 고정하기로 결정했다==\n\n\nSSD512 SGD | fail\nTrain / validation 을 나누고 batch size를 8→32로 증가시킨 뒤, AMP Dynamic을 적용하여 훈련을 시도했다.\n그런데 훈련이 진행되다가 loss가 발산하더니, 모든 loss들이 NaN이 뜨는 일이 발생했다\nMMDetection Github issue 에서 비슷한 현상을 겪은 case를 발견했다\ngithub.com/open-mmlab/mmdetection/issues/3013\nGT Box의 area가 0일 경우, 발생할 수 있다고 한다\n필요 없는 box의 refine이 필요하다고 생각했다\n\n\n\nData Refine\nDrop small annotations\n\n\n\nmodel 탐색 3번에서 고민한 내용을 토대로, 가로 혹은 세로가 20 pixel 미만의 feature와 100 pixel^2 미만의 넓이는 가지는 feature를 제외하여 새롭게 trainset을 구성했다\n(ssd512의 경우 1/2로 가로, 세로 넓이가 조정되기 때문에 20으로 선정했다)\n\n\n모델을 돌려본 결과, 정상적으로 훈련되는 것을 확인했다….만 훈련을 진행하면서 다시 발생했다\n다시 Web에서 정보를 수집한 결과,\nloss 가 너무 높은 경우 발생할 수 있다고 한다\n\n해결 방법 : Loss를 감소시키고, Warm up 의 Lr을 조정하는 것을 권장\n\n\n\nSGD의 Lr을 0.001로 설정했던 것이 원인이었다는 생각이 들었다\n경향을 보기 위해 LR를 확 감소시켰다\n\nLR : 0.001 → 0.0001\n\n\n\nNan의 발생은 해결했지만, Warm Up이 끝난 후 더 이상 loss가 감소하지 않았다\nLR을 추가적으로 더 감소시켰다\n\nLR : 0.0001 → 0.00001\n\n\n\nLR을 감소시키는 것은, 결과적으로 현명하지 못한 선택이었다\n생각해보면, loss가 낮아지지 않는다는 것은 LR이 너무 작다는 것인데, 더 줄인다는 것은 잘못된 판단이었다\nLR을 원래대로 변경하고, Wram Up 구간을 증가시켰다\n\n\nLR : 0.00001 → 0.001\n\n\nWarm up : 500 → 5000\n\n\n\n결과, 더 빠르게 Loss가 떨어지는 것을 볼 수 있었다\n\n\n\n\n\n그러나, 학습 속도가 생각보다 너무 낮았다\n시간이 얼마 남지 않았기 때문에, 기존 모델(SSD512)을 가져와 학습하는 방향으로 전략을 변경했다\n이미 훈련된 모델을 불러오는 것이기 때문에, Lr을 1/100으로 낮추고 Warm Up을 제거했다\n결과, 크게 의미있는 성능 향상이 없었다\n\nLR : 1e-3 → 1e-5\nRemove Warm up\n\n\n\n\nLR이 너무 낮은 것일까? LR을 높여서 진행해 보는 것이 필요해 보인다\n또한, 이미 훈련이 많이 진행된 model이기 때문에 epoch을 10까지만 해서 빠르게 실험하는 것이 좋다고 판단되었다\n\n\nLR : 1e-5 → 1e-3\n\n\n\n\n\nLoss가 발산하는 것이 보인다\nLR이 너무 큰거같다\n\n\nLR : 1e-3 → 1e-4\n\n\n\n마찬가지로, 성능개선이 이뤄진다는 채감이 없다\n\n\n\n\n모델 성능의 한계점에 도달하고, 최소점 근처를 배회하고 있는 것일까?\nLR을 확 낮춰보았다\n\nLR : 1e-4 → 1e-6\n\n\n\n첫 Epoch에서 1e-5로 훈련했을 때와 똑같은 결과를 얻었다 훈련이 거의 되고있지 않다고 추측된다\n\n\n\n다시 처음부터 분석해보자\n성능이 올라가기 위해서는 Loss가 줄어들어야 한다\n그렇다면 왜 Loss가 줄어들지 않을까?\n기존 실험들의 Total Loss를 살펴보면, 2~3 근처에서 계속 흔들리고 있다\nLoss는 BBox 예측에 대한 Loss, Classification에 대한 Loss의 합으로 이루어져 있다\n그 중 class에 관련한 loss의 비중이 컸는데, labeling중 기준이 모호한 것이 많았기에 이것이 원인이 아닌가 추측되었다\n\n\n\n\n\n라벨링이 모호한 것이 원인이라고 생각했고 팀원의 도움으로 다시 라벨링한 데이터를 가지고 훈련을 진행했다\n\nrelabeling\n\nv2 : relabeling + small bbox drop | submission mAp : 0.4538\n\ndrop when bbox width or height &lt; 20\n\n\nv3 : relabeling + small bbox drop + big bbox drop | submission mAp : 0.4509\n\ndrop when bbox width or height &lt; 20\ndrop when bbox is generaltrash or paper or plastic bag and area &gt; 900000\n\n\n\n\n\n\n\n\n결과, 미세한 bbox를 제거했을 때는 약간의 성능 향상이 있었지만, 큰 bbox를 제거했을 때는 성능이 하락했다\n\n\nBest Score : 0.4538(SSD512)"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---개인회고":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---개인회고","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - 개인회고.md","title":"재활용 품목 분류를 위한 Object Detection - 개인회고","links":[],"tags":[],"content":"T4063 / 김형석\n\n이번 프로젝트에서 나의 목표는 무엇이었는가?\n\nWandB를 사용해서 진행한 실험들을 빠짐 없이 기록하기\n데이터 분석을 더 면밀히 하고, 라벨링이 무조건 올바르다고 맹신하지 않기\n기본 모델 하나 선정해서 여러 실험 진행, 성능 향상 해보기\n데이터 분석을 더 면밀히 해보기\n리더보드 5위 안에 들어보기\n\n나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\n\n\nWandB를 사용해서 진행한 실험들을 빠짐 없이 기록하기\n시작과 동시에 WandB 연동을 우선시했고, 이후 실험은 WandB 연동 하에 진행했다\n\n\n데이터 분석을 더 면밀히 하고, 라벨링이 무조건 올바르다고 맹신하지 않기\nBounding Box들에 대한 RGB 분포, Size 분포 등 EDA를 통해 데이터를 파악했고 Labeling Bounding Box를 이미지에 그려 1000개 정도 샘플링하여 확인했다\n→ Category Histogram Analysis, RGB Component Analysis by Category, Annotation per Image Analysis, Size Analysis by Category\n\n\n기본 모델 하나 선정해서 여러 실험 진행, 성능 향상 해보기\n속도면에서 실무에 사용하기 좋은 Single Stage 모델을 위주로 탐색했고, 추후 Ensemble을 염두하여 다른 팀원들과 겹치지 않는 모델인 SSD를 선정했다\n\n\n리더보드 5위 안에 들어보기\n최선을 다했지만, 실패했다\n\n\n나는 어떤 방식으로 모델을 개선했는가?\n\nWarm Up : 초기에 모델의 Loss가 NaN이 되지 않도록 Learning Rate을 천천히 증가시켰다\nLearning Rate Scheduling : Loss의 경향을 보고, 일정 Epoch 이후에는 1/10으로 Learning Rate를 감소시켰다\n더 빠른 훈련을 위해 Mixed Precision Training을 적용했다\nEDA를 통해 훈련에 도움이 되지 않다고 판단되는 Bounding Box를 제거하였다\n\n내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?\n\n\nInput Size 512 x 512의 SSD를 사용하여 제출기준 mAP50 0.4538을 기록했다\n→ 한 모델에 매몰되지 말고, 더욱 다양한 실험이 필요하다고 느꼈다\n→ 또한 실험을 통해 Hyperparameter를 튜닝하는 것은 필요하지만, 시간을 효율적으로 사용하기 위해서는 0에서부터 하려고 하지 말고 모델의 논문을 참조하여 레퍼런스 얻어야 한다고 느꼈다\n→ 찾다보면 다시 부트캠프의 강의를 돌아보게 되는 일이 많았고, 부트캠프에서 가르쳐주는 기본기에 더 충실해져야겠다고 생각했다\n\n\n팀 전체가 다른 대회에 도전했고, 때문에 다른 조보다 1주일 늦게 시작했다\n→ 급한 마음에 무작정 시도만 했다고 느껴졌고, 다음에는 DOE를 해야겠다고 느꼈다\n\n\n여러 EDA를 했고, 팀원들과 공유했다\n→ 여러 EDA를 해보려고 노력했지만, 그 결과에서 Insight을 도출해 내는 작업에는 소흘했다\n→ Confusion Matrix를 그려놓고 막상 적극적으로 사용하지 않았다\n\n\n전과 비교해서, 내가 새롭게 시도한 변화는 무엇이고, 어떤 효과가 있었는가?\n\n\n이전 Mask Classification 대회에서 아쉬웠던 부분을 이번 프로젝트에서의 내 목표로 삼았고, 리더보드 상위권을 제외한 모든 목표를 이루었다\n\n\n그러나 이 목표 중, ‘라벨링을 맹신하지 않기’는 역으로 성능 향상을 위한 사고 실험에 방해되었다\n\n\n마지막에 Ensemble 자동화 코드를 만들어 팀의 성적을 최대한 끌어낼 수 있었다\n→ 제출 기준 mAp50 0.5910\n\n\n마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\n\n\n문제 정의를 제대로 하지 못했다\n이 대회는 mAP50을 최대한 높이는 사람이 승리하도록 설계했다 그러나 mAP가 어떤것인지, 어떻게 계산하는 것인지 제대로 이해하지 못하고 시작하여 결과적으로 실험의 방향을 잡지 못했다\n\n\n한 모델에 너무 매몰되어 결과를 내지 못했음\n다른 조들은 단일 모델의 Inference 결과만으로 0.5 이상의 결과를 얻었기 때문에 0.4538은 그와 비교해서 비해 매우 낮은 Score였다\n대회를 마치고 회고하면서 너무 한 모델에 매몰되어 있었다고 느꼈다\nSSD는 paperswithcode 기준으로 성능이 별로 좋은 모델이 아니다(mAP50 48.5)\n그런데 이건 생각하지 못하고 모델이 낼 수 있는 성능한계에 가까워졌다는 생각을 하지 못하고 SSD의 성능 향상에 계속 매몰되어 있었다\n\n\n대회라는 것을 망각하고 ‘올바른 예측’을 고민했다\n우리는 라벨링을 다시 하는 단계에서 ‘어떤 기준으로 라벨링을 다시 해야 옳을까?’를 고민했다\n그러나 이것은 실무가 아닌 대회이며, 우리가 해야 할 일은 ‘옳은 예측’을 하는 것이 아니라 주최측이 준비해 놓은 정답에 최대한 근접하는 예측을 할 수 있는 모델을 만드는 것이었다\n\n\n더 효율적으로 시간을 사용하지 못했다\n1주일 늦은 출발에 MMDetection 사용법을 배우는 것에 급급했고, 급한 마음에 비효율적으로 시간을 소비했다\n\n\n한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n\n‘문제 정의’를 착실하게 하겠다\n모델 탐색 및 선정은 자료를 근거로 하겠다(ex: paperswithcode)\nHyperparameter Recipe은 논문을 근거로 하겠다\n단순한 시도-결과확인이 아닌, DOE를 하겠다\n목표를 갖되, 상황에 맞게 수정하겠다\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---멘토-피드백":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Object-Detection---멘토-피드백","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Object Detection - 멘토 피드백.md","title":"재활용 품목 분류를 위한 Object Detection - 멘토 피드백","links":[],"tags":[],"content":"전체 피드백\n안녕하세요 멘토 이호민입니다. 대회 기간동안 고생 많으셨습니다. EDA에 대해서 엄청 고민을 많이 하신 흔적이 보이시네요. 단 EDA의 목적과 해석이 없어서 읽는 입장에서 이해하기가 다소 어려웠습니다. EDA를 하기 이전에 어떤 것을 확인하기 위해서 진행했는지와 확인되었다면 그 정보를 어떻게 해석했는지 그리고 그 근거로 어떤 실험을 만들려고 했는지 고민이 더 담기면 좋을 것 같습니다. 여러개의 EDA plot을 합쳐서 종합적인 의견도 포함되어도 좋을 것 같습니다. 그를 통해서 우리가 해결해야하는 문제가 어떤 것인지 정리하는 것도 좋을 것 같습니다. 예를 들어 “Data imbalance 문제로 개수가 적은 특정 class의 bounding box 사이즈 또한 작기 때문에 이를 해결하는 것이 가장 어렵고 효과가 클 것으로 예상된다. 그리하여 우리는 이를 해결하기 위해 여러 실험과 기능들을 만들고자 한다.“ 과 같이 실험 혹은 EDA를 통해서 발견한 문제점을 근거로 이러한 시도를 하였다는 자료들에 대한 정리가 있으면 좋겠습니다. 모델 또한 다양한 것들을 많이 도입하셔서 고생 많으셨던것 같습니다. 다만 모델 선택 과정에 이유가 조금 더 자세하면 좋을 것 같습니다. 물론 정확도 성능도 중요하지만 그것 뿐만 아니라 “모델의 특정 메서드가 해결하고자 하는 task에 잘작동하여서 잘되었을 것 같다.”혹은 “모델의 특정 메서드가 task의 문제를 더욱 안좋게 만든다.” 이런 해석이 있으면 더 좋을 것 같습니다. 해석을 통해서 좋은 것은 더 좋게 만들고 나쁜 것은 반대 방향으로 진행한다면 더 좋을 것 같습니다. 리포트에 작성하는 내용들이 조금 더 실험 기반으로 만들어진 것이며 객관적인 자료인 것으로 소개하면 보다 더 설명력이 있는 리포트가 될 것 같습니다.\n수행하셨던 것들 중에 성공한 것들 중심적으로 작성해주신 것 같은데 실패한 실험들도 학습에 많은 도움이 됩니다. 해당 실험들이 왜 실패했는지 대회 기간 이후에 다시 복기해보시면 떠오르지 못했던 것들을 떠오를 수도 있습니다.\n\n개인 피드백\n안녕하세요 형석님 기간동안 두 대회를 진행하셔서 대단히 고생을 많이 하셨을 것 같습니다.\n대회를 진행하시기 이전에 목표로 삼았던 것들 중에 관심이 갔던 목표가 라벨링에 대한 맹신 관련한 내용을 이야기를 해주셨네요. 이는 실무에서도 엄청 중요한 부분중에 하나입니다. 라벨링은 우리가 만들고자 하는 서비스의 방향과 밀접한 관계가 있기 때문에 라벨링을 어떻게 해야할지 많은 고민이 필요한 항목중에 하나 입니다. 이로인해 라벨링 업무를 어떻게 가이드 해야할지 혹은 미스 라벨링 감지를 어떻게 해야할지도 현업에서도 많이 고민하는 분야중에 하나입니다. 그리하여 노이즈 데이터 감지 등 별도의 연구도 발생하고 있는 실정입니다. 라벨링도 비용이 많이 발생하는 업무 중 하나임으로 MLOps에 대해서 고민을 하고 있다고 하시면 이러한 작업들을 어떻게 효율적으로 할 수 있는지 고민을 하시면 다음에 좋은 기회가 생기지 않을까 생각합니다. 대회를 진행하시면서 좋지 않은 결과에 많은 상심하시지 마시고 기간동안 시도하였던 실험들을 통해서 얻은 인사이트들 혹은 시간에 쫓겨 생각해보지 못한 생각들도 다시 복기해보시면 좋을 것 같습니다. 남은 기간도 잘 마무리 하셔서 좋은 결과 있으시면 좋겠습니다."},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/Wrap-up-report-Feedback":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/Wrap-up-report-Feedback","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report/Wrap-up report Feedback.md","title":"Wrap-up report Feedback","links":[],"tags":[],"content":"공통\n대회 기간동안 최종 프로젝트 주제 결정을 같이 진행하셔서 고생이 많으셨을 것 같습니다. 데이터 시각화와 실험관리 model performance table은 정말 잘만드셨네요. 하지만 여기서 그치는 것이 아니라 왜 그렇게 나왔는지 실험 후에 해석하는 것도 중요하다고 생각합니다. 그 과정이 있어야 다음으로 진행해야하는 것이 정해지고 무엇이 해결되었고 아직 해결되지 못한 것을 찾을 수 있을 것 같습니다. IoU score에서는 표현되지 않는 중요한 것들이 모델에 많이 담길 수 있습니다. validation dataset을 선정하는 것 처럼 프로젝트간에 해결이 되지 않았던 데이터들을 선별해서 inference를 실행하여 모델들이 어떻게 반응하는지 비교를 통해서 모델에 대한 해석을 적용하고자 하는 데이터셋의 관점에서 진행하실 수 있을 것 같습니다. 이는 모델 뿐만 아니라 loss function, lr scheduler, augmentation 등 실험 단위로도 해석이 가능할 것으로 보입니다. 물론 대회에서는 궁극적으로 IoU를 올리는 것이 목표이겠지만 IoU를 올리는 과정에서도 명확한 문제정의가 없으면 한계가 있을 수 있을 것 같으며, 실무에서는 보다 더 specific한 케이스에 대해서 작동하는지 확인하는 경우가 많습니다. (그리고 그런 해석이 없다면 포트폴리오나 면접에 가서 해당 프로젝트에 대해서 이야기를 나눈다면 “그냥 큰 모델이 좋았고 Sota 모델이 좋았다.” 정도의 이야기만 나눌 수 있을 것 같아 보다 준비가 되면 좋을 것 같습니다.) 가설을 선택해서 실험을 진행하는 것이 물론 1번으로 좋겠지만 가설 없이 진행했던 실험에 대해서도 해석을 통해서 또한 좋은 인사이트를 얻을 수 있습니다.\n개인 회고 T4063 김형석\n안녕하세요 형석님 고생많으셨습니다. 실험 관리를 위해 Notion API 부터 Wandb sweep 까지 활용하신 것을 보고 실험에 진심이신 것을 느낄 수 있었습니다. 앞으로도 이러한 부분들을 보다 자세히 진행하셔서 앞으로 많은 인사이트를 얻으시면 좋을 것 같습니다.\n느낀점에서 Sota 모델이 중요했다는 것을 이야기해주셨는데 이전 기수를 보았을 때도 그렇고 이번에도 마찬가지로 Sota 모델 활용이 이번 대회의 핵심이였던 것 같습니다. 하지만 여기에서 끝나는 것이 아니라 큰 맥락으로 왜 Sota 모델이 더 뛰어났는지 해석도 해보면 좋을 것 같습니다. 그외에 적용했던 모델 중에 좋은 성능을 보였던 모델에 대해서 해석해보는 것도 좋을 것 같습니다. 예를 들어 큰 맥락의 해석은 “Image classification 혹은 object detection에 비해 평가해야하는 요소들이 많아진다.“ Image classification은 image 한장에서 한번의 decision을 진행한다. object detection은 image 한장에서 object의 개수, segmentation에서는 image 한장에서 pixel 수 이 처럼 task 별로 benchmark dataset의 metric 점수가 의미가 다르며 작은 차이도 큰 차이를 만들어 낼 수 있다고 해석 할 수 있을 것 같습니다. 다음으로 작은 맥락의 해석은 적용 모델의 논문에서 해결하고자 했던점들이 우리의 task에 어떻게 어울리는지 해석이 필요합니다. 물론 다루는 데이터 셋이 다르고 목적이 다르기 때문에 논문에서 표현하지 않을 수 있어 보다 많은 시간을 소비해야하지만 이러한 해석이 있어야 다음 스텝으로 발전할 수 있을 것 같습니다.\n대회 기간동안 고생많으셨고 멘토링 기간동안 재미있는 질문들 많이해주셔서 감사합니다. 남은 기간 잘 마치셔서 이루고자 하는 바 다 이루셨으면 좋겠습니다.\n개인 회고 T4148 이동훈\n안녕하세요 동훈님 대회 기간동안 코로나 때문에 많이 힘드셨을 것 같은데 고생 많으셨습니다. 개인회고 중 .py 와 친해지는 것과 config 관리하는 방법을 고민하고 있으신 것 같네요. 다른 사람들이 만들어 둔 github repository를 참고를 시작으로 부분 부분을 수정하여 변형하여 사용하는 것도 방법입니다. 이와 비슷하게 저는\ngithub.com/ashleve/lightning-hydra-template\n여기 탬플릿 처럼 다른 사람들이 만들어둔 탬플릿을 활용하여 실험 관리 혹은 파이프라인 관리를 합니다. 모든 탬플릿이 자신에게 맞을 수 없으니 다양한 대회나 프로젝트를 진행하다 보면 공통적으로 진행하는 부분들을 파이프라인으로 내제화 하여 자신의 코드들을 지속적으로 추가하는 것을 추천드려요.\n멘토링 기간동안 고생많으셨습니다. 남은 기간도 잘 마무리하셔서 좋은결과 만드셨으면 좋겠습니다.\n개인 회고 T4190 전지용\n안녕하세요 지용님 대회기간동안 고생많으셨습니다. 개인회고를 보니 다양한 것들을 많이 시도하고 변화를 경험하고 있으신 것 같습니다. mmsegmentation 활용방법, 실험 관리 등 다양한 것을 학습하고 있으신 것 같아요. mmsegmentation은 많은 부분들이 이미 패키징이 되어 있고 이전에 배우셨던 PyTorch 탬플릿들과는 많은 부분들이 달라 공부하시고 적용하시는데 많은 어려움이 있으셨을 것 같습니다. 이 경우 나만의 탬플릿을 만들어 mmsegmentation 모델, dataloader 등 다양한 기능들을 가져와서 활용할 수 있게 하는 방법을 찾아 만들거나 mmsegmentation을 이해하고 거기에 자신이 자주 사용했던 기능들을 탑제하는 방법으로 활용하는 것이 가능할 것 같습니다. 자신의 탬플릿은 이전에 경험했던 것들을 축적하여 다음에 활용할 수 있게 만드는 것도 중요합니다. 이 부분에서는 효과가 좋았던 것들만 축적할 수 있겠지만 효과가 좋지 않았던 것들도 다음번에는 도움이 될수도 있습니다. 남은 기간 잘 마무리하셔서 좋은 결과 있기를 바랍니다.\n개인 회고 T4199 정원국\n안녕하세요 원국님 바쁜 와중에 몸도 안좋으셔서 수술까지 받으셨네요. 정말 고생이 많으셨습니다. 그러던 와중에도 많은 것들을 하셨네요. smp에 없는 swinT를 만드셨는데 너무 대단하신 것 같네요 나중에 시간에 남으시면 smp에 pr 신청해보는 것도 좋을 것 같아요. open source의 contributor가 생각보다 스펙이 될수도 있으니 도전해봐도 좋을 것 같아요. 그리고 PointRend, Wandb sweep 활용등 많은 것들을 진행하셨네요. 후처리에 대한 내용이 많이 없어서 어떻게 진행하셨는지 파악은 어려운데 PointRend의 특성과 다른 model들의 특성들을 잘 고려해서 Post-process와 Ensemble 도 생각해보면 좋을 것 같네요. 항상 프로젝트를 진행하실 때마다 좋은 경험들을 많이하셔서 빠르게 성장하실 것 같네요. 앞으로도 좋은 인사이트, 경험들을 많이 하셔서 좋은 결과가 남으셨으면 좋겠습니다. 수고많으셨습니다.\n개인 회고 T4226 한상준\n안녕하세요 상준님 대회기간동안 고생많으셨습니다. 회고를 보니 지난 문제에 대해서 다시 해석하시는 것이 매우 좋은 것 같습니다. 그 중 잘못된 상식이 독이된다는 이야기도 많이 공감이 되는 것 같습니다. 저도 많이 경험한 경우인데 저는 정의한 문제에 대한 해석을 위해서 실험을 근거로 증명과정이 없었고 거기에 너무 over-fit 하여 local minima를 벗어나지 못했던 것 같습니다. 데이터 사이언티스트들은 논리적인 실험을 근거로 자기 주장을 검증해내는 것이 주된 업이라 충분히 디테일해야하는 직군인 것 같습니다. 가끔은 문제들이 해결이 되지 않는다면 자신의 생각에 대해서도 다시 생각해보거나 증명을 하는 과정을 거치면 어떨까합니다. 이후 진행하는 Level 3 프로젝트는 조금 다른 방향으로 진행되어 정신이 없으시겠지만 개인적으로는 상준님에게 적합한 프로젝트라고 생각되어 많은 기대를 하고 있습니다. 마지막 프로젝트 잘 정리하셔서 좋은 결과가 따라오기를 바랍니다."},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4063-김형석---Segmentation":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4063-김형석---Segmentation","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report/개인 회고 T4063 김형석 - Segmentation.md","title":"개인 회고 T4063 김형석 - Segmentation","links":[],"tags":[],"content":"이번 프로젝트에서 나의 목표는 무엇이었는가?\n\n‘문제 정의’를 착실하게 하겠다\n모델 탐색 및 선정은 자료를 근거로 하겠다(ex: paperswithcode)\nHyperparameter Recipe은 논문을 근거로 하겠다\n단순한 시도-결과확인이 아닌, DOE를 하겠다\n목표를 갖되, 상황에 맞게 수정하겠다\n\n나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\n\n‘문제 정의’를 착실하게 하겠다\n\n대회 시작과 동시에 평가 metric인 miou의 의미를 파악\nmiou는 Class별 iou의 평균이므로, wandb에서 miou뿐만 아니라 각 Class의 iou도 모두 기록하도록 구현\n매 실험에서 단순히 miou만 보지 않고, class 별 iou를 관찰하며 진행\n\n\n모델 탐색 및 선정은 자료를 근거로 하겠다\n\n각 모델의 논문을 살펴보고 개선된 모델(ex: Deeplabv3→Deeplabv3+)인 경우, 기존 모델(Deeplabv3)는 실험에서 제외\n기본 모델(UNet)에서 시작하여 실험 전개, 결과가 가장 좋은 augmentation을 기본 augmentation으로 지정\n이후 다른 모델들을 탐색하면서 결과 Metric을 기반으로 Best 모델을 선정\n\n\nHyperparameter Recipe은 논문을 근거로 하겠다\n\n실험을 통해 Encoder로 사용하는 model에 따라 Segmentation Model에 따라 너무 많이 달라지기 때문에 의미가 없다고 판단, 시도하지 않았음\n\n\n단순한 시도-결과확인이 아닌, DOE를 하겠다\n\nNotion API를 사용해 훈련 후 자동으로 Notion Page의 DB에 결과와 Parameter가 기록되도록 구현하였음\n실험기록을 기반으로 지속적으로 성능을 향상시켰음\n\n\n목표를 갖되, 상황에 맞게 수정하겠다\n\n리더보드 상위권을 노려보고 싶었으나, 생각보다 Score가 잘 나오지 않았고 단순히 Score 향상보다 많은 실험을 해보며 이 Task에 대한 감각을 향상시키고 다양한 기능들을 구현해보며 실력을 향상시키는 것을 목표로 하였음\n\n\n\n나는 어떤 방식으로 모델을 개선했는가?\n\nUnet + Efficientnetb3로 실험을 통해 성능이 향상되는 augmentaion 조건을 선정\n이후 Unet을 다른 Segmentaion Model로 바꿔가며 가장 높은 성능을 보여주는 모델을 탐색, FPN이 가장 좋은 결과 얻음\nTimm에서 SwinTv2를 가져와 Feature map을 얻을 수 있도록 Module의 Sequence를 수정하여 Custom Encoder를 구현\nHard Voting Ensemble을 구현\n\n내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?\n\n결과\n\nSMP SwinTv2 Custom Encoder : Validation 기준 miou 0.7083\nHard Voting Ensemble을 구현 : Submission기준 miou 0.7247\n\n\n느낀점\n\n이번 Task는 Augmentation이나 최적의 모델을 찾는 것 보다 최신의 SOTA Model을 사용할수록 압도적으로 성능향상이 있었다\nTask에 따라 실험의 방향성이 다르고, 이를 빠르게 확인해야 효율적인 시간 투자가 가능하다\n\n\n\n전과 비교해서, 내가 새롭게 시도한 변화는 무엇이고, 어떤 효과가 있었는가?\n\n진행한 실험을 모두 기록했고, 실험 기록에 근거해 다음 실험을 진행하였음\n\n점진적인 성능향상을 얻음\n\n\n라이브러리에서 제공하는 모델에 안주하지 않고 새롭게 모델을 가져와 라이브러리에 적용\n\n기존의 실험들보다 더 큰 성능향상을 얻음\n\n\n\n마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\n\n한계\n\n좋은 Model을 먼저 선정하고 실험을 통해 최적의 조건을 찾아갔어야 했는데, 최적의 조건을 찾고 좋은 모델을 선정하려고 하였음\n\n\n아쉬운 점\n\nTask마다 좋은 방법은 다르겠지만, 이번 Task에서는 방향성을 잘못 잡았고 효율적이지 못하게 시간을 사용\n\n\n\n한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n\n어떤 방법을 사용해야 더 효율적으로 시간을 사용할 수 있을지 고민해보고 Workflow 작성해보고 싶음\n실험 진행 및 의사결정 시 더 체계적으로 기록해보고 싶음\nWandB Sweep등을 사용하여 최적의 조건을 자동으로 찾아보고 싶음\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4148-이동훈---Segmentation":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4148-이동훈---Segmentation","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report/개인 회고 T4148 이동훈 - Segmentation.md","title":"개인 회고 T4148 이동훈 - Segmentation","links":[],"tags":[],"content":"이번 프로젝트에서 나의 목표는 무엇이었는가?\n\n\nsegmentation 개념과 기본에 대한 이해\n\n\nmmseg와 smp 라이브러리에 대한 사용법 익히기\n\n\n나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\n\n\n강의를 최대한 잘 이해하고 들으려고 하였다.\n\n\n라이브러리 documents를 살펴보면서 최대한 사용해보려고 하였다.\n\n\n나는 어떤 방식으로 모델을 개선했는가?\n\n우선적으로 주로 쓰이는 모델들에 대하여 돌려보고 성능을 확인하였다.\n속도가 빠른 모델들을 가지고 성능을 확인해보았다.\n\n내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?\n\n\nbase config 세팅으로 다양한 모델들을 돌려봤어야했는데 그러지 못한게 아쉬웠다.\n\n\n전과 비교해서, 내가 새롭게 시도한 변화는 무엇이고, 어떤 효과가 있었는가?\n\n새롭게 시도한 부분은 albumentation을 기반으로 한 augmentation을 한 두개씩 시도해보았다.\nalbumentation demo page를 통해서 눈으로 직접 보고 적용해 보았다.\n\n마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\n\n\n건강이 많이 안좋았어서 해보려고 했던 것들을 해보지 못한게 너무나도 아쉬웠다.\n\n\n한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n\n.py 환경에 대해서 조금 더 익숙해지기 위해서 pipeline에 대한 세팅을 스스로 한번 해보고 싶다.\n프로젝트 기간동안은 아니더라도 수료를 하고 난 후에 대회를 해보면서 직접 해봐야할 듯 하다.\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4190-전지용---Segmentation":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4190-전지용---Segmentation","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report/개인 회고 T4190 전지용 - Segmentation.md","title":"개인 회고 T4190 전지용 - Segmentation","links":[],"tags":[],"content":"이번 프로젝트에서 나의 목표는 무엇이었는가?\n\n데이터 셋 분석과 그에 대한 문제점을 정확히 확인하고 그에 대한 학습 전략 수립하기\nMMSegmentation과 SMP 비교 및 완벽 이해하기\nSemantic Segmentation에 사용할 수 있는 성능 향상 기법을 최대한 사용해보기\n등수에 상관없이 완성도 있는 학습 과정을 수행하기\n협업 툴 사용에 더 나아가 Github Issue와 Pull request를 적절히 활용하기\n\n나는 내 학습 목표를 달성하기 위해 무엇을 어떻게 했는가?\n\n데이터 셋 분석\n\nEDA\n\n데이터 셋 특성을 잘 표현할 수 있도록 시각화 진행\n\n데이터의 특성이 요약된 데이터를 분석하여 의견 공유\n\n\n데이터 시각화\n\n데이터의 이미지와 Annotation을 직접 비교\n데이터의 문제점을 파악\n\n\n\n\n데이터의 특성을 활용하여 실험 방향 설정\n\n\nMMSegmentation Baseline 제작\n\nCustom Dataset\n\nCustom Dataset 모듈 구현\nCustom Dataloader 모듈 구현\n\n\nMMSegmentation을 활용하여 원활한 실험을 하기위해 Baseline Code를 제작\n\nConfig 파일 관리를 원활하기 위한 폴더 구성\n\n\nSubmission을 위한 Inference 코드 제작\n\n\nGithub 적극 활용\n\nIssue를 활용하여 실험 중 문제가 생겼을 때 즉시 공유\nPull Request를 활용하여 Code를 바로 Merge하는 것이 아닌 다른 팀원에 의해 점검 후 Merge\n\n\n\n나는 어떤 방식으로 모델을 개선했는가?\n\n학습 모델 선정\n\nDataset 특성에 맞는 모델 조사\nPretrained 가중치가 존재하는 모델을 우선적으로 실험\nSMP에는 존재하지 않고 MMseg에만 존재하는 Architecture 위주로 학습 모델을 선택\n\n\nDice CE Loss 구현\n\nClass Imbalance를 해결하기 위해 Dice Loss 실험 진행\nLoss 별 가중치를 두어 Multi Loss (Dice Loss : 0.75 + CE Loss : 0.25) 구현\n\n\nModel Ensemble &amp; Inference 기법 연구\n\n모델의 학습 특성을 파악하고 해당 장단점을 살리기 위해 Ensemble 코드 구현\nInference input size를 다양하게 주어 성능 향상 테스트\nTTA를 구현하여 성능 향상 도모\n\n\n\n내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?\n\n학습 모델 선정\n\n대회 Dataset과 가장 유사한 ADE Dataset으로 학습된 Pretrained 가중치 모델를 우선적으로 실험\n\n처음부터 학습하는 것 보다 기학습된 모델이 훨씬 더 빠르고 좋은 성능을 보였음\n\n\nSMP로는 구현할 수 없는 Architecture 학습\n\n더 다양한 모델을 실험해 봄으로써 더 많은 모델의 특성을 확인할 수 있었음\n\n\n\n\nDice CE Loss 구현\n\nDice Loss만 사용했을 때, CE Loss 만 사용했을 때 보다 0.1의 성능 향상을 보임\nDice Loss와 CE Loss를 동시에 사용했을 때 더 높은 성능을 보임\n\n각 Loss에 가중치를 두어 실험을 진행하였고 적절한 비율에서 높은 성능을 보임\n\n\nSegmentation에 자주 사용된다고 해서 항상 모든 경우에 좋은 것이 아니라는 것을 느낌\n\n실험을 진행할 땐 문제를 정확히 정의하여 비교를 해보는 것이 중요함\n\n\n\n\nModel Ensemble &amp; Inference 기법 연구\n\nInference 시 Input size를 달리하여 성능 비교\n\n원본 사이즈 (512, 512)보다 크거나 작게 Resize하면 성능이 하락하는 것을 확인\n\n\nTTA 구현\n\nRatio와 Flip을 통해 TTA를 구현하였고 TTA를 적용하지 않은 것과 비교했을 때 성능이 0.01~0.02정도 향상하는 것을 확인\n\n\n여러 모델의 특성을 고려한 Model Ensemble\n\n모델 별 장단점을 확인하고 모델 서로 간의 장점을 부각시킬 수 있도록 앙상블 진행 (0.1 정도의 성능 향상)\n모델의 특성을 미리 파악하여 학습을 진행하면 앙상블을 통해 단점을 지우고 장점을 극대화 할 수 있음을 알게됌\n\n모델 선정에도 계획이 필요함을 깨달음\n\n\n\n\n\n\n\n전과 비교해서, 내가 새롭게 시도한 변화는 무엇이고, 어떤 효과가 있었는가?\n\nGIthub 적극 활용\n\nGitLab flow → GIthub Flow\n\nPull Request를 통해 작성된 Code를 팀원들에게 검수받음\n코드에 대한 사소한 실수들을 줄일 수 있었음\n\n\nGIthub Issue 관리\n\n문제 상황을 즉시 공유할 수 있었음\n\n\n\n\nEDA에 개인적인 견해를 포함\n\n데이터를 요약하고 시각화하면서 얻은 내용을 Markdown언어를 통해 정리\n\n문제점에 대해서 더 명확히 공유할 수 있었음\n문제점을 기록함으로써 해결해야하는 문제 해결 방향을 놓치지 않음\n\n\n\n\n\n마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\n\nMMSegmentation Baseline Code를 제작하며 오류가 너무 많았다.\n\n오류가 많이 발생하여 실험이 늦어지고 그 때문에 더 많은 실험을 하지 못한 점이 아쉬웠다.\n내 상황에 맞는 모듈을 추가하며 라이브러리에 대해서 완벽히 이해했지만 이해까지 너무 오래걸렸고 더 많은 모듈을 실험해 보지 못하였다.\n실험이 늦어진 만큼 하이퍼파라미터 튜닝을 많이 고려하지 못하였다.\n\n\nEDA를 통해 나온 실험 방향에 비해 많은 실험을 하지 못했다.\n\n성능에 목 메이지 않고 차근히 실험 결과를 정리하여 중복된 실험을 줄였어야했는데 그러지 못해서 아쉬웠다.\n\n\n실험 정리를 명확하게 정리해야함을 깨달았다.\n\n실험에 대한 생각과 정리를 더 정확하고 깔끔하게 정리해야 이후에도 더 원활한 실험을 하게된다는 것을 알게되었다.\n\n\n\n한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n\n실험 정리를 보기 쉽고 깔끔하게 정리할 수 있도록 협업 툴을 수정해 볼 것이다.\n\n문서화의 중요성을 항상 명심하고 더 구체적으로 학습을 기록하는 습관을 들여야 한다.\n\n\nGitlab Flow에서 Github Flow로 변경하면서 PM의 중요성을 알게되었고 다음 프로젝트부터는 역할 분담을 좀 더 명확하게 구분할 것이다.\n아직 사용해보지 못한 CV 라이브러리도 사용해보며 CV에 대한 전문성을 기를 것이다.\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4199-정원국---Segmentation":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4199-정원국---Segmentation","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report/개인 회고 T4199 정원국 - Segmentation.md","title":"개인 회고 T4199 정원국 - Segmentation","links":[],"tags":[],"content":"📢 이번 프로젝트에서 나의 목표는 무엇이었는가?\n\n\n하나 이상의 모델 구현을 해보는 것\n\n\nPointRend를 이해하고 적용해보는것\n\n\nWandb Sweep을 통해 파라미터 튜닝을 해보는것\n\n\n데이터 분석을 통해 Augmentation을 정하는것\n\n\n전처리 or 후처리를 해보자.\n\n\n팀원에게 도움을 잘 요청하자\n\n\n📢 나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\n\n\nSMP 에서 SwinT를 구현했다. mmsegmentation 라이브러리에서 Swint의 성능이 좋다는 걸 알게 되었고, 후에 앙상블을 위해\nSMP에서도 구현을 해야 한다는 필요성을 느끼고 구현하였다.\n\n\nPointRend를 이해하고 모델의 특성을 확인하였다. 외형선을 잘 잡아주는 모델의 특성을 파악하였고, 이를 실험의 결과를 통해서 확인하였다.\n후에 segmentation task에서 외형을 잘 잡지 못하면 PointRend와 앙상블을 항상 염두 해야겠다.\n\n\nWandb Sweep을 통해 파라미터 튜닝을 진행했다. 매번 파라미터 튜닝에 많은 시간이 걸렸고, 효율적으로 진행하기 위해서 Sweep을 구현하였다.\n다른 서버에서 Auto tuning이 진행되는 동안 다른 많은 것들을 시도해 볼 수 있어서 효율성이 증가했다.\n\n\n데이터 분석을 통해 Augmentation을 정했다. 매번 Augmentation이 추가 될 때 마다 output eda를 진행하였고, 근거있는 Augmentation을 찾고 이를 검증하고자 하였다.\n\n\n후처리를 구현하였다. Tensor구조를 파악하고 Tresh hold를 조정하여 유의미한 후처리를 진행하였다.\n\n\n📢 나는 어떤 방식으로 모델을 개선했는가?\n\n\nSMP 라이브러리에 집중하여 모델을 서칭하고 좋은 스코어를 내기로 하였다.\n\n\n해당 테스크에 SwinT가 효과적인것을 파악\n\n\nSMP 에는 SwinT가 구현되어 있지 않아 SwinT를 구현하였다.\n\n\nSwinT를 구현할 때 팀원에게 도움을 요청하여 원할하게 협업이 진행되고 구현역시 빠르게 진행되었다.\n\n\nmiou 0.6을 못넘던 성능에서 한번에 0.7 까지 달성하였다.\n\n\n\n\nWandb Sweep을 통해 파라미터 튜닝을 진행했다.\n\n\nbatch size, lr , optimizer, loss, Augmentation등을 Sweep을 통해 튜닝하여 일의 효율을 향상시켰다.\n\n\n\n\n데이터 분석을 통한 Augmentation\n\n\n그림자가 있는 부분에 class를 모델이 헷갈리는것을 파악 후 brightness를 조정하여 이를 개선하였다.\n\n\n그림자가 있는 부분에 class를 모델이 헷갈리는것을 파악 후 Togray를 넣어 이를 개선하였다.\n\n\n매번 Augmentation을 추가할 때 마다 결과치를 EDA하고 타당성을 검증하였다.\n\n\n\n\n해당 모델의 한계 파악\n\n\n다른 인코더들로 내는 성능에 한계가 있음을 파악하고 SwinT로 변경해 주었다.\n\n\n파라미터 수가 더 많아 모델이 많은 Feature를 뽑아낼 것이라고 생각.\n\n\n다만 데이터 수가 충분하지 않으므로 Transformer 계열 중 작은 SwinT로 선정.\n\n\n\n\nPost Processing\n\npixel threshold\n\n\n전체적으로 Clothing을 GeneralTrash로 착각하는 경우가 많다는 걸 파악\n\n\nPixel 단위에서 GeneralTrash threshold 가 0.6 이하인 경우 0으로 처리\n\n\n결과적으로 mIou 0.02~0.05 상승 → Clothing의 경우 0.1까지도 상승\n\n\n\n\n\n\n📢 내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?\n\n\nSwintT\n\n\n필요한 모델이 있다면 두려워 하지 말고 구현을 하면 된다는 것을 깨달았다. 이전까지는 모델을 다루거나 구현하는데 두려움이 있었다면 이번 기회에 SwinT\n를 구현하면서 그런 두려움이 사라졌다. 이 구현을 통해 상위권 validation score인 0.729를 달성 하였다.\n\n\n\n\nSweep\n\n\n하이퍼 파라미터를 튜닝하는데 걸리던 시간과 노력을 대폭 줄였고, 이를 바탕으로 다른 모델구현이나 Augmentation, EDA등에 힘 쓸 수 있었다. 단순 작업들은\n자동화를 통해 효율을 높이고, 더 중요한것에 노력을 쏟는게 훨씬 효과적이란걸 깨달았다.\n\n\n\n\n데이터 분석\n\n\n이전 대회에서 느낀 데이터 분석을 통한 Augmentation적용이 이번에도 효과적이었다. 이를 통해 다시 한번 EDA의 중요성을 파악했고, 무언가 의사결정을 할 때\n얼마나 근거를 가지고 하는게 중요한지 다시금 깨닫게 되었다.\n\n\n\n\n📢 마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\n\n\n슬로우 스타터\n\n\n건강상의 문제로 대회 시작 일주일정도 뒤부터 Task에 뛰어든게 아쉽다. 다행히 수술을 통해 잘 회복하였지만 시간상의 부족으로 더 많은것들을 시도해 보지 못한게\n가장 큰 아쉬움으로 남는다.\n\n\n\n\nMM 라이브러리의 활용\n\n\nSMP 라이브러리만을 담당해서 하다보니 MM 라이브러리의 활용을 많이 못해본 것이 아쉬웠다.\n\n\n\n\nConfusion matrix\n\n\n매번 output을 하나하나 EDA하기보단 Confusion matrix를 구현했다면 훨씬 효율적으로 했을 텐데 이부분을 못한게 아쉽다.\n\n\n\n\nModel Custom\n\n\nSMP의 장점이라면 Custom이 쉽다는 점인데 이부분을 더 활용하지 못한거 같아 아쉽다.\n\n\n\n\n📢 한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n\n\n컨디션 관리가 얼마나 중요한지 다시금 깨닫게 되었고, 다음 프로젝트에는 확실한 일정관리와 컨디션 관리를 해야겠다. 스케쥴러를 통한 관리를 시도해볼 예정이다.\n\n\n3D detection을 하게 될 텐데 MM라이브러리를 잘 활용해보고 싶다.\n\n\n자동화에 좀 더 신경쓰고, 효율을 높일 수 있는 것들을 잘 구현해서 사용해야겠다. ex) Confusion matrix\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4226-한상준---Segmentation/개인-회고-T4226-한상준---Segmentation":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4226-한상준---Segmentation/개인-회고-T4226-한상준---Segmentation","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report/개인 회고 T4226 한상준 - Segmentation/개인 회고 T4226 한상준 - Segmentation.md","title":"개인 회고 T4226 한상준 - Segmentation","links":[],"tags":[],"content":"이번 프로젝트에서 나의 목표는 무엇이었는가?\n\n다양한 모델과 Augmentation 탐색을 통해 높은 성능의 모델을 찾고자 하였다.\n특히, Semantic segmentation 모델의 구조를 이해하고 각 모델이 가지는 구조적 특성을 파악하여, 본 대회에 가장 적합한 모델을 찾아가는 것을 목표로 하였다.\n또 본 대회의 경우 Train 데이터의 수가 적어 Test 데이터를 대변하기 어렵고, Class 의 불균형도 있으므로, 다양한 Augmentation 기법을 탐색하여 중 본 대회에 가장 적합한 조합을 찾아내고자 하였다.\n\n나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\n\nSegmentation Models Pytorch 라이브러리를 이용하여 아래의 경량 모델을 기준으로 하였다.\n\nEfficientnet-b3 엔코더를 가지는 Unet++ 구조 모델\nEfficientnet-b3 엔코더를 가지는 FPN 구조 모델\n\n\n위 모델을 선택한 이유로는 빠른 학습시간과 추론시간을 낼 수 있기 때문으로, 위 모델의 기준성능을 뛰어 넘을 수 있는 Augmentation 기법을 찾아내기 위하여 다양한 실험을 진행하였다.\n\n\n\n\n모델을 학습하며 Valid 데이터의 추론 결과를 얻어, 이를 WandB 를 이용하여 기록하였다.\n각각의 실험들마다 mIoU와 개별 클래스의 IoU 를 한 눈에 볼 수 있도록 하여 비교하였다.\n\n마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\n\n본 대회에서는, 빠르게 학습할 수 있는 경량 모델을 기준으로 삼아, Augmentation 기법을 다양하게 적용해보거나, Loss 함수를 커스텀하여 Cross entropy 와 Dice loss 를 Mix 하여 사용하거나, 엔코더 Loss 비율을 다르게 주거나, Learning Rate에 변화를 주며 실험을 하였다.\n실험 결과, 학습 초기에 배터리와 같은 특정 Class의 학습이 잘 되지 않는 현상을 개선하거나, Class 불균형 문제가 개선된것 처럼 보이는 학습 그래프를 얻을 수 있었다.\n하지만 결과적으로, 20k ~ 60k iteration 을 진행하며, 베이스라인으로 잡았던 점수에 모두 수렴하는 등 일정한 한계치를 뛰어넘지 못하였다.\n대회 당시에는 local minimum 에 수렴하여 global minimum을 찾지 못하는 문제로 생각하거나, 데이터 오염 등의 문제로 학습이 더 이상 되지 않는것으로 생각하였으나, 대회 종료 후 회고를 통해 알게된 사실은, 베이스라인으로 삼았던 모델이 본 대회의 쓰레기 데이터에 대해 Semantic segmentation을 하기에 충분한 파라미터를 가지고 있지 않았던 것이 가장 큰 원인일것으로 생각하게 되었다.\n본 대회를 진행하면서, 가벼운 모델을 기준으로 다양한 실험을 빠르게 하려고 했던 전략이 오히려 충분히 좋은 모델을 탐색하는데 방해가 되어 좋은 성적을 낼 수 없어 많은 아쉬움이 남게 되었다.\n\n한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n\n이번 대회를 통해 어설프게 잘못 알고 있는 상식이 얼마나 독이 되는지 새삼 느끼게 되었다.\n모델의 학습 결과를 평가함에 있어서 논리 근거가 충분하지 못한 추정으로 잘못된 문제 정의를 하는 경우 해결책을 찾아갈 수 없음을 다시금 느끼게 되었다.\n차후 이러한 유사한 상황을 맞이하게 된다면 반드시 모델에 관련한 논문을 탐독하고, 다양한 가설을 세워 어떤 문제때문에 학습이 잘 되지 않는지 실험을 한 후 근거와 결과를 바탕으로 의사결정을 하는 자세를 지향해야 하겠다.\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report/재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report.md","title":"재활용 품목 분류를 위한 Semantic Segmentation - Wrapup Report","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4063-김형석---Segmentation","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4148-이동훈---Segmentation","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4190-전지용---Segmentation","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4199-정원국---Segmentation","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/개인-회고-T4226-한상준---Segmentation/개인-회고-T4226-한상준---Segmentation","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---Wrapup-Report/Wrap-up-report-Feedback"],"tags":[],"content":"\n프로젝트 개요\n\n설명\n평가\n\n\n프로젝트 팀 구성 및 역할\n프로젝트 수행 절차 및 방법\n\n1. 협업 환경\n2. 대회 접근 전략\n3. 수행 과정\n\n\n프로젝트 수행 결과\n\n1. Data EDA\n2. Model Search\n3. Train\n4. Inference\n5. Model Ensemble\n\n\n자체 평가 의견\n\n1. 결과 분석\n2. 회고 및 향후 개선점\n\n\n\n\n프로젝트 개요\n설명\n\n\n바야흐로 대량 생산, 대량 소비의 시대. 우리는 많은 물건이 대량으로 생산되고, 소비되는 시대를 살고 있습니다. 하지만 이러한 문화는 ‘쓰레기 대란’, ‘매립지 부족’과 같은 여러 사회 문제를 낳고 있습니다.\n\n\n분리수거는 이러한 환경 부담을 줄일 수 있는 방법 중 하나입니다. 잘 분리배출 된 쓰레기는 자원으로서 가치를 인정받아 재활용되지만, 잘못 분리배출 되면 그대로 폐기물로 분류되어 매립 또는 소각되기 때문입니다.\n\n\n따라서 우리는 사진에서 쓰레기를 Segmentation하는 모델을 만들어 이러한 문제점을 해결해보고자 합니다. 문제 해결을 위한 데이터셋으로는 배경, 일반 쓰레기, 플라스틱, 종이, 유리 등 11 종류의 쓰레기가 찍힌 사진 데이터셋이 제공됩니다.\n\n\n여러분에 의해 만들어진 우수한 성능의 모델은 쓰레기장에 설치되어 정확한 분리수거를 돕거나, 어린아이들의 분리수거 교육 등에 사용될 수 있을 것입니다. 부디 지구를 위기로부터 구해주세요! 🌎\n\n\n평가\n\nTest set의 mIoU (Mean Intersection over Union)로 평가\n\n\nSemantic Segmentation에서 사용되는 대표적인 성능 측정 방법\n\n\nIoU\n\n\n\n\n\n\n프로젝트 팀 구성 및 역할\n\nT4063 김형석\n\n[SMP] Baseline code 개발, WandB 연동, Scheduler 적용, AMP 적용, 실험결과 Notion API로 자동 기록, Focal Loss + Label Smoothing 구현, Model searching, SwinTv2 encoder 구현, UpperNet decode 구현, Hard voting Ensemble 구현\n\n\nT4148 이동훈\n\nsmp, mmseg 모델 탐색\n\n\nT4190 전지용\n\nData EDA, [MMSeg] Baseline Code 개발, Loss Function Searching, Model searching, Inference 기법 연구, Model Ensemble\n\n\nT4199 정원국\n\nWandb , smp swint 구현, multi-loss , Panet 튜닝 , Model searching, 후처리, 전처리\n\n\nT4226 한상준\n\n\n프로젝트 수행 절차 및 방법\n1. 협업 환경\n\n\nCode management : GitHub\n\n\n\nGit branch strategy : GitHub Flow\n\n\n\nExperiment tracking tool : WandB\n\n\n\nProject Management : Notion, Slack, Zoom\n\n\n\n2. 대회 접근 전략\n\n\nEDA\n\n\nSMP 기본 모델을 사용한 Augmentation 결정\n\n\nSMP와 MMSegmentation 사용, 개별적인 Model Searching\n\n\nWandB, Notion을 사용한 실험 결과 관리\n\n\n3. 수행 과정\n\n\nEDA → Model Search → Ensemble\n\n\n\n프로젝트 수행 결과\n1. Data EDA\n\n\nData Imbalance 문제\n\n\n카테고리 별 annotation 분포\n\n\npaper, plastic bag, plastic, General trash에 비해 Styrofoam, Paper pack, Glass, Metal, Clothing, Battery의 annotation 수가 적음\n\n\n\n\n\nAnnotation 특징\n\n\n카테고리 별 Annotation 크기\n\n\n픽셀 단위의 아주 작은 Annotation 다수 존재\n사진의 크기와 유사한 매우 큰 Object 존재\nClothing, Battery의 경우 크기가 안정되어있지만 Styrofoam, Paper pack, Glass, Metal의 경우 크기마저 불안정한 모습을 보임\n\n\n\n카테고리 별 Annotation 위치 및 크기 비교\n\n\nobject는 대부분 중앙에 크게 존재하고 가장자리로 갈수록 크기가 작아짐\n크기가 작은 Annotation 중 이미지의 가장자리에 속한 것들은 물체가 잘린 것으로 추정\n\n\n\n\n\n데이터 시각화\n\n\n데이터 셋 분석\n\n라벨링 기준이 모호하다.\n\n일반 쓰레기의 기준, 박스와 종이, 테이프 기준\n얼마나 세부적으로 라벨링을 할 것인지\n한 물체에 포함된 물체에 대한 기준 (ex. 페트병에 감싸진 비닐)\n\n\n투명한 이미지\n\n병, 페트병, 비닐 등 투명한 이미지에 대한 오류가 심할 것 같음\n\n\nSmall Object\n\n사람 눈으로도 인식이 되지 않는 작은 물체가 존재\n\n\n라벨링 오류\n\n데이터 라벨링이 잘못 되어있거나 심한 경우 라벨링이 되어있지 않은 경우도 존재\n\n\n\n\n\n\n\n결론\n\n데이터 클린징 작업은 의미가 없을 것 이라 추정\n\nTest 데이터도 비슷하다고 가정\n\n\nData Imbalance 문제를 해결하기 위한 기법 연구 필요\nSegmentation 문제에서 물체의 모양을 잘 맞추는 것 보다 물체의 클래스를 잘 맞추는 것이 우선이 되어야 함\n라벨링 오류와 같은 Noise에 대한 해결 방안이 필요\n\n\n\n2. Model Search\nSMP와 MMSegmentation Baseline Code를 작성하고 모델을 찾음\n\n\nSMP (Segmentation Models Pytorch)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitectureBackboneValidation ScoreTest Score특이사항UNetEfficientb20.49890.4844Color Aug 사용UNetEfficientb20.62440.5452Color Aug 미사용UNetEfficientb30.63560.5715Color Aug 미사용UNetEfficientb40.63350.5698Color Aug 미사용MAnetResnext500.5495--PSPNetEfficientb30.4296--LinknetEfficientb30.3661--Deeplabv3Resnet1010.5831--FPNResnet1010.5962--FPNEfficientb30.65470.5995-FPNEfficientb3(Timm)0.62860.6073-FPNSwinTv20.2856-가중치 미사용FPNSwinTv2 Crs2240.5757--FPNSwinTv2 w24i3840.7130.6212-UpperNetSwinTv2 w24i3840.7083--PANSwinTtiny0.7290.68PANSwinTtiny0.720.66\n==→ Backbone의 경우 Swin Transfomer v2가 가장 우수했고, Segmentation Model의 경우 PAN이 가장 우수한 성능을 보였다.==\n\n\nMMSegmentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitectureBackboneValidation ScoreTest Score특이사항SegformerMIT-b00.6158-UperNetSwin_Tiny0.6217-PointRendResNet1010.6394-Architecture 전체 Pretrained 가중치KNet+UperNetSwin_Large0.750.7009UperNetSwin_Large0.73690.711Augmenation 적용PointRendSwin_Large0.750.7118Segmentorvit_tiny0.66매우 빠른 학습시간Segmentorvit_small0.690.60UperNetbeit0.13매우 긴 학습시간\n==→ Backbone의 경우 Swin Transfomer Large가 가장 우수했고, UperNet 모델과 PointRend의 장단점이 확실히 나뉘어졌다.==\n\n\n3. Train\n\nAugmentation\n\nNoise Augmentation\n\nNoise의 경우 파라미터가 적고 학습이 쉬운 Efficient model의 경우 큰 의미가 없었다.\n그러나 파라미터가 크고 많은 feature를 보게 되는 경우 학습을 지연 시키면서 더 많은 부분을 학습하게 만들어 큰 효과를 이루었다.\n\n\n\n\nLoss Function\n\nDiceCE Loss\n\nDice Loss\n\n데이터 불균형 특징이 존재하는 semantic segmentation network 구조에 많이 사용되는 손실 함수\n데이터 특성 상 Background가 많이 존재하며 데이터 불균형이 심하게 보이므로 Dice Loss 사용\n\n\nCross Entropy loss와 Dice loss를 가중치를 두고 혼합하여 DiceCE Loss 구현\n\nCross Entropy loss (0.75) + Dice loss (0.25)\nCross Entropy loss를 활용하여 정확도를 향상시키고 Dice loss를 활용하여 Class Imbalance 문제를 해결\n\n\n\n\n\n\nSweep\n\nlr , batch, optimizer, Augmentation\n\nSweep을 통해 파라미터 튜닝을 진행하여 성능 향상을 이루었다. miou 0.58 → 0.61\n\n\n\n\n\n4. Inference\n\nInference Input size\n\nInput size를 Model input size(default : 512 x 512)로 변환하여 Inference 진행\nInference 진행 후 Mask 이미지를 (256, 256)으로 변환\n\n결과 이미지는 (256, 256)으로 고정\n\n\n\n\nTTA\n\nMulti Scale\n\nTest시 이미지 크기를 0.5~1,75 배율로 조절하여 TTA 진행\n\n\nFilp\n\nTest시 RandomFlip을 사용하여 TTA 진행\n\n\n\n\nPost Processing\n\npixel threshold\n\n전체적으로 Clothing을 GeneralTrash로 착각하는 경우가 많다는 걸 파악\nPixel 단위에서 GeneralTrash threshold 가 0.6 이하인 경우 0으로 처리\n결과적으로 mIou 0.02~0.05 상승 → Clothing의 경우 0.1까지도 상승\n\n\n\n\n\n5. Model Ensemble\n\nMMSegmentation Model Ensemble\n\nMMSegmenation으로 학습된 모델에 대한 앙상블 기법\n앙상블 할 모델을 모두 추론하여 픽셀 별 Confidence score가 높은 Class를 선택\n\n\nHard voting Ensemble\n\nInference를 통해 생성된 csv파일을 Hard voting 방식으로 Ensemble, 동률의 투표 결과가 나왔다면 Submission Score가 높은 모델의 결과를 반영\n결과\n\n==[0.7118]SwinL&amp;Pointrend + [0.7057]SwinL&amp;Upernet + [0.6800]SwinTtiny&amp;PAN → 0.7278(Best)==\n[0.7093]SwinL&amp;Pointrend + [0.7110]SwinL&amp;Upernet + [0.6800]SwinTtiny&amp;PAN → 0.7247\n\n\n\n\n\n\n자체 평가 의견\n1. 결과 분석\n\nAugmentaion이나 Hyperparameter Tuning등의 기법보다 성능이 좋은 SOTA Model를 찾는 것이 더 좋은 결과를 얻었다\nEnsemble의 경우, 여러 Model의 결과를 Ensemble한 것이 미소하지만 더 좋은 결과를 얻었다.\n\n2. 회고 및 향후 개선점\n\n성장 포인트 회고\n\n지난 대회와 비교하여, 실험 기록을 Notion 에 자동으로 기록하는 코드를 작성하는 등 실험 기록의 자동화를 통한 편의성 향상을 이루었다.\n협업을 더 원활하게 하기 위해 Github을 더 적극적으로 사용하였다. (Pull Request, Issue Tracking)\n\n\n좋았던 점\n\n실험 기록들을 기반으로 점진적인 성능 향상을 이루었다.\n각자 실험할 부분을 명확하게 나누어서 실험이 진행되었고 결과가 잘 공유되었다.\n대회 종료 시까지 집중하여 성능을 끌어올릴 수 있는 모델을 계속 탐색하였고, Ensemble을 통해 더 높은 점수를 이끌어 낸 점은 잘 되었다고 생각한다.\n\n\n아쉬운 점\n\nSOTA Model을 선정하는 것이 가장 큰 성능 향상 요소였는데, 다른 실험에 몰입하여 많은 실험을 진행하지 못했다.\n가벼운 모델을 이용하여 Augmentation 기법 탐색 등 다양한 실험을 하여 의사 결정을 하는 방법에 있어서, 어설프게 잘못 알고 있는 상식으로 이를 절대 진리로 생각하여, 모델의 학습 결과를 평가함에 있어서 논리 근거가 충분하지 못한 추정으로 잘못된 문제 정의를 하는 경우 해결책을 찾아갈 수 없음을 다시금 느끼게 되었다.\nSegmentation Models PyTorch와 MMSegmentation 이외에도 Detectron2, PaddleSeg 또는 커스텀 모델들을 더 적극적으로 활용하여 SOTA 모델을 포함한 많은 모델을 실험해보았어야 했지만 이런 부분에서 협업이 원활하지 못하였다.\n대회를 시작하며 각자 하고 싶은 기법은 모두 사용해보는 Wish list를 작성하였는데 이를 모두 실험해 보지 못하였다.\n\n\n향후 개선점\n\n프로젝트 진행을 원활하게 하기 위해 PM과 팀원의 역할을 명확히 정의하고, 협업 방식을 개선하여야 하겠고, Github Flow 등 협업에 관련한 컨벤션을 정하여 팀원 모두가 적극적으로 참여할 수 있도록 해야하겠다.\n모델의 학습 결과를 평가함에 있어서 논리 근거를 충분히 하기 위하여, 각 실험에 관련한 논문을 읽고, 다양한 가설을 세워 어떤 문제 때문에 학습이 잘 되지 않는지 실험하여 근거와 결과를 바탕으로 의사 결정을 하는 자세를 지향해야 하겠다.\n\n\n\n\n개인 회고 T4063 김형석 - Segmentation\n개인 회고 T4148 이동훈 - Segmentation\n개인 회고 T4190 전지용 - Segmentation\n개인 회고 T4199 정원국 - Segmentation\n개인 회고 T4226 한상준 - Segmentation\nWrap-up report Feedback"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---개인회고":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---개인회고","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - 개인회고.md","title":"재활용 품목 분류를 위한 Semantic Segmentation - 개인회고","links":[],"tags":[],"content":"이번 프로젝트에서 나의 목표는 무엇이었는가?\n\n‘문제 정의’를 착실하게 하겠다\n모델 탐색 및 선정은 자료를 근거로 하겠다(ex: paperswithcode)\nHyperparameter Recipe은 논문을 근거로 하겠다\n단순한 시도-결과확인이 아닌, DOE를 하겠다\n목표를 갖되, 상황에 맞게 수정하겠다\n\n나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\n\n‘문제 정의’를 착실하게 하겠다\n\n대회 시작과 동시에 평가 metric인 miou의 의미를 파악\nmiou는 Class별 iou의 평균이므로, wandb에서 miou뿐만 아니라 각 Class의 iou도 모두 기록하도록 구현\n매 실험에서 단순히 miou만 보지 않고, class 별 iou를 관찰하며 진행\n\n\n모델 탐색 및 선정은 자료를 근거로 하겠다\n\n각 모델의 논문을 살펴보고 개선된 모델(ex: Deeplabv3→Deeplabv3+)인 경우, 기존 모델(Deeplabv3)는 실험에서 제외\n기본 모델(UNet)에서 시작하여 실험 전개, 결과가 가장 좋은 augmentation을 기본 augmentation으로 지정\n이후 다른 모델들을 탐색하면서 결과 Metric을 기반으로 Best 모델을 선정\n\n\nHyperparameter Recipe은 논문을 근거로 하겠다\n\n실험을 통해 Encoder로 사용하는 model에 따라 Segmentation Model에 따라 너무 많이 달라지기 때문에 의미가 없다고 판단, 시도하지 않았음\n\n\n단순한 시도-결과확인이 아닌, DOE를 하겠다\n\nNotion API를 사용해 훈련 후 자동으로 Notion Page의 DB에 결과와 Parameter가 기록되도록 구현하였음\n실험기록을 기반으로 지속적으로 성능을 향상시켰음\n\n\n목표를 갖되, 상황에 맞게 수정하겠다\n\n리더보드 상위권을 노려보고 싶었으나, 생각보다 Score가 잘 나오지 않았고 단순히 Score 향상보다 많은 실험을 해보며 이 Task에 대한 감각을 향상시키고 다양한 기능들을 구현해보며 실력을 향상시키는 것을 목표로 하였음\n\n\n\n나는 어떤 방식으로 모델을 개선했는가?\n\nUnet + Efficientnetb3로 실험을 통해 성능이 향상되는 augmentaion 조건을 선정\n이후 Unet을 다른 Segmentaion Model로 바꿔가며 가장 높은 성능을 보여주는 모델을 탐색, FPN이 가장 좋은 결과 얻음\nTimm에서 SwinTv2를 가져와 Feature map을 얻을 수 있도록 Module의 Sequence를 수정하여 Custom Encoder를 구현\nHard Voting Ensemble을 구현\n\n내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?\n\n결과\n\nSMP SwinTv2 Custom Encoder : Validation 기준 miou 0.7083\nHard Voting Ensemble을 구현 : Submission기준 miou 0.7247\n\n\n느낀점\n\n이번 Task는 Augmentation이나 최적의 모델을 찾는 것 보다 최신의 SOTA Model을 사용할수록 압도적으로 성능향상이 있었다\nTask에 따라 실험의 방향성이 다르고, 이를 빠르게 확인해야 효율적인 시간 투자가 가능하다\n\n\n\n전과 비교해서, 내가 새롭게 시도한 변화는 무엇이고, 어떤 효과가 있었는가?\n\n진행한 실험을 모두 기록했고, 실험 기록에 근거해 다음 실험을 진행하였음\n\n점진적인 성능향상을 얻음\n\n\n라이브러리에서 제공하는 모델에 안주하지 않고 새롭게 모델을 가져와 라이브러리에 적용\n\n기존의 실험들보다 더 큰 성능향상을 얻음\n\n\n\n마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\n\n한계\n\n좋은 Model을 먼저 선정하고 실험을 통해 최적의 조건을 찾아갔어야 했는데, 최적의 조건을 찾고 좋은 모델을 선정하려고 하였음\n\n\n아쉬운 점\n\nTask마다 좋은 방법은 다르겠지만, 이번 Task에서는 방향성을 잘못 잡았고 효율적이지 못하게 시간을 사용\n\n\n\n한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n\n어떤 방법을 사용해야 더 효율적으로 시간을 사용할 수 있을지 고민해보고 Workflow 작성해보고 싶음\n실험 진행 및 의사결정 시 더 체계적으로 기록해보고 싶음\nWandB Sweep등을 사용하여 최적의 조건을 자동으로 찾아보고 싶음\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---멘토-피드백":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/재활용-품목-분류를-위한-Semantic-Segmentation---멘토-피드백","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/재활용 품목 분류를 위한 Semantic Segmentation - 멘토 피드백.md","title":"재활용 품목 분류를 위한 Semantic Segmentation - 멘토 피드백","links":[],"tags":[],"content":"전체 피드백\n대회 기간동안 최종 프로젝트 주제 결정을 같이 진행하셔서 고생이 많으셨을 것 같습니다. 데이터 시각화와 실험관리 model performance table은 정말 잘만드셨네요. 하지만 여기서 그치는 것이 아니라 왜 그렇게 나왔는지 실험 후에 해석하는 것도 중요하다고 생각합니다. 그 과정이 있어야 다음으로 진행해야하는 것이 정해지고 무엇이 해결되었고 아직 해결되지 못한 것을 찾을 수 있을 것 같습니다. IoU score에서는 표현되지 않는 중요한 것들이 모델에 많이 담길 수 있습니다. validation dataset을 선정하는 것 처럼 프로젝트간에 해결이 되지 않았던 데이터들을 선별해서 inference를 실행하여 모델들이 어떻게 반응하는지 비교를 통해서 모델에 대한 해석을 적용하고자 하는 데이터셋의 관점에서 진행하실 수 있을 것 같습니다. 이는 모델 뿐만 아니라 loss function, lr scheduler, augmentation 등 실험 단위로도 해석이 가능할 것으로 보입니다. 물론 대회에서는 궁극적으로 IoU를 올리는 것이 목표이겠지만 IoU를 올리는 과정에서도 명확한 문제정의가 없으면 한계가 있을 수 있을 것 같으며, 실무에서는 보다 더 specific한 케이스에 대해서 작동하는지 확인하는 경우가 많습니다. (그리고 그런 해석이 없다면 포트폴리오나 면접에 가서 해당 프로젝트에 대해서 이야기를 나눈다면 “그냥 큰 모델이 좋았고 Sota 모델이 좋았다.” 정도의 이야기만 나눌 수 있을 것 같아 보다 준비가 되면 좋을 것 같습니다.) 가설을 선택해서 실험을 진행하는 것이 물론 1번으로 좋겠지만 가설 없이 진행했던 실험에 대해서도 해석을 통해서 또한 좋은 인사이트를 얻을 수 있습니다.\n\n개인 회고 T4063 김형석\n안녕하세요 형석님 고생많으셨습니다. 실험 관리를 위해 Notion API 부터 Wandb sweep 까지 활용하신 것을 보고 실험에 진심이신 것을 느낄 수 있었습니다. 앞으로도 이러한 부분들을 보다 자세히 진행하셔서 앞으로 많은 인사이트를 얻으시면 좋을 것 같습니다.\n느낀점에서 Sota 모델이 중요했다는 것을 이야기해주셨는데 이전 기수를 보았을 때도 그렇고 이번에도 마찬가지로 Sota 모델 활용이 이번 대회의 핵심이였던 것 같습니다. 하지만 여기에서 끝나는 것이 아니라 큰 맥락으로 왜 Sota 모델이 더 뛰어났는지 해석도 해보면 좋을 것 같습니다. 그외에 적용했던 모델 중에 좋은 성능을 보였던 모델에 대해서 해석해보는 것도 좋을 것 같습니다. 예를 들어 큰 맥락의 해석은 “Image classification 혹은 object detection에 비해 평가해야하는 요소들이 많아진다.“ Image classification은 image 한장에서 한번의 decision을 진행한다. object detection은 image 한장에서 object의 개수, segmentation에서는 image 한장에서 pixel 수 이 처럼 task 별로 benchmark dataset의 metric 점수가 의미가 다르며 작은 차이도 큰 차이를 만들어 낼 수 있다고 해석 할 수 있을 것 같습니다. 다음으로 작은 맥락의 해석은 적용 모델의 논문에서 해결하고자 했던점들이 우리의 task에 어떻게 어울리는지 해석이 필요합니다. 물론 다루는 데이터 셋이 다르고 목적이 다르기 때문에 논문에서 표현하지 않을 수 있어 보다 많은 시간을 소비해야하지만 이러한 해석이 있어야 다음 스텝으로 발전할 수 있을 것 같습니다.\n대회 기간동안 고생많으셨고 멘토링 기간동안 재미있는 질문들 많이해주셔서 감사합니다. 남은 기간 잘 마치셔서 이루고자 하는 바 다 이루셨으면 좋겠습니다.\n\n개인 회고 T4148 이동훈\n안녕하세요 동훈님 대회 기간동안 코로나 때문에 많이 힘드셨을 것 같은데 고생 많으셨습니다. 개인회고 중 .py 와 친해지는 것과 config 관리하는 방법을 고민하고 있으신 것 같네요. 다른 사람들이 만들어 둔 github repository를 참고를 시작으로 부분 부분을 수정하여 변형하여 사용하는 것도 방법입니다. 이와 비슷하게 저는 github.com/ashleve/lightning-hydra-template 여기 탬플릿 처럼 다른 사람들이 만들어둔 탬플릿을 활용하여 실험 관리 혹은 파이프라인 관리를 합니다. 모든 탬플릿이 자신에게 맞을 수 없으니 다양한 대회나 프로젝트를 진행하다 보면 공통적으로 진행하는 부분들을 파이프라인으로 내제화 하여 자신의 코드들을 지속적으로 추가하는 것을 추천드려요.\n멘토링 기간동안 고생많으셨습니다. 남은 기간도 잘 마무리하셔서 좋은결과 만드셨으면 좋겠습니다.\n\n개인 회고 T4190 전지용\n안녕하세요 지용님 대회기간동안 고생많으셨습니다. 개인회고를 보니 다양한 것들을 많이 시도하고 변화를 경험하고 있으신 것 같습니다. mmsegmentation 활용방법, 실험 관리 등 다양한 것을 학습하고 있으신 것 같아요. mmsegmentation은 많은 부분들이 이미 패키징이 되어 있고 이전에 배우셨던 PyTorch 탬플릿들과는 많은 부분들이 달라 공부하시고 적용하시는데 많은 어려움이 있으셨을 것 같습니다. 이 경우 나만의 탬플릿을 만들어 mmsegmentation 모델, dataloader 등 다양한 기능들을 가져와서 활용할 수 있게 하는 방법을 찾아 만들거나 mmsegmentation을 이해하고 거기에 자신이 자주 사용했던 기능들을 탑제하는 방법으로 활용하는 것이 가능할 것 같습니다. 자신의 탬플릿은 이전에 경험했던 것들을 축적하여 다음에 활용할 수 있게 만드는 것도 중요합니다. 이 부분에서는 효과가 좋았던 것들만 축적할 수 있겠지만 효과가 좋지 않았던 것들도 다음번에는 도움이 될수도 있습니다. 남은 기간 잘 마무리하셔서 좋은 결과 있기를 바랍니다.\n\n개인 회고 T4199 정원국\n안녕하세요 원국님 바쁜 와중에 몸도 안좋으셔서 수술까지 받으셨네요. 정말 고생이 많으셨습니다. 그러던 와중에도 많은 것들을 하셨네요. smp에 없는 swinT를 만드셨는데 너무 대단하신 것 같네요 나중에 시간에 남으시면 smp에 pr 신청해보는 것도 좋을 것 같아요. open source의 contributor가 생각보다 스펙이 될수도 있으니 도전해봐도 좋을 것 같아요. 그리고 PointRend, Wandb sweep 활용등 많은 것들을 진행하셨네요. 후처리에 대한 내용이 많이 없어서 어떻게 진행하셨는지 파악은 어려운데 PointRend의 특성과 다른 model들의 특성들을 잘 고려해서 Post-process와 Ensemble 도 생각해보면 좋을 것 같네요. 항상 프로젝트를 진행하실 때마다 좋은 경험들을 많이하셔서 빠르게 성장하실 것 같네요. 앞으로도 좋은 인사이트, 경험들을 많이 하셔서 좋은 결과가 남으셨으면 좋겠습니다. 수고많으셨습니다.\n\n개인 회고 T4226 한상준\n안녕하세요 상준님 대회기간동안 고생많으셨습니다. 회고를 보니 지난 문제에 대해서 다시 해석하시는 것이 매우 좋은 것 같습니다. 그 중 잘못된 상식이 독이된다는 이야기도 많이 공감이 되는 것 같습니다. 저도 많이 경험한 경우인데 저는 정의한 문제에 대한 해석을 위해서 실험을 근거로 증명과정이 없었고 거기에 너무 over-fit 하여 local minima를 벗어나지 못했던 것 같습니다. 데이터 사이언티스트들은 논리적인 실험을 근거로 자기 주장을 검증해내는 것이 주된 업이라 충분히 디테일해야하는 직군인 것 같습니다. 가끔은 문제들이 해결이 되지 않는다면 자신의 생각에 대해서도 다시 생각해보거나 증명을 하는 과정을 거치면 어떨까합니다. 이후 진행하는 Level 3 프로젝트는 조금 다른 방향으로 진행되어 정신이 없으시겠지만 개인적으로는 상준님에게 적합한 프로젝트라고 생각되어 많은 기대를 하고 있습니다. 마지막 프로젝트 잘 정리하셔서 좋은 결과가 따라오기를 바랍니다."},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/초보-운전자를-위한-안전-주행-보조-시스템---Wrapup-Report":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/초보-운전자를-위한-안전-주행-보조-시스템---Wrapup-Report","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/초보 운전자를 위한 안전 주행 보조 시스템 - Wrapup Report.md","title":"초보 운전자를 위한 안전 주행 보조 시스템 - Wrapup Report","links":[],"tags":[],"content":"\n팀원 소개 (\n프로젝트 개요\n\n기획 배경\n문제에 대한 수요 설문\n문제 정의\n서비스 시나리오\n\n\n기술 선정\n\nWhat we do?\nWhy 3D Object Detection?\nWhy monocular?\nHow to define Rule base Danger Level\n\n\n수행 절차 및 방법\n\n프로젝트 진행 Pipeline\nKITTI - Pretraining Dataset\n강건한(Robust) 융합 센서 객체 인식 자율주행 데이터 - Finetuning Dataset\nModel 선정\nSMOKE (Single-Stage Monocular 3D Object Detection via Keypoint Estimation)\nMetric 정의 - DDS(Danger Detection Score)\nModel Serving\n\n\n결과\n\n실험 및 결과\nInference Time Check\nModel Deploy\n\nWeb Demo(A100)\nEdge Device(Jetson Xavier)\n\n\n\n\n결론 및 토의\n\n프로젝트의 한계점\n향후 목표\n\n\n\n\n팀원 소개 (Team Notion)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이름역할링크김형석• Data Analysis, Coordinate Converting  • Visualization(3D-2D Projection, BirdeyeView)  • Inference Engine, Web Demo(Streamlit&amp;FastAPI)  • Model Train &amp; Inference  • Building a development environment(Server)Github    Notion    Linkedin이동훈• Data Analysis &amp; Converting, Coordinate Converting  • Visualization(BirdeyeView)  • Model Train &amp; Inference  • Model Research  • Presentation전지용• Data Analysis &amp; Converting, Coordinate Converting  • Visualization(Danger Object)  • Inference Engine, Web Demo(Streamlit&amp;FastAPI)  • Model Train &amp; Inference  • PresentationNotion    Github정원국• Data Analysis &amp; Converting, Coordinate Converting  • Visualization(BirdeyeView)  • Model Train &amp; Inference  • Model Research  • Presentation한상준• Model conversion  • Inference Engine, App Demo(tkinter)  • Model Train &amp; Inference  • Building a development environment(Server, Edge Device)Linkedin\n\n프로젝트 개요\n기획 배경\n운전 초보인 내겐 운전이 너무 힘들다! → 어떻게 하면 도움을 줄 수 있을까?\n\n브레이크 지금 밟아…?\n끼어들기 어떻게 대처하지?\n갑자기 사람이 튀어나오면 어쩌지..\n\n문제에 대한 수요 설문\n설문조사 참여자 : boostcamp AI Tech 캠퍼 80명 / 서울 소재 대학 재학생 60명\n\n\n\n\n초보 운전자의 운전 중 애로사항 중 급정거 및 끼어들기 대응이 약 60% 를 차지\n안전 주행 시스템의 이용 의향이 약 85% 로 수요가 높을 것으로 조사 됨\n\n문제 정의\n초보운전자의 사고 위험 향상 원인은?\n\n좁은 시야로 인한 상황 판단 미숙\n\n초보 운전자의 시야, 경력 운전자의 20% 수준\n좌우 탐색 시간은 경력 운전자의 25%에 불과\n\n\n차량의 거리 판단의 어려움\n\n전방차량과의 안전거리 확보 미숙\n\n\n\n서비스 시나리오\n초보 운전자를 위한 주행 보조 시스템\n\n초보 운전자가 차량에 탑승\n블랙박스를 주행 안전 모드로 설정\n주행 시작\n위험 상황 발생\n안전 시스템에 의해 단계별 경고 안내\n운전자는 경고를 확인하고 상황 판단\n\n\n기술 선정\nWhat we do?\n차량 검출\n\n거리 &amp; 방향 검출\n\n위험도 판정\n\n경고 전달\n\nWhy 3D Object Detection?\n거리, 방향 검출이 필요\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2D Detection3D Detection거리 탐지XO방향 탐지XOInference 속도빠름중간Task 난이도쉬움어려움\n\n2D Detection\n\n거리와 방향 같은 추가적인 정보\n\nDepth Estimation이나 장면 인식 등 추가 적인 Task 필요\n\n\n사용의 편의성을 떨어뜨리는 여러 조건 필요\n\n\n3D Detection\n\nObject의 좌표계 예측\n\nObject의 거리와 방향까지 함께 판단\n\n\nTask 난이도\n\n관련 공개 자료가 너무 적음\n기술들의 난이도가 높음\nTask에 대한 도메인 지식을 학습을 하는데 어려움\n\n\n\n\n\n\n2D Object detection\n\n3D Object detection\nWhy monocular?\n\nlidar센서 포함 Fusion 방식 vs Only Camera, Monocular 방식\n선정 Key Point\n\n서비스 Target (초보자)\n비용\n일정 수준의 성능\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLidarMonocularTarget 적합도낮음높음비용약 700만원약 30만원성능매우 높음중간Task 난이도높음높음\nHow to define Rule base Danger Level\n\n위험 상황의 정의 (모든 상황을 파악하기 어렵기 때문에 특정 상황으로 한정)\n\n옆 차선에서 끼어드는 경우\n앞 차와의 거리 조절이 안되는 경우\n\n\n위험 수준의 정의\n\n안전(Safe)\n조심(Warning)\n위험 (Danger)\n\n\n\n\n\n위험 측정 방식\n\n시속 60km 기준 : 제동거리 44m\nCase 1 : 옆 차선에서 끼어드는 경우\n\n50M 내 Yaw가 내 차선방향(±7º~30º)으로 들어올 때 → Warning\n25M 내 Yaw가 내 차선방향(±7º~30º)으로 들어올 때 → Danger\n\n\nCase 2 : 앞 차와의 거리 조절이 안되는 경우\n\n전방의 차량 간격이 50M 내로 줄어들 때 → Warning\n전방의 차량 간격이 25M 내로 줄어들 때 → Danger\n\n\n\n\n\n\n수행 절차 및 방법\n프로젝트 진행 Pipeline\n\nKITTI - Pretraining Dataset\n\n이미지 개수 : 14999장\n이미지 크기 : 1242 x 375\n데이터 구성 : streo, optical flow, visual odometry, 3D object Detection, 3D tracking\n데이터 특징 : 맑은 날씨\n클래스 구성 : car, pedestrian, cyclist, van, truck, tram, misc, person_sitting\n\n\n\nAnnotation 정보\n\nlabel (str) : 차, 보행자 등의 라벨 정보 문자열 (‘Car’, ‘Pedestrian’, …)\ntruncation (float) : 이미지상에서 물체가 잘려 있는 정도\nocclusion (int) : 폐섹 수준(camera 시야 기준으로 추측 됨), 물체가 가려진 정도\nalpha (float) : 관측각, 관측자(자율주행자동차) 기준 물체가 위치한 각도\nxmin (float) : image 상에서 물체를 감싸는 2d bbox의 left-x\nymin (float) : image 상에서 물체를 감싸는 2d bbox의 top-y\nxmax (float) : image 상에서 물체를 감싸는 2d bbox의 right-x\nymax (float) : image 상에서 물체를 감싸는 2d bbox의 bottom-y\nHeight (float) (y) : Camera 좌표계 상에서 물체의 높이(in meters)\nWidth (float) (z) : Camera 좌표계 상에서 물체의 너비(in meters)\nLength (float) (x) : Camera 좌표계 상에서 물체의 길이(in meters)\ntx (float) : Camera 좌표계 상에서 물체의 x(in meters)\nty (float) : Camera 좌표계 상에서 물체의 y(in meters)\ntz (float) : Camera 좌표계 상에서 물체의 z(in meters)\nry (float) : Camera 좌표계 상에서 물체의 yaw (pi:좌측 ~ 0:정면 ~ pi:우측)\n\n\n\n강건한(Robust) 융합 센서 객체 인식 자율주행 데이터 - Finetuning Dataset\n\n이미지 개수 : 360,000개\n이미지 크기 : 1920 X 1200\n데이터 구성 : Image , 2D 바운딩 박스, 2D 세그멘 테이션, 3D 바운딩 박스\n데이터 특징 : 2가지 촬영 시간대, 2가지 날씨, 5가지 도로 유형으로 매우 다양한 상황에서 촬영됨\n클래스 구성 : bicycle, bus, car, motorcycle, other vehicles, pedestrian, truck\n\n\n\nKitti Format 으로 Finetuning Dataset 변환\n\nAi-hub 좌표축을 KITTI 좌표축으로 변환\n시각화를 위한 Carlibration 수치 보정\nYaw(방향) 값 기준 통일\nClass 병합 (car, pedstrian, cyclist)\n\n\n최종 Camera 기준 Kitti format label Data 작성\n\n\nModel 선정\n\n선정 기준\n\nCamera Only로 사용 가능해야 함 → Monocular Model\nMMDetection 3D에서 사용 가능 → Smoke &amp; PGD\nReal Time Inference가 가능해야 함→ Inference Time 0.2s\n적정 수준의 성능\n\n\n\n\n→ SMOKE Model 최종 선택\nSMOKE (Single-Stage Monocular 3D Object Detection via Keypoint Estimation)\n2D bbox 예측하고 이를 바탕으로 3D bbox를 예측하는 기존의 방식과는 다르게 바로 3D bbox 예측\n\nOne-Stage architecture 제안\nmulti-step disentanglement approach 제안\n\n중심/크기/각도로 분리하여 3D bbox 예측\n\n\n\n\n\nMetric 정의 - DDS(Danger Detection Score)\n\n\n위험 기준 설정\n\n\n정면 차간 거리 (25m~50m)\n\n\n끼어들기 차량의 각도 (7도~30도)\n\n\n옆 차선의 차량과의 거리 (1.5m)\n\n\n\n\n경고 레벨에 따른 confusion matrix\n\n\n\nModel Serving\n\n\nWeb\n\n개발 목적 : Inference Engine 개발, 협업, 데모\n\n개발도중 Edge device 환경 구성에 문제 발생, Edge device 없이 Inference Engine 개발 필요\n원활한 협업을 위해 기존 개발 환경(ai-stages server)에서 바로 동작하고, 각자의 작업물을 병합하여 결과를 볼 수 있는 도구 필요\n다른 사람들에게 손쉽게 보여줄 수 있는 방법이 필요\n\n\n\n\nWeb Demo Flow\n\n\nInference Engine\n\nInference Engine Structure\n\nSet Engine Sequence\n\nRun Engine Sequence\n\n\nEdge Device (APP)\n\n개발 목적 : 프로젝트 실현 가능성 검증 , 향후 실험\n\n프로젝트의 실현 가능성을 확인하기 위해서는 Edge Device에서의 사용 가능 여부 검증 필요\n짧은 일정으로 실제 목표인 블랙박스에 Serving하지 못하는 상황, 그러나 가능성이 있는지 확인\n\n\n\n\n\n\nModel serving flow for edge divide(Jetson Xavier)\n\nInference flow in Jetson Xavier\n\n결과\n실험 및 결과\n\n→ Fine Tuning + Augmentation을 사용하여 프로젝트 목표에 맞게 결과를 도출\n\nNo1. Inference Image\n\nNo2. Inference Image\n\n\n\n\nInference Time Check\n\n\nCUDA 코어 개수 차이 10:1 / 소비 전력 차이 8:1\n\n→ 큰 차이에도 속도는 2배 정도 밖에 안 느려지지 않았고, 경량화 가능성 확인하였음\nModel Deploy\nWeb Demo(A100)\n\nLeft : only KITTI dataset / Right : Our Model(KITTI + Finetuning)\nEdge Device(Jetson Xavier)\n\n결론 및 토의\n프로젝트의 한계점\n\n현재 실시간으로 수집되는 data들은 고려되지 못함\n경고 자체가 Rull base로 이루어짐 (추론 결과값을 가지고 후처리)\n너무 가깝거나 바로 옆에 있는 자동차의 경우는 잘 탐지하지 못하는 경우도 존재\nMonocular 모델 자체의 성능 개선 한계가 존재\n\n향후 목표\n\nActive learning 또는 Self-supervised Learning을 통해 Data 관련 문제를 보완\n새롭게 정의한 Metric을 전문가의 피드백을 받은 후에 경고 자체를 학습하도록 구조 개선\n거리에 따라서 서로 다른 가중치를 주어 위험도를 학습하여 가까운 물체를 더 잘 탐지하도록 함\nKnowledge distillation, pseudo labeling 사용하여 모델의 성능 향상\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/초보-운전자를-위한-안전-주행-보조-시스템---개인-회고":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/초보-운전자를-위한-안전-주행-보조-시스템---개인-회고","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/초보 운전자를 위한 안전 주행 보조 시스템 - 개인 회고.md","title":"초보 운전자를 위한 안전 주행 보조 시스템 - 개인 회고","links":[],"tags":[],"content":"이번 프로젝트에서 나의 목표는 무엇이었는가?\n\nMLOps를 Pipeline을 구성해보기\nFrontend-Backend 구조로 서비스를 만들기\n사용해보지 않은 기술 사용해보기\n실험적 접근으로 모델 성능 향상시켜 보기\n\n나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?\n\nMLOps를 Pipeline을 구성해보기\n\nMLOps Pipeline 구성은 적용하지 못했습니다.\n\n\nFrontend-Backend 구조로 서비스를 만들기\n\nWeb Demo를 만들 때 Streamlit과 FastAPI를 사용하여 데모를 만들었습니다.\n\n\n==사용해보지 않은 기술 사용해보기==\n\n3D Object Detection을 했습니다.\n3차원 좌표계의 좌표계 간 변환을 했습니다.\n3D 공간의 BBox를 2D Plane에 Projection하고, 이미지에 시각화했습니다.\n\n\n실험적 접근으로 모델 성능 향상시켜 보기\n\n일정상 개발해야할 것이 많았고, 데이터 변환이 너무 늦게 완료되어 시도하지 못했습니다.\n\n\n\n내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?\n이번 프로젝트에서는 결과물을 내기 위해 필요한 전반적인 모든 부분들에 관여했습니다.\n\n좌표계 변환(Ridar to Camera, Camera to image)를 수행했습니다.\n\n로보틱스에서 배웠던 Forward Kinematics, 영상처리에서 배운 Homography가 코드 이해에 크게 도움이 됬습니다. 전혀 다른 분야에서의 배움이 이해에 도움이 된다는 사실이 매우 신기했습니다.\n\n\nImage 상에서의 3D BBox 시각화, Bird eyes view 시각화를 수행했습니다.\n\n3D Object Detection의 Inference 결과는 3차원공간(Camera Coordinate)을 기준으로 나옵니다. 그러므로 결과가 잘되었는지 아닌지를 알기 매우 어려웠고, 다시금 시각화의 소중함을 깨달았습니다.\n그러나 시각화 자체가 좌표계 변환이 필요하기 때문에 기존 2D Detection과 다르게 시각화 작업 자체가 하나의 진입장벽이라고 느껴졌습니다.\n\n\n==MMDet3D의 tools/test.py를 분석했고 이를 토대로 Inference만 할 수 있는 코드를 만들어 팀원들에게 공유했습니다.==\n\n다른 사람이 만들어놓은 도구는 처음에는 편하지만, 내가 원하는 기능을 추가하거나 구현할 때 매우 힘들어질 수 있다는 것을 다시 깨닫게 되었습니다.\n\n\n==Inference Engine을 만들었고 ONNX, TensorRT, MMDet3D 모델 class를 추상화하여 Model Factory를 만들었습니다.==\n\n\n파이썬으로도 여러 디자인패턴을 구현할 수 있다는 것이 흥미로웠습니다.\n\n\n딥러닝 파트가 프로젝트에서 차지하는 비중이 매우 적다는 것을 느꼈습니다.\n(물론 3D Object Detection이라고 하는 조금 특수한 기술을 사용하는 프로젝트여서 일 수도 있습니다)\n\n\n정말 큰 기업이어서 각자의 R&amp;R이 매우 확실하게 정의되어 있는 조직이 아니라면, 개발 능력이 없는 DL Engineer는 도태될 것이라고 느꼈습니다.\n\n\n\n==Streamlit과 FastAPI를 이용하여 Web Demo를 만들어 Model을 serving했습니다.==\n\n실시간으로 Video를 Inference 할 수 있는 구조를 만들어야 했는데, 기존 이미지 한 장만 Inference하는 것보다 더 어려웠고, 특히 Streamlit은 직접적으로 이미지 스트리밍을 지원하지 않아 불편했습니다.\nStreamlit은 빠르게 결과를 보거나 간이 서비스를 만들 때는 유용하고 실시간으로 갱신해야 하는 경우 부적절하다고 느꼈습니다.\n\n\n\n전과 비교해서, 내가 새롭게 시도한 변화는 무엇이고, 어떤 효과가 있었는가?\n\n이전에는 단순히 대회를 위해 모델 훈련을 위한 구조를 개발했다면, 이번에는 Streamlit과 FastAPI를 사용하여 실시간 Video Inference를 할 수 있는 Web Service를 개발했습니다.\n\nService를 돌리기 위해서 직접 Engine을 개발했해야 했고 처음으로 Python을 사용하여 Model, Coordinate Converter, Renderer 등 객체지향적으로 Class를 만들어 돌아가게 했습니다. 결과, Class별로 역할을 명확하게 구분됬고 다른 팀원들의 작업을 용이하게 했습니다.\n\n\n==Web Server는 직접 개발한 Infrence Engine으로 결과를 반환하는 구조로 만들었고, 이전에는 Pytorch Model만 Inference 해봤다면 이번에는 ONNX와 TensorRT도 Inference 해봤습니다.==\n\n인퍼런스를 무엇으로 할 것인지에 따라서 class를 다르게 불러와야 하지만 사용하는 단계에서 class별로 개별적인 사용을 할 필요는 없다고 생각했고 MMdet3D, TensorRT, ONNX Model을 ModelBase로 추상화해서 Model Factory를 통해 불러와 사용하도록 구현했습니다. 결과, 전혀 다른 Inference Library를 사용하지만 모두 같은 방식으로 사용할 수 있도록 만들어 유지보수를 용이하게 했습니다.\n\n\n\n마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?\n3D Object Detection이라는 것이 진입장벽이 생각보다 더 높았습니다. 팀원들과 저는 기반지식을 찾는 것과 데이터 이해 및 변환 등에 많은 시간을 할애해야 했고, 결과 매우 촉박한 스케줄로 개발을 진행해야 했습니다.\n때문에 시간이 매우 부족했고, 아래 세 가지 아쉬운 점이 남았습니다.\n\n모델을 많이 다뤄보지 못한 것\n실제 Edge Device에서 Natave App을 통해 동작하게 하지 못한 것\nMLOps Pipeline을 적용해보지 못한 것\n\n한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?\n\n너무 어려운 기술, 사용해보지 않은 기술로 프로젝트를 시작한다면 생각보다 목표를 보수적으로 잡아야 할 수 있다고 느꼈습니다. 다음에는 사용해본 기술을 토대로 프로젝트를 진행하고 점진적으로 목표를 확장해가고 싶습니다.\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-1/팀-프로젝트-개인-아이디어-정리-1":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-1/팀-프로젝트-개인-아이디어-정리-1","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/팀 프로젝트 개인 아이디어 정리 1/팀 프로젝트 개인 아이디어 정리 1.md","title":"팀 프로젝트 개인 아이디어 정리 1","links":[],"tags":[],"content":"\nLEGO AI Renderer\n\n개요\n제공하려는 서비스\n데이터의 취득\nPipeline\n\n\n\n\nLEGO AI Renderer\n개요\n\nLEGO는 남녀노소 즐길 수 있는 장난감이며, 많은 사람들이 어린 시절 LEGO 혹은 그 유사제품을 가지고 놀아본 경험이 있을 것이다.\n기술의 발전으로, LEGO 또한 컴퓨터로 즐길 수 있게 되었다.\n이는 LEGO CAD라고 불리며 LEGO사에서 만든 LDD부터 커뮤니티가 자체적으로 만들어낸 StudIO까지 수 많은 버전이 존재한다.\n사람들은, 3D CAD로 LEGO를 설계할 수 있는 툴을 커뮤니티 레벨에서 만들어낼 정도로 엄청난 열정을 갖고 있지만 문제가 하나 존재한다.\n\nRendering 전\n\nRendering 후\n한 개의 LEGO Model에는 적게는 수십개에서 많게는 수천, 수만개의 부품이 들어간다.\n이를 3D 공간상에 동적으로 불러와서 어셈블리를 수행하므로, 당연히 구조상 실시간 렌더링이 불가능하며 최대한 추상화하고 심플하게 표현할 필요가 있다.\n따라서 사람들은 먼저 설계를 완료한 뒤, 별도의 렌더링 툴을 이용해서 현재 Camera View를 기준으로 렌더링을 수행한다.\n이 때, 렌더링 해상도를 키울수록 그에 비례해서 필요한 시간이 매우 증가하게 된다.\n이 파이프라인은 LEGO CAD에 국한된 것이 아니며 산업에 사용되는 상용 3D CAD인 카티아, 프로이, 솔리드웍스 등도 동일하며, 해당 프로그램들도 마찬가지로 동일한 문제를 내포하고 있다.\n그렇다면 저해상도로 Rendering해서 빠른 속도로 Rendering 이미지를 얻고, AI를 이용해 고해상도로 복원해줄 수 없을까?\n혹은, AI를 이용해서 Rendering 이미지를 생성할 수 없을까?\n제공하려는 서비스\nRendering 전의 이미지를 업로드하면, Rendering을 수행해서 다운로드할 수 있는 서비스를 제공한다.\n데이터의 취득\n이미 만들어진 많은 LEGO Model이 존재하며, 이를 사용해서 Rendering을 수행할 수 있다.\n따라서 Model을 Load후, 동일한 View에서 Rendering 전 이미지와 Rendering 후의 이미지를 획득할 수 있다.\nPipeline\n\nCreate Rendering Image\nUpload to database\nPeed new Data and Train\nRaw Image → Inference(Style Gen) → Low Resolution Rendering Image\nEvaluation\nLow Resolution Rendering Image → Inference(Super Resolution) → High Resolution Rendering Image\nEvaluation\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-2/이미지/이미지":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-2/이미지/이미지","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/팀 프로젝트 개인 아이디어 정리 2/이미지/이미지.md","title":"이미지","links":[],"tags":[],"content":"\n\n"},"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-2/팀-프로젝트-개인-아이디어-정리-2":{"slug":"vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-2/팀-프로젝트-개인-아이디어-정리-2","filePath":"vault/Notion/DB/DB Blog Post/Naver Connect - Boostcamp AI Tech 4기/팀 프로젝트 개인 아이디어 정리 2/팀 프로젝트 개인 아이디어 정리 2.md","title":"팀 프로젝트 개인 아이디어 정리 2","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/팀-프로젝트-개인-아이디어-정리-2/이미지/이미지"],"tags":[],"content":"Web상에서 획득할 수 있는 데이터는?\n어떤 키워드에 대한 무작위 검색 정보\n\n\nWeb Crawling을 통한 이미지 획득\n\n\n                  \n                  Web Crawling -이론 및 실습 \n파이썬을 활용한 웹 크롤링 여기서 시리즈로 시작해보자!\nvelog.io/@changhtun1/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9B%B9-%ED%81%AC%EB%A1%A4%EB%A7%81\n                  \n                \n\n\n\nAI Painter[링크]\n이미지\n\nColorization\n\nImage Colorization: A Survey and Dataset\n\n\nHint-Guided Anime Colorization\n\narxiv.org/pdf/1808.03240.pdf\n\n\n\n\n\n\n\nYoutube 영상\n\n\nColorization, super resolution, motion interpolation\n\n\n현대 축구경기 영상을 사용한 과거 축구 영상 고화질 복원\n\n\n과거 애니메이션, 영화 컬러 및 고화질 복원\n\n저작권 소멸 영상들이 존재\n\n\nwww.youtube.com/channel/UCWjw68d5tDf975za7XWVUaA\n\n\nwww.youtube.com/channel/UCEVI45za5i1hS8jA-mGR66A\n\n\n\n\n\n\n\n\nAI Hub\n\n배송용 로봇 자율주행 [링크]\nNeRF를 이용한 Real Map Scanning(드론 주행 영상 사용?)\n\nNeRF 단점과 후속 연구 소개\nNeRF Factory\n카카오 브레인에서 NeRF Factory 출시 → 쉽게 사용할 수 있게 만들어진 pytorch 기반 NeRF Lib\n\n\nNeRF를 이용한 자율주행시뮬레이션용 3D 공간 제작\n\n\n\nkaggle\n\n프로젝트화 가능한 Dataset 없어보임\n\n\n\netc\n\nNeRF를 이용한 Lego Rendering Video to Lego Render 3D model 제작 서비스\nNeRF를 이용한 3D Scan Video 제작 서비스\n\nyoutu.be/ZBBBUQpxaZ4\n\n\nRealtime Face landmark detection을 사용한 Vitual Character Rendering\n\n너무어렵겠다\nscience3m.tistory.com/446\nwww.ppap.blog/버튜버-하는데-필요한-비용/\n\n\n\n\n\nCCTV로 획득할 수 있는 데이터는?\n특정 환경에 대한 연속적 정보\n\noptical flow를 사용한 야생동물 탐지\n"},"vault/Notion/DB/DB-Blog-Post/Neck/Neck":{"slug":"vault/Notion/DB/DB-Blog-Post/Neck/Neck","filePath":"vault/Notion/DB/DB Blog Post/Neck/Neck.md","title":"Neck","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10"],"tags":[],"content":"\n참조\n등장 배경\nNeck\n\n\n참조\nvelog.io/@peterkim/Object-Detection에서-말하는-Backbone-Neck-Head\nWeek 10\n등장 배경\n기존 2-Stage Detector를 사용하면서 “왜 마지막 feature map 만을 사용해야하는가?”에 대한 의문에서 시작하여 연구를 통해 Neck이라는 구조가 생겨났습니다.\n\n기존 2-Stage Detector의 Pipline\n\nInput → Backbone → RPN→ Prediction\nBackbone에서 마지막 Feature Map만을 사용\n\n\n연구를 통해 중간 과정의 Feature map들도 사용할 수 있겠다는 결론\n중간단계의 Feature map들도 활용 시작\n크기 별로 feaute를 추출하기 때문에 검출에 유리\n\nLow Level의 Feature는 Semantic 정보가 약하고, Local한 정보가 강함\nHigh Level의 Feature는 Semantic 정보는 강하나 Local한 정보가 약함\n\n\n다양한 크기의 객체를 더 잘 탐지하기 위해 필요\n\nNeck\n\nBackbone에서 extract된 feature들을 적절하게 조화시키는 계층입니다.\n\nFPN, PANet, BiFPN 등이 대표적인 예시\n이전 map을 upsampling하여 크기 키우고, backbone에서의 feature map을 concat 등의 방식으로 같이 반영\nTop-down 방식, bottom-up 방식 모두 존재\n"},"vault/Notion/DB/DB-Blog-Post/Norm,-L1-Norm,-L2-Norm":{"slug":"vault/Notion/DB/DB-Blog-Post/Norm,-L1-Norm,-L2-Norm","filePath":"vault/Notion/DB/DB Blog Post/Norm, L1 Norm, L2 Norm.md","title":"Norm, L1 Norm, L2 Norm","links":[],"tags":[],"content":"\n참초\nNorm이란?\n\nL1 Norm\nL2 Norm\n\n\n\n\n참초\nlight-tree.tistory.com/125\nNorm이란?\nNorm 은 벡터의 크기(혹은 길이)를 측정하는 방법(혹은 함수)입니다.\n두 벡터 사이의 거리를 측정하는 방법이기도 합니다.\nL1 Norm\nTaxicab Norm 혹은 맨허튼 노름(Manhattan norm)으로 불림\n벡터 요소에 대한 절대값의 합을 의미\n|+|-3|+|4|+|5|) = 15$$\n\n### L2 Norm\n\n$$x=[-1,2,3]\\\\||x||_2 = sqrt((-1)^2+(2)^2+(3)^2)$$"},"vault/Notion/DB/DB-Blog-Post/ONNX(Open-Neural-Network-Exchange)란/ONNX(Open-Neural-Network-Exchange)란":{"slug":"vault/Notion/DB/DB-Blog-Post/ONNX(Open-Neural-Network-Exchange)란/ONNX(Open-Neural-Network-Exchange)란","filePath":"vault/Notion/DB/DB Blog Post/ONNX(Open Neural Network Exchange)란/ONNX(Open Neural Network Exchange)란.md","title":"ONNX(Open Neural Network Exchange)란","links":[],"tags":[],"content":"\n참조\n개요\nONNX(Open Neural Network Exchange)란?\nONNX로 변환하면 실제로 어떤 결과물을 얻을까?\n무슨 변환이 이루어질까?\nPyTorch Model을 ONNX Model로 변환하기\nONNX의 opset_version\n마치며\n\n\n참조\nonnx.ai/\nchat.openai.com/\ngaussian37.github.io/dl-pytorch-deploy/#onnx에-shape-정보-저장-1\ntech.kakaopay.com/post/model-serving-framework/\ntutorials.pytorch.kr/advanced/super_resolution_with_onnxruntime.html\nonnx.ai/onnx/index.html\n개요\n최근, 회사에서 서비스화를 진행하는 AI 모델의 경량화를 맡게 되었습니다. 모델은 Nvidia GPU를 사용하는 서버에서 구동될 예정이어서 TensorRT로 모델을 경량화 하는 것으로 방향을 잡았습니다.\nTensorRT를 사용하여 경량화 하기 위해서는 우선 모델을 ONNX로 변환해야 할 필요가 있었습니다. 물론, 현재 Torch-TensorRT라는 것이 있긴 하지만, 제가 원하는 Int8 양자화 기능을 찾지 못했습니다. 때문에 겸사겸사 ONNX에 대하여 스터디해 보았습니다.\nONNX(Open Neural Network Exchange)란?\n\nONNX는 ML Model을 표현하기 위한 Format의 한 종류이며, Open Source로 공개되어 있습니다.\nAI 개발자가 다양한 프레임워크, 도구, 런타임 및 컴파일러와 함께 모델을 사용할 수 있도록 공통 연산자 세트(기계 학습 및 딥 러닝 모델의 빌딩 블록)와 공통 파일 형식을 정의합니다.\n\n세상에는 현재 많은 ML/DL Framework(ex: Pytorch, TensorFlow, Caffe)가 존재합니다. 우리는 이 Framework을 토대로 모델을 훈련하고, 그 결과 훈련된 모델을 얻습니다.\n여기서 ‘훈련된 모델’은 어떤 Framework를 사용해서 훈련했느냐에 따라서 그 내부적인 구조와 구현도 제각각입니다. 똑같은 Resnet50 아키텍처라도 Framework간에 서로 호환되지 않습니다. 예를 들어 Pytorch에서 훈련한 Resnet50을 TensorFlow에서는 사용할 수 없습니다.\n하지만, 서로에게 약속된 공용 포맷이 있다면 변환을 통해 사용할 수 있을 것입니다.\n==예) Pytorch Resnet50 → ( 어떤 공용 포맷 ) → TensorFlow Resnet50\n\nONNX는 위 예시에서의 ‘어떤 공용 포맷’ 역할을 해줍니다.\n==\n\n\nONNX는 딥러닝 모델을 표준화된 형식으로 표현하고 공유할 수 있게 해주는 오픈소스 프로젝트입니다. ONNX는 다양한 딥러닝 프레임워크 간에 모델을 쉽게 변환하고 공유할 수 있도록 함으로써, 다른 프레임워크를 사용하는 환경에서 모델의 이식성을 높일 수 있습니다.\nONNX는 딥러닝 모델을 표현하는데 사용되는 중간 표현(Intermediate Representation) 형식입니다. 이 중간 표현은 모델의 아키텍처, 가중치, 그래프 연산 등을 포함하고 있어 다양한 딥러닝 프레임워크에서 해석할 수 있습니다. ONNX를 사용하면 훈련된 모델을 변환하고, 추론 엔진에서 실행하고, 다른 프레임워크로 내보낼 수 있습니다.\nONNX로 변환하면 실제로 어떤 결과물을 얻을까?\n우리가 기존에 모델을 불러와 사용할 때 두 가지가 필요합니다.\n\n\n모델의 아키텍쳐의 정보(ex: Resnet50.py)\n\n\n그 아키텍처의 Weight 파일(ex: Resnet.50_epoch10.pth\n\n\n그러나 ONNX로 변환하면 이 두 가지가 합쳐져서 .onnx 파일 하나만 생성됩니다.\n무슨 변환이 이루어질까?\n우리가 만약 PyTorch로 훈련한 Resnet18을 ONNX로 변환한다면, 내부적으로는 어떤 일을 하는 것일까요? ONNX는 ‘공용 포맷’ 입니다. 따라서 어떤 Framework의 코드도 사용할 수 없습니다. 특정 Framework의 코드를 사용한다면, 이미 공용이 아니고 해당 Framework에 종속된 것이니까요. 따라서 ONNX로 변환할 시에는 우리가 만든 모델의 아키텍처를 하나 하나 뜯어서 ONNX가 자체적으로 제공하는 Operator로 변환하는 작업을 합니다.\n우리가 직접 Resnet을 구현해야 한다면 대부분 Residual Block을 구현한 뒤, Residual Block을 가져와 Resnet을 종류별로 구현할 것입니다. 이렇게 해야 코드가 늘어나는 것을 피할 수 있고, 쓰는 우리도 편리하고, 이해하기도 편합니다. 이곳을 보시면 TorchVision의 Resnet18에 대한 구현을 볼 수 있습니다.\n이 이야기를 한 이유는 ONNX로 변환한 결과는 위 모듈 구조들이 모두 사라지고, 연산을 위한 Operator들 간의 Graph가 남기 때문입니다. 이곳에는 Resnet18을 onnx로 변환한 모델이 있습니다. 이 파일을 다운로드한 뒤, Netron이라는 ONNX Viewer로 열어보면 아래와 같은 구조를 볼 수 있습니다.\n\n이 결과를 통해 알 수 있는 점은 다음과 같습니다.\n\nONNX로 변환하면, 사람이 짜놓은 모듈 구조들이 모두 무너진다.\n모델이 복잡해질수록 ONNX 파일만 보고 모델의 구조를 파악하는 것은 어려울 것이다.\nONNX를 다시 변환 전의 상태로 변환할 수 없다.\n\n\nResnet18.onnx를 가지고 TorchVision의 Resnet18처럼 모듈화된 아키텍처를 만들 수 없다.\n\n\n\n\n따라서, ONNX 변환을 통해 다양한 Framework에서 사용할 수 있다고 할지라도, 이는 Inference만 해당될 것입니다. (사실 Inference가 가능하다는 것은 Train도 가능하다는 것을 의미하지만, 모델을 ONNX 변환한 뒤 훈련할 이유는 없다고 생각합니다)\nPyTorch Model을 ONNX Model로 변환하기\n아래 코드는 Pytorch 공식 문서에서 제공하는 것으로, Super Resolution Model을 ONNX로 export 하는 예시를 보여줍니다.\n# 필요한 import문\nimport io\nimport numpy as np\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx\nimport torch.nn as nn\nimport torch.nn.init as init\n \n \n# PyTorch에서 구현된 초해상도 모델\nclass SuperResolutionNet(nn.Module):\n    def __init__(self, upscale_factor, inplace=False):\n        super(SuperResolutionNet, self).__init__()\n \n        self.relu = nn.ReLU(inplace=inplace)\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n \n        self._initialize_weights()\n \n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n \n    def _initialize_weights(self):\n        init.orthogonal_(self.conv1.weight, init.calculate_gain(&#039;relu&#039;))\n        init.orthogonal_(self.conv2.weight, init.calculate_gain(&#039;relu&#039;))\n        init.orthogonal_(self.conv3.weight, init.calculate_gain(&#039;relu&#039;))\n        init.orthogonal_(self.conv4.weight)\n \n# 위에서 정의된 모델을 사용하여 초해상도 모델 생성\ntorch_model = SuperResolutionNet(upscale_factor=3)\n \n# 미리 학습된 가중치를 읽어옵니다\nmodel_url = &#039;s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth&#039;\nbatch_size = 1    # 임의의 수\n \n# 모델을 미리 학습된 가중치로 초기화합니다\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n \n# 모델을 추론 모드로 전환합니다\ntorch_model.eval()\n \n# 모델에 대한 입력값\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\ntorch_out = torch_model(x)\n \n# 모델 변환\ntorch.onnx.export(torch_model,               # 실행될 모델\n                  x,                         # 모델 입력값 (튜플 또는 여러 입력값들도 가능)\n                  &quot;super_resolution.onnx&quot;,   # 모델 저장 경로 (파일 또는 파일과 유사한 객체 모두 가능)\n                  export_params=True,        # 모델 파일 안에 학습된 모델 가중치를 저장할지의 여부\n                  opset_version=10,          # 모델을 변환할 때 사용할 ONNX 버전\n                  do_constant_folding=True,  # 최적화시 상수폴딩을 사용할지의 여부\n                  input_names = [&#039;input&#039;],   # 모델의 입력값을 가리키는 이름\n                  output_names = [&#039;output&#039;], # 모델의 출력값을 가리키는 이름\n                  dynamic_axes={&#039;input&#039; : {0 : &#039;batch_size&#039;},    # 가변적인 길이를 가진 차원\n                                &#039;output&#039; : {0 : &#039;batch_size&#039;}})\n위 과정을 요약하면 다음과 같습니다.\n\nPytorch 모델 인스턴스를 생성하고, 학습된 가중치를 모델에 로드한다.\n모델 입력 값을 선언한다.\n모델을 추론 모드로 전환하고, 선언한 입력 값을 1회 인퍼런스 한다.\n모델 인스턴스와 모델 입력 값을 파라미터로 onnx로 export 한다.\n\nONNX의 opset_version\n위 과정에서 눈여겨 볼 부분은 onnx로 export하는 부분입니다. 잘 보면 파라미터 중에 아래와 같은 문구가 있습니다.\nopset_version=10, # 모델을 변환할 때 사용할 ONNX 버전\nONNX 변환에 별도로 버전을 파라미터로 준다니, 버전은 높으면 무조건 좋은 것이 아닐까요? 해답은 공식 문서에 있었습니다. 이곳에는 ONNX가 제공하는 Operator가 있습니다. 아래 Table은 그 일부를 발췌한 것입니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noperatorversionsdifferencesAbs13, 6, 113/6, 13/1, 6/1Acos7Acosh9Add14, 13, 7, 6, 114/13, 14/7, 13/7, 14/6, 13/6, 7/6, 14/1, 13/1, 7/1, 6/1And7, 17/1\n앞서 ONNX의 변환은 해당 Framework의 operator를 ONNX operator로 변환하는 과정이라고 설명드렸습니다. 이것은 즉 ONNX가 지원하지 않는 Operator를 모델에서 사용했다면 변환할 수 없다는 것을 의미합니다. Table을 쭉 훑어보면, DeformConv Operator(deformable convolution)의 경우 19 버전에서 추가되었으므로, 그 이전 버전으로는 변환할 수 없습니다.\n또한 Table의 우측 Column은 같은 Operator라도 버전에 따라서 차이가 존재할 수 있음을 암시합니다. 이 링크는 위 Table에서 Abs operator의 13과 6 버전의 차이에 관해 설명된 글입니다. 내부 구현이 조금 달라진 것을 알 수 있습니다. 따라서 opset_version에 따라 모델의 성능이 달라질 수 있다는 것을 알 수 있는 부분입니다.\n마치며\n이번 포스팅에서는 ONNX의 기초 개념과 onnx 파일 내부가 어떻게 되어 있는지 알아보았고 Pytorch 모델을 ONNX로 변환하는 코드에서 “opset_version”이 의미하는 것이 무엇인지를 다뤄보았습니다.\nPyTorch를 배우면서 느꼈던 생각이\n“작은 모델이라면 상관 없지만 복잡하고 큰 모델일수록 여러 script의 모듈을 참조하고 혹은 추가 패키지까지 설치하게 되는데 서빙하는 단에서 이 종속성을 모두 챙겨야 한다면 너무 불편할 것 같다”\n였습니다.\n하지만 현업에서 ONNX로 export하여 서빙하는 곳들이 있다는 것을 알게 되었고, 이번에 ONNX에 대해 공부하게 되면서 그 이유를 조금이나마 알게 된 것 같습니다.\nONNX로 변환하게 되면 더 이상 수정하기 어려운 상태가 되지만 별도로 다른 부속품들을 챙길 필요 없이 onnx 파일과 파일을 Inference 할 수 있는 Engine만 있으면 되기 때문입니다.\nONNX에는 ONNX-Runtime이라는 강력한 인퍼런스 엔진이 있는데, 이것도 굉장히 매력적인 프로젝트라고 느껴집니다. 때문에 다음에 기회가 되면 다뤄보고 싶습니다."},"vault/Notion/DB/DB-Blog-Post/ONNX-Runtime은-무엇일까/ONNX-Runtime은-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/ONNX-Runtime은-무엇일까/ONNX-Runtime은-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/ONNX-Runtime은 무엇일까/ONNX-Runtime은 무엇일까.md","title":"ONNX-Runtime은 무엇일까","links":["vault/Notion/DB/DB-Blog-Post/ONNX(Open-Neural-Network-Exchange)란/ONNX(Open-Neural-Network-Exchange)란"],"tags":[],"content":"\n참조\n개요\nONNX-Runtime\nONNX-Runtime이 지원하는 환경\nONNX-Runtime Project는 누가 관리하고 있을까?\nONNX-Runtime 설치 및 사용해보기\n마치며\n\n\n참조\nonnxruntime.ai\nchat.openai.com\n개요\n회사에서 Model을 Serving하는 일을 맡으면서, ONNX를 공부하게 되었습니다. 그러나 변환된 onnx 파일을 사용(인퍼런스) 하기 위해서는 Pytorch, Tensorflow와 같은 Engine이 추가적으로 필요합니다. 따라서 이번에는 onnx 파일을 인퍼런스 할 수 있는 engine 중 하나인 onnx-runtime에 대해서 조사해 보았습니다.\n관련 글 : ONNX(Open Neural Network Exchange)란\nONNX-Runtime\n\nONNX Runtime은 오픈 소스 인퍼런스 엔진으로, 딥 러닝 모델의 추론(예측)을 실행하기 위한 플랫폼입니다. ONNX Runtime은 ONNX 모델을 실행하기 위한 최적화된 엔진으로 여러 하드웨어 및 운영 체제에서 딥 러닝 모델을 효율적으로 실행할 수 있습니다.\nONNX-Runtime이 지원하는 환경\n\n위 이미지는 ONNX Runtime 공식 페이지에 있는 Table을 캡쳐한 것 입니다. Training Tab도 보이지만, 이 글에서는 다루지 않습니다.\n위 Table은 상당히 중요한데, 각 Platform 별로 지원되는 API가 다르며 Architecture, HW Acceleration에 따라서도 다릅니다.\n따라서 자신이 인퍼런스 할 환경에서 사용이 가능한지 체크할 필요가 있습니다.(예: IOS는 Python이 지원되지 않습니다.)\n\nPlatform은 OS 입니다. Windows부터 시작하여 심지어 web browser까지 매우 다양한 플랫폼을 지원합니다.\nAPI는 사용할 수 있는 프로그래밍 언어입니다.\nArchitecture는 CPU Architecture의 종류입니다.\n\n자세한 의미는 이 글을 참조해주세요.\n\n\nHW Acceleration은 실제 연산(인퍼런스)을 할 때 사용할 도구입니다. CPU로 할 시, CPU에서 연산을 수행하게 되며 CUDA로 할 시, CUDA를 사용할 수 있는 GPU에서 연산을 수행하게 됩니다.\n\n생각보다 매우 많은 가속기가 지원되서 놀랐습니다. 예로, ROCm을 지원하므로(잘 되는지는 모르겠습니다만..) 가지고 있는 device가 nvidia gpu가 아니더라도 Model Inference를 할 수 있습니다.\n\n\n\nONNX-Runtime Project는 누가 관리하고 있을까?\ngithub.com/microsoft/onnxruntime\nTable을 보면, 절대 개인이 할 수 있는 프로젝트가 아니라는 것이 느껴졌습니다. 그런데도 Open Source라니 누가 관리하고 있는지 궁금해졌습니다.\ngoogle에 ‘onnx-runtime github’를 검색하면 바로 나오는데, 바로 MS가 관리하고 있었습니다.\nMS를 사랑할 수 밖에 없는 이유가 하나 더 늘었습니다..\nONNX-Runtime 설치 및 사용해보기\nONNX-Runtime의 설치는 사용 환경에 따라 다르겠지만, 이 글은 python에서의 사용을 기준으로 작성되었습니다.\n설치는 pip를 이용하여 바로 수행할 수 있습니다. 여기서 주의할 점은 cpu용과 gpu용 패키지가 다르다는 것입니다.\n\n\nCPU Inference용 런타임 설치\npip install onnxruntime\n\n\nGPU Inference용 런타임 설치\n\ngpu용 런타임을 설치했다고 해서 cpu 인퍼런스가 안되는 것은 아니므로 gpu 가속이 가능하신 분들은 고민하지 마시고 gpu 버전을 설치하시면 됩니다.\n\npip install onnxruntime-gpu\n\n\n인퍼런스 코드 예시\nimport onnxruntime as ort\nimport numpy as np\nx, y = test_data[0][0], test_data[0][1]\nproviders=[&#039;CUDAExecutionProvider&#039;, &#039;CPUExecutionProvider&#039;]\nort_sess = ort.InferenceSession(&#039;fashion_mnist_model.onnx&#039;, providers)\noutputs = ort_sess.run(None, {&#039;input&#039;: x.numpy()})\n \n# Print Result \npredicted, actual = classes[outputs[0][0].argmax(0)], classes[y]\nprint(f&#039;Predicted: &quot;{predicted}&quot;, Actual: &quot;{actual}&quot;&#039;)\n인퍼런스 방법은 매우 간단합니다.\n먼저 ‘InferenceSession’이라는 인스턴스를 생성합니다. providers는 연산에 사용할 장치 후보입니다. 만약 CUDA Excution Provider를 사용할 수 없다면 CPU Excution Provider를 사용합니다.\n인스턴스의 ‘run’ 메소드에 입력 값을 전달하면 됩니다. 이 때, 첫 번째 값은 None이고 두 번째 값은 {’input’: x.numpy()} 로 되어 있습니다. None은 출력 값을 선택할 수 있게 해주는 파라미터인데 자세한 내용은 api 문서를 참조하여 주시기 바랍니다. 두 번째 값은 dict 형태로 되어있습니다. 여기서 key에 해당하는 값은 onnx 를 생성할 때 지정했던 입력 파라미터명 입니다. 그리고 value에 해당하는 값은 실제 입력 값 입니다.\ntorch를 사용해보신 분들은 ~.to(device)가 없어서 의아해 하실 수 있습니다. onnx-runtime는 위와 같이 입력해도 내부에서 알아서 gpu로 데이터를 변환해줍니다.\n그리고 outputs 또한 마찬가지로 연산을 수행한 장치의 데이터가 아닌, 우리가 바로 다루고 처리할 수 있는 ram에 존재하는 데이터로 돌려줍니다.\n\n\n성능 최적화 - I/O Binding\n\n\n위 방식는 처음엔 편리하지만, 최적화와 고속처리가 필요 할 때 문제가 될 수 있습니다.(메모리 관련 처리가 어떻게 되고 있는지 불확실하고, 더 low level에서의 제어하고 싶을 때가 있습니다) 이 부분은 문서에서 다음과 같이 말하고 있습니다.\n\nCPU가 아닌 실행 공급자로 작업할 때는 그래프를 실행하기 전에(Run() 호출) 대상 장치에 입력(및/또는 출력)을 정렬하는 것이 가장 효율적입니다(사용된 실행 공급자에 의해 추상화됨). 입력이 대상 장치에 복사되지 않으면 ORT는 Run() 호출의 일부로 CPU에서 입력을 복사합니다. 마찬가지로 출력이 디바이스에 미리 할당되지 않은 경우 ORT는 출력이 CPU에서 요청된 것으로 가정하고 Run() 호출의 마지막 단계로 디바이스에서 복사합니다. 이것은 그래프의 실행 시간을 잠식하여 대부분의 시간이 이러한 복사본에 소비될 때 ORT가 느리다고 생각하도록 사용자를 오도합니다.\n이 문제를 해결하기 위해 IOBinding이라는 개념을 도입했습니다. 핵심 아이디어는 \nRun()을 호출하기 전에 입력을 장치에 복사하고 출력을 장치에 미리 할당하도록 정렬하는 것입니다. IOBinding은 모든 언어 바인딩에서 사용할 수 있습니다.\n\n\n\n요약하자면 미리 메모리를 할당할 수 있다는 것을 의미합니다.\n\n\n더 정확한 설명과 사용예시는 이 문서를 참조하시면 됩니다.\n\n\n\n\n마치며\n위 onnx-runtime을 지원하는 환경에 대한 테이블을 보고 든 생각은 적어도 Model Inference에 있어서는 Python을 고집 할 필요가 없지 않을까? 라는 생각이었습니다. 특히 제가 하고있는 일은 고속처리가 필요해서 컴퓨팅 자원을 최대한 활용할 수 있어야 하기 때문에 python이라는 언어로는 한계를 느끼고 있습니다.\n다음에 기회가 되면 다른 언어에서 문제없이 잘 인퍼런스 되는지 확인해보려고 합니다."},"vault/Notion/DB/DB-Blog-Post/Object-Detection-History(2001~2022)/Object-Detection-History(2001~2022)":{"slug":"vault/Notion/DB/DB-Blog-Post/Object-Detection-History(2001~2022)/Object-Detection-History(2001~2022)","filePath":"vault/Notion/DB/DB Blog Post/Object Detection History(2001~2022)/Object Detection History(2001~2022).md","title":"Object Detection History(2001~2022)","links":[],"tags":[],"content":"\n참조\nObject Detection History(2001~2022)\n\n\n참조\n\n\n                  \n                  Object Detection in 20 Years: A Survey \n                  \n                \n\n\nObject detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years.\narxiv.org/abs/1905.05055\n\n\n\nyeomko.tistory.com/14\nyeomko.tistory.com/15\nherbwood.tistory.com/18\nyeomko.tistory.com/20\ndeep-learning-study.tistory.com/504\ntalktato.tistory.com/m/19\nm.blog.naver.com/phj8498/222345095231\nkeyog.tistory.com/32\nwikidocs.net/145910\nObject Detection History(2001~2022)\n\n\n\nRCNN\n\n선택적 검색을 통해 일련의 객체 Proposal(객체 후보 상자)을 추출\nProposal은 고정된 크기의 이미지로 재조정되고 ImageNet(예: AlexNet)에서 사전 훈련된 CNN 모델로 Feature 추출\nLinear SVM 분류기로 각 영역 내 객체의 존재를 예측하고 객체 범주를 인식하는 데 사용\n한 이미지에서 2000개 이상의 Proposal이 있고, 이를 모두 고정 크기로 조절한 뒤 CNN을 통과해야 하기 때문에 매우 느린 속도를 보임\n\n\n\nSPPNet\n\n\nSpatial Pyramid Pooling Layer 구조를 제안, 입력 이미지 크기와 상관 없이 fixed-length represntation(고정 크기의 vector) 생성 가능\n\nFeature Map에서 21 bin = [4x4, 2x2, 1x1] 등으로 고정 크기로 Feature를 Max Pooling 하는 Layer\n\n\nFC Layer 때문에 수행하던 crop / warp의 필요성이 사라져 속도 향상\n\n(RCNN 대비 20배 빠른 속도)\n\n\n\n\n\nFast RCNN\n\nRCNN이 selective search로 찾아낸 모든 RoI에 대해서 CNN inference를 하는 문제를 CNN inference를 전체 이미지에 대하여 1회만 수행하고, 이 피쳐맵을 공유하는 방식으로 해결\nROI Pooling 제안\n더 이상 SVM을 사용하지 않고 end-to-end 훈련이 가능해짐\nR-CNN보다 200배 이상 빠른 Inference 속도\n\n\n\nFPN\n\n\nPyramid Networks 제안, CNN은 순방향 전파를 통해 자연적으로 피쳐 피라미드를 형성하기 때문에 FPN은 다양한 규모의 객체를 감지하는 데 큰 발전을 보임\n1x1 conv 연산을 적용하여 모두 256 channel을 가지도록 조정하며 Up Sampling을 통해 width, height를 맞춰 Lateral connections 과정을 통해 pyramid level 바로 아래 있는 feature map과 element-wise addition 연산을 수행\n\n\n\nYOLO(You Only Look Once)\n\n최초의 1 Stage Detector\n매우 큰 inference 속도 향상\n일부 작은 물체의 경우 2 Stage Detector에 비해 위치 파악 정확도가 떨어짐\n\n\n\nSSD(Single Shot MultiBox Detector)\n\n\n앞단 CNN Feature Map을 끌어와 사용하여 Detail을 잡아내고, Faster RCNN의 anchor 개념을 가져와 다양한 형태의 Object들도 잡아낼 수 있도록 구조 설계\nyolo의 약점이었던 일부 작은 물체에 대해 검출 정확도를 크게 향상, inference 속도 향상\n\n\n\nRetinaNet\n\n1 Stage Detector들이 2 Stage Detector들에 비해 정확도가 떨어지는 이유를 탐구, Dence Detector 훈련 중 발생하는 전경과 배경간의 극단적인 Class Imbalance가 원인임을 발견\nCross Entropy Loss를 수정하여 Focal Loss라는 새로운 손실 함수 제안\n1 Stage Detector가 매우 높은 검출 속도를 유지하면서 2 Stage Detector와 비슷한 정확도를 달성할 수 있었음\n\n\n\nCornerNet\n\n앵커를 사용하지 않고 Corner를 예측을 수행\n\n이전 연구들은 앵커 박스를 사용하며 Bounding box regression, Classification을 수행했으나 Detection 된 객체는 수, 위치, 크기, 비율 등의 측면에서 자주 변형이 나타남\nCNN을 통해서 top-left, bottom-right corner를 위한 각각의 heat map을 예측\nCorner Grouping을 위한 Embedding Vector 생성, Vector간 distance를 기준으로 Grouping 수행\n\nNewell, A. and Deng, J. (2017). Pixels to graphs by associative embedding\n\n\n작은 해상도의 heat map에서 원본 이미지로 복원하는 과정에서 위치가 부정확해지는 것을 막기 위한 offset 예측 수행\n\n\n당시 대부분의 1단계 검출기보다 높은 성능 달성(COCO mAP@.5=57.8%)\n\n\n\nCenterNet\n\nkeypoint-based detection 패러다임을 따르지만 CornerNet에서 사용하던 keypoint grouping 방식과 NMS 등의 비용이 높은 post process를 제거한 End-to-End Network을 제안\nObject를 Single Point로 간주하고(Object Center) 해당 Point를 기준으로 하는 크기, 방향, 위치를 Regression\n높은 성능 달성(COCO mAP@.5=61.1%)\n\n\n\nDETR(DEtection TRansformer)\n\nObject Detection을 Set Prediction 문제로 보고 Transformer를 사용한 end-to-end detection network를 제안\n모든 객체를 한번에 예측\n수작업(앵커나 NMS같은 작업) 컴포넌트가 없기 때문에 간단한 파이프라인을 가짐\nCOCO dataset에 대해서 Faster R-CNN baseline 급의 정확도와 런타임 성능을 보임\nLarge object에 대해 좋은 퍼포먼스, Small object대해서는 낮은 퍼포먼스를 보임\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Open-Source-Contiribution":{"slug":"vault/Notion/DB/DB-Blog-Post/Open-Source-Contiribution","filePath":"vault/Notion/DB/DB Blog Post/Open Source Contiribution.md","title":"Open Source Contiribution","links":[],"tags":[],"content":"\nMMDetection 3D - fix local_visualizer.py: only visualize when there is at least one instance\n\ngithub.com/open-mmlab/mmdetection3d/pull/2231"},"vault/Notion/DB/DB-Blog-Post/OpenEXR-Python":{"slug":"vault/Notion/DB/DB-Blog-Post/OpenEXR-Python","filePath":"vault/Notion/DB/DB Blog Post/OpenEXR-Python.md","title":"OpenEXR-Python","links":[],"tags":[],"content":"github.com/404Vector/OpenEXR-Python\n개요\nOpenEXR-Python은 Python 라이브러리로, C++ 기반의 OpenEXR 라이브러리에 대한 파이썬 바인딩을 제공합니다. 이 프로젝트는 개발 환경에서의 편의성을 개선하기 위해 인텔리센스를 지원하도록 설계되었습니다.\n주요 특징\n\n인텔리센스 지원: Python 개발 환경에서 자동 완성과 문서 조회를 지원합니다.\n영화 및 이미지 산업 표준 지원: 고동적 범위 및 다채널 이미지 데이터를 효과적으로 저장하고 처리합니다.\n\n설치 방법\n\n\nOpenEXR 라이브러리 설치\n\n\npip를 사용한 OpenEXR-Python 설치\npip install openexr-python\n\n"},"vault/Notion/DB/DB-Blog-Post/Optimization에서-중요한-것들/Optimization에서-중요한-것들":{"slug":"vault/Notion/DB/DB-Blog-Post/Optimization에서-중요한-것들/Optimization에서-중요한-것들","filePath":"vault/Notion/DB/DB Blog Post/Optimization에서 중요한 것들/Optimization에서 중요한 것들.md","title":"Optimization에서 중요한 것들","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-3"],"tags":[],"content":"\n참조\nOptimization에서 중요한 것들\n\n\n참조\n\n\n                  \n                  Bootstrapping (statistics) - Wikipedia \n                  \n                \n\n\nBootstrapping is any test or metric that uses random sampling with replacement (e.\nen.wikipedia.org/wiki/Bootstrapping_(statistics)\n\n\n\n\n\n                  \n                  배깅 - 위키백과, 우리 모두의 백과사전 \n                  \n                \n\n\n배깅(bagging)은 bootstrap aggregating의 줄임말로 통계적 분류와 회귀 분석에서 사용되는 기계 학습 알고리즘의 안정성과 정확도를 향상시키기 위해 고안된 일종의 앙상블 학습법의 메타 알고리즘이다.\nko.wikipedia.org/wiki/%EB%B0%B0%EA%B9%85\n\n\n\n\n\n                  \n                  What is Bagging? \n                  \n                \n\n\nLearn how bootstrap aggregating, or bagging, can improve the accuracy of your machine learning models, enabling you to develop better insights.\nwww.ibm.com/cloud/learn/bagging\n\n\n\n\n\n                  \n                  Boosting (machine learning) - Wikipedia \n                  \n                \n\n\nIn machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.\nen.wikipedia.org/wiki/Boosting_(machine_learning)\n\n\n\n\n\n                  \n                  What is Boosting? \n                  \n                \n\n\nLearn about boosting algorithms and how they can improve the predictive power of your data mining initiatives.\nwww.ibm.com/cloud/learn/boosting\n\n\n\nWeek 3\nOptimization에서 중요한 것들\n\n\nGeneralization\n\nModel이 훈련에서 사용하지 않은 Data에 대해서도 성능을 발휘할수 있어야 함\n\n\n\nUnder-fitting vs Over-fitting\n\nUnderfitting\n\n네트워크가 특성을 나타내기 너무 간단한 경우\n모델의 훈련이 부족한 경우\n\n\nOverfitting\n\n네트워크를 너무 복잡한 경우\n훈련을 너무 과하게 한 경우\n데이터의 양이 충분하지 못한 경우\n\n\n\n\n\nCross validation\n\n\nTrain set을 train set과 validatin set으로 분리한 뒤, validation set을 사용해 검증하는 방식\n\n\nTest set은 그 어떤 방식으로든 훈련에 사용해서는 안되기 때문에 위와 같이 분리\n\n\nk-겹 교차 검증\n\n\n                  \n                  Cross-validation (statistics) - Wikipedia \nCross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\nen.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\n                  \n                \n\n\n가장 일반적인 교차검증 방법\nk개의 sub sample중 하나를 validation으로 사용하고 나머지 k-1개의 sub sample은 훈련에 사용하는 방식\n이후 교차검증을 정확히 k번 반복하여 교차검증을 수행\n\n\n\n\nHyper parameter를 찾기 위해 사용\n\n\n\n\nBias-variance tradeoff\n\nBias : 정확도\nVariance : 정밀도\n\n\n\nBootstrapping(in machine learning)\n\n중복을 허용하는 Random sampling을 하는 실험, 또는 metric을 의미한다\n\n\n\nBagging(Bootstrap aggregating)\n\n\n통계적 분류 및 회귀에 사용되는 기계 학습 알고리즘의 안정성과 정확성을 개선하기 위해 설계된 앙상블 메타 알고리즘(ensemble meta-algorithm)\n잡음이 많은 데이터 세트 내에서 분산을 줄이고 과적합을 방지\nn개의 전체 데이터 중, 복원추출(Sampling with replacement)방식으로 m개 새로운 훈련 세트를 생성\n\n이러한 샘플을 Bootstrap sample이라고 부름)\n\n\n회귀의 경우 Soft voting으로 최종 결과 값 산출\n분류의 경우 Hard voting으로 최종 결과 값 산출\n\n\n\nBoosting\n\n\n지도학습의 편향과 분산을 주로 줄이기 위한 앙상블 메타 알고리즘(ensemble meta-algorithm)\n약한 학습자 세트를 강한 학습자 세트로 결합하는 앙상블 학습 방법\n종류\n\nAdaBoost(or Adaptive boosting)\nGradient Boosting\nXGBoost(or Extreme Gradient Boosting)\n\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/PANet(Path-Aggregation-Network)/PANet(Path-Aggregation-Network)":{"slug":"vault/Notion/DB/DB-Blog-Post/PANet(Path-Aggregation-Network)/PANet(Path-Aggregation-Network)","filePath":"vault/Notion/DB/DB Blog Post/PANet(Path Aggregation Network)/PANet(Path Aggregation Network).md","title":"PANet(Path Aggregation Network)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10"],"tags":[],"content":"\n참조\nFPN의 단점\nPANet(Path Aggregation Network)\n\n\n참조\n\n\n                  \n                  Path Aggregation Network for Instance Segmentation \n                  \n                \n\n\nThe way that information propagates in neural networks is of great importance.\narxiv.org/abs/1803.01534\n\n\n\nWeek 10\ndeep-learning-study.tistory.com/637\nFPN의 단점\nBottom-Up의 과정에서 실제로는 매우 많은 CNN Layer를 거치기 때문에 상위 Level의 Layer로 Feature를 재대로 전달하는지 장담 할 수 없습니다.\n\nex : ResNet의 긴 CNN 구조)\n\nPANet(Path Aggregation Network)\n\nPANet은 Bottom-Up Path Augmentation를 위한 Network의 추가와 Adaptive feature pooling이라는 아이디어를 제안합니다.\n\n\nBottom-up Path Augmentation\n\n\nBottom-Up 방향으로 진행하는 새로운 Network 생성, 기존 방식으로 만든 stage 별 feature map인 P를 더하며 위로 진행\n\nlow-level feature 정보를 손쉽게 high-level로 전달\n정보가 전달되는 information flow를 축소하여 low-level 정보를 최대한 보존하는 역할\n\n\n\n\n\nAdaptive Feature Pooling\n\n\n기존의 stage mapping의 문제(수 pixel 차이로 stage가 변하는 경계선상에 있는 roi들이 존재)를 해결하기 위해 모든 Stage에서 ROI Pooling 수행\n\nN2~N5 각각의 feature map에 RPN이 적용되여 RoI를 생성\n생성된 RoI는 RoI Align을 거쳐서 일정한 크기의 벡터를 생성\n생성된 벡터를 max 연산으로 하나로 결합\n결합된 벡터에서 class와 box를 예측\n\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Photometric-Stereo를-사용한-형상추출-알고리즘-개발/Photometric-Stereo를-사용한-형상추출-알고리즘-개발":{"slug":"vault/Notion/DB/DB-Blog-Post/Photometric-Stereo를-사용한-형상추출-알고리즘-개발/Photometric-Stereo를-사용한-형상추출-알고리즘-개발","filePath":"vault/Notion/DB/DB Blog Post/Photometric Stereo를 사용한 형상추출 알고리즘 개발/Photometric Stereo를 사용한 형상추출 알고리즘 개발.md","title":"Photometric Stereo를 사용한 형상추출 알고리즘 개발","links":[],"tags":[],"content":"\n개요\n현상 / 이슈\n문제 정의\n해결방법\n\nPhotometric Stereo 알고리즘 구현\n광학계의 구성\n\n\n결과\n\n알고리즘 구현\n카메라 모듈과의 처리 결과 비교\nGithub 링크\n\n\n\n\n\n기존에 진행했던 프로젝트를 완전히 잊어버리기 전에 기록으로 남기고 싶어서 글을 쓰게 되었습니다. 이 프로젝트는 회사 재직중에 업무의 일환으로 진행되었기 때문에 기밀에 해당될 수 있는 정보는 다루지 않으며 일반적인 정보만을 다룹니다.\n개요\n\n배터리 타입별 특징 - 티스토리 개인 블로그\n원통형 2차 전지는 2차전지의 종류 중 하나이며 다른 2차전지 형태에 비해 공정이 쉽고, 표준화되어 있는 등 여러 장점들을 갖고 있습니다.\n\n원통형 배터리의 모습 - 국제신문 기사\n원통형 2차 전지는 일상생활에서 쓰는 배터리와 형태가 크게 다르지 않습니다. 저 초록색 사각형으로 된 부분이 상부인데, 저 표면이 깨끗해야 추후 문제가 발생할 가능성이 적어집니다. 따라서 저 상부가 오염되었거나, 뭔가 결함이 있을 경우 공정에서 제외할 필요가 있습니다.\n여기서 한 가지 문제가 있는데, 저 상단은 금속의 표면이며 바라보는 방향에 따라 어떤 결함은 보이기도하고 어떤 결함은 안보이기도 합니다.\n우리가 일상생활에서 사용하는 거울, 스마트폰 등의 물건들을 각도를 바꿔가며 보다 보면, 어떤 각도에서는 스크래치가 보이고 어떤 각도에서는 스크래치가 보이지 않는 것과 같습니다.\n이를 해결하기 위해서는, 다각도로 카메라를 배치해서 검사하거나 다각도로 조명을 투사해보면서 확인할 수 밖에 없습니다.\n당시 회사에서는 후자의 방법으로 3D Suface 이미지를 획득할 수 있는 “LumiTrax”를 사용하여 검사를 수행했습니다\n\n현상 / 이슈\n\n\nCamera Device에서 모든 처리를 하고 보내주기 때문에 오래 걸림, 여러 종류(Albedo, Gradient, etc)의 이미지를 사용해야 하는데 차례로 한 장씩 전송됨\nwww.youtube.com/watch\n\n\n전송을 모두 기다리고 검사를 시작하면 Tact time over\n\n\n전송 받을 이미지를 모두 Task를 사용하여 비동기적으로 처리\n모든 처리를 기다린 후, 병합하여 판정하는 방식으로 해결\n\n\n그러나 여전히, 너무 오래 걸리기에 실제 결함을 검사할 시간이 촉박하여 장기적으로 더 많은 검사 항목이 생기거나 더 정밀하게 검사해야 했을 때 문제 발생소지가 있음\n\n\n\n\n비주기적으로 Camera가 Black Frame을 전송\n\n임시로, Black Frame일 경우 검사에서 제외\n초기에는 개발하고 있는 프로그램의 문제라고 판단, 그러나 Test 프로그램(제조사에서 제공하는 Camera Test 프로그램)에서도 동일한 문제 발생\n카메라 제조사의 본사(일본)와 같이 트러블슈팅에 나섰으나 해결하지 못하였음\n\nJumbo Packet 설정, Receive Buffer 설정, GigE 전용 랜카드 사용, CAT.7 Cable 사용 등 모두 효과를 보지 못함\n\n\n\n\n\n\n문제 정의\nLumiTrax에서 제공하는 3D Surface 이미지 획득 방식보다 빠르고 정확한 형상추출 방법이 필요합니다.\n\n느린 이유는 무엇일까?\n\nLumiTrax 카메라는 여러 방향으로 빛을 순차적으로 조사하고, 카메라로 이미지를 각각 촬상하는 방식\n\nA 방향 빛 On → Camera 촬영 → A 방향 빛 Off → B 방향 빛 On → Camera 촬영 - …\n\n\n저렴하지만 번거롭고 시간 추가 소요\n\n움직이는 물체의 경우, Camera 자체적으로 Object Detection 및 정합까지 수행 필요\n\n\n\n\nBlack Frame이 들어오는 이유는 무엇일까?\n\n카메라 자체에서 누락시키는 것인지, API 내부 오류인지 판단 불가\n\n\n\n\n해결방법\n\n\n                  \n                  느리면서 Frame까지 누락된다 \n                  \n                \n\n→ 차라리 직접 다방향으로 조명을 조사하여 3D Surface Image를 직접 만들어보면 어떨까?\n\n순차적으로 조사하고, 카메라로 이미지를 각각 촬상\n\n한 장씩 찍다보면 시간이 오래걸린다. 한 번에 찍을 수 있는 방법이 없을까?\n한 번에 찍더라도 분리 할 수 있으면 되지 않을까?(RGB or Polar)\n\n논문 탐색\n→\n금속 표면 미세 결함에 대한 신뢰성 있는 실시간 3차원 형상 추출 시스템 개발\n\n\n직접 광학계를 구성하여 논문의 방법으로 Surface Normals를 구할 수 있다면, 두 문제를 모두 해결할 수 있음\n\n\n\nPhotometric Stereo 알고리즘 구현\n\n\nPhotometric Stereo : 다양한 조명조건에서 물체를 관찰하여 Surface Normals를 추정하기 위한 Computer Vision 기술\n\n더 자세한 이론은 이 자료를 추천합니다.\n\n\n\n공식\nI : 반사광(Diffuse Reflection)의 강도\n\n빛의 반사되는 상은 정반사(Specular Reflection)와 난반사(Diffuse Reflection)의 합입니다.\n그러나 기본적으로 ROI에 해당하는 영역은 결함이 존재하지 않는 이상 일반적으로 평면이므로, Specular Reflection이 일어나지 않습니다.\n그러므로 Diffuse Reflection만을 고려했습니다.\n\\vec{s} : Light Source Vector\n\\vec{n} : 물체의 Surface Normal Vector\nc : Camera Gain\n\\rho : Albedo, 물체가 빛을 받았을 때 반사하는 정도를 나타내는 단위(0.0~1.0)\nk : Light Source의 Brightness\nI = c{\\rho \\over \\pi }k(\\vec{s}\\cdotp \\vec{n}) : 카메라에서 취득하는 반사광의 강도에 대한 Equation\n반사광의 강도는 관찰자의 관찰 각도와 관계없이 일정하다고 가정합니다.(Lambert’s Law)\n여기서 c{\\rho \\over \\pi }k를 1로 가정하면, 식은 아래과 같이 단순해집니다.\n→ I = \\vec{s}\\cdotp \\vec{n}\nI(x,y) = \\vec{s}\\cdotp \\vec{n}(x,y)\n기존 식을 Image의 각 Pixel별 Intensity값인 점을 고려하여 표현한다면 위와 같이 Intensity I와 Surface Normal Vector n은 위와 같이 position x와 y에 대한 함수로 표현할 수 있으며, Light Source Vector는 모든 Pixel 위치에서 동일하다고 가정합니다.\n이 상태에서 I(x,y)는 촬상된 이미지의 해당 위치의 밝기 값이므로 상수가 되며, s(물체에 대한 조명의 조사 각도)는 Hardware 구성을 통해 정해지는 값이므로 마찬가지로 상수가 됩니다.\n\\vec{n}(x,y) = [n_x(x,y), n_y(x,y), n_z(x,y)]\n따라서 우리가 구하고자 하는 Surface Normal Vector n만이 미지수로 남게되며, n은 3차원 공간에서의 Vector이므로 Pixel별로 총 3개의 미지수가 존재하게 됩니다.\n때문에 Surface Normal을 추정하기 위해서는 다른 각도에서 조명을 조사하여 촬영한 이미지들이 최소 3장 필요하다는 것을 알 수 있습니다.\n\\begin{bmatrix} I_1(x,y) \\\\ I_2(x,y) \\\\... \\\\I_t(x,y) \\end{bmatrix} = \\begin{bmatrix} s_{1x}(x,y), s_{1y}(x,y), s_{1z}(x,y) \\\\s_{2x}(x,y), s_{2y}(x,y), s_{2z}(x,y) \\\\... \\\\s_{tx}(x,y), s_{ty}(x,y), s_{tz}(x,y) \\end{bmatrix} \\times \\begin{bmatrix} n_x(x,y) \\\\n_y(x,y) \\\\n_z(x,y) \\end{bmatrix}\n이를 matrix화하여 표현하면 위와 같습니다. 여기서 n을 구하기 위해서는 우측에 있는 s에 대한 matrix를 제거해줘야 합니다.\n행렬의 차원은 3xn이므로 Moore–Penrose pseudo inverse matrix등을 사용하여 역행렬을 구할 필요가 있습니다.\nn=3일 경우에도, 수학적으로 역행렬이 존재하지 않는 경우가 있으므로 pseudo inverse matrix를 꼭 사용해야 합니다.\nG=\\begin{bmatrix} I_1(x,y) \\\\ I_2(x,y) \\\\... \\\\I_t(x,y) \\end{bmatrix} , S= \\begin{bmatrix} s_{1x}(x,y), s_{1y}(x,y), s_{1z}(x,y) \\\\s_{2x}(x,y), s_{2y}(x,y), s_{2z}(x,y) \\\\... \\\\s_{tx}(x,y), s_{ty}(x,y), s_{tz}(x,y) \\end{bmatrix} , N= \\begin{bmatrix} n_x(x,y) \\\\n_y(x,y) \\\\n_z(x,y) \\end{bmatrix}\nS^+ : 행렬 S의 pseudo inverse matrix, 3 x t 차원\nS^+(x,y)\\times G(x,y)=N\n여기서 반사광 값 I는 0.01.0 사이의 값 입니다. 기본적으로 이미지는 0255의 byte array로 이루어져 있기 때문에 Normalize가 필요합니다.\n\n\n광학계의 구성\n\n\n논문과 같은 방법으로 광학계를 구성하였고, 추가적으로 그림 (b)에 있는 Camera들의 Pixel Align이 틀어지지 않도록 미세조정할 수 있도록 별도 SW를 구성하였습니다.\n\n\n결과\n알고리즘 구현\n검증의 사용한 이미지는 2MB 크기의 Gray Scale Image 3장이며, 검증은 4번에 나눠서 수행했습니다.\n\nPython &amp; Jupyter Notebook을 이용한 Prototyping\n\n빠른 구현을 위해 Python과 Jupyter Notebook을 사용하여 구현했습니다.\n구현 결과, 3장의 이미지를 사용하여 1개의 Surface Normal을 추정하는데 걸리는 시간은 30 Sec 이상이었습니다.\n\n이 때, Numpy 혹은 OpenCV를 최대한 활용하여 연산을 했다면 시간을 더 줄일 수 있었을 것입니다. 하지만 당시 배움이 부족하여, 아쉽게도 고려하지 못했습니다.\n\n\nPython의 연산이 빠를 것이라고 기대하지는 않았지만, 너무 심각하게 느려서 사용할 수 없는 수준이었습니다.\n\n\nC#으로 Porting\n\nFramework가 C#기반으로 되어있었고, 기존에 구현되어있는 api들이 있었으므로 C#으로 Porting하면 빠르게 Application상에서 돌려볼 수 있었습니다.\n구현 결과, 400ms로 기존에 비해 획기적으로 단축되었습니다. 그러나 마찬가지로, 아직 처리를 위해 사용할 수 있는 수준은 아니었습니다.\n\n\nLUT Mapping을 이용한 연산 최적화\n\n기존 알고리즘에서 미리 계산해둘 수 있는 부분을 Caching하여 LUT을 만들었습니다.\n결과, 200ms로 기존 대비 50%로 프로세싱 시간을 단축했습니다.\n\n\nC++으로 Porting\n\nImage는 byte형태이며, C#에서 byte는 blittable type이기 때문에 C++ DLL을 Wrapping해서 사용해도 빠르게 사용할 수 있습니다. 때문에 low level 연산이 훨신 빠른 C++에서 알고리즘을 구현했습니다.\n결과, 40ms로 처리 시간을 기존 대비 80% 단축했고, 사용할 가치가 있는 속도를 얻을 수 있었습니다.\n\n\n\n카메라 모듈과의 처리 결과 비교\n알고리즘을 통해 얻은 Surface Normal을 사용하여 추가적인 Processing을 한 뒤, LumiTrax와 비교했고 완전히 동일한 이미지를 얻을 수 있었습니다.(PSNR Inf)\n다만, LumiTrax에서 말하는 형상 이미지(Shape Image)의 경우 기본 Parameter의 상태에서는 동일하지만 Parameter를 조작할 경우 이미지가 왜곡되는데, 이 기능은 필요하지 않다고 판단했습니다.(형상 이미지에서 Feature가 있어야 왜곡이 되지만, 애초에 Feature가 있다면 이미지를 더 왜곡하지 않아도 검사를 통해 잡을 수 있습니다.)\n\nGithub 링크\n업무에 사용한 이미지와 코드를 올릴 수는 없기 때문에, 인터넷에서 새롭게 예제 이미지를 구해서 간단하게 SurfaceNormal을 구하는 코드를 Jupyter Notebook으로 만들었습니다.\nPython에서 기본적으로 사용하는 라이브러리들만 이용했기 때문에 기본적인 Python 개발 환경을 갖고있는 분들은 손쉽게 돌려볼 수 있습니다.\ngithub.com/404Vector/Study.PhotometricStereo"},"vault/Notion/DB/DB-Blog-Post/Pix2Pix/Pix2Pix":{"slug":"vault/Notion/DB/DB-Blog-Post/Pix2Pix/Pix2Pix","filePath":"vault/Notion/DB/DB Blog Post/Pix2Pix/Pix2Pix.md","title":"Pix2Pix","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-5/Week-5"],"tags":[],"content":"참조\ndeep-learning-study.tistory.com/645\nvelog.io/@wilko97/논문실습-Pix2Pix\narxiv.org/abs/1611.07004\nWeek 5\nPix2Pix란?\nPix2Pix는 image를 image로 변환하도록 generator을 학습합니다. 예를 들어, generator의 입력값으로 스케치 그림을 입력하면 완성된 그림이 나오도록 학습할 수 있습니다.\n\n\nPix2Pix의 특징\n\n\nLoss에 Ground-truth와의 차이도 반영\n\n기존의 GAN과 다르게 Loss에 GT와의 차이를 반영합니다.\n\n\n\nNoise Vector를 사용하지 않음\n\nPix2Pix GAN은 CGAN의 아이디어를 확장시켜서, Paired Image-to-Image translation을 추가한 것입니다. Pix2Pix GAN은 generator에서 노이즈 벡터의 개념을 사용하지 않습니다.\n이미지가 generator의 입력으로 들어가고, translated된 이미지가 출력됩니다.\nDiscriminator는 conditional discriminator로 real/fake 이미지와 condition을 입력으로 받습니다. 역할은 기존처럼 real/fake를 판단하는 것입니다.\nPix2Pix GAN의 최종적인 목표는 다른 GANs과 동일합니다. Discriminator를 속이는 이미지들을 Generator가 생성하게 만드는 것입니다.\n\n\n\nUNet Generator 사용\n\n\n초창기 GANs 아키텍쳐처럼 노이즈 벡터를 입력으로 받는것이 아니라, 이미지들을 입력으로 받아서 마치 AutoEncoder 형태로 generator를 구성합니다.\n따라서 Generator에는 encoder,decoder networks가 있습니다. Pix2Pix는 UNET을 Generator로 사용하게 되는데, 이것은 mirrored layers 사이에 skip-connections이 있다는 특징이 있습니다.\n\n\n\nPatchGAN Discriminator\n\n\nPix2Pix는 PatchGAN이라 불리는 Discriminator를 사용하는데, output으로 확률값(scalar)을 내놓는 대신에, 영역의 Tensor값들을 반환합니다. 즉, 입력 이미지에 대해서 discriminator는 행렬값을 반환하게 되는데, 이미지 전체를 한번에 판단하는 것보다, 세부 영역들에 대해 구분한 값을 반환하게 됩니다.\n\n\n\nLoss Function\n\n\nDiscriminator\n\n\n이전의 GANs모델과 같은 loss함수를 사용합니다. 즉, real과 fake를 구별하기 위해, negative log-likelihood를 최소화하는 것이 목표입니다.  또한 저자는 Generator보다 빨리 학습하는 것을 방지하기 위해 2로 나누어 주었습니다.\n\n\n\nGenerator\n\n\n실제 라벨값들은 Generator를 학습시키는데 사용됩니다.\n또한 추가적인 L1 loss 항을 본 논문에서는 더해주었는데, error를 최소화하는데 사용됩니다. L1 loss값들은 실제 정답과 예측값들 간의 차이의 절댓값이며, L1 규제를 통해서 translated된 이미지가 target하고 유사하지 않는 경우 penelty를 부여하는 역할을 합니다.\nL1을 사용하는 이유는 저자가 실험적으로 기존의 다른 Loss(ex: L2)보다 덜 blurry한 이미지를 얻었기 때문입니다.\n\n\n\nTotal Loss\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Post-Template":{"slug":"vault/Notion/DB/DB-Blog-Post/Post-Template","filePath":"vault/Notion/DB/DB Blog Post/Post Template.md","title":"Post Template","links":[],"tags":[],"content":"\n서론\n본론\n결론\n\n\n서론\n본론\n결론"},"vault/Notion/DB/DB-Blog-Post/Program-vs-Process-vs-Thread/Program-vs-Process-vs-Thread":{"slug":"vault/Notion/DB/DB-Blog-Post/Program-vs-Process-vs-Thread/Program-vs-Process-vs-Thread","filePath":"vault/Notion/DB/DB Blog Post/Program vs Process vs Thread/Program vs Process vs Thread.md","title":"Program vs Process vs Thread","links":[],"tags":[],"content":"\n\n                  \n                  프로세스와 스레드의 차이 \n                  \n                \n\n\nCS 첫 포스팅으로는 프로세스와 스레드의 차이에 대해 설명하는 글을 쓰기로 결정했다.\nvelog.io/@raejoonee/%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80-%EC%8A%A4%EB%A0%88%EB%93%9C%EC%9D%98-%EC%B0%A8%EC%9D%B4\n\n\n\n설명\n\n\nProgram\n\n\n파일이 저장 장치에 저장되어 있지만 메모리에는 올라가 있지 않은 정적인 상태\n윈도우의 *.exe 파일이나 MacOS의 *.dmg 파일 등등 사용자가 눌러서 실행하기 전의 파일\n\n\n\nProcess\n\n\n메모리 상에서 실행중인 프로그램\n모든 Process는 메모리에 독립적인 공간을 할당받고, 안정성을 위해 프로세스는 다른 프로세스의 변수나 자료에 접근할 수 없음\n\n\n\nThread\n\n\n\n프로세스 안에서 실행되는 흐름 단위\nProcess는 최소 1개 이상의 Thread를 보유하고 있음\n프로세스 소속의 스레드끼리는 메모리를 공유\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Programmers---가장-가까운-글자":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---가장-가까운-글자","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 가장 가까운 글자.md","title":"Programmers - 가장 가까운 글자","links":[],"tags":[],"content":"\n참조\nProgrammers - 가장 가까운 글자\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/142086\n\n\n\nProgrammers - 가장 가까운 글자\ndef solution(s):\n    answer = []\n    history = {}\n    for i, c in enumerate(s):\n        if c not in history:\n            history[c] = i\n            answer.append(-1)\n            continue\n        answer.append(i - history[c])\n        history[c] = i\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---개인정보-수집-유효기간":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---개인정보-수집-유효기간","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 개인정보 수집 유효기간.md","title":"Programmers - 개인정보 수집 유효기간","links":[],"tags":[],"content":"\n참조\nProgrammers - 개인정보 수집 유효기간\n\n\n참조\nschool.programmers.co.kr/learn/courses/30/lessons/150370\nProgrammers - 개인정보 수집 유효기간\ndef cvt_ydm2int(ydm:str): return (int(ydm[0])*12*28) + (int(ydm[1])*28) + int(ydm[2])\n \ndef solution(today, terms, privacies):\n    answer = []\n    day_now = cvt_ydm2int(today.split(&#039;.&#039;)) # convert string to total day\n    rule_map = {s.split()[0]:int(s.split()[1]) for s in terms} # get rule map\n    for idx, privacy in enumerate(privacies): # loop for privacies\n        privacy = privacy.split() # split privacy to ydm &amp; rule\n        day_privacy = cvt_ydm2int(privacy[0].split(&#039;.&#039;)) + int(rule_map[privacy[1]])*28 # get total day of privacy\n        if day_now &gt;= day_privacy:\n            answer.append(idx+1) # add if not expired\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---공원-산책":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---공원-산책","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 공원 산책.md","title":"Programmers - 공원 산책","links":[],"tags":[],"content":"\n참조\nProgrammers - 공원 산책\n\n\n참조\nschool.programmers.co.kr/learn/courses/30/lessons/172928#\nProgrammers - 공원 산책\nimport numpy as np\n \ndef find_start_pos(park): # find starting pos\n    for j, row in enumerate(park):\n        if not &#039;S&#039; in row: continue\n        return [j, row.index(&#039;S&#039;)]\n    return [-1,-1]\n \ndef decode_route(route): # decode route to vector\n    rs = route.split(&#039; &#039;)\n    norm_vector = {&#039;N&#039;:[-1,0], &#039;S&#039;:[1,0], &#039;W&#039;:[0,-1], &#039;E&#039;:[0,1]}[rs[0]]\n    return np.array(norm_vector) * int(rs[1])\n \ndef get_target_pos(current_pos, route): # calc target_pos\n    return decode_route(route) + current_pos\n \ndef eval_target_pos(park, current_pos, target_pos): # eval target position\n    if not len(park) &gt; target_pos[0] or not len(park[0]) &gt; target_pos[1]:\n        return False\n    if 0 &gt; target_pos[0] or 0 &gt; target_pos[1]:\n        return False\n    dp = ((target_pos - current_pos) / max(abs(target_pos - current_pos))).astype(np.int32)\n    next_pos = current_pos\n    while any(next_pos != target_pos):\n        next_pos = next_pos + dp\n        if park[next_pos[0]][next_pos[1]] == &#039;X&#039;: return False\n    return True\n    \ndef solution(park, routes):\n    answer = []\n    current_pos = np.array(find_start_pos(park))\n \n    for route in routes:\n        target_pos = get_target_pos(current_pos, route)\n        if not eval_target_pos(park, current_pos, target_pos): continue\n        current_pos = target_pos\n    return current_pos.tolist()"},"vault/Notion/DB/DB-Blog-Post/Programmers---단어-변환":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---단어-변환","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 단어 변환.md","title":"Programmers - 단어 변환","links":["tags/1문제에","tags/2target인"],"tags":["1문제에","2target인"],"content":"\n문제 설명\n\n제한사항\n입출력 예\n입출력 예 설명\n\n\n풀이\n\n\n문제 설명\n두 개의 단어 begin, target과 단어의 집합 words가 있습니다. 아래와 같은 규칙을 이용하여 begin에서 target으로 변환하는 가장 짧은 변환 과정을 찾으려고 합니다.\n1. 한 번에 한 개의 알파벳만 바꿀 수 있습니다. 2. words에 있는 단어로만 변환할 수 있습니다.\n예를 들어 begin이 “hit”, target가 “cog”, words가 [“hot”,“dot”,“dog”,“lot”,“log”,“cog”]라면 “hit” → “hot” → “dot” → “dog” → “cog”와 같이 4단계를 거쳐 변환할 수 있습니다.\n두 개의 단어 begin, target과 단어의 집합 words가 매개변수로 주어질 때, 최소 몇 단계의 과정을 거쳐 begin을 target으로 변환할 수 있는지 return 하도록 solution 함수를 작성해주세요.\n제한사항\n\n각 단어는 알파벳 소문자로만 이루어져 있습니다.\n각 단어의 길이는 3 이상 10 이하이며 모든 단어의 길이는 같습니다.\nwords에는 3개 이상 50개 이하의 단어가 있으며 중복되는 단어는 없습니다.\nbegin과 target은 같지 않습니다.\n변환할 수 없는 경우에는 0를 return 합니다.\n\n입출력 예\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbegintargetwordsreturn”hit&quot;&quot;cog”[“hot”, “dot”, “dog”, “lot”, “log”, “cog”]4”hit&quot;&quot;cog”[“hot”, “dot”, “dog”, “lot”, “log”]0\n입출력 예 설명\n예제 1문제에 나온 예와 같습니다.\n예제 2target인 “cog”는 words 안에 없기 때문에 변환할 수 없습니다.\n풀이\ndef word_norm(w1, w2):\n    distance = len(w1)\n    for i in range(len(w1)):\n        if w1[i] != w2[i]:continue\n        distance -= 1\n    return distance\n \ndef set_map(words, target_idx, idx_map):\n    added = 0\n    for idx in range(len(words)):\n        w1 = words[target_idx]\n        w2 = words[idx]\n        if idx == target_idx: continue\n        if word_norm(w1, w2) != 1: continue\n        idx_map[target_idx].append(idx)\n        added += 1\n    return added\n \ndef bfs(idx_map, history, begin_idx, target_idx, depth):\n    if history[begin_idx] == True: return -1 # skip search\n    my_depth = depth + 1\n    history[begin_idx]=True # check search\n    idxs = idx_map[begin_idx]\n    if target_idx in idxs:\n        return my_depth\n    for idx in idxs:\n        child_depth = bfs(idx_map, history, idx, target_idx, my_depth)\n        if child_depth &gt; -1:\n            return child_depth\n    return -1\n \ndef solution(begin, target, words):\n    answer = 0\n    # skip if target is not in words\n    if target not in words: return 0\n    words.append(begin)\n    # init idx map\n    idx_map = [[] for _ in range(len(words))]\n    # init history\n    history = [False for _ in range(len(words))]\n    # create graph\n    for idx in range(len(words)):\n        _ = set_map(words, idx, idx_map)\n    # bfs\n    answer = bfs(idx_map, history, len(words)-1, words.index(target),0)\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---당구-연습":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---당구-연습","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 당구 연습.md","title":"Programmers - 당구 연습","links":[],"tags":[],"content":"\n참조\nProgrammers - 당구 연습\n\n\n참조\nschool.programmers.co.kr/learn/courses/30/lessons/169198#\nProgrammers - 당구 연습\n# get porojected position\ndef get_projected_pos(m, n, ball):\n    return [\n        [   -ball[0],    -ball[1]], \\#left top\n        [    ball[0],    -ball[1]], \\#top\n        [2*m-ball[0],    -ball[1]], \\#right top\n        [   -ball[0],     ball[1]], \\#left\n        [2*m-ball[0],     ball[1]], \\#right\n        [   -ball[0], 2*n-ball[1]], \\#left bot\n        [    ball[0], 2*n-ball[1]], \\#bot\n        [2*m-ball[0], 2*n-ball[1]], \\#right bot\n    ]\n \n# judge target ball position is in line\ndef is_in_line(pos_start, pos_end, pos_target):\n    if pos_end[0] == pos_start[0]:\n        return min(pos_start[1], pos_end[1]) &lt;= pos_target[1] and max(pos_start[1], pos_end[1]) &gt;= pos_target[1]\n    elif pos_end[1] == pos_start[1]:\n        return min(pos_start[0], pos_end[0]) &lt;= pos_target[0] and max(pos_start[0], pos_end[0]) &gt;= pos_target[0]\n    m = (pos_end[1] - pos_start[1]) / (pos_end[0] - pos_start[0])\n    b = pos_start[1] - (m*pos_start[0])\n    return pos_target[1] == (m*pos_target[0]) + b\n \n# get distance between two position\ndef get_distance(pos1, pos2):\n    return (pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2\n \ndef solution(m, n, startX, startY, balls):\n    answer = []\n    projected_poss = [get_projected_pos(m, n, ball) for ball in balls]\n    for ball, projected_pos in zip(balls, projected_poss):\n        distances = [get_distance((startX,startY), pos) for pos in projected_pos]\n        is_inlines = [is_in_line((startX,startY), pos, ball) for pos in projected_pos]\n        max_val = max(distances)\n        min_val = min([dst if not inline else max_val for dst, inline in zip(distances, is_inlines)])\n        answer.append(min_val)\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---대충-만든-자판":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---대충-만든-자판","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 대충 만든 자판.md","title":"Programmers - 대충 만든 자판","links":[],"tags":[],"content":"\n참조\nProgrammers - 대충 만든 자판\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/160586\n\n\n\nProgrammers - 대충 만든 자판\ndef solution(keymap, targets):\n    answer = []\n    for target in targets: # loop for targets\n        count = 0 # set key push count to 0\n        for c in target: # loop for target&#039;s key\n            m = min([km.index(c) if c in km else 999 for km in keymap]) # find minimum key push count\n            if m == 999: # if there is no way, break loop and set count to -1\n                count = -1\n                break\n            count += m+1 # add key push count\n        answer.append(count) # add total key push count for target\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---덧칠하기":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---덧칠하기","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 덧칠하기.md","title":"Programmers - 덧칠하기","links":[],"tags":[],"content":"\n참조\nProgrammers - 덧칠하기\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/161989\n\n\n\nProgrammers - 덧칠하기\ndef solution(n, m, section):\n    answer = 0\n    target = section[-1]+1 # the section is sorted list. thus, set target value to section[-1]+1\n    while section: # loop until section is not empty\n        sec = section.pop() # pop element\n        if sec &gt; target: # judge element is in target range\n            continue\n        target = sec - m # set new target range\n        answer += 1 # add painting count\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---둘만의-암호":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---둘만의-암호","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 둘만의 암호.md","title":"Programmers - 둘만의 암호","links":[],"tags":[],"content":"\n참조\nProgrammers - 둘만의 암호\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/155652\n\n\n\nProgrammers - 둘만의 암호\ndef solution(s, skip, index):\n    answer = &#039;&#039;\n    char_map = []\n    for v in range(ord(&#039;a&#039;),ord(&#039;z&#039;)+1,1):\n        c = chr(v)\n        if c in skip:\n            continue\n        char_map.append(c)\n    for c in s:\n        idx = (char_map.index(c)+index)%len(char_map)\n        c_shifted = char_map[idx]\n        answer += c_shifted\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---리코쳇-로봇":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---리코쳇-로봇","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 리코쳇 로봇.md","title":"Programmers - 리코쳇 로봇","links":[],"tags":[],"content":"\n참조\nProgrammers - 리코쳇 로봇\n\n\n참조\nschool.programmers.co.kr/learn/courses/30/lessons/169199\nProgrammers - 리코쳇 로봇\ndef is_moveable(x, y, board):\n    rows, cols = len(board), len(board[0])\n    if x &lt; 0 or x &gt;= cols: return False\n    elif y &lt; 0 or y &gt;= rows: return False\n    elif board[y][x] == &#039;D&#039;: return False\n    return True\n \ndef move(board, pos, vector):\n    x, y = pos\n    while is_moveable(x+vector[0], y+vector[1], board):\n        x += vector[0]\n        y += vector[1]\n    return [x, y]\n    \ndef solution(board):\n    rows, cols = len(board), len(board[0])\n    start_pos = None\n    end_pos = None\n    move_map = {}\n    for row, item in enumerate(board):\n        if &#039;R&#039; in item:\n            start_pos = [item.index(&#039;R&#039;), row]\n        if &#039;G&#039; in item:\n            end_pos = [item.index(&#039;G&#039;), row]\n    print(f&quot;pos for x:{move(board, start_pos, [-1,0])} / pos for y:{move(board, start_pos, [0,1])}&quot;)\n    print(f&quot;start_pos:{start_pos} , end_pos:{end_pos}&quot;)\n    step = 0\n    visited = []\n    next_positions = [start_pos]\n    while next_positions:\n        _next_positions = []\n        for pos in next_positions:\n            if pos in visited: # skip already visited position\n                continue\n            visited.append(pos)\n            if pos == end_pos: # escape loop if find the goal\n                return step\n            _next_positions.append(move(board, pos, [-1,0]))\n            _next_positions.append(move(board, pos, [+1,0]))\n            _next_positions.append(move(board, pos, [0,-1]))\n            _next_positions.append(move(board, pos, [0,+1]))\n        next_positions = _next_positions\n        step += 1\n    return -1"},"vault/Notion/DB/DB-Blog-Post/Programmers---바탕화면-정리":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---바탕화면-정리","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 바탕화면 정리.md","title":"Programmers - 바탕화면 정리","links":[],"tags":[],"content":"\n참조\nProgrammers - 바탕화면 정리\n\n\n참조\nschool.programmers.co.kr/learn/courses/30/lessons/161990\nProgrammers - 바탕화면 정리\ndef get_points(wallpaper):\n    points = []\n    for row, buffer in enumerate(wallpaper):\n        for col, char in enumerate(buffer):\n            if char != &#039;#&#039;: continue\n            points.append([row, col])\n    return points\n \ndef calc_min_rect(points):\n    x_min = 51\n    x_max = -1\n    y_min = 51\n    y_max = -1\n    for point in points:\n        x_min = min(x_min, point[1])\n        x_max = max(x_max, point[1])\n        y_min = min(y_min, point[0])\n        y_max = max(y_max, point[0])\n    return [y_min, x_min, y_max+1, x_max+1]\n \ndef solution(wallpaper):\n    points = get_points(wallpaper)\n    answer = calc_min_rect(points)\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---숫자-짝궁":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---숫자-짝궁","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 숫자 짝궁.md","title":"Programmers - 숫자 짝궁","links":[],"tags":[],"content":"\n참조\nProgrammers - 숫자 짝궁\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/131128\n\n\n\nProgrammers - 숫자 짝궁\ndef get_map(char_list:str):\n    map_result = {}\n    for c in char_list:\n        if c not in map_result:\n            map_result[c] = 1\n        else:\n            map_result[c] += 1\n    return map_result\n \ndef solution(X, Y):\n    answer = &#039;&#039;\n    map_X = get_map(X)\n    map_Y = get_map(Y)\n    keys = [str(i) for i in range(9, -1, -1)]\n    common_keys = {}\n    for key in keys:\n        if key not in map_Y or key not in map_X:\n            continue\n        common = min(map_Y[key], map_X[key])\n        answer += key*common\n    return &#039;-1&#039; if answer == &#039;&#039; else &#039;0&#039; if answer[0] == &#039;0&#039; else answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---숫자의-표현":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---숫자의-표현","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 숫자의 표현.md","title":"Programmers - 숫자의 표현","links":[],"tags":[],"content":"\n참조\nProgrammers - 숫자의 표현\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/12924\n\n\n\nProgrammers - 숫자의 표현\ndef solution(n):\n    # x : start value, m : number_of_sequential_value\n    # EQ : n = x*m + m*(m-1)/2 = (m/2) * (2*x+m-1)\n    #      x = 2*n/m -m + 1 \n    answer = 0\n    x = 1\n    m = 1\n    while x &gt; 0:\n        x = ((2*n/m) -m + 1)/2\n        m += 1\n        if not x - int(x) &gt; 0:\n            answer += 1\n    return answer-1 # subtract x=0 case"},"vault/Notion/DB/DB-Blog-Post/Programmers---옹알이(2)":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---옹알이(2)","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 옹알이(2).md","title":"Programmers - 옹알이(2)","links":[],"tags":[],"content":"\n참조\nProgrammers - 옹알이(2)\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/133499\n\n\n\nProgrammers - 옹알이(2)\ndef is_speakable(word, VOCA):\n    last_speak = &#039;&#039;\n    while word != &#039;&#039;:\n        can_speak = False\n        for v in VOCA:\n            if v not in word: continue\n            if word.index(v) != 0 : continue\n            can_speak = True\n            if v == last_speak:\n                return False\n            else:\n                last_speak = v\n                word = word[len(v):]\n                break\n        if can_speak == False:\n            return False\n    return True\n \ndef solution(babbling):\n    VOCA = [&quot;aya&quot;, &quot;ye&quot;, &quot;woo&quot;, &quot;ma&quot;]\n    return sum(is_speakable(ba, VOCA) for ba in babbling)"},"vault/Notion/DB/DB-Blog-Post/Programmers---카드뭉치":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---카드뭉치","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 카드뭉치.md","title":"Programmers - 카드뭉치","links":[],"tags":[],"content":"\n참조\nProgrammers - 카드뭉치\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/159994\n\n\n\nProgrammers - 카드뭉치\ndef solution(cards1, cards2, goal):\n    for word in goal:\n        if cards1:\n            if cards1[0] == word:\n                cards1 = cards1[1:]\n                continue\n        if cards2:\n            if cards2[0] == word:\n                cards2 = cards2[1:]\n                continue\n        return &#039;No&#039;\n    return &#039;Yes&#039;"},"vault/Notion/DB/DB-Blog-Post/Programmers---콜라-문제":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---콜라-문제","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 콜라 문제.md","title":"Programmers - 콜라 문제","links":[],"tags":[],"content":"\n참조\nProgrammers - 콜라 문제\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/132267#\n\n\n\nProgrammers - 콜라 문제\ndef solution(a, b, n):\n    answer = 0\n    while n &gt;= a : # loop util b &gt; 1\n        number_of_exchange = n // a # calc how many coke can exchange\n        n -= a * number_of_exchange # drink cokes\n        new_coke = b * number_of_exchange # get new cokes\n        n += new_coke # add new cokes\n        answer += new_coke\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---크기가-작은-부분-문자열":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---크기가-작은-부분-문자열","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 크기가 작은 부분 문자열.md","title":"Programmers - 크기가 작은 부분 문자열","links":[],"tags":[],"content":"\n참조\nProgrammers - 크기가 작은 부분 문자열\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/147355\n\n\n\nProgrammers - 크기가 작은 부분 문자열\ndef solution(t, p):\n    answer = 0\n    for i in range(0, len(t) - len(p) + 1, 1):\n        if int(t[i:i+len(p)]) &lt;= int(p):\n            answer +=1\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programmers---햄버거-만들기":{"slug":"vault/Notion/DB/DB-Blog-Post/Programmers---햄버거-만들기","filePath":"vault/Notion/DB/DB Blog Post/Programmers - 햄버거 만들기.md","title":"Programmers - 햄버거 만들기","links":[],"tags":[],"content":"\n참조\nProgrammers - 햄버거 만들기\n\n\n참조\n\n\n                  \n                  Info\n                  \n                \n\n\nschool.programmers.co.kr/learn/courses/30/lessons/133502\n\n\n\nProgrammers - 햄버거 만들기\ndef solution(ingredient):\n    answer = 0\n    stacked_items = []\n    for item in ingredient:\n        stacked_items.append(item)\n        if len(stacked_items) &lt; 4: continue\n        if stacked_items[-4:] == [1,2,3,1]:\n            stacked_items.pop()\n            stacked_items.pop()\n            stacked_items.pop()\n            stacked_items.pop()\n            answer += 1\n    return answer"},"vault/Notion/DB/DB-Blog-Post/Programming-language-Dart-1-기본기/Programming-language-Dart-1-기본기":{"slug":"vault/Notion/DB/DB-Blog-Post/Programming-language-Dart-1-기본기/Programming-language-Dart-1-기본기","filePath":"vault/Notion/DB/DB Blog Post/Programming language Dart 1 기본기/Programming language Dart 1 기본기.md","title":"Programming language Dart 1 기본기","links":[],"tags":[],"content":"\n참조\n개요\n[[#Dart #1 기본기]]\n\nDart Pad 실행하기\nDart 변수 선언\nDart 상수 선언\nNullable 선언\nMap\n조건문\n반복문\nEnum 선언\n함수 선언\nMethod Return Type\nArrow Method\nTypedef\n\n\n\n\n참조\n\n\n                  \n                  [코드팩토리] [입문] Dart 언어 4시간만에 완전정복 - 인프런 | 강의 \n                  \n                \n\n\n이 강의를 통해 Dart 언어를 배우면 Flutter를 시작할 수 있을 정도의 수준으로의 업그레이드가 가능합니다!\nwww.inflearn.com/course/dart-언어-입문/dashboard\n\n\n\nbrunch.co.kr/@mystoryg/118\n개요\n플러터라는 Framework에 평소 관심이 있었고, 배우고 싶다고 생각하고 있었습니다.\nAI Engineer로 취업도 잘 안되고 있어서 분노의(?) 스터디를 시작합니다.\nDart #1 기본기\nDart Pad 실행하기\n개발환경에 안드로이드 스튜디오가 있다면 상관이 없지만, 그렇지 않은 분들은 Dart 하나를 위해 환경를 꾸리는 것이 부담스러울 수 있습니다.\n이 때, Google에 DartPad라고 검색하면 Web형 IDE를 쓸 수 있습니다.\n→ dartpad.dev/\n\n알아두면 좋은 단축키\n\n\nctrl + x : 줄 삭제\n\n\nctrl + enter : compile\n\n\n\n\nDart 변수 선언\nDart에서는 ‘var’ 키워드로 변수를 선언합니다.\nvoid main() {\n  var x = &quot;hello world!&quot;;\n  print(x);\n}\n물론 기존 C++, C#과 같이 자료형을 직접 표기할 수도 있습니다.\nvoid main(){\n\tint number = 2;\n}\n기본적인 사칙연산을 모두 지원합니다.( + - * / )\n또한 print에 바로 넣을 수 있는 것으로 보아, 내부적으로 String변환 메서드를 갖고 있는거 같습니다.\nvoid main(){\n\tint number1 = 1;\n\tint number2 = 2;\n\tprint(number1 + number2)\n}\n\n\nDart Built-in types[link]\n\n\n\nObject Class\n\n\nC# 과 마찬가지로, 모든 객체는 Object를 상속합니다.\n따라서 아래 표현이 가능합니다.\n\n\nObject balance = 1000;\n\n\nDart 상수 선언\n상수는 ‘final’ 또는 ‘const’ 키워드를 사용하여 선언할 수 있습니다. final로 선언한 경우, 변경이 불가능합니다.\nfinal int PRICE = 1000;\nPRICE = 2000; // ERROR!\n상수 선언의 경우, 아래와 같이 type 선언을 생략할 수 있습니다.\nfinal PRICE = 1000;\n‘final’과 ‘const’의 차이는 build-time에 값을 알 수 있는지의 여부입니다.\nDateTime.now()의 경우, 해당 구문이 실행될 때의 현재 시간을 반환하므로 build 하는 시점에서는 값을 알 수 없습니다.\nfianl now1 = DateTime.now();\n \nconst now2 = DateTime.now(); // ERROR!\nNullable 선언\n자료형 뒤에 ‘?’를 붙여서 nullable type을 선언할 수 있습니다.\nnullable인 경우, ‘??=’ operator를 사용할 수 있습니다. 좌항의 변수가 null인 경우 우항의 값을 대입합니다.\ndouble? number = null\nnumber ??= 3.0\nMap\n원하는 type으로 dictionary를 만들 수 있습니다.\nMap&lt;String, String&gt; dictionary = {\n\t&#039;a&#039;: &#039;1234&#039;,\n\t&#039;b&#039;: &#039;a134&#039;,\n\t&#039;c&#039;: &#039;3423&#039;,\n};\ndictionary[&#039;d&#039;] = &#039;223123&#039; // add new item\n조건문\nif문 사용법은 아래와 같습니다.\nif(number % 3 == 0)\n{\n\tprint(&#039;나머지는 0&#039;);\n}\nelse if(number % 3 == 1)\n{\n\tprint(&#039;나머지는 1&#039;);\n}\nelse\n{\n\tprint(&#039;나머지는 0&#039;);\n}\nswitch문 사용법은 아래와 같습니다.\nbreak와 default가 있어야 한다는 점에서 사용 방식은 C++, C#과 유사합니다.\nswitch(number % e){\n\tcase 0:\n\t\tprint(&#039;나머지는 0&#039;);\n\t\tbreak;\n\tcase 1:\n\t\tprint(&#039;나머지는 1&#039;);\n\t\tbreak;\n\tdefault:\n\t\tprint(&#039;나머지는 2&#039;);\n\t\tbreak;\n}\n반복문\nFor Loop 사용방식은 아래와 같습니다.\nfor(int i = 0; i &lt; 30; i++)\n{\n\t//some code\n}\nList&lt;int&gt; numbers = [1,2,3,4];\nfor(var number in numbers\n{\n\t//some code\n}\nwhile 및 do-while의 사용방식은 아래와 같습니다.\nint total = 0;\nwhile(total&lt;10){\n\t//some code\n}\ndo{\n\t//some code\n}while(total&gt;0)\nEnum 선언\n아래와 같이 enum을 선언할 수 있습니다.\nenum Status{\n\tapproved,\n\tpending,\n\trejected,\n}\n함수 선언\n함수 선언 방식은 다음과 같습니다.\n중괄호를 사용하여 optional parameter를 선언할 수 있습니다.\naddNumbers(int x, int y, int z, [int? a, int b=2]){\n\t// some code\n\treturn x + y + z\n}\n아래 방법으로 named parameter를 선언할 수 있습니다. named parameter로 선언되면 명시적으로 parameter 이름을 입력해야 합니다. ‘requred’ 키워드를 넣어야 하며, 없다면 optional parameter로 취급됩니다.\naddNumbers({\n\trequired int x,\n\trequired int y,\n\trequired int z,\n  int a=3,\n  int b=9,\n}){\n\t//some code\n}\n \naddNumbers(x:11, y:2, z:23)\naddNumbers(y:1, x:202, z:3)\naddNumbers(1,202,3) // ERROR!\nMethod Return Type\nvoid는 리턴타입이 없는 Method를 의미하며 생략할 수 있습니다.\nint형 변수를 반환하고 싶을 경우, int를 입력합니다\nvoid simplePrint(String word){\n\tprint(word)\n}\n \nsimplePrint(String word){\n\tprint(word)\n}\n \nint simpleSum(int a, int b){\n\treturn a+b\n}\nArrow Method\n간단한 메서드일 경우, 더 간단하게 표현할 수 있습니다.\nint add(int a, int b) =&gt; a+b;\nTypedef\n메서드 Delegate를 만들 수 있습니다.\ntypedef custom_op = int Function(int x, int y, int z);\nint add(int x, int y, int z) =&gt; x + y + z;\nint subtract(int x, int y, int z) =&gt; x - y - z;\n \ncustom_op op = add\nint result = op(1,2,3); // 6\nop = subtract\nresult = op(1,2,3) // -4"},"vault/Notion/DB/DB-Blog-Post/Programming-language-Dart-2-OOP":{"slug":"vault/Notion/DB/DB-Blog-Post/Programming-language-Dart-2-OOP","filePath":"vault/Notion/DB/DB Blog Post/Programming language Dart 2 OOP.md","title":"Programming language Dart 2 OOP","links":[],"tags":[],"content":"\n참조\n[[#Dart #2 OOP]]\n\nClass 선언 및 인스턴스 생성\nConst Constructor\nGetter &amp; Setter\nPrivate 선언\n상속\nMethod Override\nStatic\nInterface\nGeneric\n\n\n\n\n참조\n\n\n                  \n                  [코드팩토리] [입문] Dart 언어 4시간만에 완전정복 - 인프런 | 강의 \n                  \n                \n\n\n이 강의를 통해 Dart 언어를 배우면 Flutter를 시작할 수 있을 정도의 수준으로의 업그레이드가 가능합니다!\nwww.inflearn.com/course/dart-언어-입문/dashboard\n\n\n\nDart #2 OOP\nClass 선언 및 인스턴스 생성\n아래와 같이 class 선언하며 및 인스턴스 생성이 가능합니다.\n’this’ 키워드는 C++과 같이 인스턴스 자기 자신을 가르키는 키워드입니다.\n’final’로 정의한 멤버변수는 instance 생성 시 값을 할당할 수 있으며, 이후 변경할 수 없습니다.\nclass Idol{\n\tfinal String name;\n \n\tList&lt;String&gt; members = [&#039;지수&#039;,&#039;제니&#039;,&#039;리사&#039;,&#039;로제&#039;];\n \n\tvoid sayHello(){\n\t\tprint(&#039;안녕하세요?&#039;)\n\t}\n \n\tIdol(string name){\n\t\tthis.name = name\n\t}\n}\n \nvar blackPink = Idol(&#039;블랙핑크&#039;);\nConst Constructor\n‘const’ 키워드를 사용해서 인스턴스를 생성할 수 있습니다. 이 경우, 빌드 타임에 값을 알 수 있어야 합니다.\n또한 기존 instance와는 별도로, 내부 값이 같다면 다른 같은 메모리를 가르키게 됩니다.\nclass Idol{\n\tfinal String name;\n\tfinal List&lt;String&gt; members;\n\tconst Idol(this.name, this.members);\n}\n \nvar idol1 = const Idol(&#039;bp&#039;, [&#039;a&#039;,&#039;b&#039;,&#039;c&#039;]);\nvar idol2 = const Idol(&#039;bp&#039;, [&#039;a&#039;,&#039;b&#039;,&#039;c&#039;]);\nprint(idol1 == idol2); // true!!!\n\nGetter &amp; Setter\n아래와 같이 getter, setter를 정의할 수 있습니다.\nclass Idol{\n\tString name;\n\tList&lt;String&gt; members;\n \n\t//some code\n \n\tString get firstMember{\n\t\treturn this.members[0];\n\t} \n \n\tset firstMember(String name){\n\t\tthis.members[0]= name;\n\t}\n}\nPrivate 선언\n‘_’를 이름 앞에 붙여서 Private로 선언할 수 있습니다.\n_가 붙은 class , 함수, 변수는 다른 파일에서 참조할 경우 접근할 수 없습니다.\nclass _Idol{\n\t//some code\n}\n상속\n‘extends’ 키워드를 사용하여 상속할 수 있습니다. 이 경우, ‘super’를 사용하여 부모의 constructor를 호출해야 합니다. 부모객체의 요소들을 ‘super.*’ 으로 참조할 수 있습니다.\nclass Idol{\n\tIdol(int a, int b){\n\t}\n\t//some code\n}\n \nclass BoyGroup extends Idol{\n\tBoyGorup(int a, int b):super(a: a,b: b);\n\t// some code\n}\nMethod Override\n‘@override’를 Method 위에 적어 override 할 수 있습니다.\nclass TimesTwo{\n\tfinal int number;\n\tTimesTwo(this.number);\n\tint calc(){\n\t\treturn number * 2;\n\t}\n}\nclass TimesFour extends TimesTwo{\n\tTimesFour(int number) : super(number);\n\t@override\n\tint calc(){\n\t\treturn super.number * 4; // = return number = return this.number\n\t}\n}\nStatic\n‘static’ 키워드를 사용하여 정적 변수, 정적 메서드를 만들 수 있습니다.\nclass Employee{\n\tstatic String building = &#039;A2&#039;;\n\tstatic String PrintBuilding(){\n\t\treturn Employee.building;\n\t}\n}\n \nprint(Employee.building);\nEmployee.PrintBuilding();\nInterface\n상속 시 extends 대신 ‘implements’ 키워드를 사용해서 interface처럼 구현을 강제할 수 있습니다.\n또한 Interface로 사용할 class 앞에 ‘abstract’를 붙이면 해당 class는 인스턴스로 생성할 수 없고, 함수 body 구현도 생략할 수 있습니다.\nabstract class IdolInterface{\n\tString name;\n\tvoid sayName();\n\tIdolInterface(this.name);\n}\n \nclass BoyGroup implements IdolInterface{\n\tString name;\n\tvoid sayName(){\n\t\treturn sayName;\n\t};\n\tBoyGroup(this.name);\n}\nGeneric\nclass 옆에 ‘’을 추가하여 Generic class를 생성할 수 있습니다.\nclass Lecture&lt;T&gt; {\n\tfinal T id;\n\tfinal String name;\n}\n \nvar lec = Lecture&lt;int&gt;();"},"vault/Notion/DB/DB-Blog-Post/Project-Template":{"slug":"vault/Notion/DB/DB-Blog-Post/Project-Template","filePath":"vault/Notion/DB/DB Blog Post/Project Template.md","title":"Project Template","links":[],"tags":[],"content":"[이곳에 github repos 주소를 추가합니다]\n\n개요\n현상 / 이슈\n문제 정의\n해결방법\n결과\n\n\n개요\n\n현상 / 이슈\n\n문제 정의\n\n해결방법\n\n결과"},"vault/Notion/DB/DB-Blog-Post/PyTorch-딥러닝-학습의-기본-순서":{"slug":"vault/Notion/DB/DB-Blog-Post/PyTorch-딥러닝-학습의-기본-순서","filePath":"vault/Notion/DB/DB Blog Post/PyTorch 딥러닝 학습의 기본 순서.md","title":"PyTorch 딥러닝 학습의 기본 순서","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\nPyTorch 딥러닝 학습의 기본 순서\n\n\n참조\nWeek 2\nPyTorch 딥러닝 학습의 기본 순서\n# Optimize 대상인 각 parameter들의 gradient 값 초기화\noptimizer.zero_grad()\n \n# 예측값 계산\noutput = model(inputs) # \n \n# Ground_truth와 예측 값 사이의 loss 계산\nloss = loss_function(output, ground_truth)\n \n# loss 값으로 각 parameter의 gradient 값 계산\nloss.backward()\n \n# gradient 값으로 각 parameter 갱신\noptimizer.step()"},"vault/Notion/DB/DB-Blog-Post/Pytorch-Performance-Tuning-Practices/Pytorch-Performance-Tuning-Practices":{"slug":"vault/Notion/DB/DB-Blog-Post/Pytorch-Performance-Tuning-Practices/Pytorch-Performance-Tuning-Practices","filePath":"vault/Notion/DB/DB Blog Post/Pytorch Performance Tuning Practices/Pytorch Performance Tuning Practices.md","title":"Pytorch Performance Tuning Practices","links":[],"tags":[],"content":"\n참조\nGeneral optimizations\n\nUse async data loading\nPin memory, transfer data asynchronously\nEfficiently zero-out gradients\nIncrease batch size\n\n\nGPU specific optimizations\n\nUse 16-bit precision\nEnable cuDNN autotuner\nAvoid unnecessary CPU-GPU synchronization\nConstruct tensors directly on GPUs\n\n\nDistributed optimizations\n\nUse DistributedDataParallel not DataParallel\n\n\n\n\n참조\n\n\n                  \n                  Top 10 Performance Tuning Practices for Pytorch \n                  \n                \n\n\nPytorch 모델의 학습 및 추론을 가속화 할 수 있는 10가지 팁을 공유드립니다.\nmedium.com/naver-shopping-dev/top-10-performance-tuning-practices-for-pytorch-e6c510152f76\n\n\n\nGeneral optimizations\nUse async data loading\n\ntorch.utils.data.DataLoader(dataset, num_workers=num_workers)\n\nnum_workers = 0: 메인 프로세스가 데이터를 디스크에서 동기식으로 로딩합니다.\nnum_workers &gt; 0: 여러 프로세스를 사용하여 디스크에서 데이터를 비동기식으로 읽고, 학습과 데이터로딩이 overlapping될 수 있도록 허용합니다. CPU의 데이터 로딩을 빠르게 처리하는 용도로 사용합니다.\n\n\n\nPin memory, transfer data asynchronously\n\n\ntorch.utils.data.DataLoader(dataset, pin_memory=True) batch.to(device, non_blocking=True)\n\nGPU가 pageable host 메모리에서 곧바로 데이터를 가져올 수 없기 때문에, pinned (page-locked) 메모리를 활용\npin_memory=True\n\n데이터 텐서를 자동으로 pinned 메모리로 가져오기 때문에, 데이터 전송이 빠릅니다.\n\n\npin_memory=True, non_blocking=True\n\npinned 메모리에 있는 데이터에 한해서 GPU로 비동기식으로 데이터를 전송합니다.\nGPU 데이터 전송 이후의 연산이 GPU 데이터를 필요로 하지 않는 경우, 속도 개선 효과를 볼 수 있습니다. 데이터 전송이 모두 완료되기 전에, 기다리지 않고 즉시 연산을 실행하기 때문입니다.\npage-locked memory은 다른 작업에 의해 memory deallocation 되지 않기 때문에, 너무 많은 메모리를 점유하게 될 경우, 다른 데이터가 메모리에 못 올라오는 문제가 생길 수 있습니다.\n\n\n\n\n\nEfficiently zero-out gradients\n\nmodel.zero_grad(set_to_none=True)\n\nmodel.zero_grad() 대신 사용합니다.\n모든 파라미터마다 memset을 실행하지 않습니다.\nGradient를 업데이트할 때, “+=” (read+write)이 아닌 “=” (write)를 사용합니다.\n\n\n\nIncrease batch size\n\n배치 크기를 키워서 GPU 메모리를 최대한 활용하는 것이 학습 시간을 단축하는데 큰 도움이 됩니다.\n배치 크기가 크면, 수렴이 느려질 수 있기 때문에 아래와 같은 방법을 사용해서 보완할 수 있습니다.\n\nTune learning rate, tune weight decay\nAdd learning rate warm-ups &amp; decay\n\n\n\nGPU specific optimizations\nUse 16-bit precision\nscaler = torch.cuda.amp.GradScaler()\nwith torch.cuda.amp.autocast(enabled=use_fp16):\n    output = model(input)\n    loss = loss_fn(output, target)\nif use_fp16:\n    scaler.scale(loss).backward()\n    if max_norm is not None:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n    scaler.step(optimizer)\n    scaler.update()\nMixed precision training은 FP16, FP32를 같이 사용해서 학습하는 방법입니다. 일반적으로 2단계로 이루어집니다.\n\nFP16으로 casting.\nFP16 숫자가 0으로 되지 않도록 loss / gradient scaling.: FP16이 나타낼 수 있는 수의 최소 범위 (2²⁴) 보다 숫자가 작아서 0으로 강제 변환하는 문제를 scaling으로 해결.\n\nMixed precision training을 사용했을 때 다음과 같은 이점이 있습니다.\n\nFP32으로만 학습할 때와 비슷한 정확도.\n필요한 메모리 사이즈 감소.\n학습 시간 감소.Tensor core 를 지원하는 gpu는(e.g. V100) mixed precision training 을 위한 하드웨어 가속을 제공하기 때문에 효과 극대화. V100 기준, 1.5~5배 speedup.\n\nEnable cuDNN autotuner\ntorch.backends.cudnn.benchmark = True\n\nNvidia cuDNN은 convolution (CNN)을 계산하기 위해 다양한 알고리즘을 지원하고 있습니다.\nAutotuner는 짧은 benchmark 실행하고, 하드웨어와 input 크기에 최적화된 알고리즘을 선택합니다.\n(주의) 고정된 input 크기일 때만 효과적이고, input 크기가 동적으로 변하면 매번 최적화된 알고리즘을 찾게 되어 시간이 더 오래 걸릴 수도 있습니다.\n(참고) Batch size, input / output size가 최소 64, 이상적으로는 256으로 나뉘어지는 수로 선택하기를 권장합니다.\n\nAvoid unnecessary CPU-GPU synchronization\n# don&#039;t\n.item()\n.cuda()\n.cpu()\n.to(device)\n.nonzero()\nprint(tensor)\n\n불필요하게 GPU, CPU간 데이터를 전송하는 경우, 성능이 크게 저하됩니다.\ncuda tensor의 operation에 의존하는 경우, 성능이 저하됩니다.e.g. (cuda_tensor != 0).all()\n\nConstruct tensors directly on GPUs\n# don&#039;t\nt = tensor.rand(2,2).cuda()\n \n# do\nt = tensor.rand(2,2, device=torch.device(&#039;cuda:0&#039;))\n\nDon’t: CPU에 tensor를 생성한 후에 GPU로 전송하기 때문에, 시간이 오래 걸립니다.\nDo: Tensor를 device에 곧바로 생성하는 것을 권장합니다.\n\nDistributed optimizations\nUse DistributedDataParallel not DataParallel\n\n\nSpawn processes\n\n\nnprocs: 현재 머신에서 생성(spawn) 할 프로세스 수(GPU당 하나의 프로세스를 생성하는 경우, nprocs=gpu_num)\n\ntorch.multiprocessing.spawn(main_worker, nprocs=args.gpu_num, args=(args,))\n\nEnvironment variable initialization\n\n\nMASTER_ADDR: rank 0 머신의 주소.하나의 머신으로 학습할 경우, “127.0.0.1”로 설정.\nMASTER_PORT: rank 0 머신의 free port.\nWORLD_SIZE: init_process_group에서 세팅 가능.\nRANK: init_process_group에서 세팅 가능.\nrank 0 머신이 모든 connection을 setup.\n\nos.environ[&#039;MASTER_ADDR&#039;] = master_address\nos.environ[&#039;MASTER_PORT&#039;] = str(master_port)\n\nInitialize process group\n\n\nbackend: nccl (GPU용. backend 가장 빨라서 권장) / gloo (CPU용)\ninit_method: peer process를 어디서/어떻게 찾을 수 있는 지 설정. [참고]환경변수로 MASTER_ADDR, MASTER_PORT 세팅했으면, ‘env://‘로 설정 가능.\nworld_size: 동시에 실행되는 총 애플리케이션 프로세스 수.\nrank: 모든 프로세스 중 global rank\n\ntorch.distributed.init_process_group(backend=&#039;nccl&#039;,\n                                     init_method=&#039;env://&#039;,\n                                     world_size=world_size,\n                                     rank=rank)\n\nDistributed Data Parallel\n\n\ndevice_ids: 코드가 작동할 GPU device id. 일반적으로 프로세스의 local rank.\n\nmodel = torch.nn.parallel.DistributedDataParallel(\n    model,\n    device_ids=[gpu],\n    output_device=gpu,\n)"},"vault/Notion/DB/DB-Blog-Post/Quick-Sort/Quick-Sort":{"slug":"vault/Notion/DB/DB-Blog-Post/Quick-Sort/Quick-Sort","filePath":"vault/Notion/DB/DB Blog Post/Quick Sort/Quick Sort.md","title":"Quick Sort","links":[],"tags":[],"content":"참조\ngmlwjd9405.github.io/2018/05/10/algorithm-quick-sort.html\ngyoogle.dev/blog/algorithm/Quick Sort.html\nlatte-is-horse.tistory.com/197\nQuick Sort란?\nQuick sort는 분할정복 알고리즘의 하나로, 평균적으로 매우 빠른 수행 속도를 자랑하는 정렬 방법입니다.\n\n불안정 정렬에 속하며, 다른 원소와의 비교만으로 정렬을 수행하는 비교 정렬에 속한다.\n시간복잡도\n\n최선의 경우 : O(n\\log_2n)\n평균 : O(n\\log_2n)\n최악의 경우 : O(n^2)\n\n\n공간 복잡도 : O(n)\n\n주어진 배열 안에서 교환(swap)을 통해, 정렬이 수행됨\n\n\n\n과정\n\n\n노란색이 pivot,\n초록색이 pivot보다 작은 데이터,\n보라색이 pivot보다 큰 데이터,\n주황색이 정렬이 완료된 데이터\n\n리스트 안에 있는 한 요소(pivot)를 선택한다.\npivot을 기준으로 pivot보다 작은 요소들은 모두 pivot의 왼쪽으로 옮기고 pivot보다 큰 요소들은 모두 pivot의 오른쪽으로 옮긴다.\npivot을 제외한 왼쪽 리스트와 오른쪽 리스트를 다시 정렬한다.\n\n분할된 왼쪽 리스트와 오른쪽 리스트도 다시 pivot을 정하고 pivot을 기준으로 2개의 부분리스트로 나눈다.\n재귀를 사용하여 부분 리스트들이 더이상 분할이 불가능 할 때까지 반복한다.\n\n\n\n장점 &amp; 단점\n\n장점\n\n불필요한 데이터의 이동을 줄이고 먼 거리의 데이터를 교환할 뿐만 아니라, 한 번 결정된 피벗들이 추후 연산에서 제외되는 특성 때문에, 시간 복잡도가 O(nlog₂n)를 가지는 다른 정렬 알고리즘과 비교했을 때도 가장 빠르다.\n정렬하고자 하는 배열 안에서 교환하는 방식이므로, 다른 메모리 공간을 필요로 하지 않는다.\n\n\n단점\n\n불안정 정렬(Unstable Sort) 이다.\n정렬된 배열에 대해서는 Quick Sort의 불균형 분할에 의해 오히려 수행시간이 더 많이 걸린다.\n\n\n\n코드구현(C)\n# include &lt;stdio.h&gt;\n# define MAX_SIZE 9\n# define SWAP(x, y, temp) ( (temp)=(x), (x)=(y), (y)=(temp) )\n \n// 1. 피벗을 기준으로 2개의 부분 리스트로 나눈다.\n// 2. 피벗보다 작은 값은 모두 왼쪽 부분 리스트로, 큰 값은 오른쪽 부분 리스트로 옮긴다.\n/* 2개의 비균등 배열 list[left...pivot-1]와 list[pivot+1...right]의 합병 과정 */\n/* (실제로 숫자들이 정렬되는 과정) */\nint partition(int list[], int left, int right){\n  int pivot, temp;\n  int low, high;\n \n  low = left;\n  high = right + 1;\n  pivot = list[left]; // 정렬할 리스트의 가장 왼쪽 데이터를 피벗으로 선택(임의의 값을 피벗으로 선택)\n \n  /* low와 high가 교차할 때까지 반복(low&lt;high) */\n  do{\n    /* list[low]가 피벗보다 작으면 계속 low를 증가 */\n    do {\n      low++; // low는 left+1 에서 시작\n    } while (low&lt;=right &amp;&amp; list[low]&lt;pivot);\n \n    /* list[high]가 피벗보다 크면 계속 high를 감소 */\n    do {\n      high--; //high는 right 에서 시작\n    } while (high&gt;=left &amp;&amp; list[high]&gt;pivot);\n \n    // 만약 low와 high가 교차하지 않았으면 list[low]를 list[high] 교환\n    if(low&lt;high){\n      SWAP(list[low], list[high], temp);\n    }\n  } while (low&lt;high);\n \n  // low와 high가 교차했으면 반복문을 빠져나와 list[left]와 list[high]를 교환\n  SWAP(list[left], list[high], temp);\n \n  // 피벗의 위치인 high를 반환\n  return high;\n}\n \n// 퀵 정렬\nvoid quick_sort(int list[], int left, int right){\n \n  /* 정렬할 범위가 2개 이상의 데이터이면(리스트의 크기가 0이나 1이 아니면) */\n  if(left&lt;right){\n    // partition 함수를 호출하여 피벗을 기준으로 리스트를 비균등 분할 -분할(Divide)\n    int q = partition(list, left, right); // q: 피벗의 위치\n \n    // 피벗은 제외한 2개의 부분 리스트를 대상으로 순환 호출\n    quick_sort(list, left, q-1); // (left ~ 피벗 바로 앞) 앞쪽 부분 리스트 정렬 -정복(Conquer)\n    quick_sort(list, q+1, right); // (피벗 바로 뒤 ~ right) 뒤쪽 부분 리스트 정렬 -정복(Conquer)\n  }\n \n}\n \nvoid main(){\n  int i;\n  int n = MAX_SIZE;\n  int list[n] = {5, 3, 8, 4, 9, 1, 6, 2, 7};\n \n  // 퀵 정렬 수행(left: 배열의 시작 = 0, right: 배열의 끝 = 8)\n  quick_sort(list, 0, n-1);\n \n  // 정렬 결과 출력\n  for(i=0; i&lt;n; i++){\n    printf(&quot;%d\\n&quot;, list[i]);\n  }\n}"},"vault/Notion/DB/DB-Blog-Post/RNN-내부에서-sigmoid-또는-tanh를-쓰는-이유":{"slug":"vault/Notion/DB/DB-Blog-Post/RNN-내부에서-sigmoid-또는-tanh를-쓰는-이유","filePath":"vault/Notion/DB/DB Blog Post/RNN 내부에서 sigmoid 또는 tanh를 쓰는 이유.md","title":"RNN 내부에서 sigmoid 또는 tanh를 쓰는 이유","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\nRNN 내부에서 sigmoid 또는 tanh를 쓰는 이유\n\n\n참조\ngruuuuu.github.io/machine-learning/lstm-doc2/\nWeek 1\nRNN 내부에서 sigmoid 또는 tanh를 쓰는 이유\n\nRNN은 구조상 반복된 곱연산이 발생한다. 따라서 뒤로 갈수록 값이 무한정 커질 수 밖에 없다.\n하지만 값을 tanh 또는 sigmoid를 거쳐가게 하면 sigmoid의 경우 항상 [0,1]사이의 값 되도록 보장할 수 있고 tanh의 경우 [-1,1]사이의 값이 되도록 보장할 수 있다.\n그러므로 계속 곱연산이 일어나도 결과 값이 발산하지 않을 수 있다.\n"},"vault/Notion/DB/DB-Blog-Post/RNN(Recurrent-Neural-Networks)/RNN(Recurrent-Neural-Networks)":{"slug":"vault/Notion/DB/DB-Blog-Post/RNN(Recurrent-Neural-Networks)/RNN(Recurrent-Neural-Networks)","filePath":"vault/Notion/DB/DB Blog Post/RNN(Recurrent Neural Networks)/RNN(Recurrent Neural Networks).md","title":"RNN(Recurrent Neural Networks)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\nRNN(Recurrent Neural Networks)\n\n\n참조\nratsgo.github.io/natural language processing/2017/03/09/rnnlstm/\nWeek 1\nRNN(Recurrent Neural Networks)\n\n\n녹색 박스는 hidden state를 의미\n빨간 박스는 input data를 의미\n파란 박스는 output data를 의미\n현 시점의 hidden state(h_t)는 이전 시점의 hidden state(h_{t-1})을 받아 갱신됨\n첫 번째 RNN의 경우, 이전 hidden state가 존재하지 않으므로 0을 입력\n\n\n"},"vault/Notion/DB/DB-Blog-Post/RPN(Region-Proposal-Network)/RPN(Region-Proposal-Network)":{"slug":"vault/Notion/DB/DB-Blog-Post/RPN(Region-Proposal-Network)/RPN(Region-Proposal-Network)","filePath":"vault/Notion/DB/DB Blog Post/RPN(Region Proposal Network)/RPN(Region Proposal Network).md","title":"RPN(Region Proposal Network)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-9/Week-9","vault/Notion/DB/DB-Blog-Post/NMS(Non-Maximum-Suppression)"],"tags":[],"content":"\n참조\nRPN(Region Proposal Network)\nRPN Inference 과정\nNMS의 Train 및 Loss\n\n\n참조\ntowardsdatascience.com/region-proposal-network-a-detailed-view-1305c7875853\nvelog.io/@suminwooo/RPNRegion-Proposal-Network-정리\nblog.paperspace.com/faster-r-cnn-explained-object-detection/\nWeek 9\nRPN(Region Proposal Network)\n\nRPN은 Feature Map에서 Object가 존재할 수 있는 Region을 예측하는 Network입니다.\nRPN Inference 과정\n\n\nInput Feature Map\n\nInput Shape : : H x W x C\nRegion proposal을 생성하기 위해 feature map위에 nxn window를 Sliding, 객체 존재여부 예측과 Bounding Box Regression을 수행\n\n\n\nObject Probability Prediction per pixel of each anchor box\n\nAnchor 내부에 Object가 있을 확률\nOutput Shape : H x W x 2k\n2k : 2(Object 존재 여부에 대한 확률) x k(anchor box 수 만큼의 channel)\n\n\n존재 여부라면, 0과 1로 표현할 수 있으므로(Sigmoid) k개로도 가능\n\n\n그렇다면 왜 2k(2-class softmax)를 사용했는가?[논문 link]\n→ 저자는 간단함 때문이라고 합니다.\n\n3.1.1 Anchors\nAt each sliding-window location, we simultaneously\npredict multiple region proposals, where the number\nof maximum possible proposals for each location is\ndenoted as k.\nSo the reg layer has 4k outputs encoding\nthe coordinates of k boxes, and the cls layer outputs\n2k scores that estimate probability of object or not\nobject for each proposal.*\n\nFor simplicity we implement the cls layer as a two-class\nsoftmax layer. Alternatively, one may use logistic regression to\nproduce k scores.\n\n\n\n\n\n\n\n\nBounding Box Regression, 각 anchor box 별 픽셀 별 delta)\n\nOutput Shape : H x W x 4k\n4k : 각 anchor box의 delta(dx, dy ,dh, dw)와 anchor box 의 총 수 k\nAnchor Box\n\nFaster R-CNN에서는 3개의 Scale(128128, 256256, 512*512), 3 개의 비율(1:1, 1:2, 2:1)을 이용해 k=9개의 Anchor Box(미리 정의된 형태를 가진 박스)를 생성\n\n\nDelta : Anchor의 크기와 위치를 조정하기 위한 값, 모델을 통해 학습 됨\n\nAnchor 하나 당 Delta 하나가 대응 됨\nAhchor 구성: (Y1,X1,Y2,X2)\nDelta :구성 (deltaCenterY, deltaCenterX, deltaHeight, deltaWidth)\n이해한 것\n\nImage Space 위에서, 어떤 Anchor Box를 생성할 때, Anchor Box의 중점이 위치할 수 있는 경우의 수는 이미지 공간의 총 픽셀 수(H x W)와 같음\n임의의 위치(x,y)를 중심으로 하는 Anchor box가 어떤 Object 위에 있다고 가정했을 때, 해당 Anchor box가 그 Object를 온전히 감싸려면 얼마나 이동하고(dx,dy) 얼마나 크기를 조정해야(dh, dw) Object를 온전히 감쌀 수 있는 Bounding Box가 되는가에 대한 추측 값\n만들어질 수 있는 Anchor Box 위치의 모든 경우의 수는 H x W x k\n각 경우마다 가지고 있는 정보의 수는 2(object probability) + 4(delta)\n\n\n\n\nRPN의 출력 차원은 H x W x k x 6 → 무수히 많은 Bounding Box가 생성 됨\n\n\n\nBounding Box Selection\n\nH x W x k x 6개의 box는 너무 많은 양이기 때문에 줄이는 것이 필요하다.(후보군 선택)\n따라서 object 존재 확률이 가장 높은 anchor box position부터 내림차순으로 n개를 선택한다. 논문에서는 2000개 정도를 선택했다고 합니다.\n하지만 그럼에도 불구하고, 너무 많은 box이며 나아가 겹치는 box들 또한 존재합니다.\n그러므로 NMS(Non-maximum suppression)을 통해 불필요한 box를 한번 더 제거합니다.\n\n\n\nNMS\nNMS(Non-Maximum Suppression) 참조\n\n\nNMS의 Train 및 Loss\n\n\nTrain\n\nIoU &gt; 0.7인 anchor box를 positive\nIoU &lt; 0.3인 anchor box를 negative\n다른 경우 사용 안함\n\n\n\nLoss\nL({\\{p_i\\}},{\\{t_i\\}})={1 \\over N_{cls}}{\\sum_i}L_{cls}(p_i, p_i^*) \\\\+\\lambda{1 \\over N_{reg}}{\\sum_i}{p_i^*}{L_{reg}}(t_i, t_i^*)\n\n"},"vault/Notion/DB/DB-Blog-Post/Receptive-Field(수용필드,-수용장)/Receptive-Field(수용필드,-수용장)":{"slug":"vault/Notion/DB/DB-Blog-Post/Receptive-Field(수용필드,-수용장)/Receptive-Field(수용필드,-수용장)","filePath":"vault/Notion/DB/DB Blog Post/Receptive Field(수용필드, 수용장)/Receptive Field(수용필드, 수용장).md","title":"Receptive Field(수용필드, 수용장)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4"],"tags":[],"content":"\n참조\nReceptive Field\n\n\n참조\nblog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807\nwww.baeldung.com/cs/cnn-receptive-field-size\nwww.youtube.com/watch\nWeek 4\nReceptive Field\n\n\n특정 CNN Feature에 대해서 입력공간이 영향을 받는 크기\n\nex ) 첫 번째 CNN 계층의 Feature의 입력 공간에 대한 Receptive Field는 3x3이다.\nex ) 두 번째 CNN 계층의 Feature의 입력 공간에 대한 Receptive Field는 5x5이다.\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Regularization---Batch-normalization/Regularization---Batch-normalization":{"slug":"vault/Notion/DB/DB-Blog-Post/Regularization---Batch-normalization/Regularization---Batch-normalization","filePath":"vault/Notion/DB/DB Blog Post/Regularization - Batch normalization/Regularization - Batch normalization.md","title":"Regularization - Batch normalization","links":[],"tags":[],"content":"\n참조\nBatch normalization(Batch Norm)\nBatch normalization - 학습단계\nBatch normalization - 추론단계\nInternal Covariant Shift\n\n\n참조\nen.wikipedia.org/wiki/Batch_normalization\ngaussian37.github.io/dl-concept-batchnorm/\nBatch normalization(Batch Norm)\n\nbatch normalization은 학습 과정에서 각 배치 단위 별로 데이터가 다양한 분포를 가지더라도 각 배치별로 평균과 분산을 이용해 정규화하는 것을 뜻합니다.\n\ngradient descent에서는 gradient를 한번 업데이트 하기 위하여 모든 학습 데이터를 사용합니다.\n그러나 대용량의 데이터를 한번에 처리하지 못하기 때문에 데이터를 batch 단위로 나눠서 학습을 하는 방법을 사용하는 것이 일반적이며, SGD 등을 사용합니다.\nBatch 단위로 학습을 하게 되면 Internal Covariant Shift 문제가 발생할 수 있으며 Batch Normalization으로 이를 해결할 수 있습니다.\n\nBatch normalization - 학습단계\nBN(X)=\\gamma ({{X - \\mu_{batch}}\\over{\\sigma_{batch}}})+\\beta\n\\mu_{batch}={1\\over B}\\sum_i{x_i}\n\\sigma^2_{batch}={1\\over B}\\sum_i{x_i-\\mu_{batch}}\n\n\n학습 단계의 BN을 구하기 위하여 사용된 평균과 분산을 구할 때에는 배치별로 계산\n되어야 의미가 있습니다.\n\n\nbatch normalization은 activation function 앞에 적용됩니다.\n\n\nbatch normalization을 적용하면 weight의 값이 평균이 0, 분산이 1인 상태로 분포가 되어지는데, 이 상태에서 ReLU가 activation으로 적용되면 전체 분포에서 음수에 해당하는 (1/2 비율) 부분이 0이 되어버립니다. 기껏 정규화를 했는데 의미가 없어져 버리게 됩니다.\n\n\n따라서 γ,β가 정규화 값에 곱해지고 더해져서 ReLU가 적용되더라도 기존의 음수 부분이 모두 0으로 되지 않도록 방지해 주고 있습니다. 물론 이 값은 학습을 통해서 효율적인 결과를 내기 위한 값으로 찾아갑니다.\n\n\nBatch normalization - 추론단계\nBN(X)=\\gamma ({{X - \\mu_{BN}}\\over{\\sigma_{BN}}})+\\beta\n\\mu_{BN}={1\\over N}\\sum_i{\\mu_{i_{batch}}}\n\\sigma^2_{BN}={1\\over B}\\sum_i{\\sigma_{i_{batch}}}\n추론 과정에서는 framework에서 옵션을 지정하여 평균과 분산을 moving average/variance\n를 사용하도록 해야합니다.\n\n추론 과정에서는 BN에 적용할 평균과 분산에 고정값을 사용합니다.\n추론 시에는 배치 단위로 데이터를 넣지 않기 때문에, 학습 단계에서 배치 단위의 평균/분산을 저장해 놓고 테스트 시에는 평균/분산을 사용합니다.\n고정된 평균과 분산은 학습 과정에서 이동 평균(moving average) 또는 지수 평균(exponential average)을 통하여 계산한 값입니다.\n\n학습 하였을 때의 최근 N개에 대한 평균 값을 고정값으로 사용하는 것입니다.\n이동 평균을 하면 N개 이전의 평균과 분산은 미반영 되지만 지수 평균을 사용하면 전체 데이터가 반영됩니다.\n\n\n\nInternal Covariant Shift\n\nInternal Covariant Shift는 학습 과정에서 계층 별로 입력의 데이터 분포가 달라지는 현상입니다.\n이와 유사하게 Batch 단위로 학습을 하게 되면 Batch 단위간에 데이터 분포의 차이가 발생할 수 있습니다."},"vault/Notion/DB/DB-Blog-Post/Regularization---Data-Augmentation":{"slug":"vault/Notion/DB/DB-Blog-Post/Regularization---Data-Augmentation","filePath":"vault/Notion/DB/DB Blog Post/Regularization - Data Augmentation.md","title":"Regularization - Data Augmentation","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-3"],"tags":[],"content":"\n참조\nData Augmentation\n\n\n참조\nen.wikipedia.org/wiki/Data_augmentation\nWeek 3\nData Augmentation\n\n약간의 수정이 가해진 복제본을 추가하여 데이터의 수를 늘리는 기법\n"},"vault/Notion/DB/DB-Blog-Post/Regularization---Dropout/Regularization---Dropout":{"slug":"vault/Notion/DB/DB-Blog-Post/Regularization---Dropout/Regularization---Dropout","filePath":"vault/Notion/DB/DB Blog Post/Regularization - Dropout/Regularization - Dropout.md","title":"Regularization - Dropout","links":[],"tags":[],"content":"\n참조\nDropout\n\n\n참조\ndeepestdocs.readthedocs.io/en/latest/004_deep_learning_part_2/0042/\nDropout\n\n매 훈련 과정에서 랜덤으로 일부 퍼셉트론을 없애버림\n\n퍼셉트론에 연결된 가중치들도 의미없는 값이 됩니다.\n이 과정에서 약간씩 다른, 엄청 다양한 구조가 학습된다고 생각할 수 있습니다.\n\n\n구현할 때는 일단 계산한 후 0 or 1 마스크를 곱해 결과값을 0으로 만들어버리는 방식으로 구현합니다.\n\n"},"vault/Notion/DB/DB-Blog-Post/Regularization---Early-Stopping":{"slug":"vault/Notion/DB/DB-Blog-Post/Regularization---Early-Stopping","filePath":"vault/Notion/DB/DB Blog Post/Regularization - Early Stopping.md","title":"Regularization - Early Stopping","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-3"],"tags":[],"content":"\n참조\nEarly Stopping\n\n\n참조\nen.wikipedia.org/wiki/Early_stopping\nWeek 3\nEarly Stopping\n\n과적합을 피하기 위해 사용하는 정규화 방법의 일종\n훈련을 반복할 때 마다 어느정도까지는 훈련 세트의 외부의 데이터에 대한 학습자의 성능이 향상되나 그 지점을 지나면 over-fit되어 generalization error 증가\ngeneralization error가 증가하기 전 학습을 종료\n"},"vault/Notion/DB/DB-Blog-Post/Regularization---Label-Smoothing":{"slug":"vault/Notion/DB/DB-Blog-Post/Regularization---Label-Smoothing","filePath":"vault/Notion/DB/DB Blog Post/Regularization - Label Smoothing.md","title":"Regularization - Label Smoothing","links":[],"tags":[],"content":"\n참조\nLabel Smoothing\n\n\n참조\nratsgo.github.io/insight-notes/docs/interpretable/smoothing\nLabel Smoothing\n레이블 스무딩이란 Szegedy et al. (2016)이 제안한 기법으로, 라벨을 스무딩하여 모델 일반화 성능을 꾀합니다.\ny_{k}^{L.S.}=y_{k}(1-\\alpha)+{\\alpha}/K\n예시)\ny_1 = [0,1,0,0]\\ \\ \\ \\alpha=0.1\ny_{1}^{L.S.} = y_1(1-0.1)+0.1/4 \\\\=[0.25,0.925,0.025,0.025]"},"vault/Notion/DB/DB-Blog-Post/ResNet/ResNet":{"slug":"vault/Notion/DB/DB-Blog-Post/ResNet/ResNet","filePath":"vault/Notion/DB/DB Blog Post/ResNet/ResNet.md","title":"ResNet","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4"],"tags":[],"content":"참조\narxiv.org/abs/1512.03385\nphil-baek.tistory.com/entry/ResNet-Deep-Residual-Learning-for-Image-Recognition-논문-리뷰\nWeek 4\n배경\n당시 연구결과에 따르면 네트워크의 깊이는 매우 중요한데, 실제로 많은 모델들이 깊은 네트워크를 통해 좋은 성능을 보였다고 합니다.\nNetwork의 Depth가 중요해지면서, layer를 많이 쌓았을 때 발생하는 Vanishing / Exploding gradient 현상은 큰 방해 요소였습니다.\nResNet이란?\n\nResNet은 컴퓨터 비전 관련 딥러닝 모델로, 2015년 ImageNet 대회에서 우승한 모델입니다.\nResNet은 각 레이어에서 입력 데이터를 단순화하고 보다 정확하게 분류하기 위해 특별한 구조인 Skip-Connection를 사용합니다. Skip-Connection은 모델이 보다 깊은 레이어를 학습하고 잘 작동할 수 있게 해줍니다.\nResNet의 특징\n\nShortcut connection을 도입하여 기존보다 더 깊게 층을 쌓을 수 있게 되었습니다.\n\n기존 네트워크는 입력 x를 받고 layer를 거쳐 H(x)를 출력하는데, 이는 입력값 x를 타겟값 y로 mapping 하는 함수 H(x)를 얻는 것이 목적입니다.\n하지만 ResNet의 Residual Learning은 H(x)가 아닌 출력과 입력의 차인 H(x) - x를 얻도록 목표를 수정했습니다.\n\n\n\n곱셈 연산에서 덧셈 연산으로 변형되어 몇 개의 layer를 건너뛰는 효과가 있었는데, 이 덕에 forward와 backward path가 단순해지는 효과가 있었으며, gradient의 소멸 문제를 해결 할 수 있었다고 합니다.\n\nResNet 구현\n\n\nConvBlock\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super().__init__()\n \n        #\\#fill it##\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding) # kernel size = ...\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n \n    def forward(self, x):\n        \n        #\\#fill it##\n        x = self.conv(x)\n        x = self.batchnorm(x)\n        return x\n\n\nResBlock\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, pool_stride = 1):\n        super().__init__()\n \n        self.kernel_size = 3\n        self.padding = 1\n        self.stride = 1\n        self.relu = nn.ReLU()\n        self.pool_stride = pool_stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        if(in_channels == out_channels):\n          self.skip = torch.nn.Identity()\n        else:\n          self.skip = torch.nn.Conv2d(in_channels=in_channels, \n                               out_channels=out_channels, \n                               kernel_size=1, \n                               stride=self.pool_stride)\n          \n        self.conv1 = ConvBlock(in_channels=in_channels, \n                               out_channels = out_channels, \n                               kernel_size=self.kernel_size, \n                               stride=self.stride, \n                               padding=self.padding)\n        \n        self.conv2 = ConvBlock(in_channels=out_channels, \n                               out_channels = out_channels, \n                               kernel_size=self.kernel_size, \n                               stride=self.pool_stride, \n                               padding=self.padding)\n \n    def forward(self, x):\n        \n        #\\#fill##\n        y = self.conv1(x)\n        y = self.relu(y)\n        y = self.conv2(y)\n        return  self.relu(y + self.skip(x))\n\n\nResnet Model\nclass ResNet(nn.Module):\n    def __init__(self, in_channels, out_channels, nker=64, nblk=[3,4,6,3]):\n        super(ResNet, self).__init__()\n \n        self.enc = ConvBlock(in_channels, nker, kernel_size=7, stride=2, padding=1)\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.average_pool = nn.AvgPool2d(kernel_size=5, stride=1)\n \n \n        #\\#fill##\n        self.relu = nn.ReLU()\n        layers = []\n        for j, b in enumerate(nblk):\n          __out_chennel = 64 * (j+1)\n          for i in range(b):\n            if(j != 0 and i == 0):\n              __pool_stride = 2\n              __in_chennel = 64 * j\n \n            else:\n              __pool_stride = 1\n              __in_chennel = __out_chennel\n            print(__in_chennel, __out_chennel, __pool_stride)\n            layers.append(ResBlock(__in_chennel, __out_chennel, __pool_stride))\n        print(&#039;complete auto layer making&#039;)\n        self.conv = nn.Sequential(*layers)\n \n        self.fc = nn.Linear(nker*2*2, 10)\n \n    def forward(self, x):\n        x = self.enc(x)\n        x = self.max_pool(x)\n \n        #\\#fill##\n        x = self.conv(x)\n        x = self.average_pool(x)\n        x = x.view(x.shape[0], -1)\n        out = self.fc(x)\n \n        return out\n\n"},"vault/Notion/DB/DB-Blog-Post/RetinaNet/RetinaNet":{"slug":"vault/Notion/DB/DB-Blog-Post/RetinaNet/RetinaNet","filePath":"vault/Notion/DB/DB Blog Post/RetinaNet/RetinaNet.md","title":"RetinaNet","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10"],"tags":[],"content":"\n참조\nRetinaNet\nFocal Loss\n\n\n참조\n\n\n                  \n                  Focal Loss for Dense Object Detection \n                  \n                \n\n\nThe highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations.\narxiv.org/abs/1708.02002\n\n\n\nWeek 10\ndeep-learning-study.tistory.com/504\ncsm-kr.tistory.com/5\ntalktato.tistory.com/13\nRetinaNet\n1 Stage Detector는 ‘Class imbalance’라는 고질적인 문제는 갖고 있었습니다.\n\nClass imbalance\n\nGrid로 나눠서 Cell 마다 모두 Bbox를 추정하게 함\nPositive Sample(Object area) &lt; Negative Sample(BG Area)\n\n\n\n\nRetinaNet은 One-stage Detector입니다. Retinanet은 Class imbalance 문제를 해결하기 위해 Focal Loss를 제안했으며 이를 통해 낮은 확률의 클래스에 대한 학습 성능을 향상시켰고, ResNet 구조의 FPN Network를 Backbone으로 사용하여 Two-stage Detector인 Faster R-CNN의 정확도를 능가했습니다.\n\n\nOne-Stage Detector인 이유\nclass+box subnet을 보면 class를 예측하는 network와 box를 예측하는 network가 나누어져 있습니다.\n이 구조를 보면 two stage detector가 아닌가 생각 할 수도 있지만, 자세히 살펴보면 class subnet은 box subnet의 결과를 전혀 사용하지 않고 독립적으로 anchor 별 class 예측을 한다는 것을 알 수 있습니다.\n그러므로 RetinaNet은 One-Stage Detector입니다.\n\n\n\nFocal Loss\n\nFocal loss는 one-stage object detector의 극단적인 class imbalance 문제를 해결하기 위해 design 된 loss function입니다.\nL_{focal\\ loss}(p_t)=-\\alpha_t(1-p_t)^{\\gamma}\\log (p_t)\n저자는 실험적으로 alpha=0.25, gamma = 2가 가장 성능이 좋았다고 합니다."},"vault/Notion/DB/DB-Blog-Post/Rust-Study":{"slug":"vault/Notion/DB/DB-Blog-Post/Rust-Study","filePath":"vault/Notion/DB/DB Blog Post/Rust Study.md","title":"Rust Study","links":[],"tags":[],"content":"\n스터디 멤버\n참조\n[[#]]\n[[#]]\n[[#]]\n[[#]]\n[[#]]\n[[#]]\n[[#]]\n[[#]]\n[[#]]\nToy Project\n추가 자료 조사 - Smart Pointer\n\n\n스터디 멤버\n김태훈\n김형석\n유재상\n참조\ntourofrust.com/TOC_ko.html\n1장 - 기초\nRust 놀이터\n별도의 환경을 구성하지 않더라도, “Rust 놀이터”라고 하는 Web Service를 통해 기본적인 코드를 실행해볼 수 있다.\n변수\n\n‘let’ 키워드르 통해 변수 선언, 자료형은 일반적으로 적지 않아도 유추합니다.\n\n(마치 파이썬처럼..)\n그러나 type을 명시할 수도 있습니다.\n\n\nvariable shadowing\n\n이미 선언한 변수 이름으로 다시 선언할 수 있습니다.\n다시 선언하는 변수는 기존의 변수와 전혀 다른 타입일 수 있습니다.\n\n\n\nfn main() {\n    // rust가 x의 자료형을 유추합니다\n    let x = 13;\n    println!(&quot;{}&quot;, x);\n \n    // 자료형을 명시적으로 지정할 수도 있습니다\n    let x: f64 = 3.14159;\n    println!(&quot;{}&quot;, x);\n \n    // 선언 후 나중에 초기화도 가능하지만, 보통 그렇게 하진 않습니다\n    let x;\n    x = 0;\n    println!(&quot;{}&quot;, x);\n}\n변수의 값 변경하기\n\nmutable vs immutable\n\n\nimmutable(기본)\n\n기본적으로 변수를 선언하면 c++의 const 키워드를 추가한 것처럼 변경이 불가한 상태가 됩니다.\n다만, const와는 다릅니다. 어디까지나 런타임 데이터입니다.\n\nRust에서 불변 변수와 const 변수의 차이점은 무엇인가요?\n\n\n\n\n\nmutable\n\n변경 가능한 값은 mut 키워드를 추가하여 선언해야 합니다.\n\nfn main() {\n    let mut x = 42;\n    println!(&quot;{}&quot;, x);\n    x = 13;\n    println!(&quot;{}&quot;, x);\n}\n\n\n\n\n기본 자료형\n\nRust에는 다양하지만 익숙한 자료형\n\n부울 값 - 참/거짓 값을 나타내는 bool\n부호가 없는 정수형 - 양의 정수를 나타내는 u8 u16 u32 u64 u128\n부호가 있는 정수형 - 양/음의 정수를 나타내는 i8 i16 i32 i64 i128\n포인터 사이즈 정수 - 메모리에 있는 값들의 인덱스와 크기를 나타내는 usize isize\n부동 소수점 - f32 f64\n튜플(tuple) - stack에 있는 값들의 고정된 순서를 전달하기 위한 (값, 값, ...)\n배열(array) - 컴파일 타임에 정해진 길이를 갖는 유사한 원소들의 모음(collection)인 [값, 값, ...]\n슬라이스(slice) - 런타임에 길이가 정해지는 유사한 원소들의 collection\nstr(문자열 slice) - 런타임에 길이가 정해지는 텍스트\n\n텍스트는 다른 익숙한 언어에서보다 복잡한데, 이는 Rust가 시스템 프로그래밍 언어이며 여러분이 지금까지 익숙하지 않았을 메모리 문제에 신경쓰기 때문입니다. 이에 대해서는 나중에 더 자세히 다루겠습니다.\n\n\n\n\n숫자형 자료형들은 숫자 뒤에 자료형 이름을 붙여 명시적으로 지정할 수 있습니다 (예: 13u32, 2u8).\n\nfn main() {\n    let x = 12; // 기본적으로 i32\n    let a = 12u8;\n    let b = 4.3; // 기본적으로 f64\n    let c = 4.3f32;\n    let bv = true;\n    let t = (13, false);\n    let sentence = &quot;hello world!&quot;;\n    println!(\n        &quot;{} {} {} {} {} {} {} {}&quot;,\n        x, a, b, c, bv, t.0, t.1, sentence\n    );\n}\n기본 자료형 변환\n\nas 키워드를 사용해 자료형을 매우 쉽게 변환할 수 있습니다.\n\nfn main() {\n    let a = 13u8;\n    let b = 7u32;\n    let c = a as u32 + b;\n    println!(&quot;{}&quot;, c);\n \n    let t = true;\n    println!(&quot;{}&quot;, t as u8);\n}\n상수\n\nconst 키워드를 사용하여 상수를 선언할 수 있습니다.\nconst로 선언된 값은 컴파일 타임에 텍스트 지정자를 직접 값으로 대체합니다.\n\nconst PI: f32 = 3.14159;\n \nfn main() {\n    println!(\n        &quot;아무 재료 없이 애플 {}를 만들려면, 먼저 우주를 만들어야 한다.&quot;,\n        PI\n    );\n}\n배열\n\n_배열(array)_은 고정된 길이로 된 모두 같은 자료형의 자료를 가진 collection입니다.\n\nfn main() {\n    let nums: [i32; 3] = [1, 2, 3];\n    println!(&quot;{:?}&quot;, nums);\n    println!(&quot;{}&quot;, nums[1]);\n}\n함수\n\nfn 키워드르 메서드를 선언할 수 있습니다.\ntyping hint가 추가된 python 메서드와 같이 [변수명:타입 → 리턴 값] 타입의 형태로 선언됩니다.\n\nfn add(x: i32, y: i32) -&gt; i32 {\n    return x + y;\n}\n \nfn main() {\n    println!(&quot;{}&quot;, add(42, 13));\n}\n여러개의 리턴 값\n\n튜플 형을 리턴함으로써 여러개의 값을 리턴할 수 있습니다.\n또한 python 같은 destructuring 방법을 지원합니다.\n\nfn swap(x: i32, y: i32) -&gt; (i32, i32) {\n    return (y, x);\n}\n \nfn main() {\n    // 리턴 값의 튜플을 리턴\n    let result = swap(123, 321);\n    println!(&quot;{} {}&quot;, result.0, result.1);\n \n    // 튜플을 두 변수명으로 분해\n    let (a, b) = swap(result.0, result.1);\n    println!(&quot;{} {}&quot;, a, b);\n}\n아무것도 리턴하지 않기\n\n아무것도 반환하지 않는 메서드는 자동으로 empty tuple = ‘()’을 반환합니다.\n\nfn make_nothing() -&gt; () {\n    return ();\n}\n \n// 리턴 자료형은 ()로 암시\nfn make_nothing2() {\n    // 리턴할 것이 지정되지 않으면 이 함수는 ()를 리턴함\n}\n \nfn main() {\n    let a = make_nothing();\n    let b = make_nothing2();\n \n    // 아무 것도 없는 것은 출력하기 힘들기 때문에\n    // a와 b의 디버그 문자열을 출력한다\n    println!(&quot;The value of a: {:?}&quot;, a);\n    println!(&quot;The value of b: {:?}&quot;, b);\n}\n2장 - 기초적인 흐름 제어\nif/else if/else\n\n기존의 논리 연산자를 그대로 사용 할 수 있음\n\n==, !=, &lt;, &gt;, &lt;=, &gt;=, !, ||, &amp;&amp;\n\n\npython 처럼 조건문에 중괄호 ‘()’가 필요하지 않음\n\nfn main() {\n    let x = 42;\n    if x &lt; 42 {\n        println!(&quot;42보다 작다&quot;);\n    } else if x == 42 {\n        println!(&quot;42와 같다&quot;);\n    } else {\n        println!(&quot;42보다 크다&quot;);\n    }\n}\nloop\n\nloop { … }로 반복\n\nfn main() {\n    let mut x = 0;\n    loop {\n        x += 1;\n        if x == 42 {\n            break;\n        }\n    }\n    println!(&quot;{}&quot;, x);\n}\nwhile\n\nwhlie condition { … }로 반복\n\nfn main() {\n    let mut x = 0;\n    while x != 42 {\n        x += 1;\n    }\n}\nfor\n\nfor item in iterator { … }로 반복\n\nfn main() {\n    for x in 0..5 {\n        println!(&quot;{}&quot;, x);\n    }\n \n    for x in 0..=5 {\n        println!(&quot;{}&quot;, x);\n    }\n}\nmatch\n\nswitch의 대용, 모든 case를 처리해야 한다.\n\nfn main() {\n    let x = 42;\n \n    match x {\n        0 =&gt; {\n            println!(&quot;0 발견&quot;);\n        }\n        // 여러 개 값과 대조할 수 있다\n        1 | 2 =&gt; {\n            println!(&quot;1 또는 2 발견!&quot;);\n        }\n        // 범위로 대조할 수 있다\n        3..=9 =&gt; {\n            println!(&quot;3에서 9까지의 숫자 발견&quot;);\n        }\n        // 찾은 숫자를 변수에 바인딩할 수 있다\n        matched_num @ 10..=100 =&gt; {\n            println!(&quot;10에서 100까지의 숫자 {} 발견!&quot;, matched_num);\n        }\n        // 모든 케이스가 처리되지 않았을 경우 반드시 존재해야 하는 기본 match\n        _ =&gt; {\n            println!(&quot;뭔가 다른거 발견!&quot;);\n        }\n    }\n}\nloop에서 값 리턴하기\n\nbreak 뒤에 값을 두면, loop 후 값이 리턴됨\n\nfn main() {\n    let mut x = 0;\n    let v = loop {\n        x += 1;\n        if x == 13 {\n            break &quot;13 찾았다&quot;;\n        }\n    };\n    println!(&quot;loop에서: {}&quot;, v);\n}\n블록 표현에서 값 리턴하기\n\n다른 언어들과 많이 다른 부분\n‘return’이 없더라도, if, match, 함수, 또는 범위 블록의 마지막 구문에 ’;‘가 없다면 Rust는 그 값을 블록의 리턴 값으로 간주\n\nfn example() -&gt; i32 {\n    let x = 42;\n    // Rust의 3항 연산 표현\n    let v = if x &lt; 42 { -1 } else { 1 };\n    println!(&quot;if로부터: {}&quot;, v);\n \n    let food = &quot;햄버거&quot;;\n    let result = match food {\n        &quot;핫도그&quot; =&gt; &quot;핫도그다&quot;,\n        // 리턴문 하나 뿐이라면 중괄호는 필수가 아님\n        _ =&gt; &quot;핫도그가 아니다&quot;,\n    };\n    println!(&quot;음식 판별: {}&quot;, result);\n \n    let v = {\n        // 이 범위 블록은 함수 범위를 더럽히지 않고 값을 가져오게 해준다\n        let a = 1;\n        let b = 2;\n        a + b\n    };\n    println!(&quot;block에서: {}&quot;, v);\n \n    // Rust에서 함수 마지막에 값을 리턴하는 관용적 표현\n    v + 4\n}\n \nfn main() {\n    println!(&quot;function에서: {}&quot;, example());\n}\n2장 - 마무리\n3장 - 기본 데이터 구조 자료형\n구조체\n\nstruct는 필드(field)들의 collection\n이 정의는 메모리 상에 field들을 어떻게 배치할지에 대한 컴파일러의 청사진\n\n정의한 순서에 따라 memory 순서까지 달라지는 것일까?\n\n\n\nstruct SeaCreature {\n    // String은 struct다\n    animal_type: String,\n    name: String,\n    arms: i32,\n    legs: i32,\n    weapon: String,\n}\n메소드 호출하기\n\nc++과 비슷한 느낌으로 호출\n스태틱 메소드(static methods) - 자료형 그 자체에 속하는 메소드로서, :: 연산자를 이용하여 호출\n인스턴스 메소드(instance methods) - 자료형의 인스턴스에 속하는 메소드로서, . 연산자를 이용하여 호출\n\nfn main() {\n    // static method를 사용하여 String의 instance를 생성\n    let s = String::from(&quot;Hello world!&quot;);\n    // instance의 메소드를 사용\n    println!(&quot;{}의 글자 수는 {}입니다.&quot;, s, s.len());\n}\n메모리\n\nRust 프로그램에는 데이터가 저장되는 세 가지의 메모리 영역이 존재\n데이터 메모리(data memory)\n\n크기가 고정 되었으며 static한 데이터\n\n예: “Hello World!”\n이 텍스트의 바이트들은 오직 한 곳에서만 읽히므로 이 영역에 저장될 수 있음\n이런 종류의 데이터는 컴파일러가 많은 최적화를 수행\n위치가 알려져 있고 고정되어 있기 때문에 일반적으로 사용하기에 매우 빠름\n\n\n\n\n스택 메모리(stack memory)\n\n함수 내에서 변수로 선언되는 데이터용\n\n이 메모리의 위치는 함수 호출 동안에는 절대 변하지 않음\n때문에 컴파일러가 코드를 최적화할 수 있으며, 이로 인해 접근하기에 매우 빠름\n\n\n\n\n힙 메모리(heap memory)\n\n애플리케이션이 실행되는 동안 생성되는 데이터용\n\n이 영역의 데이터는 추가하거나, 이동하거나, 제거하거나, 크기를 바꾸거나, 등을 할 수 있음\n일반적으로 사용하기에 느리지만, 훨씬 더 창의적인 메모리 사용이 가능\n데이터가 이 영역에 추가되면 할당(allocation)\n데이터가 이 영역에서 제거되면 해제(deallocation)\n\n\n\n\n\n메모리에 데이터 생성하기\n\n데이터가 stack에 할당되는지, heap에 할당되는지에 대한 차이는 c++과 매우 유사\nstruct의 field 값들은 . 연산자를 통해 접근\n\n \nlet ferris = SeaCreature {\n\t\t// String struct도 stack에 있지만,\n\t\t// heap에 있는 데이터에 대한 참조를 갖고 있음\n\t\tanimal_type: String::from(&quot;게&quot;),\n\t\tname: String::from(&quot;Ferris&quot;),\n\t\tarms: 2,\n\t\tlegs: 4,\n\t\tweapon: String::from(&quot;집게&quot;),\n};\nTuple 같은 구조체\n\n튜플형태로 struct를 생성할 수 있음\n\nstruct Location(i32, i32);\n \nfn main() {\n    // 이것도 여전히 stack에 있는 struct임\n    let loc = Location(42, 32);\n    println!(&quot;{}, {}&quot;, loc.0, loc.1);\n}\nUnit 같은 구조체\n\nstruct 내에 아무것도 선언하지 않는다면 빈 튜플과 같아짐\n\nunit은 빈 튜플의 또 다른 이름\n\n\n\nstruct Location(i32, i32);\n \nfn main() {\n    // 이것도 여전히 stack에 있는 struct임\n    let loc = Location(42, 32);\n    println!(&quot;{}, {}&quot;, loc.0, loc.1);\n}\n열거형\n\n열거형(enumeration)은 enum 키워드를 통해 몇 가지 태그된 원소의 값을 가질 수 있는 새로운 자료형을 생성\n\nenum Species {\n    Crab,\n    Octopus,\n    Fish,\n    Clam,\n}\n열거형과 데이터\n\n내용이 난해해서 원문 복붙..\n\n\nenum의 원소들은 C의 _union_처럼 동작할 수 있도록 한 개 이상의 자료형을 가질 수 있습니다.\nenum이 match를 통해 패턴 일치될 때, 각각의 데이터 값에 변수명을 붙일 수 있습니다.\nenum의 메모리 상세:\n\nenum 데이터 값은 가장 큰 원소의 메모리 크기와 같은 메모리 크기를 가집니다. 이는 가능한 모든 값이 동일한 메모리 공간에 들어갈 수 있게 해줍니다.\n원소의 자료형(있는 경우)에 더하여, 각 원소는 무슨 태그에 해당하는지 나타내는 숫자값도 갖습니다.\n\n다른 상세 정보:\n\nRust의 enum은 _tagged union_으로도 알려져 있습니다.\nRust가 _대수적 자료형(algebraic types)_을 갖고 있다고 할 때 이는 자료형을 조합하여 새 자료형을 만드는 것을 의미합니다.\n\n\n\nRust의 enum은 _tagged union_으로도 알려져 있음\n\ntagged union\n\n80000coding.oopy.io/e1fcc2c2-8149-49a8-9b9b-544103481c9c\nenum 내에 선언된 원소 중 가장 큰 data size와 같은 메모리 크기를 가짐\n\n가능한 모든 값이 동일 메모리 공간 안에 들어갈 수 있게 해줌\n예\n\n\nenum SomeType {\nstruct_1() // size = 3 byte,\nstruct_2() // size = 5 byte,\nstruct_3() // size = 1 byte,\n}\n\n\nSomeType의 data 크기는 5 byte\n\n\n\n\n\n\n\n즉, enum 내에 전혀 다른 data 크기의 값을 열거 할 수 있으며, 해당 enum의 data 크기는 열거된 원소 중 가장 큰 data size로 결정된다는 의미\n\n\n\nenum PoisonType {\n    Acidic,\n    Painful,\n    Lethal,\n}\nenum Size {\n    Big,\n    Small,\n}\nenum Weapon {\n    Claw(i32, Size),\n    Poison(PoisonType),\n    None,\n}\n \nstruct SeaCreature {\n    species: Species,\n    name: String,\n    arms: i32,\n    legs: i32,\n    weapon: Weapon,\n}\n \nfn main() {\n    // SeaCreature의 데이터는 stack에 있음\n    let ferris = SeaCreature {\n        // String struct도 stack에 있지만,\n        // heap에 있는 데이터에 대한 참조를 갖고 있음\n        species: Species::Crab,\n        name: String::from(&quot;Ferris&quot;),\n        arms: 2,\n        legs: 4,\n        weapon: Weapon::Claw(2, Size::Small),\n    };\n \n    match ferris.species {\n        Species::Crab =&gt; match ferris.weapon {\n            Weapon::Claw(num_claws, size) =&gt; {\n                let size_description = match size {\n                    Size::Big =&gt; &quot;큰&quot;,\n                    Size::Small =&gt; &quot;작은&quot;,\n                };\n                println!(\n                    &quot;ferris는 {}개의 {} 집게를 가진 게이다&quot;,\n                    num_claws, size_description\n                )\n            }\n            _ =&gt; println!(&quot;ferris는 다른 무기를 가진 게이다&quot;),\n        },\n        _ =&gt; println!(&quot;ferris는 다른 동물이다&quot;),\n    }\n}\n3장 - 마무리\n4장 - Generic 자료형\nGeneric 자료형이란?\n\ngeneric 자료형은 struct나 enum을 부분적으로 정의하여, 컴파일러가 컴파일 타임에 코드 사용을 기반으로 완전히 정의된 버전을 만들 수 있게 해줍니다.\n\nstruct BagOfHolding&lt;T&gt; { item: T, }\n\n\nRust는 일반적으로 인스턴스화 하는 것을 보고 최종 자료형을 유추할 수 있습니다.\n\nlet float_bag = BagOfHolding { item: 3.14 };\n\n\n그러나::&lt;T&gt; 연산자를 사용해 언제든 명시적으로 자료형을 지정할 수 있습니다.\n\nlet i32_bag = BagOfHolding::&lt;i32&gt; { item: 42 };\n\n\n\n아무 것도 없는 것을 표현하기 &amp; 옵션\n\nRust에는 null이 없습니다.\n한 개 이상의 선택 가능한 값에 대해 None 선택지를 제공하는 방법은 null 값이 없는 Rust에서 매우 흔한 패턴입니다.\n\n\n실제 Rust에서 제공하는 generic enum : Option\n\n참조 : showx123.tistory.com/58\nT와 Option는 다른Type\nnull pointer dereference와 같은 메모리 취약점을 없앨 수 있다.\nRust 컴파일러가 Option을 포인터로 컴파일 타임에 변환, 런타임 오버헤드는 사실상 0\nEnume은 원소 중 가장 큰 type의 사이즈를 갖도록 구현되어 있기 때문에 포인터의 사이즈(32비트면 4바이트, 64비트면 8바이트)보다 작은 타입의 경우 메모리를 아주 조금 낭비하게 됨.\n\nenum Option&lt;T&gt; {\n    Some(T),\n    None,\n}\n \nstruct BagOfHolding {\n    item: Option&lt;Inventory&gt;,\n}\n\n\n\n\n결과\n\n\nRust에는 실패할 가능성이 있는 값을 리턴할 수 있도록 해주는 Result라 불리는 내장된 generic enum\n\nRust에서 오류 처리를 하는 관용적인 방법\n\nenum Result&lt;T, E&gt; {\n    Ok(T),\n    Err(E),\n}\n\n\n아래 코드를 돌려보면 “Standard Error”로 결과가 출력된다.\n\n단순한 ‘표현’이 아닌듯.. throw exception 같은 표현일까?\n\nfn do_something_that_might_fail(i: i32) -&gt; Result&lt;f32, String&gt; {\n    if i == 42 {\n        Ok(13.0)\n    } else {\n        Err(String::from(&quot;맞는 숫자가 아닙니다&quot;))\n    }\n}\n \nfn main() {\n    let result = do_something_that_might_fail(12);\n \n    // match는 Result를 우아하게 분해하고, 모든 케이스를 처리하도록 해준다!\n    match result {\n        Ok(v) =&gt; println!(&quot;{} 발견&quot;, v),\n        Err(e) =&gt; println!(&quot;오류: {}&quot;, e),\n    }\n}\n\n\n실패할 수 있는 메인\n\n\nmain 은 Result를 반환할 수 있음\nfn do_something_that_might_fail(i: i32) -&gt; Result&lt;f32, String&gt; {\n    if i == 42 {\n        Ok(13.0)\n    } else {\n        Err(String::from(&quot;맞는 숫자가 아닙니다&quot;))\n    }\n}\n \n// Main은 아무 값도 리턴하지 않지만, 오류를 리턴할 수 있다!\nfn main() -&gt; Result&lt;(), String&gt; {\n    let result = do_something_that_might_fail(12);\n \n    match result {\n        Ok(v) =&gt; println!(&quot;{} 발견&quot;, v),\n        Err(_e) =&gt; {\n            // 이 오류를 우아하게 처리한다\n \n            // main으로부터 무슨 일이 발생했는지 새 오류를 리턴한다!\n            return Err(String::from(&quot;main에서 뭔가 잘못 되었습니다!&quot;));\n        }\n    }\n \n    // 모든 일이 잘 끝났음을 표현하기 위해\n    // Result Ok 안에 unit 값을 쓰고 있는걸 잘 봐두십시오\n    Ok(())\n}\n\n\n우아한 오류 처리\n\n\nResult와 함께 쓸 수 있는 강력한 연산자 ?\n\nmatch ~ 문을 이용한 예외처리를 ? 로 대체할 수 있음\n\nfn do_something_that_might_fail(i: i32) -&gt; Result&lt;f32, String&gt; {\n    if i == 42 {\n        Ok(13.0)\n    } else {\n        Err(String::from(&quot;맞는 숫자가 아닙니다&quot;))\n    }\n}\n \nfn main() -&gt; Result&lt;(), String&gt; {\n    // 얼마나 코드를 줄였는지 보세요!\n    let v = do_something_that_might_fail(42)?;\n    println!(&quot;{} 발견&quot;, v);\n    Ok(())\n}\n\n\n추한 옵션/결과 처리\n\nnullable, OK/Err 반환에 대한 예외 처리를 아래와 같이 간결하게 사용 가능\n\nnullable\n\n\n기존\nmatch my_option { Some(v) =&gt; v, None =&gt; panic!(&quot;some error message generated by Rust!&quot;), }\n\n\nunwrap 사용\nlet v = my_option.unwrap()\n\n\n\nOK / Err\n\n\n기존\nmatch my_result { Ok(v) =&gt; v, Err(e) =&gt; panic!(&quot;some error message generated by Rust!&quot;), }\n\n\nunrwap 사용\nlet v = my_result.unwrap()\n\n\n\n\n\n\n벡터\n\nVec\n\nstruct로 표현되는 가변 크기의 리스트\nstruct이나, 내부적으로는 내용물이 heap에 있는 고정 리스트에 대한 참조를 포함\n기본 용량을 갖고 시작\n용량보다 많은 내용물이 추가될 경우, 큰 용량을 가진 새 고정 리스트를 위해 heap에 데이터를 재할당\n\n\nvec!\n\nvector를 수동으로 일일이 만드는 대신, 손쉽게 생성할 수 있게 해줌\n\n\n.iter()\n\nfor 반복문에 손쉽게 넣을 수 있도록 vector로부터 반복자를 생성하는 메소드\n\n\n\nfn main() {\n    // 자료형을 명시적으로 할 수 있음\n    let mut i32_vec = Vec::&lt;i32&gt;::new(); // turbofish &lt;3\n    i32_vec.push(1);\n    i32_vec.push(2);\n    i32_vec.push(3);\n \n    // 하지만 Rust가 얼마나 똑똑하게 자료형을 자동으로 결정하는지 보십시오\n    let mut float_vec = Vec::new();\n    float_vec.push(1.3);\n    float_vec.push(2.3);\n    float_vec.push(3.4);\n \n    // 아름다운 macro입니다!\n    let string_vec = vec![String::from(&quot;Hello&quot;), String::from(&quot;World&quot;)];\n \n    for word in string_vec.iter() {\n        println!(&quot;{}&quot;, word);\n    }\n}\n4장 - 마무리\n5장 - 소유권과 데이터 대여\n소유권\n\n할당(binding)\n\n자료형을 인스턴스화 하여 변수명에 하는 행위\nRust 컴파일러가 전체 **생명주기(lifetime)**동안 검증할 메모리 리소스를 생성하는 것\n할당된 변수는 리소스의 **소유자(owner)**라고 불립니다.\n\n\n\nstruct Foo {\n    x: i32,\n}\n \nfn main() {\n    // struct를 인스턴스화 하고 변수에 bind하여\n    // 메모리 리소스를 생성함\n    let foo = Foo { x: 42 };\n    // foo가 owner임\n}\n범위 기반 리소스 관리\n\nRust는 범위(scope)가 끝나는 곳에서 리소스를 소멸하고 할당 해제합니다.\n이 소멸과 할당 해제를 의미하는 용어로 drop을 사용합니다.\nRust에는 가비지 컬렉션이 없습니다.\n\nstruct Foo {\n    x: i32,\n}\n \nfn main() {\n    let foo_a = Foo { x: 42 };\n    let foo_b = Foo { x: 13 };\n \n    println!(&quot;{}&quot;, foo_a.x);\n \n    println!(&quot;{}&quot;, foo_b.x);\n    // foo_b가 여기서 drop 됩니다\n    // foo_a가 여기서 drop 됩니다\n}\nDropping은 계층적이다\n\nstruct가 drop 될 때\n\nstruct 자신이 제일 먼저 drop 됩니다\n이후에 그 자식들이 각각 drop 되며, 등의 순서대로 처리됩니다.\n\n\nRust에서는 메모리를 자동으로 해제함으로써 메모리 누수가 덜 일어나도록 합니다.\n\n자식들이 포인터 형인 경우에도, heap에 있을 경우에도 알아서 해제해줄까?\n\n\n메모리 리소스는 단 한 번 drop 될 수 있습니다.\n\nstruct Bar {\n    x: i32,\n}\n \nstruct Foo {\n    bar: Bar,\n}\n \nfn main() {\n    let foo = Foo { bar: Bar { x: 42 } };\n    println!(&quot;{}&quot;, foo.bar.x);\n    // foo가 먼저 drop 되고\n    // 그 다음에 foo.bar가 drop 됩니다\n}\n소유권 이전\n\nowner가 함수의 인자로 전달되면, ownership은 그 함수의 매개변수로 이동(move)됩니다.\nmove 이후에는 원래 함수에 있던 변수는 더 이상 사용할 수 없습니다.\n\nmove 중에는 owner 값의 stack 메모리가 함수 호출의 매개변수 stack 메모리로 복사됩니다.\n\n\n\nstruct Foo {\n    x: i32,\n}\n \nfn do_something(f: Foo) {\n    println!(&quot;{}&quot;, f.x);\n    // f가 여기서 drop 됩니다\n}\n \nfn main() {\n    let foo = Foo { x: 42 };\n    // foo가 do_something으로 move 됩니다\n    do_something(foo);\n    // foo는 더 이상 사용할 수 없습니다\n}\n소유권 리턴하기\n\nownership은 함수에서도 리턴될 수 있습니다.\n\n기존 c++, c#에서는 포인터가 아닌 이상 값의 복사가 일어 났는데, 러스트는 기본적으로 메모리 주소 자체를 아에 넘겨버리는 것으로 보임\n\n\n\nstruct Foo {\n    x: i32,\n}\n \nfn do_something() -&gt; Foo {\n    Foo { x: 42 }\n    // ownership이 밖으로 move 됩니다\n}\n \nfn main() {\n    let foo = do_something();\n    // foo가 owner가 되었습니다\n    // 함수의 scope 끝에 도달했기 때문에 foo는 drop 됩니다\n}\n참조로 소유권 대여하기\n\n&amp; 연산자를 통해 참조로 리소스에 대한 접근권한을 대여할 수 있습니다.\n\n참조도 다른 리소스와 마찬가지로 drop 됩니다.\n\n\n\nstruct Foo {\n    x: i32,\n}\n \nfn main() {\n    let foo = Foo { x: 42 };\n    let f = &amp;foo;\n    println!(&quot;{}&quot;, f.x);\n    // f는 여기서 drop 됩니다\n    // foo는 여기서 drop 됩니다\n}\n참조로 변경 가능한 소유권 대여하기\n\n\n&amp;mut 연산자를 통해 리소스에 대한 mutable한 접근 권한도 대여할 수 있습니다.\n\n리소스의 owner는 mutable하게 대여된 상태에서는 move 되거나 변경될 수 없습니다.\nRust는 데이터 경합의 가능성 때문에 소유된 값을 변경하는 방법이 여러 개 생기는 것을 방지합니다.\n\n→ 소유권을 대여해주면, 원래의 소유권을 가졌던 참조는 소유권이 반환될 때까지 변경 권한을 잃는다.\n\n\nstruct Foo {\n    x: i32,\n}\n \nfn do_something(f: Foo) {\n    println!(&quot;{}&quot;, f.x);\n    // f는 여기서 drop 됩니다\n}\n \nfn main() {\n    let mut foo = Foo { x: 42 };\n    let f = &amp;mut foo;\n \n    // FAILURE: do_something(foo) 은 실패할 것입니다\n    // 왜냐하면 foo는 mutable하게 borrow된 상태에서는 move될 수 없기 때문입니다\n \n    // FAILURE: foo.x = 13; 는 여기서 실패할 것입니다\n    // 왜냐하면 foo는 mutable하게 borrow된 상태에서는 변경할 수 없기 때문입니다\n \n    f.x = 13;\n    // f는 이 시점 이후 더 이상 사용되지 않기 때문에 여기서 drop 됩니다\n \n    println!(&quot;{}&quot;, foo.x);\n \n    // 모든 mutable 참조가 drop 되었으므로 이제 문제 없이 동작합니다\n    foo.x = 7;\n \n    // foo의 ownership을 함수로 move 합니다\n    do_something(foo);\n}\n역참조\n\n&amp;mut 참조를 이용해 * 연산자로 owner의 값을 설정할 수 있습니다.\n* 연산자로 own된 값의 복사본도 가져올 수 있습니다 (복사 가능한 경우만)\n\nfn main() {\n    let mut foo = 42;\n    let f = &amp;mut foo;\n    let bar = *f; // owner의 값의 복사본을 가져옴\n    *f = 13; // 참조의 owner의 값을 설정함\n    println!(&quot;{}&quot;, bar);\n    println!(&quot;{}&quot;, foo);\n}\n대여한 데이터 전달하기\n\nRust의 참조 규칙은 다음과 같이 요약될 수 있습니다:\n\nRust는 단 하나의 mutable한 참조 또는 여러개의 non-mutable 참조만 허용하며, 둘 다는 안됨.\n참조는 그 owner보다 더 오래 살 수 없음.\n\n\n첫 번째 참조 규칙은 데이터 경합을 방지합니다.\n\n데이터를 읽는 행위가 동시에 데이터를 쓰는 이의 존재로 인해 동기화가 어긋날 가능성이 있을 때 일어납니다.\n\n\n두 번째 참조 규칙은 존재하지 않는 데이터를 바라보는 잘못된 참조를 사용하는 것을 방지\n\n이를 C에서는 허상 포인터(dangling pointers)라고 부름\n\n\n\nstruct Foo {\n    x: i32,\n}\n \nfn do_something(f: &amp;mut Foo) {\n    f.x += 1;\n    // mutable 참조 f는 여기서 drop 됩니다\n}\n \nfn main() {\n    let mut foo = Foo { x: 42 };\n    do_something(&amp;mut foo);\n    // 모든 mutable 참조가 do_something 함수 내에서 drop 되므로,\n    // 하나 더 생성할 수 있습니다.\n    do_something(&amp;mut foo);\n    // foo는 여기서 drop 됩니다\n}\n참조의 참조\n\n참조는 심지어 참조에도 사용될 수 있습니다.\n\nstruct Foo {\n    x: i32,\n}\n \nfn do_something(a: &amp;Foo) -&gt; &amp;i32 {\n    return &amp;a.x;\n}\n \nfn main() {\n    let mut foo = Foo { x: 42 };\n    let x = &amp;mut foo.x;\n    *x = 13;\n    // 여기서 x가 drop 되어 non-mutable 참조를 생성할 수 있습니다\n    let y = do_something(&amp;foo);\n    println!(&quot;{}&quot;, y);\n    // y는 여기서 drop 됩니다\n    // foo는 여기서 drop 됩니다\n}\n명시적인 생명주기\n\nRust 컴파일러는 모든 변수의 lifetime을 이해하며 참조가 절대로 그 owner보다 더 오래 존재하지 못하도록 검증을 시도합니다.\n함수에서는 어떤 매개변수와 리턴 값이 서로 같은 lifetime을 공유하는지 식별할 수 있도록 심볼로 표시하여 명시적으로 생명주기를 지정할 수 있습니다.\nlifetime 지정자는 언제나 &#039;로 시작합니다. (예: &#039;a, &#039;b, &#039;c)\n\nstruct Foo {\n    x: i32,\n}\n \n// 매개변수 foo와 리턴 값은 동일한 lifetime을 공유함\nfn do_something&lt;&#039;a&gt;(foo: &amp;&#039;a Foo) -&gt; &amp;&#039;a i32 {\n    return &amp;foo.x;\n}\n \nfn main() {\n    let mut foo = Foo { x: 42 };\n    let x = &amp;mut foo.x;\n    *x = 13;\n    // x가 여기서 drop 되어, non-mutable 참조를 생성할 수 있음\n    let y = do_something(&amp;foo);\n    println!(&quot;{}&quot;, y);\n    // y는 여기서 drop 됨\n    // foo는 여기서 drop 됨\n}\n여러 개의 생명주기\n\nlifetime 지정자는 컴파일러가 스스로 함수 매개변수들의 lifetime을 판별하지 못하는 경우, 이를 명시적으로 지정할 수 있게 도와줍니다.\n\n아래 코드에서 ‘ 를 제외하면 에러 발생\n\n\n\nstruct Foo {\n    x: i32,\n}\n \n// foo_b와 리턴 값은 동일한 lifetime을 공유함\n// foo_a는 무관한 lifetime을 가짐\nfn do_something&lt;&#039;a, &#039;b&gt;(foo_a: &amp;&#039;a Foo, foo_b: &amp;&#039;b Foo) -&gt; &amp;&#039;b i32 {\n    println!(&quot;{}&quot;, foo_a.x);\n    println!(&quot;{}&quot;, foo_b.x);\n    return &amp;foo_b.x;\n}\n \nfn main() {\n    let foo_a = Foo { x: 42 };\n    let foo_b = Foo { x: 12 };\n    let x = do_something(&amp;foo_a, &amp;foo_b);\n    // 여기 이후에는 foo_b의 lifetime만 존재하므로 foo_a만 drop 됨\n    println!(&quot;{}&quot;, x);\n    // 여기서 x가 drop 됨\n    // 여기서 foo_b가 drop 됨\n}\n정적인 생명주기\n\nstatic 변수는 컴파일 타임에 생성되어 프로그램의 시작부터 끝까지 존재하는 메모리 리소스입니다.\n\n이들은 명시적으로 자료형을 지정해 주어야 합니다.\n\n\nstatic lifetime은 프로그램이 끝날 때까지 무한정 유지되는 메모리 리소스입니다.\n\n이 정의에 따르면, 어떤 static lifetime의 리소스는 런타임에 생성될 수도 있습니다.\nstatic lifetime을 가진 리소스는 &#039;static이라는 특별한 lifetime 지정자를 갖습니다.\n&#039;static한 리소스는 절대 drop 되지 않습니다.\nstatic lifetime을 가진 리소스가 참조를 포함하는 경우, 그들도 모두 &#039;static이어야 합니다\n\n그 이하의 것들은 충분히 오래 살아남지 못합니다\n\n\n\n\nstatic 변수는 어느 누구에 의해서든 전역적으로 접근 가능하기 때문에, 이를 변경하는 것은 데이터 경합을 유발하는, 본질적으로 위험한 행위입니다.\nRust에서는 unsafe { ... } 블록을 이용하여 특정 동작에 대해 컴파일러가 메모리 검사를 하지 않도록 할 수 있습니다.\n\n없을 경우 : error[E0133]: use of mutable static is unsafe and requires unsafe function or block\n아래 예제의 경우, unsafe를 없애면 SECRET = &quot;abracadabra&quot;;와 println!(&quot;{}&quot;, SECRET);에 모두 오류가 발생\n\n\n\nstatic PI: f64 = 3.1415;\n \nfn main() {\n    // static 변수는 함수 scope 안에도 넣을 수 있습니다\n    static mut SECRET: &amp;&#039;static str = &quot;swordfish&quot;;\n \n    // string 값들은 &#039;static lifetime을 갖습니다\n    let msg: &amp;&#039;static str = &quot;Hello World!&quot;;\n    let p: &amp;&#039;static f64 = &amp;PI;\n    println!(&quot;{} {}&quot;, msg, p);\n \n    // 일부 규칙은 깰 수 있으나, 반드시 명시적으로 해야 합니다\n    unsafe {\n        // SECRET에 string 값을 설정할 수 있는데, 이 값 역시 &#039;static이기 때문입니다\n        SECRET = &quot;abracadabra&quot;;\n        println!(&quot;{}&quot;, SECRET);\n    }\n데이터 자료형의 생명주기\n\n함수와 마찬가지로, 데이터 자료형의 구성원들도 lifetime 지정자로 지정할 수 있습니다.\nRust는 참조가 품고 있는 데이터 구조가 참조가 가리키는 owner보다 절대 오래 살아남지 못하도록 검증합니다.\n\nstruct Foo&lt;&#039;a&gt; {\n    i:&amp;&#039;a i32\n}\n \nfn main() {\n    let x = 42;\n    let foo = Foo {\n        i: &amp;x\n    };\n    println!(&quot;{}&quot;,foo.i);\n}\n5장 - 마무리\n6장 - 텍스트\n문자열\n\n언제나 유니코드로 되어 있음\n문자열의 자료형은 &amp;&#039;static str입니다:\n\n&amp;는 메모리 내의 장소를 참조하고 있다는 의미\n&amp;mut가 빠졌다는 것은 컴파일러가 값의 변경을 허용하지 않을 것이라는 뜻\n&#039;static은 string 데이터가 프로그램이 끝날 때까지 유효하다는 의미(절대 drop 되지 않음)\nstr은 언제나 유효한 utf-8인 바이트 열을 가리키고 있다는 의미\n\n\nRust 컴파일러는 문자열을 프로그램 메모리의 데이터 세그먼트에 저장\nlet a: &amp;&#039;static str = &quot;hi 🦀&quot;;\n\nutf-8이란 무엇인가\n\nutf-8은 1에서 4 바이트의 가변 길이 바이트로 도입 되었음\n\n사용 가능한 문자의 범위를 엄청나게 늘어남\nASCII 문자에 쓸데 없는 바이트를 필요로 하지 않음(utf-8에서도 여전히 1 바이트만 필요로 함)\n단순한 인덱싱(예: my_text[3]으로 네 번째 문자를 가져옴)시, O(1) 상수 시간으로 문자를 찾을 수 없음\n\n바로 앞의 글자가 가변 길이를 가질 수 있으므로, 바이트 열에서 4번째 문자가 실제로 시작하는 위치가 달라질 수도 있음\nutf-8 바이트 열을 하나하나 돌면서 각각의 유니코드 문자가 실제로 어디에서 시작하는지 찾아야 함 (O(n) 선형 시간)\n\n\n\n\n\n예외처리문자\n\n\n어떤 문자들은 시각적으로 표현하기 어려우므로, 예외처리 코드(escape code) 로 대체하여 사용\n\n\nRust는 C 기반 언어들의 일반적인 예외처리 코드를 지원\n\n\\n - 줄바꿈\n\\r - 캐리지리턴\n\\t - 탭\n\\\\ - 역슬래시\n\\0 - null\n\\&#039; - 작은 따옴표\n\n→ 전체 목록은 이곳에서\n\n\n여러 줄로 된 문자열\n\nRust의 문자열은 기본적으로 여러 줄로 되어 있음\n줄바꿈 문자를 원하지 않을 경우, 줄 맨 뒤에 \\를 사용\n\nfn main() {\n    let haiku: &amp;&#039;static str = &quot;\n        나는 쓰고, 지우고, 다시 쓴다\n        다시 지우고, 그러고 나면\n        양귀비 꽃이 핀다.\n        - 가쓰시카 호쿠사이&quot;;\n    println!(&quot;{}&quot;, haiku);\n \n    println!(\n        &quot;hello \\\n    world&quot;\n    ) // w 앞의 공백이 무시 되었음을 주의하세요\n}\n원시 문자열\n\nr#&quot;로 시작하고 &quot;#로 끝남\n문자열을 있는 그대로 쓸 수 있음\n\n큰 따옴표나 역슬래시 같은 문자들을 바로 사용할 수 있음\n\n\n\nfn main() {\n    let a: &amp;&#039;static str = r#&quot;\n        &lt;div class=&quot;advice&quot;&gt;\n            원시 문자열은 일부 상황에서 유용합니다.\n        &lt;/div&gt;\n        &quot;#;\n    println!(&quot;{}&quot;, a);\n}\n파일에서 문자열 가져오기\n\ninclude_str! macro를 사용하여 local에 있는 문자열을 가져올 수 있음\n\nlet 00_html = include_str!(&quot;00_ko.html&quot;);\n\n\n\n문자열 슬라이스\n\n문자열 slice는 메모리 상의 바이트 열에 대한 참조\n\nutf-8의 특성 상, char에 대한 index 접근과 다르다.\nslice의 문자열 slice (sub-slice)도 역시 유효한 utf-8이어야 함\n\nslice의 결과물이 문자열을 만들 지 못할 때 EROR\n\n\n\n\n흔히 사용되는 메소드\n\nlen은 문자열의 바이트 길이(글자수가 아님)\nstarts_with/ends_with는 기본적인 비교에 사용\nis_empty는 길이가 0일 경우 true를 반환\nfind는 주어진 텍스트가 처음 등장 하는 위치인 Option&lt;usize&gt; 값을 반환\n\n\n\n문자\n\nutf-8 바이트 열을 char 자료형의 vector로 돌려주는 기능을 제공\nchar 하나는 4 바이트(각각의 문자를 효율적으로 찾을 수 있음)\n\nfn main() {\n    // 문자들을 char의 vector로 가져옵니다\n    let chars = &quot;hi 🦀&quot;.chars().collect::&lt;Vec&lt;char&gt;&gt;();\n \n    println!(&quot;{}&quot;, chars.len()); // 4여야 합니다\n \n    // char가 4 바이트이므로 u32로 변환할 수 있습니다\n    println!(&quot;{}&quot;, chars[3] as u32);\n}\n스트링\n\nString은 utf-8 바이트 열을 heap memory에 소유하는 struct\nheap에 있기 때문에, 문자열과는 달리 늘리거나 변경하거나 기타 등등을 할 수 있음\npush_str은 string의 맨 뒤에 utf-8 바이트들을 더 붙일 때 사용\nreplace는 utf-8 바이트 열을 다른 것으로 교체할 때 사용\nto_lowercase/to_uppercase는 대소문자를 바꿀 때 사용\ntrim은 공백을 제거할 때 사용\n\nfn main() {\n    let mut helloworld = String::from(&quot;hello&quot;);\n    helloworld.push_str(&quot; world&quot;);\n    helloworld = helloworld + &quot;!&quot;;\n    println!(&quot;{}&quot;, helloworld);\n}\n함수의 매개변수로서의 텍스트\n\n문자열과 string은 일반적으로 함수에 문자열 slice 형태로 전달\n\nfn say_it_loud(msg: &amp;str) {\n    println!(&quot;{}!!!&quot;, msg.to_string().to_uppercase());\n}\n \nfn main() {\n    // say_it_loud는 &amp;&#039;static str을 &amp;str로 대여할 수 있습니다\n    say_it_loud(&quot;hello&quot;);\n    // say_it_loud는 또한 String을 &amp;str로 대여할 수 있습니다\n    say_it_loud(&amp;String::from(&quot;goodbye&quot;));\n}\n스트링 만들기\n\nconcat과 join은 string을 만드는 간단하지만 강력한 방법\n\nfn main() {\n    let helloworld = [&quot;hello&quot;, &quot; &quot;, &quot;world&quot;, &quot;!&quot;].concat();\n    let abc = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;].join(&quot;,&quot;);\n    println!(&quot;{}&quot;, helloworld);\n    println!(&quot;{}&quot;,abc);\n}\n→ hello world!\n→ a,b,c\n스트링 양식 만들기\n\nformat! macro는 값이 어디에 어떻게 놓일지 매개변수화 된 (예: {}) string을 정의하여 string을 생성\n\nfn main() {\n    let a = 42;\n    let f = format!(&quot;삶, 우주, 그리고 모든 것에 대한 해답: {}&quot;, a);\n    println!(&quot;{}&quot;, f);\n}\n스트링 변환\n\nto_string을 이용하여 string으로 변환될 수 있음(모든 자료형이 되는 것은 아님)\ngeneric 함수인 parse로 string이나 문자열을 다른 자료형을 가진 값으로 변환할 수 있음\n\n실패할 수도 있기 때문에 Result를 리턴\n\n\n\nfn main() -&gt; Result&lt;(), std::num::ParseIntError&gt; {\n    let a = 42;\n    let a_string = a.to_string();\n    let b = a_string.parse::&lt;i32&gt;()?;\n    println!(&quot;{} {}&quot;, a, b);\n    Ok(())\n}\n6장 - 마무리\n7장 - 객체 지향 프로그래밍\nOOP란 무엇인가?\n\n다음과 같은 상징적 특징을 가진 프로그래밍 언어를 뜻함\n\n캡슐화 (encapsulation) - _객체_라 불리는 단일 유형의 개념적 단위에 데이터와 함수를 연결지음.\n추상화 (abstraction) - 데이터와 함수를 숨겨 객체의 상세 구현 사항을 알기 어렵게 함.\n다형성 (polymorphism) - 다른 기능적 관점에서 객체와 상호작용하는 능력.\n상속 (inheritance) - 다른 객체로부터 데이터와 동작을 상속받는 능력.\n\n\n\nRust는 OOP가 아니다\n\nRust에서는 어떠한 방법으로도 데이터와 동작의 상속이 불가능\n\nstruct는 부모 struct로부터 field를 상속받을 수 없습니다.\nstruct는 부모 struct로부터 함수를 상속받을 수 없습니다.\n\n\n\n메소드 캡슐화 하기\n\n\n모든 메소드의 첫번째 매개변수는 메소드 호출과 연관된 인스턴스에 대한 참조여야 함\n\n&amp;self - 인스턴스에 대한 immutable한 참조.\n&amp;mut self - 인스턴스에 대한 mutable한 참조.\nex: instanceOfObj.foo()\n\n\n\n메소드는 impl 키워드를 쓰는 구현 블록 안에 정의\nimpl MyStruct {\n    ...\n    fn foo(&amp;self) {\n        ...\n    }\n}\n\n\nstruct SeaCreature {\n    noise: String,\n}\n \nimpl SeaCreature {\n    fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \nfn main() {\n    let creature = SeaCreature {\n        noise: String::from(&quot;blub&quot;),\n    };\n    println!(&quot;{}&quot;, creature.get_sound());\n}\n선택적 노출을 통한 추상화\n\n기본적으로, filed와 메소드들은 그들이 속한 module에서만 접근 가능\npub 키워드는 struct의 field와 메소드를 module 밖으로 노출시킴\n\nstruct SeaCreature {\n    pub name: String,\n    noise: String,\n}\n \nimpl SeaCreature {\n    pub fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \nfn main() {\n    let creature = SeaCreature {\n        name: String::from(&quot;Ferris&quot;),\n        noise: String::from(&quot;blub&quot;),\n    };\n    println!(&quot;{}&quot;, creature.get_sound());\n}\n다형성과 Trait\n\ntrait은 메소드의 집합을 struct 자료형에 연결할 수 있게 해줌\nstruct가 trait을 구현할 때, 실제 자료형이 무엇인지 알지 못하더라도 그 trait 자료형을 통해 간접적으로 struct와 상호작용할 수 있도록 (예: &amp;dyn MyTrait) 협약을 맺게 됨\n\nstruct SeaCreature {\n    pub name: String,\n    noise: String,\n}\n \nimpl SeaCreature {\n    pub fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \ntrait NoiseMaker {\n    fn make_noise(&amp;self);\n}\n \nimpl NoiseMaker for SeaCreature {\n    fn make_noise(&amp;self) {\n        println!(&quot;{}&quot;, &amp;self.get_sound());\n    }\n}\n \nfn main() {\n    let creature = SeaCreature {\n        name: String::from(&quot;Ferris&quot;),\n        noise: String::from(&quot;blub&quot;),\n    };\n    creature.make_noise();\n}\nTrait에 구현된 메소드\n\ntrait에 메소드를 구현해 넣을 수 있음\nstruct 내부의 field에 직접 접근할 수는 없지만, trait 구현체들 사이에서 동작을 공유할 때 유용\n\nstruct SeaCreature {\n    pub name: String,\n    noise: String,\n}\n \nimpl SeaCreature {\n    pub fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \ntrait NoiseMaker {\n    fn make_noise(&amp;self);\n    \n    fn make_alot_of_noise(&amp;self){\n        self.make_noise();\n        self.make_noise();\n        self.make_noise();\n    }\n}\n \nimpl NoiseMaker for SeaCreature {\n    fn make_noise(&amp;self) {\n        println!(&quot;{}&quot;, &amp;self.get_sound());\n    }\n}\n \nfn main() {\n    let creature = SeaCreature {\n        name: String::from(&quot;Ferris&quot;),\n        noise: String::from(&quot;blub&quot;),\n    };\n    creature.make_alot_of_noise();\n}\nTrait 상속\n\ntrait은 다른 trait의 메소드들을 상속 받을 수 있음\n\nstruct SeaCreature {\n    pub name: String,\n    noise: String,\n}\n \nimpl SeaCreature {\n    pub fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \ntrait NoiseMaker {\n    fn make_noise(&amp;self);\n}\n \ntrait LoudNoiseMaker: NoiseMaker {\n    fn make_alot_of_noise(&amp;self) {\n        self.make_noise();\n        self.make_noise();\n        self.make_noise();\n    }\n}\n \nimpl NoiseMaker for SeaCreature {\n    fn make_noise(&amp;self) {\n        println!(&quot;{}&quot;, &amp;self.get_sound());\n    }\n}\n \nimpl LoudNoiseMaker for SeaCreature {}\n \nfn main() {\n    let creature = SeaCreature {\n        name: String::from(&quot;Ferris&quot;),\n        noise: String::from(&quot;blub&quot;),\n    };\n    creature.make_alot_of_noise();\n}\n동적 vs 정적 디스패치\n\n정적 디스패치 (static dispatch) - 인스턴스의 자료형을 알고 있는 경우, 어떤 함수룰 호출해야 하는지 정확히 알고 있음\n동적 디스패치 (dynamic dispatch) - 인스턴스의 자료형을 모르는 경우, 올바른 함수를 호출할 방법을 찾아야 함\n\ntrait 자료형인 &amp;dyn MyTrait은 동적 디스패치를 통해 객체의 인스턴스들을 간접적으로 작동시킬 수 있게 함\n동적 디스패치를 사용할 경우, Rust에서는 사람들이 알 수 있도록 trait 자료형 앞에 dyn을 붙일 것을 권고\n\n\n동적 디스패치는 실제 함수 호출을 위한 포인터 추적으로 인해 조금 느릴 수 있음\n\nstruct SeaCreature {\n    pub name: String,\n    noise: String,\n}\n \nimpl SeaCreature {\n    pub fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \ntrait NoiseMaker {\n    fn make_noise(&amp;self);\n}\n \nimpl NoiseMaker for SeaCreature {\n    fn make_noise(&amp;self) {\n        println!(&quot;{}&quot;, &amp;self.get_sound());\n    }\n}\n \nfn static_make_noise(creature: &amp;SeaCreature) {\n    // 실제 자료형을 압니다\n    creature.make_noise();\n}\n \nfn dynamic_make_noise(noise_maker: &amp;dyn NoiseMaker) {\n    // 실제 자료형을 모릅니다\n    noise_maker.make_noise();\n}\n \nfn main() {\n    let creature = SeaCreature {\n        name: String::from(&quot;Ferris&quot;),\n        noise: String::from(&quot;blub&quot;),\n    };\n    static_make_noise(&amp;creature);\n    dynamic_make_noise(&amp;creature);\n}\nTrait 객체\n\n객체의 인스턴스를 &amp;dyn MyTrait 자료형을 가진 매개변수로 넘길 때, 이를 _trait 객체_라고 부름\ntrait 객체\n\n인스턴스의 올바른 메소드를 간접적으로 호출할 수 있게 해줌\n인스턴스에 대한 포인터와 인스턴스 메소드들에 대한 함수 포인터 목록을 갖고있는 struct\n\n\n이런 함수 목록을 C++에서는 _vtable_이라고 함\n\n크기를 알 수 없는 데이터 다루기\n\ntrait을 다른 struct에 저장하는 것\n\ntrait은 원본 struct를 알기 어렵게 하느라 원래 크기 또한 알기 어렵게 함\n\n\nRust에서 크기를 알 수 없는 값이 struct에 저장될 때는 다음의 두 가지 방법으로 처리\n\ngenerics - 매개변수의 자료형을 효과적으로 활용하여 알려진 자료형 및 크기의 struct/함수를 생성\nindirection - 인스턴스를 heap에 올림으로써 실제 자료형의 크기 걱정 없이 그 포인터만 저장\n\n\n\nGeneric 함수\n\n\nRust의 generic은 trait과 함께 작동\n\n\n매개변수 자료형 T를 정의할 때 해당 인자가 어떤 trait을 구현해야 하는지 나열함으로써 인자에 어떤 자료형을 쓸 수 있는지 제한할 수 있음\nfn my_function&lt;T&gt;(foo: T)\nwhere\n    T:Foo\n{\n    ...\n}\n\n\ngeneric을 이용하면 컴파일 시 자료형과 크기를 알 수 있는 정적 자료형의 함수가 만들어짐\n\n정적 디스패치와 함께 크기가 정해진 값으로 저장할 수 있게 됨\n\n\n\nstruct SeaCreature {\n    pub name: String,\n    noise: String,\n}\n \nimpl SeaCreature {\n    pub fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \ntrait NoiseMaker {\n    fn make_noise(&amp;self);\n}\n \nimpl NoiseMaker for SeaCreature {\n    fn make_noise(&amp;self) {\n        println!(&quot;{}&quot;, &amp;self.get_sound());\n    }\n}\n \nfn generic_make_noise&lt;T&gt;(creature: &amp;T)\nwhere\n    T: NoiseMaker,\n{\n    // 컴파일 타임에 실제 자료형을 알게 됩니다\n    creature.make_noise();\n}\n \nfn main() {\n    let creature = SeaCreature {\n        name: String::from(&quot;Ferris&quot;),\n        noise: String::from(&quot;blub&quot;),\n    };\n    generic_make_noise(&amp;creature);\n}\nGeneric 함수 줄여쓰기\n\n\ntrait으로 제한한 generic은 다음과 같이 줄여쓸 수 있음\nfn my_function(foo: impl Foo) {\n    ...\n}\n \n// 줄여쓰기 전\nfn my_function&lt;T&gt;(foo: T)\nwhere\n    T:Foo\n{\n    ...\n}\n\n\nstruct SeaCreature {\n    pub name: String,\n    noise: String,\n}\n \nimpl SeaCreature {\n    pub fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \ntrait NoiseMaker {\n    fn make_noise(&amp;self);\n}\n \nimpl NoiseMaker for SeaCreature {\n    fn make_noise(&amp;self) {\n        println!(&quot;{}&quot;, &amp;self.get_sound());\n    }\n}\n \nfn generic_make_noise(creature: &amp;impl NoiseMaker) {\n    // 컴파일 타임에 실제 자료형을 알게 됩니다\n    creature.make_noise();\n}\n \nfn main() {\n    let creature = SeaCreature {\n        name: String::from(&quot;Ferris&quot;),\n        noise: String::from(&quot;blub&quot;),\n    };\n    generic_make_noise(&amp;creature);\n}\nBox\n\nstack에 있는 데이터를 heap으로 옮길 수 있게 해주는 데이터 구조\n_smart pointer_로도 알려진 struct이며 heap에 있는 데이터를 가리키는 포인터를 들고 있음\n크기가 알려져 있는 struct이므로 (왜냐하면 그저 포인터만 들고 있으므로) field의 크기를 알아야 하는 struct에 뭔가의 참조를 저장할 때 종종 사용 됨\n\nstruct SeaCreature {\n    pub name: String,\n    noise: String,\n}\n \nimpl SeaCreature {\n    pub fn get_sound(&amp;self) -&gt; &amp;str {\n        &amp;self.noise\n    }\n}\n \ntrait NoiseMaker {\n    fn make_noise(&amp;self);\n}\n \nimpl NoiseMaker for SeaCreature {\n    fn make_noise(&amp;self) {\n        println!(&quot;{}&quot;, &amp;self.get_sound());\n    }\n}\n \nstruct Ocean {\n    animals: Vec&lt;Box&lt;dyn NoiseMaker&gt;&gt;,\n}\n \nfn main() {\n    let ferris = SeaCreature {\n        name: String::from(&quot;Ferris&quot;),\n        noise: String::from(&quot;blub&quot;),\n    };\n    let sarah = SeaCreature {\n        name: String::from(&quot;Sarah&quot;),\n        noise: String::from(&quot;swish&quot;),\n    };\n    let ocean = Ocean {\n        animals: vec![Box::new(ferris), Box::new(sarah)],\n    };\n    for a in ocean.animals.iter() {\n        a.make_noise();\n    }\n}\nGeneric 구조체 다시 보기\n\ngeneric struct는 trait으로 제한된 매개변수 자료형을 가질 수 있음\n\nstruct MyStruct&lt;T&gt;\nwhere\n    T: MyTrait\n{\n    foo: T\n    ...\n}\n \n//매개변수 자료형은 generic structure의 구현 블록 안에 표시\nimpl&lt;T&gt; MyStruct&lt;T&gt; {\n    ...\n}\n7장 - 마무리\n8장 - 스마트 포인터\n참조 다시 보기\n\n참조는 메모리 상의 어떤 바이트들의 시작 위치를 가리키는 숫자\n용도\n\n특정 자료형의 데이터가 어디에 존재하는지에 대한 개념을 나타내는 것\n\n\n일반 숫자와의 차이점\n\nRust에서 참조가 가리키는 값보다 더 오래 살지 않도록 lifetime을 검증하는 것\n\n\n\n원시 포인터\n\n참조는 더 원시적인 자료형인 _raw pointer_로 변환될 수 있음\nraw pointer는 숫자와 마찬가지로 거의 제한 없이 여기저기 복사하고 이동할 수 있음\n\nRust는 raw pointer가 가리키는 메모리 위치의 유효성을 보증하지 않습니다.\n\n\nraw pointer에는 두 종류\n\nconst T - 자료형 T의 데이터를 가리키는 절대 변경되지 않는 raw pointer.\nmut T - 자료형 T의 데이터를 가리키는 변경될 수 있는 raw pointer.\n\n\nraw pointer는 숫자와 상호 변환이 가능\n\n예: usize\n\n\nraw pointer는 _unsafe_한 코드의 데이터에 접근할 수 있음\n참조 vs raw pointer\n\nRust에서의 참조\n\n사용 방법에 있어서 C의 pointer와 매우 유사하나, 저장되는 방식이나 다른 함수에 전달되는 부분에 있어 훨씬 많은 컴파일 타임의 제약을 받음\n\n\nRust에서의 raw pointer\n\n복사하고 전달하고 심지어 pointer 연산을 할 수 있는 숫자 자료형으로 변환할 수도 있다는 점에서 C의 pointer와 유사\n\n\n\n\n\nfn main() {\n    let a = 42;\n    let memory_location = &amp;a as *const i32 as usize;\n    println!(&quot;데이터는 여기 있습니다: {}&quot;, memory_location);\n}\n \n-&gt; 데이터는 여기 있습니다: 140725281338716\n역참조\n\n참조 (i.e. &amp;i32)를 통해 참조되는 데이터를 접근/변경하는 것\n참조로 데이터에 접근/변경하는 데에는 다음의 두 가지 방법(=역참조 방법)\n\n변수 할당 중에 참조되는 데이터에 접근\n참조되는 데이터의 field나 메소드에 접근\n\n\n\n* 연산자\n\n\n* 연산자는 참조를 역참조 하는 명시적인 방법\n\n\ni32는 ==Copy trait을 구현하는 기본 자료형이기 때문에, stack에 있는 변수 a==의 바이트들은 변수 ==b==의 바이트들로 복사\nvelog.io/@undefcat/Rust-T-mut-T-그리고-Copy\n\ncopy trait : 일종의 shallow copy? ↔ clone trait\n\ntrait : rust의 object\n\n\n\n\n\nfn main() {\n    let a: i32 = 42;\n    let ref_ref_ref_a: &amp;&amp;&amp;i32 = &amp;&amp;&amp;a;\n    let ref_a: &amp;i32 = **ref_ref_ref_a;\n    let b: i32 = *ref_a;\n    println!(&quot;{}&quot;, b)\n}\n \n-&gt; 42\n. 연산자\n\n\n참조의 field와 메소드에 접근하는 데에 쓰임, 이건 좀 더 미묘하게 동작\nlet f = Foo { value: 42 };\nlet ref_ref_ref_f = &amp;&amp;&amp;f;\nprintln!(&quot;{}&quot;, ref_ref_ref_f.value);\n\n. 연산자가 참조 열을 자동으로 역참조\n저 마지막 줄은 컴파일러에 의해 자동적으로 다음과 같이 바뀌게 됨\n\nprintln!(&quot;{}&quot;, (***ref_ref_ref_f).value);\n\n\n\n\n\n스마트 포인터\n\n&amp; 연산자로 이미 존재하는 데이터의 참조를 생성하는 기능과 더불어, Rust에서는 smart pointer라 불리는 참조 같은 struct를 생성하는 기능을 제공\n고수준에서 보자면 참조는 다른 자료형에 대한 접근을 제공하는 자료형이라고 볼 수 있음\nsmart pointer가 일반적인 참조와 다른 점은, 프로그래머가 작성하는 내부 로직에 기반해 작동한다는 것\n일반적으로 smart pointer는 struct가 *와 . 연산자로 역참조될 때 무슨 일이 발생할지 지정하기 위해 Deref, DerefMut, 그리고 Drop trait을 구현\n\nuse std::ops::Deref;\nstruct TattleTell&lt;T&gt; {\n    value: T,\n}\nimpl&lt;T&gt; Deref for TattleTell&lt;T&gt; {\n    type Target = T;\n    fn deref(&amp;self) -&gt; &amp;T {\n        println!(&quot;{} was used!&quot;, std::any::type_name::&lt;T&gt;());\n        &amp;self.value\n    }\n}\nfn main() {\n    let foo = TattleTell {\n        value: &quot;secret message&quot;,\n    };\n    // foo가 `len` 함수를 위해 자동참조된 후\n    // 여기서 역참조가 즉시 일어납니다\n    println!(&quot;{}&quot;, foo.len());\n}\n-&gt; &amp;str was used!\n   14\n==→ 문자열은 필드에 존재하는데, foo의 참조를 재정의해서 self.value의 참조를 반환하도록 했기 때문에 len을 호출 할 수 있게 되었다.==\n위험한 스마트 코드\n\nsmart pointer는 _unsafe_한 코드를 꽤 자주 쓰는 경향이 있음.\nsmart pointer는 Rust에서 가장 저수준의 메모리를 다루기 위한 일반적인 도구\nunsafe한 코드\n\nRust 컴파일러가 보증할 수 없는 몇 가지 기능이 있다는 예외사항을 제외하고는 일반적인 코드와 완전히 똑같이 동작\nunsafe한 코드의 주기능은 _raw pointer를 역참조_하는 것\n이는 _raw pointer_를 메모리 상의 위치에 가져다 놓고 “데이터 구조가 여깄다!”고 선언한 뒤 사용할 수 있는 데이터 표현으로 변환하는 것을 의미\n\n예: *const u8을 u8\n\n\nRust에는 메모리에 쓰여지는 모든 바이트의 의미를 추적하는 방법은 없음\nRust는 _raw pointer_로 쓰이는 임의의 숫자에 무엇이 존재하는지 보증할 수 없기 때문에, 역참조를 unsafe { ... } 블록 안에 넣음\n\n\n\nfn main() {\n    let a: [u8; 4] = [86, 14, 73, 64];\n    // 이게 원시 pointer입니다.\n    // 무언가의 메모리 주소를 숫자로 가져오는 것은 완전히 안전한 일입니다\n    let pointer_a = &amp;a as *const u8 as usize;\n    println!(&quot;데이터 메모리 주소: {}&quot;, pointer_a);\n    // 숫자를 원시 pointer로, 다시 f32로 변환하는 것 역시\n    // 안전한 일입니다.\n    let pointer_b = pointer_a as *const f32;\n    let b = unsafe {\n        // 이건 unsafe한데,\n        // 컴파일러에게 우리의 pointer가 유효한 f32라고 가정하고\n        // 그 값을 변수 b로 역참조 하라고 하고 있기 때문입니다.\n        // Rust는 이런 가정이 참인지 검증할 방법이 없습니다.\n        *pointer_b\n    };\n    println!(&quot;맹세하건대 이건 파이다! {}&quot;, b);\n}\n \n-&gt; 데이터 메모리 주소: 140725241194940 \n   맹세하건대 이건 파이다! 3.1415\n익숙한 친구들\n\nVec&lt;T&gt;\n\n바이트들의 메모리 영역을 소유하는 smart pointer\nRust 컴파일러는 이 바이트들에 뭐가 존재하는지 모름\nsmart pointer\n\n관리하는 메모리 영역에서 내용물을 꺼내기 위해 자기가 뭘 의미하는지 해석\n데이터 구조가 그 바이트들 내 어디에서 시작하고 끝나는지 추적\nraw pointer를 데이터 구조로, 또 쓰기 편한 멋지고 깔끔한 인터페이스로 역참조\n\n\n\n\nString\n\n바이트들의 메모리 영역을 추적\n쓰여지는 내용물이 언제나 유효한 utf-8이도록 프로그램적으로 제한\n메모리 영역을 &amp;str 자료형으로 역참조할 수 있도록 도와줌\n\n\n이 데이터 구조들 둘 다, 자기 할 일을 하기 위해 raw pointer에 대한 unsafe한 역참조를 사용\n\nuse std::alloc::{alloc, Layout};\nuse std::ops::Deref;\n \nstruct Pie {\n    secret_recipe: usize,\n}\n \nimpl Pie {\n    fn new() -&gt; Self {\n        // 4 바이트를 요청해 봅시다\n        let layout = Layout::from_size_align(4, 1).unwrap();\n \n        unsafe {\n            // 메모리 위치를 숫자로 할당하고 저장합니다\n            let ptr = alloc(layout) as *mut u8;\n            // pointer 연산을 사용해 u8 값 몇 개를 메모리에 써봅시다\n            ptr.write(86);\n            ptr.add(1).write(14);\n            ptr.add(2).write(73);\n            ptr.add(3).write(64);\n \n            Pie {\n                secret_recipe: ptr as usize,\n            }\n        }\n    }\n}\nimpl Deref for Pie {\n    type Target = f32;\n    fn deref(&amp;self) -&gt; &amp;f32 {\n        // secret_recipe pointer를 f32 raw pointer로 변환합니다\n        let pointer = self.secret_recipe as *const f32;\n        // 역참조 하여 &amp;f32 값으로 리턴합니다\n        unsafe { &amp;*pointer }\n    }\n}\nfn main() {\n    let p = Pie::new();\n    // Pie struct의 smart pointer를 역참조 하여\n    // &quot;파이를 만듭니다&quot;\n    println!(&quot;{:?}&quot;, *p);\n}\n \n-&gt; 3.1415\n힙에 할당된 메모리\n\nBox는 데이터를 stack에서 heap으로 옮길 수 있게 해주는 smart pointer\n이를 역참조하면 마치 원래 자료형이었던 것처럼 heap에 할당된 데이터를 편하게 쓸 수 있음\n\nstruct Pie;\n \nimpl Pie {\n    fn eat(&amp;self) {\n        println!(&quot;heap에 있으니 더 맛있습니다!&quot;)\n    }\n}\n \nfn main() {\n    let heap_pie = Box::new(Pie);\n    heap_pie.eat();\n}\n \n-&gt;heap에 있으니 더 맛있습니다!\n실패할 수 있는 메인 다시 보기\n\nRust 코드에는 많고도 많은 오류 표현 방법이 있지만, 그 중에도 standard library에는 오류를 설명하기 위한 범용 trait인 std::error::Error가 있음\nsmart pointer인 Box를 사용하면 Box&lt;dyn std::error::Error&gt;를 오류 리턴 시 공통된 자료형으로 사용할 수 있음\n이는 오류를 heap에 전파하고 특정한 자료형을 몰라도 고수준에서 상호작용할 수 있도록 해주기 때문\nTour of Rust 초반에 main은 오류를 리턴할 수 있다고 배웠습니다.\n이제 우리는 오류의 데이터 구조가 Rust의 일반적인 Error trait을 구현하는 한, 프로그램에서 발생할 수 있는 거의 모든 종류의 오류를 설명할 수 있는 자료형을 리턴할 수 있습니다.\n\nuse core::fmt::Display;\nuse std::error::Error;\n \nstruct Pie;\n \n#[derive(Debug)]\nstruct NotFreshError;\n \nimpl Display for NotFreshError {\n    fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;&#039;_&gt;) -&gt; std::fmt::Result {\n        write!(f, &quot;이 파이는 신선하지 않군요!&quot;)\n    }\n}\n \nimpl Error for NotFreshError {}\n \nimpl Pie {\n    fn eat(&amp;self) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n        Err(Box::new(NotFreshError))\n    }\n}\n \nfn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let heap_pie = Box::new(Pie);\n    heap_pie.eat()?;\n    Ok(())\n}\n-&gt;\nStandard Error\n   Compiling playground v0.0.1 (/playground)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.61s\n     Running `target/debug/playground`\nError: NotFreshError\n참조 카운팅\n\nRc\n\nstack에 있는 데이터를 heap으로 옮겨주는 smart pointer\nheap에 놓인 데이터를 immutable하게 대여하는 기능을 가진 다른 Rc smart pointer들을 복제할 수 있게 해줌\nsmart pointer가 drop 될 때에만 heap에 있는 데이터가 할당 해제\n\n\n\nuse std::rc::Rc;\n \nstruct Pie;\n \nimpl Pie {\n    fn eat(&amp;self) {\n        println!(&quot;heap에 있으니 더 맛있습니다!&quot;)\n    }\n}\n \nfn main() {\n    let heap_pie = Rc::new(Pie);\n    let heap_pie2 = heap_pie.clone();\n    let heap_pie3 = heap_pie2.clone();\n \n    heap_pie3.eat();\n    heap_pie2.eat();\n    heap_pie.eat();\n \n    // 모든 참조 카운트 smart pointer가 여기서 drop 됩니다\n    // heap 데이터인 Pie가 드디어 할당 해제됩니다\n}\n-&gt; heap에 있으니 더 맛있습니다!\n   heap에 있으니 더 맛있습니다!\n   heap에 있으니 더 맛있습니다!\n접근 공유하기\n\nRefCell\n\n보통 smart pointer가 보유하는 컨테이너 데이터 구조\n데이터를 가져오거나 안에 있는 것에 대한 mutable 또는 immutable한 참조를 대여할 수 있게 해줌\n데이터를 대여할 때, Rust는 런타임에 다음의 메모리 안전 규칙을 적용하여 남용을 방지\n\n단 하나의 mutable한 참조 또는 여러개의 immutable한 참조만 허용\n둘 다는 불가\n이 규칙을 어기면 RefCell은 panic을 일으킴\n\n\n\n\n\nuse std::cell::RefCell;\n \nstruct Pie {\n    slices: u8,\n}\n \nimpl Pie {\n    fn eat(&amp;mut self) {\n        println!(&quot;heap에 있으니 더 맛있습니다!&quot;);\n        self.slices -= 1;\n    }\n}\n \nfn main() {\n    // RefCell은 런타임에 메모리 안전성을 검증합니다\n    // 주의: pie_cell은 mut가 아닙니다!\n    let pie_cell = RefCell::new(Pie { slices: 8 });\n \n    {\n        // 그렇지만 mutable 참조를 대여할 수 있습니다!\n        let mut mut_ref_pie = pie_cell.borrow_mut();\n        mut_ref_pie.eat();\n        mut_ref_pie.eat();\n \n        // mut_ref_pie는 scope의 마지막에 drop 됩니다\n    }\n \n    // 이제 mutable 참조가 drop 되고 나면 immutable하게 대여할 수 있습니다\n    let ref_pie = pie_cell.borrow();\n    println!(&quot;{} 조각 남았습니다&quot;, ref_pie.slices);\n}\n \n-&gt; heap에 있으니 더 맛있습니다!\n\t heap에 있으니 더 맛있습니다!\n\t 6 조각 남았습니다\n쓰레드 간에 공유하기\n\nMutex\n\n보통 smart pointer가 보유하는 컨테이너 데이터 구조\n데이터를 가져오거나 내부 데이터에 대한 mutable 또는 immutable한 참조를 대여할 수 있게 해줌\n잠긴 대여를 통해 운영체제가 동시에 오직 하나의 CPU만 데이터에 접근 가능\n원래 쓰레드가 끝날 때까지 다른 쓰레드들을 막음으로써 대여가 남용되는 것을 방지Mutex는 여러 개의 CPU 쓰레드가 같은 데이터에 접근하는 것을 조율하는 근본적인 부분\n\n\nArc\n\n특별한 smart pointer인 Arc도 있는데, 쓰레드-안전성을 가진 참조 카운트 증가 방식을 사용한다는 것을 제외하고는 Rc와 동일\n\n동일한 Mutex에 다수의 참조를 가질 때 종종 사용\n\n\n\n\n\nuse std::sync::Mutex;\n \nstruct Pie;\n \nimpl Pie {\n    fn eat(&amp;self) {\n        println!(&quot;지금은 오직 나만이 파이를 먹는다!&quot;);\n    }\n}\n \nfn main() {\n    let mutex_pie = Mutex::new(Pie);\n    // 파이에 대한 잠겨있는 immutable한 참조를 빌려봅시다\n    // lock은 실패할 수도 있기 때문에 그 결과는 unwrap 해야합니다\n    let ref_pie = mutex_pie.lock().unwrap();\n    ref_pie.eat();\n    // 잠긴 참조는 여기서 drop 되며, mutex로 보호되는 값은 다른 이에 의해 쓰일 수 있습니다\n}\n스마트 포인터 조합하기\n\nsmart pointer는 한계가 있는 것처럼 보이지만, 조합해서 사용하면 매우 강력해질 수 있음\n\nRc&lt;Vec&lt;Foo&gt;&gt;\n\nheap에 있는 immutable한 데이터 구조의 동일한 vector를 대여할 수 있는 복수의 smart pointer를 복제할 수 있게 해줌\n\n\nRc&lt;RefCell&lt;Foo&gt;&gt; \n\n복수의 smart pointer가 동일한 Foo struct를 mutable/immutable하게 대여할 수 있게 해줌\n\n\nArc&lt;Mutex&lt;Foo&gt;&gt;\n\n복수의 smart pointer가 임시의 mutable/immutable한 대여를 CPU 쓰레드 독점 방식으로 잠글 수 있게 해줌\n\n\n\n\n“내부 가변성” 패턴\n\nRust의 컴파일 타임 체크와 동일 수준의 안전성으로 런타임의 메모리 사용 규칙을 변경할 수 있는 패턴\n내부 데이터를 변경하기 위해 immutable한 데이터 유형(복수의 smart pointer가 소유할 수 있음)을 사용\n\n\n\nuse std::cell::RefCell;\nuse std::rc::Rc;\n \nstruct Pie {\n    slices: u8,\n}\n \nimpl Pie {\n    fn eat_slice(&amp;mut self, name: &amp;str) {\n        println!(&quot;{}가 한 조각 먹었습니다!&quot;, name);\n        self.slices -= 1;\n    }\n}\n \nstruct SeaCreature {\n    name: String,\n    pie: Rc&lt;RefCell&lt;Pie&gt;&gt;,\n}\n \nimpl SeaCreature {\n    fn eat(&amp;self) {\n        // mutable 대여를 위해 파이에 대한 smart pointer를 사용\n        let mut p = self.pie.borrow_mut();\n        // 한 입 먹자!\n        p.eat_slice(&amp;self.name);\n    }\n}\n \nfn main() {\n    let pie = Rc::new(RefCell::new(Pie { slices: 8 }));\n    // ferris와 sarah에겐 파이에 대한 smart pointer의 복제가 주어집니다\n    let ferris = SeaCreature {\n        name: String::from(&quot;ferris&quot;),\n        pie: pie.clone(),\n    };\n    let sarah = SeaCreature {\n        name: String::from(&quot;sarah&quot;),\n        pie: pie.clone(),\n    };\n    ferris.eat();\n    sarah.eat();\n \n    let p = pie.borrow();\n    println!(&quot;{} 조각 남았습니다&quot;, p.slices);\n}\n \n-&gt; ferris가 한 조각 먹었습니다!\n   sarah가 한 조각 먹었습니다!\n   6 조각 남았습니다\n8장 - 마무리\n9장 - 프로젝트 구성과 구조\n모듈\n\n크레이트(crate)\n\n모든 Rust 프로그램이나 라이브러리(library)\n모든 crate는 _모듈(module)_의 계층구조로 이루어짐\n모든 crate에는 최상위(root) module이 있음\n\n\nmodule\n\n전역변수, 함수, struct, trait, 또는 다른 module까지도 포함될 수 있음\nRust에서는 파일과 module 트리 계층구조 간의 1:1 대응은 없음\nmodule의 트리 구조는 코드로 직접 작성\n\n\n\n프로그램 작성하기\n\n프로그램은 main.rs라 불리는 파일에 root module을 가짐\n\n프로그램인 경우, main.rs가 진입점\n\n\n\n라이브러리 작성하기\n\nlibrary는 lib.rs라 불리는 파일에 root module을 가짐\n\nlibrary인 경우, lib.rs가 진입점\n\n\n\n다른 모듈과 크레이트 참조하기\n\nmodule 내의 항목은 전체 module 경로를 이용해 참조 가능\n\n예 : std::f64::consts::PI\n\n\n더 간단한 방법 : use 키워드 사용\n\nmodule에서 쓰고자 하는 특정 항목을 전체 경로를 쓰지 않고도 코드 어디에서든 사용할 수 있음\nuse std::f64::consts::PI를 쓰면 main 함수에서 PI만으로 사용할 수 있음\n\n\nstd\n\n유용한 데이터 구조 및 OS와 상호 작용할 수 있는 함수로 가득한 **표준 라이브러리(standard library)**의 crate\n\n\n커뮤니티 crate\n\ncrates.io/\n\n\n\nuse std::f64::consts::PI;\n \nfn main() {\n    println!(&quot;놀이터에 오신 것을 환영합니다!&quot;);\n    println!(&quot;{} 한 조각 먹고 싶군요!&quot;, PI);\n}\n \n-&gt; 놀이터에 오신 것을 환영합니다!\n   3.141592653589793 한 조각 먹고 싶군요!\n여러 개의 항목을 참조하기\n\n복수의 항목을 하나의 module 경로로 참조하는 경우 {} 사용\n\n예: use std::f64::consts::{PI,TAU}\n\n\n\n모듈 작성하기\n\nRust에서 module을 선언하는 데에는 두 가지 방법\n\nfoo.rs라는 이름의 파일\nfoo라는 이름의 디렉토리에 들어있는 파일 mod.rs\n\n\n\n모듈 계층구조\n\nmodule과 하위모듈(sub-module)\n\n한 module은 다른 module에 의존할 수 있음\n\n\n하위 모듈 만들기\n\n부모 module에 다음과 같은 코드를 작성\n\nmod foo;\n\nfoo.rs 파일이나 foo/mod.rs 파일을 찾아 이 scope 내의 foo module안에 그 내용물을 삽입\n\n\n\n\n\n\n\n인라인 모듈\n\n\nsub-module\n\nmodule의 코드 내에 직접 치환(inline)됨\n\n\n\ninline module의 가장 흔한 용도\n\n단위 테스트를 만들 때\n\n// 이 macro는 Rust가 테스트 모드가 아닐 경우\n// 이 inline module을 제거합니다.\n#[cfg(test)]\nmod tests {\n    // 부모 module에 즉시 접근이 가능하지 않다는 데에 주의하세요.\n    // 반드시 명시적으로 써줘야 합니다.\n    use super::*;\n \n    ... tests go here ...\n}\n\n\n내부 모듈 참조하기\n\nuse 경로에 사용할 수 있는 몇 가지 키워드\n\ncrate - root module\nsuper - 현재 module의 부모 module\nself - 현재 module\n\n\n\n내보내기\n\npub 키워드\n\n사용하면 module의 구성원들을 접근 가능하게 할 수 있음\n\n\n기본적으로 _module_의 구성원들은 외부에서 접근이 불가능\n\n그 자식 module도 접근 불가\n\n\n기본적으로 _crate_의 구성원들도 외부에서 접근이 불가능\n\ncrate의 root module (lib.rs 또는 main.rs)\npub을 표시하면 구성원들을 접근 가능하게 할 수 있음\n\n\n\n구조체 가시성\n\n함수와 마찬가지로, structure도 module 외부로 무엇을 노출할 지를 pub을 사용해 선언할 수 있음\n\n// SeaCreature struct는 우리 module 외부에서도 사용 가능합니다\npub struct SeaCreature {\n    pub animal_type: String,\n    pub name: String,\n    pub arms: i32,\n    pub legs: i32,\n    // 우리의 무기는 비밀로 남겨둡시다\n    weapon: String,\n}\n전주곡 (Prelude)\n\nprelude module\n\nuse로 가져오지도 않았는데 어떻게 어디서나 Vec나 Box를 쓸 수 있는 이유\nRust의 standard library에서는 std::prelude::*로 내보내기 된 모든 것들이 어디에서든 자동으로 사용 가능\n\nVec와 Box가 바로 이런 경우이며, 다른 것들(Option, Copy, 기타 등등)도 마찬가지\n\n\n\n\n\n여러분만의 Prelude\n\nstandard library의 prelude로 인해, 흔히들 library마다 고유의 prelude module을 만듬\n\nlibrary 사용을 위해 필요한 가장 흔한 데이터 구조들을 모두 가져오는 시작점으로 사용\n\n예: use my_library::prelude::*\n\n\nstandard library와 달리 프로그램이나 library에서 자동으로 쓸 수 있는 것은 아니지만, library 사용자들이 어디서부터 시작할지 도움을 줄 좋은 습관\n\n\n\n9장 - 마무리\n\n참고하면 좋은 자료 : rust 사용 시 가이드라인\n\n\nrust-lang.github.io/api-guidelines/\n\n\n\n\nToy Project\n\nRust Backend를 사용한 inference server (Image Denoising)\n\nImage input → Pre Processing → GPU Inference → Post Processing → image output\n\nGPU Inference : ONNX Runtime - TensorRT\n할 수 있는 만큼 최적화, 그리고 속도 check\n\n\nActix\n\nactix.rs/\n\n\nRust OpenCV\n\nwww.google.com/search+opencv&amp;rlz=1C5GCEM_enKR1054KR1054&amp;oq=rust+opencv&amp;gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDQ2MzNqMGo3qAIAsAIA&amp;sourceid=chrome&amp;ie=UTF-8\n\n\nRust ONNX Runtime\n\ndocs.rs/onnxruntime/latest/onnxruntime/\ncuda 가능 할 것인가\n\n\n위 구현 완료 후, 여유가 된다면 +@ 계획 및 진행\n\n\n\n추가 자료 조사 - Smart Pointer\nc/c++의 포인터처럼 작동하지만 추가적인 메타 데이터와 기능들을 가지고 있는 “데이터 구조”\n스마트 포인터를 사용하여 구현한 대표적인 자료형\n\n\nString\n\n\nVec\n\n\nRust의 스마트 포인터는 Rust의 소유권 시스템의 기본 요소이며 메모리를 효율적으로 관리하는 데 사용 됨\n\n\n스마트 포인터는 변수의 수명을 관리하는 데 사용되는 데이터 유형\n\n\n데이터가 더 이상 필요하지 않을 때 자동으로 메모리를 해제\n\n\n복잡한 메모리 상황을 관리하고 널 포인터 참조와 메모리 누수와 같은 일반적인 버그를 방지하는 데 유용\n\n\nRust의 세 가지 내장 스마트 포인터 유형\n\n\nBox\n\n소유권 및 힙에 대한 할당을 제공, 힙에 메모리를 할당하고 거기에 값을 저장하는 데 사용\n\nfn main() {\n\t\t// i32형 데이터를 힙에 할당! 스택에는 이 힙 메모리에 대한 주소 값이 할당 됨\n    let b = Box::new(5);\n    println!(&quot;b = {}&quot;, b); // 주의깊게 볼 점 : &#039;b&#039;는 i32 type이 아닌 Box형 type\n}\n\n\nRc\n\n동일한 값에 대한 여러 소유자를 허용, 코드의 여러 부분 간에 값 소유권을 공유 하려는 경우 사용\n\n\n\nRefCell\n\n\n내부 가변성을 제공하여 그렇지 않은 경우 값의 가변 대여를 허용\n\n\n\n\n참조자와 스마트 포인터 간의 추가적인 차이점은?\n\n\n참조자 : 데이터를 오직 빌리기만 하는 포인터\n\n\n스마트 포인터 : (많은 경우)그들이 가리키고 있는 데이터를 소유\n\n\n스마트 포인터의 구현\n\n\nDeref와 Drop 트레잇을 구현\n\n\nDeref\n\n\n스마트 포인터 구조체의 인스턴스가 참조자처럼 동작하도록 하여 참조자나 스마트 포인터 둘 중 하나와 함께 작동하는 코드를 작성하게 해 줌\n\n\n우리가 (곱하기 혹은 글롭 연산자와는 반대 측에 있는) 역참조 연산자 (dereference operator) * 의 동작을 커스터마이징 하는 것을 허용\n\n\n기존 역참조 vs Deref\n// 기본적인 역참조 예시\nfn main() {\n    let x = 5;\n    let y = &amp;x;\n \n    assert_eq!(5, *y);\n}\n \n// Box로 힙에 할당한 데이터에 대한 역참조 예시\nfn main() {\n    let x = 5;\n    let y = Box::new(x);\n \n    assert_eq!(5, x);\n    assert_eq!(5, *y);\n}\n\n\n예시로 이해해보기\n// Box와 유사한 trait 구현\nuse std::ops::Deref;\nstruct MyBox&lt;T&gt;(T);\n \n// new 메서드 구현\nimpl&lt;T&gt; MyBox&lt;T&gt; {\n    fn new(x: T) -&gt; MyBox&lt;T&gt; {\n        MyBox(x)\n    }\n}\n \n// deref 메서드 구현\nimpl&lt;T&gt; Deref for MyBox&lt;T&gt; {\n    type Target = T;\n \n    fn deref(&amp;self) -&gt; &amp;T {\n\t\t\t\t// MyBox 타입은 T 타입의 하나의 요소를 가진 튜플 구조체\n\t\t\t\t// 아래 코드를 통해 자신이 가지고 있는 데이터를 리턴\n        &amp;self.0\n    }\n}\n \nfn main() {\n    let x = 5;\n    let y = MyBox::new(x);\n \n    assert_eq!(5, x);\n\t\t// 위에서 Deref를 구현했으므로 러스트는 실제로 &quot;*(y.deref())&quot;를 실행해 줌\n    assert_eq!(5, *y);\n}\n\n\n\n\nDrop\n\n\n스마트 포인터의 인스턴스가 스코프 밖으로 벗어 났을 때 실행되는 코드를 커스터마이징 가능하도록 해 줌\n\n\n특정한 코드는 파일이나 네트워크 연결 같은 자원을 해제하는 데에 사용될 수 있음. 일종의 Dispose\n\n\n예시로 이해하기\n// 인스턴스가 스코프 밖으로 벗어났을 때 Dropping CustomSmartPointer!를 출력\n// 하는 커스텀 기능만을 갖춘 CustomSmartPointer 구조체\n \n// 구조 정의\nstruct CustomSmartPointer {\n    data: String,\n}\n \n// Drop 구현\nimpl Drop for CustomSmartPointer {\n    fn drop(&amp;mut self) {\n        println!(&quot;Dropping CustomSmartPointer with data `{}`!&quot;, self.data);\n    }\n}\n \nfn main() {\n    let c = CustomSmartPointer { data: String::from(&quot;my stuff&quot;) };\n    let d = CustomSmartPointer { data: String::from(&quot;other stuff&quot;) };\n    println!(&quot;CustomSmartPointers created.&quot;);\n}\n변수들은 만들어진 순서의 역순으로 버려지므로\n출력 →\nCustomSmartPointers created.\nDropping CustomSmartPointer with data\nother stuff!\nDropping CustomSmartPointer with data\nmy stuff!\n\n\n주의할 점\n\n\nDrop은 Rust상에서 자동적으로 호출되는 메서드이므로, 우리가 직접 호출 할 수 없음\nerror[E0040]: explicit use of destructor method\n⇒ src/main.rs:14:7\n|\n14 | c.drop();\n| ^^^^ explicit destructor calls not allowed\n\n\n만약 직접 호출할 필요가 있다면 std::mem::drop을 가져와서 구현해야 함\n\n\n\n\n\n\n참조\n\nrinthel.github.io/rust-lang-book-ko/ch15-00-smart-pointers.html\n\n\n"},"vault/Notion/DB/DB-Blog-Post/SSD(Single-Shot-Multibox-Detector)/SSD(Single-Shot-Multibox-Detector)":{"slug":"vault/Notion/DB/DB-Blog-Post/SSD(Single-Shot-Multibox-Detector)/SSD(Single-Shot-Multibox-Detector)","filePath":"vault/Notion/DB/DB Blog Post/SSD(Single Shot Multibox Detector)/SSD(Single Shot Multibox Detector).md","title":"SSD(Single Shot Multibox Detector)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10"],"tags":[],"content":"\n참조\nSSD(Single Shot Multibox Detector)\nLoss\n\n\n참조\n\n\n                  \n                  SSD: Single Shot MultiBox Detector \n                  \n                \n\n\nWe present a method for detecting objects in images using a single deep neural network.\narxiv.org/abs/1512.02325\n\n\n\nWeek 10\nyeomko.tistory.com/20\ntaeu.github.io/paper/deeplearning-paper-ssd/\nSSD(Single Shot Multibox Detector)\n\nYolo의 문제점은 입력 이미지를 7x7 크기의 그리드로 나누고, 각 그리드 별로 Bounding Box Prediction을 진행하기 때문에 그리드 크기보다 작은 물체를 잡아내지 못하는 문제가 있었습니다.\n그리고 신경망을 모두 통과하면서 컨볼루션과 풀링을 거쳐 coarse한 정보만 남은 마지막 단 피쳐맵만 사용하기 때문에 정확도가 하락하는 한계가 있었습니다.\n\nSSD는 이를 해결하고자 앞단 CNN Feature Map을 끌어와 사용하여 Detail을 잡아내고 Faster RCNN의 Anchor 개념을 가져와서 다양한 형태의 Object들도 감지하려고 시도했습니다.\n\nyolo v1은 7x7 grid로 2개의 Bounding Box를 예측했습니다. 하지만 SSD의 Feature Map을 보면 매우 다양한 dimension으로 이루어져 있는 것을 알 수 있습니다.\n\nFeature Map 1 : 38 x 38 x 512 → 38 x 38 x B개의 Bounding Box 예측\nFeature Map 2 : 19 x 19 x 1024 → 19 x 19 x B개의 Bounding Box 예측\nFeature Map 3 : 10 x 10 x 512 → 10 x 10 x B개의 Bounding Box 예측\nFeature Map 4 : 5 x 5 x 256 → 5 x 5 x B개의 Bounding Box 예측\nFeature Map 5 : 3 x 3 x 256 → 3 x 3 x B개의 Bounding Box 예측\nFeature Map 6 : 1 x 1 x 256 → 1 x 1 x B개의 Bounding Box 예측\n\n또한 yolo v1에서는 Bounding Box와 별개로 Grid의 Class를 예측했다면, SSD는 Bonding Box 별로 Class를 예측한다는 것을 Conv Filter Size 공식을 보면 알 수 있습니다.\nConv Filter Size = 3 x 3 x (B x (C + offset))\n\nstride = 1, padding = 1\nB : Bounding Box 개수\nC : Class 개수\noffset : Bounding Box의 center x, center y, width, height\n\nLoss\n\n\nLoss Function\n\n\n\nLocalization Loss\n\n\n\nConfidence Loss\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Sample-Distribution-vs-Sampling-Distribution":{"slug":"vault/Notion/DB/DB-Blog-Post/Sample-Distribution-vs-Sampling-Distribution","filePath":"vault/Notion/DB/DB Blog Post/Sample Distribution vs Sampling Distribution.md","title":"Sample Distribution vs Sampling Distribution","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\nSample Distribution vs Sampling Distribution\n\n\n참조\nko.wikipedia.org/wiki/표본_분포\nvelog.io/@regista/표본분포와-표집분포-Sampling-distribution-vs.-Sample-distribution\nWeek 1\nSample Distribution vs Sampling Distribution\n\n표본분포(sample distribution)\n\n확률표본(확률표본, random sample)의 함수\n\n\n표집분포(sampling distribution)\n\n표본통계량이 이론적으로 따르는 확률분포\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Simple-ML-Flow-vs-Competition-Flow":{"slug":"vault/Notion/DB/DB-Blog-Post/Simple-ML-Flow-vs-Competition-Flow","filePath":"vault/Notion/DB/DB Blog Post/Simple ML Flow vs Competition Flow.md","title":"Simple ML Flow vs Competition Flow","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-6/Week-6"],"tags":[],"content":"\n참조\nSimple ML Flow vs Competition Flow\n\n\n참조\nWeek 6\nSimple ML Flow vs Competition Flow\n대회에서는 Deploy를 하지 않는다.\n\n\nSimple ML Flow\n\nDomain understanding\nData analysis\nData processing\nModeling\nTraining\nDeploy\n\n\n\nCompetition Flow\n\nDomain understanding\nData analysis\nData processing\nModeling\nTraining\n\n\n"},"vault/Notion/DB/DB-Blog-Post/StyleGAN--A-Style-Based-Generator-Architecture-for-GANs/StyleGAN--A-Style-Based-Generator-Architecture-for-GANs":{"slug":"vault/Notion/DB/DB-Blog-Post/StyleGAN--A-Style-Based-Generator-Architecture-for-GANs/StyleGAN--A-Style-Based-Generator-Architecture-for-GANs","filePath":"vault/Notion/DB/DB Blog Post/StyleGAN- A Style-Based Generator Architecture for GANs/StyleGAN- A Style-Based Generator Architecture for GANs.md","title":"StyleGAN- A Style-Based Generator Architecture for GANs","links":[],"tags":[],"content":"\n참조\nStyleGAN: A Style-Based Generator Architecture for GANs\n\n\n참조\nblog.lunit.io/2019/02/25/a-style-based-generator-architecture-for-generative-adversarial-networks/\nStyleGAN: A Style-Based Generator Architecture for GANs\nGenerator를 통한 이미지 합성 과정은 여전히 block box로 여겨지며, 이로 인해 합성되는 이미지의 attribute (성별, 연령, 헤어스타일 등) 을 조절하기가 매우 어렵다는 한계가 있습니다.\n또한 생성되는 이미지 퀄리티가 불안정하여 실제로는 논문에 리포트된 것과 달리 부자연스러운 이미지도 다수 생성됩니다.\n\n이러한 문제를 해결하기위해 Style Transfer에 기반한 새로운 generator 구조인 StyleGAN을 제안합니다. StyleGAN은 이미지를 style의 조합으로 보고, generator의 각 layer 마다 style 정보를 입히는 방식으로 이미지를 합성합니다. 이 때 각 layer에서 추가되는 style은 이미지의 coarse feature (성별, 포즈 등) 부터 fine detail (머리색, 피부톤 등) 까지 각기 다른 level의 visual attribute를 조절할 수 있습니다.  뿐만 아니라 StyleGAN은 기존의 방법들보다 훨씬 안정적이고 높은 퀄리티의 이미지를 생성하게 됩니다."},"vault/Notion/DB/DB-Blog-Post/Super-resolution-GAN/Super-resolution-GAN":{"slug":"vault/Notion/DB/DB-Blog-Post/Super-resolution-GAN/Super-resolution-GAN","filePath":"vault/Notion/DB/DB Blog Post/Super resolution GAN/Super resolution GAN.md","title":"Super resolution GAN","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-5/Week-5"],"tags":[],"content":"\n참조\nSuper resolution GAN\n\n\n참조\narxiv.org/abs/1609.04802\nWeek 5\nSuper resolution GAN\n\n\nGan을 이용해서 Super resolution을 수행"},"vault/Notion/DB/DB-Blog-Post/Swin-Transformer/Swin-Transformer":{"slug":"vault/Notion/DB/DB-Blog-Post/Swin-Transformer/Swin-Transformer","filePath":"vault/Notion/DB/DB Blog Post/Swin Transformer/Swin Transformer.md","title":"Swin Transformer","links":[],"tags":[],"content":"\n참조\n등장 배경\nSwin Transformer\nShifted Windows\nHierarchical Stages\nRelative position bias\nPatch Merging\nInference Sequence\n\n\n참조\narxiv.org/abs/2103.14030\nheeya-stupidbutstudying.tistory.com/entry/DL-Swin-Transformer-논문-리뷰-Hierarchical-Vision-Transformer-using-Shifted-Windows\nzhangtemplar.github.io/swin-transformer/\n등장 배경\n\n\nTransformer based approach의 문제점\n\ncomputational complexity on high-resolution images\n\n이미지의 해상도, 픽셀이 늘어나면 늘어날수록 모든 patch의 조합에 대해 self-attention을 수행하는 것은 불가능해짐\n\n\n\nSwin Transformer\n\nSwin Transformer에서는 hierarchical feature map을 구성함으로써 이미지 크기에 대해 linear complexity를 가질 수 있도록 고안된 아키텍처를 제시합니다.\n이것은 다양한 비전 분야의 작업에 있어 general-purpose backbone으로 적합하게 만들며, 단일 해상도의 feature map을 만들고 quadratic complexity를 갖는 이전의 트랜스포머 기반 아키텍처와 차이점을 가집니다.\nSwin Transformer Block은 Window 기반의 W-MSA와 SW-MSA 모듈로 이루어지며, 각각의 MSA 모듈을 포함한 2개의 연속적인 트랜스포머로 하나의 Swin block이 형성됩니다.\n그 이의의 부분(Layer Normalization, MLP, GELU 등)은 ViT와 같습니다.\nShifted Windows\n\n각 Block에서 Patch들을 Window로 분할하는 것을 두 가지 방식으로 진행합니다.\n\nW-MSA(window based multi-head self attention)\n\nfeature map을 M개의 window로 나누는 것 (regular - 위 그림에서 왼쪽)\n\n\nSW-MSA(shifted window based multi-head self attention)\n\n\nW-MSA 모듈에서 발생한 패치로부터([M/2, M/2])칸 떨어진 patch에서 window 분할\n\n\n\n\nSelf-attention방식\n\nWindow들 내부에서만 patch끼리의 self-attention을 계산\n\n\ncomputational complexity의 한계를 극복하기위한 방안\n\n\nWindow : M개의 인접한 patch들로 구성되어 있는 patch set\n\n\n\n\nSW-MSA의 문제\n\nW-MSA에서 (H/M) * (W/M)이였던 Window 수가 SW-MSA 모듈에서는 (H/M + 1) * (W/M + 1)로 달라지게 됩니다.\n\n\nex) W-MSA에서 2x2였던 Window 수가 SW-MSA에서는 3x3이 됩니다.\n\n\n\n\n저자는 SW-MSA의 문제에 대해서는 두 가지 Approach를 제시합니다.\n\n\nNaive Solution\n\n작아진 window들에 padding을 두어 크기를 다시 으로 맞춰주고, attention을 계산할 때 padding된 값들을 마스킹\n연산량의 증가(2x2→3x3 , 2.25배 더 상승)\n\n\n\nCyclic-shift(Efficient Batch Computation for Shifted Configuration)\n\n\n왼쪽 상단을 향해 cyclic하게 회전하기\nBatch Window는 Feature map에서 인접하지 않은 여러 하위 window로 구성될 수 있음\n\nself-attention 계산을 각 하위 window 내에서 제한하기 위해 마스킹 메커니즘 사용\n\n\nBatch Window 개수가 regular window partitioning 때와 동일하게 유지되므로 효율적(low latency)\n\n\n\nHierarchical Stages\nHierarchical Stages\n\npatch들을 합치며 계층적인 구조로 각 단계마다 representation을 갖기 때문에 다양한 크기의 entity를 다루어야 하는 비전 분야에서 좋은 성능을 낼 수 있음\n\nViT vs Swin\n\nViT : image→patch\n\n이미지를 작은 patch들로 쪼개는 방식\nViT의 Patch size : 16 x 16\n\n\nSwin : patch→proportion of image→image\n\nViT보다 더 작은 단위의 patch로부터 시작해서 점점 patch들을 merge\nSwin의 Patch size : 4 x 4\n\n\n\n\n\nRelative position bias\n\nAttention(Q,K,V)=SoftMax(QK^T/\\sqrt d+B)V\nSwin은 position embedding이 없으며, 이를 대신하여 Attention 시 Relative position bias(B)를 추가했습니다.\n저자는 bias 항이 없거나 absolute position embedding을 사용했을 때보다 상당한 모델 성능의 향상을 보였다고 합니다.\n\nPatch Merging\n\n\n인접한(2×2)=4개의 patch들끼리 결합하여 하나의 큰 patch를 새롭게 만듭니다.\npatch를 합치는 과정에서 차원이 4C로 늘어나기 때문에 linear layer를 통과하여 2C로 조정합니다(feature transformation).\n\nInference Sequence\n\nInput\n\nPatch Partition\n\n이미지를 Patch 단위로 나누기\nH x W x 3 → (H/4) x (H/4) x 48\n48 : 3 x M → 3 x 16\n\n\n\n\nStage 1\n\nLinear embedding\n\n(H/4) × (W/4) × 48 → (H/4) × (W/4) × C\n\n\nSwin Transformer block\n\n(H/4) × (W/4) × C → (H/4) × (W/4) × C\n\n\n\n\nStage 2\n\nPatch Merging\n\n(H/4) × (W/4) × C → (H/8) × (W/8) × 4C\n(H/4) × (W/4) × 4C → (H/8) × (W/8) × 2C\n\n\nSwin Transformer block\n\n(H/4) × (W/4) × 2C → (H/8) × (W/8) × 2C\n\n\n\n\nStage 3\n\nPatch Merging\n\n(H/8) × (W/8) × 2C → (H/16) × (W/16) × 8C\n(H/16) × (W/16) × 8C → (H/16) × (W/16) × 4C\n\n\nSwin Transformer block\n\n(H/16) × (W/16) × 8C → (H/16) × (W/16) × 4C\n\n\n\n\nStage 4\n\nPatch Merging\n\n(H/16) × (W/16) × 8C → (H/32) × (W/32) × 16C\n(H/32) × (W/32) × 16C → (H/32) × (W/32) × 8C\n\n\nSwin Transformer block\n\n(H/32) × (W/32) × 8C → (H/32) × (W/32) × 8C\n\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Text-Region-검출기/Text-Region-검출기":{"slug":"vault/Notion/DB/DB-Blog-Post/Text-Region-검출기/Text-Region-검출기","filePath":"vault/Notion/DB/DB Blog Post/Text Region 검출기/Text Region 검출기.md","title":"Text Region 검출기","links":["vault/Notion/DB/DB-Blog-Post/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선"],"tags":[],"content":"\n\n                  \n                  GitHub - 404Vector/App.Streamlit-FastAPI.Text.Detector: OCR Task 중 하나인 Text Detection을 수행하는 Web Service 저장소 입니다. \n                  \n                \n\n\nYou can’t perform that action at this time.\ngithub.com/404Vector/App.Streamlit-FastAPI.Text.Detector\n\n\n\n\n개요\n설치 및 실행\n결과\n\n\n개요\n\n목적\n\n이 프로젝트는 사용자가 업로드한 이미지에서 Text가 있는 위치를 예측해주는 Web 기반 Service를 개발하는 것이 목적입니다.\n\n\nModel, Weight\n\nText Region의 예측은 기존에 대회에서 훈련시킨 모델을 가져와 사용했습니다.\nNAVER Connect 학습 데이터 추가 및 수정을 통한 이미지 속 글자 검출 성능 개선\n\n\nWeb\n\n\nFrontend : Streamlit\n\n\nBackend : FastAPI\n\n\n\n\n\n설치 및 실행\n\n\n설치\n\n주의 : Python 3.8.15, curl, git-lfs가 필요합니다.\n\n# (optional)curl이 없는 경우\napt-get install curl\n \n# (optional)git-lfs가 없는 경우\ncurl -s packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\napt install git-lfs\n \n# git repos clone\ngit clone github.com/404Vector/App.Streamlit-FastAPI.Text.Detector.git\ncd App.Streamlit-FastAPI.Text.Detector\n \n# install package\npip3 install -r requirments.txt\n# or\npipenv install -r requirments.txt\n\n\nError Case\nERROR: Could not build wheels for lanms-neo, which is required to install pyproject.toml-based projects\n→ build-essential 설치 후 다시 시도\n(개발에 필요한 기본 라이브러리와 헤더파일이 설치됨)\napt-get install build-essential -y\n \n# install package\npip3 install -r requirments.txt\n# or\npipenv install -r requirments.txt\n\n\n\n\n실행\n\n\nFrontend 실행\npython -m frontend\n\n\nBackend 실행\npython -m backend\n\n\n실행화면\n\n\n\n\n\n\n결과\n\nStreamlit과 FastAPI를 사용하여 훈련한 모델로 결과를 얻을 수 있었다.\nWeb Application을 만들어 본 것은 두 번째여서 Streamlit과 FastAPI 사이의 정보 교환은 공부하며 진행해야 했다.\nStreamlit은 변경사항이 있을 때 마다 rendering을 다시 하며 위에서 아래로 순차적으로 script를 읽기 때문에 state(st.session_state)를 적극적으로 활용해야 작업이 편리했다.\nFrontend와 Backend로 분리되어 있어서 한쪽만 수정해도 되는 작업의 경우 매우 편리했다.\n"},"vault/Notion/DB/DB-Blog-Post/Tmux-설치-및-사용법":{"slug":"vault/Notion/DB/DB-Blog-Post/Tmux-설치-및-사용법","filePath":"vault/Notion/DB/DB Blog Post/Tmux 설치 및 사용법.md","title":"Tmux 설치 및 사용법","links":[],"tags":[],"content":"\n원격 서버로 훈련할 때의 문제\ntmux 란?\n설치\n사용\n\n기본 세션 시작\n명명된 세션 시작\n세션 분리\n세션 접속\n현재 세션 목록\n\n\n결론\n\n\n원격 서버로 훈련할 때의 문제\nssh를 사용해 원격 서버로 훈련을 진행한다면, 세션을 계속 유지하는 것이 부담스러울 수 있다.\n이 문제를 tmux를 사용해 해결할 수 있다.\ntmux 란?\ntmux는 터미널 멀티플렉서이며, 여러 개의 창(window)을 하나의 터미널 세션에서 사용할 수 있게 해준다. 그리고 ssh 연결이 끊어지더라도 실행 중인 프로그램을 보존할 수 있다.\n설치\nFor Ubuntu, Debian\nsudo apt install tmux\nFor CentOS, Fedora\nsudo yum install tmux\nFor MacOS\nbrew install tmux\n사용\n기본 세션 시작\ntmux\n명명된 세션 시작\ntmux new -s {session_name}\n세션 분리\n\nCtrl + b d\n\n세션 접속\ntmux attach -t {session_name}\n현재 세션 목록\ntmux ls\n결론\ntmux를 사용하면 지속적으로 원격서버와의 세션을 유지할 필요 없이 훈련이 가능하므로, 장시간 모델 훈련이 필요할 때 유용하다."},"vault/Notion/DB/DB-Blog-Post/Transfer-Learning/Transfer-Learning":{"slug":"vault/Notion/DB/DB-Blog-Post/Transfer-Learning/Transfer-Learning","filePath":"vault/Notion/DB/DB Blog Post/Transfer Learning/Transfer Learning.md","title":"Transfer Learning","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4"],"tags":[],"content":"\n참조\nTransfer Learning\n\n\n참조\nen.wikipedia.org/wiki/Transfer_learning\nkr.mathworks.com/discovery/transfer-learning.html\nWeek 2\nWeek 4\nTransfer Learning\n\n잘 만들어진 dataset으로 잘 만든 model(pre-trained model)을 가져와 활용하는 기술입니다.\n현실에서는 고품질 혹은 대량의 dataset을 얻기 힘들지만, 전이학습을 이용하면 적은 데이터로 높은 성능을 기대할 수 있습니다.\n\n하나의 문제를 풀 때 얻은 지식을 관련있는 다른 문제를 풀 때 적용하는 것\n이미 훈련된 모델을 유사 작업 수행 모델의 시작점으로 활용하는 딥러닝 접근법\n"},"vault/Notion/DB/DB-Blog-Post/Tree란-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/Tree란-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/Tree란 무엇일까.md","title":"Tree란 무엇일까","links":[],"tags":[],"content":"\n참조\nTree\nTree의 성질\n\n\n참조\nratsgo.github.io/data structure&amp;algorithm/2017/10/21/tree/\nTree\nTree는 그 모양이 뒤집어 놓은 나무와 같다고 해서 이런 이름이 붙었습니다. 예컨대 다음 그림과 같습니다.\n\n\nnode : 위 그림에서 검정색 동그라미, 데이터가 담기는 곳\nedge : node와 node 사이를 이어주는 선, 노드와의 관계를 표시\npath : edge로 연결된, 즉 인접한 노드들로 이뤄진 시퀀스(sequence)를 의미, 경로의 길이(length)는 경로에 속한 edge의 수\ntree height : root node에서 말단 node에 이르는 가장 긴 경로의 edge 수\ntree level : tree의 특정 깊이를 가지는 node의 집합\nleaf node : 자식 node가 없는 node\ninternal node : 잎새노드를 제외한 node\nroot node : 부모노드가 없는 node\n\nTree의 성질\nTree의 속성 중 가장 중요한 것이 ‘루트노드를 제외한 모든 노드는 단 하나의 부모노드만을 가진다’는 것입니다. 이 속성 때문에 트리는 다음 성질을 만족합니다.\n\n임의의 노드에서 다른 노드로 가는 경로(path)는 유일\n회로(cycle)가 존재하지 않음\n모든 노드는 서로 연결되어 있음\n엣지(edge)를 하나 자르면 트리가 두 개로 분리됨\n엣지(edge)의 수 |E| 는 노드의 수 |V|에서 1을 뺀 것과 같다.\n"},"vault/Notion/DB/DB-Blog-Post/Two-stage-detector-vs-One-stage-detector/Two-stage-detector-vs-One-stage-detector":{"slug":"vault/Notion/DB/DB-Blog-Post/Two-stage-detector-vs-One-stage-detector/Two-stage-detector-vs-One-stage-detector","filePath":"vault/Notion/DB/DB Blog Post/Two-stage detector vs One-stage detector/Two-stage detector vs One-stage detector.md","title":"Two-stage detector vs One-stage detector","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-4/Week-4"],"tags":[],"content":"\n참조\nTwo-stage detector vs One-stage detector\n\n\n참조\nstackoverflow.com/questions/65942471/one-stage-vs-two-stage-object-detection\nWeek 4\narxiv.org/pdf/1905.05055.pdf\nTwo-stage detector vs One-stage detector\n\n2 Stage Detector는 느린 속도 대신 정밀성을 확보할 수 있고 1 Stage Detector는 조밀하고 작은 물체를 감지할 때 성능이 저하되는 대신 빠른 속도를 예측할 수 있습니다.\n\nObject detection 시, ROI Search와 ROI 내 Image Classification을 별도로 수행하면 Two-stage detector, 한번에 수행한다면 One-stage detector\n\n속도 : Two Stage detector &lt; One-stage detector(better)\n정확도 : Two Stage detector(better) &lt; One-stage detector\n\n\nTraditional Methods\n\nRaw Image Segmentation → 유사 영역 Merge → 후보 영역(Candidate box) 추출\n\n\nTwo-stage detector\n\n\nR-CNN\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1311.2524.pdf\n\n\n\n\n\nFast RCNN\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\n\n\n\n\n\nFaster RCNN\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1506.01497.pdf\n\n\n\n\n\n\nOne-stage detector\n\n\nYOLO\n\n\n                  \n                  Info\n                  \n                \n\n\nwww.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\n\n\n\n\n\nSSD\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1512.02325.pdf\n\n\n\n\n\nRetinaNet\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/1708.02002.pdf\n\n\n\n\n\n\netc. Detection with Transformer\n\n\nDETR\n\n\n                  \n                  Info\n                  \n                \n\n\narxiv.org/pdf/2005.12872.pdf\n\n\n\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/VGGNet/VGGNet":{"slug":"vault/Notion/DB/DB-Blog-Post/VGGNet/VGGNet","filePath":"vault/Notion/DB/DB Blog Post/VGGNet/VGGNet.md","title":"VGGNet","links":[],"tags":[],"content":"참조\narxiv.org/pdf/1409.1556.pdf\ninhovation97.tistory.com/44\nmedium.com/@msmapark2/vgg16-논문-리뷰-very-deep-convolutional-networks-for-large-scale-image-recognition-6f748235242a\nVGGNet이란?\n\nVGGNet은 딥러닝 모델에서 사용되는 이미지 인식 알고리즘으로, 2014년 옥스퍼드 대학교의 Visual Geometry Group(VGG)에서 개발한 합성곱 신경망입니다. VGGNet은 CNN Layer 및 Pooling Layer와 FC Layer로 이루어져 있습니다.\nCNN Layer는 이미지의 특징을 추출하는 데 사용되며 FC Layer는 Classification에 사용됩니다.\nVGGNet의 특정\nVGGNet은 이전까지 발전해왔던 모델들과는 다른 부분(depth)에 초점을 맞춘 모델입니다.\n\n\n\n매우 작은 커널 사이즈(3x3)을 사용\n\nVGG 모델 이전에 Convolutional Network를 활용하여 이미지 분류에서 좋은 성과를 보였던 모델들은 비교적 큰 Receptive Field를 갖는 11x11필터나 7x7 필터를 사용했습니다.\n그러나 VGG 모델은 오직 3x3 크기의 작은 필터만 사용했음에도 이미지 분류 정확도를 비약적으로 개선시켰습니다.\n여기서 얻을 수 있는 통찰은 Stride가 1일 때, 3차례의 3x3 Conv 필터링을 반복한 특징맵은 한 픽셀이 원본 이미지의 7x7 Receptive field의 효과를 볼 수 있다 입니다.\n\n7x7 필터를 이용해 이미지에 대해 한 번 Convolution을 수행한 것과 3x3 필터로 세 번 Convolution을 수행한 것의 차이\n\n비선형성 증가 : 3x3을 여러번 하는 것이 Activation Function를 여러번 사용됨\n파라미터 수 감소 : 7x7x1 = 49 vs 3x3x3 = 27\n\n\n\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/ViT/ViT":{"slug":"vault/Notion/DB/DB-Blog-Post/ViT/ViT","filePath":"vault/Notion/DB/DB Blog Post/ViT/ViT.md","title":"ViT","links":[],"tags":[],"content":"\n참조\nViT란 무엇인가?\nViT의 특징\nViT Inference 과정\nClass Token이 있는 이유는?\nLayer Normalization\nViT의 결과\nPytorch를 이용한 ViT 구현\n\n참조\narxiv.org/abs/2010.11929\nvisionhong.tistory.com/25\nkmhana.tistory.com/27\ndeep-learning-study.tistory.com/716\njalammar.github.io/illustrated-transformer/\ndaebaq27.tistory.com/108\ngaussian37.github.io/dl-concept-vit/\ndatascience.stackexchange.com/questions/90649/class-token-in-vit-and-bert\nlearnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/\nViT란 무엇인가?\nTransformer를 image patch의 sequence에 적용하여 classification을 수행하는 모델입니다.\nViT는 일반적인 CNN과 다르게 공간에 대한, “Inductive bias”이 없습니다. 따라서 더 많은 데이터를 통해, 원초적인 관계를 Robust 하게 학습시켜야 하기 때문에 매우 많은 데이터가 필요합니다.\n\n\nVision Transformer는 Transformer의 전체 아키텍쳐를 크게 변경하지 않은 상태에서 이미지 처리를 위한 용도로 사용되는데 의의가 있습니다.\n기존의 이미지 분야에서 attention 기법을 사용할 경우 대부분 CNN과 함께 사용되거나 전체 CNN 구조를 유지하면서 CNN의 특정 구성 요소를 대체하는 데 사용되어 왔습니다.\n또는 attention 만을 이용한 모델도 있었지만 기존의 CNN을 기반으로 하는 모델의 성능을 넘지는 못하였습니다.\n하지만 Vision Transformer에서는 CNN에 의존하지 않고 이미지 패치의 시퀀스를 입력값으로 사용하는 transformer를 적용하여 CNN 기반의 모델의 성능을 넘는 성능을 보여주었습니다. 이미지를 이미지 패치의 시퀀스로 입력하는 방법은 뒤에서 다루겠습니다.\n\nViT의 특징\n\n매우 많은 데이터가 필요합니다. ImageNet와 같은 Mid-sized 데이터셋으로 학습 시, ResNet보다 낮은 성능을 보입니다.\nViT는 이미지를 patch로 분할하고, 이 patch들을 linear embedding에 전달합니다. 그리고 이것을 transformer의 입력값으로 사용합니다. 즉, 하나 하나의 patch를 NLP의 token으로 간주합니다.\nEmbedding은 NLP의 transformer와 동일하게 learnable positional embedding과 element-wise sum으로 결합된 embedding을 사용하며 0번째 patch + position embedding에는 class를 부여합니다. 그리고 이것을 patch embedding이라고 부릅니다.\nTransformer encoder 출력값에 하나의 hidden layer를 가진 MLP를 사용하여 pre-train 합니다. pine-tunning 시에는 랜덤 초기화된 하나의 linear layer를 사용합니다.\n\nViT Inference 과정\n\n\n\n==Before Input==\n\n\n(C,H,W) 크기의 이미지를 크기가 (P,P)인 패치 N개로 자른 후, N개의 1-D 벡터 (P^2⋅c차원)로 flatten\nN=HW/P^2로 계산되며, P는 하이퍼 파라미터.\n\n예시는 참고용이며, 실제 실험에서는 모델 크기에 따라 P=14,16,32 등 다양\n\n\n이후, Linear projection을 수행하여 크기가 D인 벡터의 시퀀스로 차원 변경\n\n이때, D는 모든 레이어 전체에 고정된 값.\n\n\n\n\n\n==Linear Projection==\n\nxp 를 Embedding 하기 위하여 행렬 E 와 곱 연산 수행\n\nE 의 shape : (P^2 x C, D)\nD : Embedding dimension, P^2 x C 크기의 벡터를 D 로 변경하겠다는 의미\n\n\nx_p E \\in \\R^{(N \\times D)}\n\nxp shape : ( N,P^2 x C )\nE shape : ( P^2 x C,D )\n\n\n\n\n\n==Embedding (P=16, H=W=224, N=196, D=768)==\n\n\nClass token 추가\n\n가장 첫번째 패치 임베딩 앞에 학습 가능한 임베딩 벡터를 추가\n추후 이미지 전체에 대한 표현을 나타내게 됨\n\n\nPosotional Embeding\n\nN+1개의 학습 가능한 1D 포지션 임베딩 벡터 (이미지 패치의 위치를 특정하기 위함)를 생성\n기존 임베딩 벡터와 합 연산\n\n\n만들어진 임베딩 패치를 Transformer Encoder에 입력\n\n\n\n==Transformer Encoder==\n\n\nTransformer의 Encoder는 L 번 반복하기 위해 입력과 출력의 크기가 같도록 유지\n입력값 z0 에서 시작하여 L 번 반복하여 얻은 zL 이 최종 Encoder의 출력\nEncoder 출력에서 Class Token 부분만 Classification에 사용, 마지막에 추가적으로 MLP Head를 붙여 분류 수행\n\n\n\nClass Token이 있는 이유는?\nClass Token은 BERT에서 제시된 아이디어이며 학습 가능한 임베딩이 있는 입력으로, 입력 패치 임베딩이 앞에 추가되고 MSA를 사용하여 모든 패치에서 정보를 수집합니다. \n트랜스포머는 기본적으로 시퀀스-시퀀스 네트워크입니다.\nViT에 디코더 레이어가 없기 때문에 입력 시퀀스(패치 수)의 길이는 출력 시퀀스의 길이와 같습니다.\n따라서 목표가 분류인 경우 두 가지 선택지가 있습니다.\n\nmodel 위에 완전 연결된 계층을 적용\n\n입력 이미지 해상도를 고정해야 하기 때문에 좋은 생각이 아닙니다\n\n\n출력 시퀀스의 하나의 항목에 분류 계층을 적용\n\n해당 패치에 대하여 선호도가 생겨버립니다\n\n\n\n따라서 최선의 해결책은 더미 입력인 클래스 토큰을 추가하고 해당 출력 항목에 분류 계층을 적용하는 것입니다\nLayer Normalization\n전설적인 Geoffrey Hinton 교수의 연구실에서 처음 제안한 레이어 정규화는 배치 정규화의 약간 다른 버전입니다. 우리는 모두 컴퓨터 비전에서 배치 정규화에 익숙합니다. 그러나 배치 정규화는 반복 아키텍처에 직접 적용할 수 없습니다. \n또한 배치 정규화의 평균(μ) 및 표준 편차(σ) 통계량이 미니 배치에 대해 계산되기 때문에 결과는 배치 크기에 따라 달라집니다. 미니 배치의 각 샘플은 서로 다른 μ, σ를 갖지만 평균 및 표준 편차는 레이어의 모든 뉴런에 대해 동일합니다.\n일반적인 모델 크기의 경우 레이어 정규화가 배치 표준보다 느리다는 점에 유의해야 합니다. \n따라서 DEST(속도를 위해 설계되었지만 여기서는 소개하지 않음)와 같은 일부 아키텍처는 엔지니어링 트릭을 사용하여 훈련을 안정적으로 유지하면서 배치 정규화를 사용합니다. \n그러나 가장 널리 사용되는 ViT의 경우 레이어 정규화가 사용되며 성능에 매우 중요합니다.\nViT의 결과\n\n\n가장 왼쪽의 RGB embedding filter 그림의 시사점은 ViT 또한 CNN과 같은 형태로 학습이 되었다는 점\n\nRGB embedding filter : Transformer Encoder에 입력되기 전 Embedding을 할 때 사용한 filter\nfilter의 일부분을 가져와서 시각화 하였을 때, 위 그림과 같은 형태가 나타남\n핵심은 CNN에서의 low level layer에서와 유사한 형태의 결과가 시각화로 나타난다는 점\n즉, CNN 처럼 학습이 잘 되었다는 것을 의미\n\n\n중간의 Positional Embedding Similarity 그림의 시사점은 Positional Embedding이 데이터의 위치를 잘 의미하도록 학습이 잘 되었다는 점\n\nPositional Embedding 에는 각 패치 마다 대응되는 Embedding 벡터가 존재\n모든 패치 p(i,pj) 에 대하여 cosine similarity를 구하였을 때 각 row, col에 해당하는 부분의 패치가 similarity가 높은 것을 통해 Position이 의미가 있도록 학습이 잘 되었다는 것을 확인\n\n\n가장 오른쪽의 그래프는 attention이 관심을 두는 위치의 편차를 나타냄\n\nlow level layer : 가까운 곳에서부터 먼 곳까지 모두 살펴 보고 있음\nhigh level layer : 전체적으로 보고 있음\n\ny축의 distance의 의미는 어떤 query 위치를 기준으로 의미있는 영역까지의 평균 거리\n이 거리가 짧을수록 가까운 영역에 대하여 attention\n이 거리가 길수록 먼 영역에 대하여 attention\n\n\n\n\nCNN 또한 convolution 연산의 특성상 layer가 깊어질수록 점점 더 큰 영역을 보게되는데 Vision Transformer 또한 그러한 성질을 가지는 것을 확인할 수 있었습니다.\n\nPytorch를 이용한 ViT 구현\nimport torch\nimport torch.nn as nn\n \nclass LinearProjection(nn.Module):\n \n    def __init__(self, patch_vec_size, num_patches, latent_vec_dim, drop_rate):\n        super().__init__()\n        self.linear_proj = nn.Linear(patch_vec_size, latent_vec_dim)\n        self.cls_token = nn.Parameter(torch.randn(1, latent_vec_dim))\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, latent_vec_dim))\n        self.dropout = nn.Dropout(drop_rate)\n \n    def forward(self, x):\n        batch_size = x.size(0)\n        x = torch.cat([self.cls_token.repeat(batch_size, 1, 1), self.linear_proj(x)], dim=1)\n        x += self.pos_embedding\n        x = self.dropout(x)\n        return x\n \nclass MultiheadedSelfAttention(nn.Module):\n    def __init__(self, latent_vec_dim, num_heads, drop_rate):\n        super().__init__()\n        device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n        self.num_heads = num_heads\n        self.latent_vec_dim = latent_vec_dim\n        self.head_dim = int(latent_vec_dim / num_heads)\n        self.query = nn.Linear(latent_vec_dim, latent_vec_dim)\n        self.key = nn.Linear(latent_vec_dim, latent_vec_dim)\n        self.value = nn.Linear(latent_vec_dim, latent_vec_dim)\n        self.scale = torch.sqrt(latent_vec_dim*torch.ones(1)).to(device)\n        self.dropout = nn.Dropout(drop_rate)\n \n    def forward(self, x):\n        batch_size = x.size(0)\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        q = q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n        k = k.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,3,1) # k.t\n        v = v.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n        attention = torch.softmax(q @ k / self.scale, dim=-1)\n        x = self.dropout(attention) @ v\n        x = x.permute(0,2,1,3).reshape(batch_size, -1, self.latent_vec_dim)\n \n        return x, attention\n \nclass TFencoderLayer(nn.Module):\n    def __init__(self, latent_vec_dim, num_heads, mlp_hidden_dim, drop_rate):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(latent_vec_dim)\n        self.ln2 = nn.LayerNorm(latent_vec_dim)\n        self.msa = MultiheadedSelfAttention(latent_vec_dim=latent_vec_dim, num_heads=num_heads, drop_rate=drop_rate)\n        self.dropout = nn.Dropout(drop_rate)\n        self.mlp = nn.Sequential(nn.Linear(latent_vec_dim, mlp_hidden_dim),\n                                 nn.GELU(), nn.Dropout(drop_rate),\n                                 nn.Linear(mlp_hidden_dim, latent_vec_dim),\n                                 nn.Dropout(drop_rate))\n \n    def forward(self, x):\n        z = self.ln1(x)\n        z, att = self.msa(z)\n        z = self.dropout(z)\n        x = x + z\n        z = self.ln2(x)\n        z = self.mlp(z)\n        x = x + z\n \n        return x, att\n \nclass VisionTransformer(nn.Module):\n    def __init__(self, patch_vec_size, num_patches, latent_vec_dim, num_heads, mlp_hidden_dim, drop_rate, num_layers, num_classes):\n        super().__init__()\n        self.patchembedding = LinearProjection(patch_vec_size=patch_vec_size, num_patches=num_patches,\n                                               latent_vec_dim=latent_vec_dim, drop_rate=drop_rate)\n        self.transformer = nn.ModuleList([TFencoderLayer(latent_vec_dim=latent_vec_dim, num_heads=num_heads,\n                                                         mlp_hidden_dim=mlp_hidden_dim, drop_rate=drop_rate)\n                                          for _ in range(num_layers)])\n \n        self.mlp_head = nn.Sequential(nn.LayerNorm(latent_vec_dim), nn.Linear(latent_vec_dim, num_classes))\n \n    def forward(self, x):\n        att_list = []\n        x = self.patchembedding(x)\n        for layer in self.transformer:\n            x, att = layer(x)\n            att_list.append(att)\n        x = self.mlp_head(x[:,0])\n \n        return x, att_list"},"vault/Notion/DB/DB-Blog-Post/Video2Video":{"slug":"vault/Notion/DB/DB-Blog-Post/Video2Video","filePath":"vault/Notion/DB/DB Blog Post/Video2Video.md","title":"Video2Video","links":[],"tags":[],"content":"github.com/404Vector/Video2Video\n개요\nVideo2Video는 FFMPeg와 FFMPeg-Python을 사용하여 비디오를 비디오로 변환하는 라이브러리입니다. 이 라이브러리는 비디오 프레임에 대한 추가 처리를 통해 비디오 편집의 맞춤화를 가능하게 합니다.\n주요 기능\n\n비디오에서 이미지 추출: 비디오를 개별 프레임으로 분리\n이미지에서 비디오 생성: 단일 이미지 또는 이미지 시퀀스로부터 비디오 생성\n오디오 추출: 비디오에서 오디오 트랙 분리\n오디오 비디오 병합: 오디오 트랙을 비디오에 다시 삽입\n\n설치\npip install video2video"},"vault/Notion/DB/DB-Blog-Post/Window-10-OpenSSH-서버-활성화-및-실행하기":{"slug":"vault/Notion/DB/DB-Blog-Post/Window-10-OpenSSH-서버-활성화-및-실행하기","filePath":"vault/Notion/DB/DB Blog Post/Window 10 OpenSSH 서버 활성화 및 실행하기.md","title":"Window 10 OpenSSH 서버 활성화 및 실행하기","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-6/Week-6"],"tags":[],"content":"\n참조\nWindow 10 OpenSSH 서버 활성화 및 실행하기\n\n\n참조\nwww.lainyzine.com/ko/article/how-to-run-openssh-server-and-connect-with-ssh-on-windows-10/\nWeek 6\nWindow 10 OpenSSH 서버 활성화 및 실행하기\nOpenSSH Server 활성화\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\n\n\n출력\nPath :\nOnline : True\nRestartNeeded : False\n\n\nOpenSSH Server 실행\nStart-Service sshd\n부팅 시점에 자동으로 서비스가 실행되도록 설정\nSet-Service -Name sshd -StartupType &#039;Automatic&#039;\n동작 확인\nGet-NetFirewallRule -Name OpenSSH-Server-In-TCP\n\n\n출력\nName : OpenSSH-Server-In-TCP\nDisplayName : OpenSSH SSH Server (sshd)\nDescription : Inbound rule for OpenSSH SSH Server (sshd)\nDisplayGroup : OpenSSH Server\nGroup : OpenSSH Server\n==Enabled : True ← True일 경우 정상==\nProfile : Any\nPlatform : {}\nDirection : Inbound\nAction : Allow\nEdgeTraversalPolicy : Block\nLooseSourceMapping : False\nLocalOnlyMapping : False\nOwner :\nPrimaryStatus : OK\nStatus : 저장소에서 규칙을 구문 분석했습니다. (65536)\nEnforcementStatus : NotApplicable\nPolicyStoreSource : PersistentStore\nPolicyStoreSourceType : Local\n\n\n"},"vault/Notion/DB/DB-Blog-Post/Yolo-v1(You-Only-Look-Once)/Yolo-v1(You-Only-Look-Once)":{"slug":"vault/Notion/DB/DB-Blog-Post/Yolo-v1(You-Only-Look-Once)/Yolo-v1(You-Only-Look-Once)","filePath":"vault/Notion/DB/DB Blog Post/Yolo v1(You Only Look Once)/Yolo v1(You Only Look Once).md","title":"Yolo v1(You Only Look Once)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-10/Week-10"],"tags":[],"content":"\n참조\nYolo v1(You Only Look Once)\nYolo v1 Inference\nYolo v1 장점 vs 단점\nYolo History(v1~v5)\nYolo Loss Function\n\n\n참조\n\n\n                  \n                  You Only Look Once: Unified, Real-Time Object Detection \n                  \n                \n\n\nWe present YOLO, a new approach to object detection.\narxiv.org/abs/1506.02640\n\n\n\nWeek 10\nvelog.io/@skhim520/YOLO-v1-논문-리뷰-및-코드-구현\nYolo v1(You Only Look Once)\n\nYolo v1은 GoogLeNet의 변형 구조로, Region Proposal 단계가 없는 최초의 1 Stage Detector입니다. Yolo는 이미지의 Bbox와 Classification을 동시에 예측하는 방식을 사용합니다.\n\nYolo v1 Inference\nYolo v1은 아래 순서로 Inference 됩니다.\n\n입력 이미지를 SxS 그리드 영역으로 나누기\n\n논문 : 7x7 사용\n\n총 grid 수 : 49\n\n\n\n\n각 그리드 영역마다 B개의 Bounding box와 Confidence score 계산\n\n논문 : B=2 사용\n\n5개 : 1번 Bonding Box 정보(center x, center y, width, height, confidence score)\n5개 : 2번 Bonding Box 정보(center x, center y, width, height, confidence score)\n\n\nConfidence = Pr(Object) x IOU(truth;pred)\n\n\n각 그리드 영역마다 C개의 Class에 대한 확률 계산\n\n논문 : C=20\nConditional class probability = Pr(Class_i | Object)\n\n\nScore Thresholding\n\n너무 작은 Score는 0으로 Drop\n\n\nSort Descending\n\n내림차순으로 정렬\n\n\nNMS\n\n네트워크에서 생성되는 총 Bounding Box의 수는 SxSxB = 7x7x2 = 98개 입니다. 그리고 각 Bounding Box는 5개의 차원을 갖습니다. BBox의 차원이 C+5가 아니라 5인 이유는 Yolo에서는 BBox 별로 Class를 예측하지 않고 Grid의 Class를 예측하기 때문입니다.\n따라서 Inference 되는 결과 Vector의 차원은 아래와 같습니다.\nModel_{yolo1}(x) = y_{pred}\\in \\R^{S \\times S \\times (5B+C) } \\\\ {S \\times S \\times (5B+C) } = 7 \\times 7 \\times (10+20)\nYolo v1 장점 vs 단점\n장점\n\nFaster R-CNN에 비해 6배 빠른 속도\n다른 real-time detector에 비해 2배 높은 정확도\n물체의 일반화된 표현을 학습(학습하지 않은 다른 도메인의 이미지에서도 좋은 성능을 보임)\n\n단점\n\nGrid 보다 작은 크기의 물체 검출 불가능\n마지막 feature만을 사용\n\nYolo History(v1~v5)\nYolo v1은 Region Proposal Network(RPN)을 사용하지 않고, 이미지의 전체를 한 번에 처리하기 때문에 다른 모델에 비해 속도가 빠르고, 정확도도 높은 편입니다. 이후 Yolo v2, v3, v4, v5 등의 버전이 출시되면서 기능과 성능이 계속해서 개선되고 있습니다.\nYolo v2부터는 batch normalization과 다양한 기법들이 도입되어 정확도와 신뢰성이 향상되었습니다.\nYolo v3부터는 multi-scale feature maps를 사용하여 더 다양한 크기의 물체들을 인식할 수 있게 되었습니다.\nYolo v4에서는 최신 딥러닝 기술인 BagOf Freebies(BOF)와 Bag of Specials(BOS) 를 사용하여 정확도와 속도가 한층 높아졌습니다.\nYolo v5에서는 크기별로 모델 구성이 가능해져서 다양한 하드웨어에서 유연하게 사용할 수 있게 되었습니다. Yolo는 높은 속도와 정확도를 보장하면서도 단일 네트워크에서 객체 검출 및 분류를 수행할 수 있어서, 실시간 영상 처리나 자율주행 자동차 등에 널리 사용되고 있습니다.\n\n\nv1 : 이미지의 Bbox와 Classification을 동시에 예측하는 1 Stage Detector 등장\n\n\nv2 : 빠르고 강력하고 더 좋게 향상\n\n\nv3 : multi-scale feature maps 사용\n\n\nv4 : 최신 딥러닝 기술(BagOf Freebies=BOF, Bag of Specials=BOS) 사용\n\n\nv5 : 크기별로 모델 구성(Small, Medium, Lage, Xlarge)\n\n\nYolo Loss Function\n"},"vault/Notion/DB/DB-Blog-Post/mIoU(Mean-Intersection-over-Union)":{"slug":"vault/Notion/DB/DB-Blog-Post/mIoU(Mean-Intersection-over-Union)","filePath":"vault/Notion/DB/DB Blog Post/mIoU(Mean Intersection over Union).md","title":"mIoU(Mean Intersection over Union)","links":[],"tags":[],"content":"mIoU는 다중 오브젝트 클래스의 교차검출비(Intersection over Union, IoU)의 평균입니다. 교차검출비(IoU)는 감지된 오브젝트와 실제 오브젝트 사이의 겹치는 면적을 두 오브젝트의 총 면적의 비율로 계산됩니다.\n\nIoU\n\n\n\nExample of IoU (when Class is [Red, Green, Blue, Yellow])\n\n\n\nExample of mIoU (when Class is [Red, Green, Blue, Yellow])\n\n"},"vault/Notion/DB/DB-Blog-Post/numpy.ndarray.view와-numpy.ndarray.reshape의-차이":{"slug":"vault/Notion/DB/DB-Blog-Post/numpy.ndarray.view와-numpy.ndarray.reshape의-차이","filePath":"vault/Notion/DB/DB Blog Post/numpy.ndarray.view와 numpy.ndarray.reshape의 차이.md","title":"numpy.ndarray.view와 numpy.ndarray.reshape의 차이","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\nnumpy.ndarray.view와 numpy.ndarray.reshape의 차이\n\n\n참조\nstackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\nWeek 2\nnumpy.ndarray.view와 numpy.ndarray.reshape의 차이\n\n\nview는 항상 같은 메모리임을 보장, reshape의 경우 항상 같은 메모리임을 보장하지 않음\n\n\nview는 contiguity를 보장하지만 reshape는 보장하지 않음\n\n\n\nReturns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\n\n\n"},"vault/Notion/DB/DB-Blog-Post/optimizer.zero_grad()은-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/optimizer.zero_grad()은-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/optimizer.zero_grad()은 무엇일까.md","title":"optimizer.zero_grad()은 무엇일까","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\noptimizer.zero_grad()\n\n\n참조\npytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html zero_grad#torch.optim.Optimizer.zero_grad\nWeek 2\noptimizer.zero_grad()\n\n호출 시 parameter(학습해야 할 tensor)의 grad 변수를 초기화\n\nmodel.backward()를 사용할 시, 내부 Parameter Tensor의 grad에는 편미분한 결과 값이 저장됨\n그러나 역전파(optimizer.step()) 가 끝난 후에도 유지되므로, 다음 훈련 전에 초기화 필요\n\n\n"},"vault/Notion/DB/DB-Blog-Post/pipenv-설치-및-사용":{"slug":"vault/Notion/DB/DB-Blog-Post/pipenv-설치-및-사용","filePath":"vault/Notion/DB/DB Blog Post/pipenv 설치 및 사용.md","title":"pipenv 설치 및 사용","links":["vault/Notion/DB/DB-Blog-Post/pyenv-설치-및-사용-방법"],"tags":[],"content":"\n개요\n설치\n사용\nPipfile &amp; pipfile.lock\n\n\n개요\nPipenv는 Python의 패키지 관리자이며 단일 디렉토리 내의 가상 환경을 구축하고 관리하는 데 도움이 됩니다. Pipenv의 사용법을 다음과 같이 설명합니다.\n설치\n\n\nPipenv를 사용하기 위해서는 다음 명령어를 사용해 설치해야 합니다.\npipenv --python version\n \n# example\npipenv --python 3.8.5\n\n\n생성시 사용하는 version의 경우 현재 pc에 설치되어 있는 것만 사용할 수 있습니다.\n\n\npyenv로 여러 python version을 설치했다면, 보다 자유롭게 version을 선택할 수 있습니다.\npyenv 설치 및 사용 방법\n\n\n사용\n\n\n가상 환경 활성화\npipenv shell\n\n\n가상 환경 비활성화\nexit\n\n\n가상환경 내 패키지 설치\npipenv install [패키지명]\n \n# example\npipenv install numpy\n\n\nPipfile &amp; pipfile.lock\n\n\n해당 가상환경에 설치된 파이썬 버전, 패키지별 이름과 버전을 기록\n\n\n패키지를 새롭게 설치하면 Pipfile.lock 파일 생성\n\n\npackages와 dev-packages 간의 차이\n\npackages (i.e., dependencies)\n\n실제 프로그램이 동작할 때 반드시 필요한 패키지 리스트\n프로그램을 실제 배포할 때 반드시 포함해야 하는 패키지 리스트\n\n\ndev-packages (i.e., devDependencies)\n\n개발이나 테스트 시 필요한 패키지 리스트\n프로그램 배포 시 해당 패키지는 포함시키지 않음\n\n\n프로그램 빌드 시간을 줄이기 위해 꼭 필요한 패키지만 포함시키기 위해 패키지 용도를 구분해 놓은 것\n\n\n\npipfile.lock\n\n패키지 등의 정보를 자동으로 Hash를 생성\n\n보안 이유\n\n\nPipfile과 다르게 텍스트가 암호화\n\n\n"},"vault/Notion/DB/DB-Blog-Post/pyenv-설치-및-사용-방법":{"slug":"vault/Notion/DB/DB-Blog-Post/pyenv-설치-및-사용-방법","filePath":"vault/Notion/DB/DB Blog Post/pyenv 설치 및 사용 방법.md","title":"pyenv 설치 및 사용 방법","links":[],"tags":[],"content":"\n개요\n설치\n원하는 Python 버전 설치하기\n핵심 명령어 모음\n\n\n개요\n개발자들이 실행하는 스크립트 등의 환경을 정교하게 관리할 수 있는 Python 개발 환경 매니저 “Pyenv”를 소개합니다.\n설치\n\n\n필요파일 설치\nsudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev\n\n\npyenv git 가져오기\ngit clone github.com/pyenv/pyenv.git ~/.pyenv\n\n\n환경설정\necho &#039;export PYENV_ROOT=&quot;$HOME/.pyenv&quot;&#039; &gt;&gt; ~/.bashrc\necho &#039;export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;&#039; &gt;&gt; ~/.bashrc\necho &#039;eval &quot;$(pyenv init --path)&quot;&#039; &gt;&gt; ~/.bashrc\necho &#039;eval &quot;$(pyenv init -)&quot;&#039; &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n\n원하는 Python 버전 설치하기\n\n\n설치가능한 버전 확인\npyenv install --list\n\n\n파이썬 설치\npyenv install 3.x.x\n\n\n핵심 명령어 모음\n\n\n현재 사용중인 인터프리터 위치\nwhich python\n\n\n현재 설치된 모든 버전 확인\npyenv versions\n\n\n전역 python 버전 설정 or 기본 python버전으로 변경\npyenv global 3.x.x\n\n\n전역 python 버전 설정\n== 기본 python버전으로 변경\npyenv global 3.x.x\n\n\n로컬 python 버전 설정\n== 현재 폴더에만 python 버전을 변경\npyenv local 3.x.x\n\n\n가상 환경 생성\npyenv virtualenv 3.x.x 가상_환경_이름\n\n\n가상 환경 활성화\npyenv activate 가상_환경_이름\n\n\n가상 환경 비활성화\npyenv deactivate\n\n\n가상 환경을 이용하여 전역 python 버전 설정\n== 기본 python버전으로 변경\npyenv global 가상_환경_이름\n\n\n가상 환경을 이용하여 로컬 python 버전 설정\n== 현재 폴더에만 python 버전을 변경\npyenv local 가상_환경_이름\n\n"},"vault/Notion/DB/DB-Blog-Post/torch.mm-vs-torch.matmul-차이점/torch.mm-vs-torch.matmul-차이점":{"slug":"vault/Notion/DB/DB-Blog-Post/torch.mm-vs-torch.matmul-차이점/torch.mm-vs-torch.matmul-차이점","filePath":"vault/Notion/DB/DB Blog Post/torch.mm vs torch.matmul 차이점/torch.mm vs torch.matmul 차이점.md","title":"torch.mm vs torch.matmul 차이점","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\nTorch.mm vs Torch.matmul 차이점\n\n\n참조\nneos518.tistory.com/178\nWeek 2\nTorch.mm vs Torch.matmul 차이점\n\ntorch.matmul\n\ntorch.matmul은 broadcast를 지원합니다.\n\n\ntorch.mm\n\ntorch.mm은 broadcast를 지원하지 않습니다.\n따라서 mm은 정확하게 matrix 곱의 사이즈가 맞아야 사용이 가능합니다.\n\n\nBroadcast\n\n\nbroadcast 기능은 아래의 예제와 같이 T1(10, 3, 4) T2(4)을 곱할 때, 맨 앞의 dim이 3개 일 때는 첫 dim을 batch로 간주하고 T1 (3, 4) tensor의 10개의 batch와 각각 T2(4)랑 곱을 해주는 것입니다.\n\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/torch.nn.Module은-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/torch.nn.Module은-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/torch.nn.Module은 무엇일까.md","title":"torch.nn.Module은 무엇일까","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\ntorch.nn.Module\n\n\n참조\nWeek 2\ntorch.nn.Module\n\nai 모델을 구성하는 layer의 base class\ninput, output, forward, backward를 정의해야 함\n학습 대상이 되는 parameter(tensor)를 정의해야 함\n"},"vault/Notion/DB/DB-Blog-Post/torch.nn.Parameter은-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/torch.nn.Parameter은-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/torch.nn.Parameter은 무엇일까.md","title":"torch.nn.Parameter은 무엇일까","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\ntorch.nn.Parameter\n\n\n참조\nWeek 2\ntorch.nn.Parameter\n\nTensor를 상속받는 Class\nnn.Moudule 내에 Attribute가 될 때, required_grad=True로 지정되어 자동으로 학습대상이 됨\n"},"vault/Notion/DB/DB-Blog-Post/torch.no_gard()의-역할":{"slug":"vault/Notion/DB/DB-Blog-Post/torch.no_gard()의-역할","filePath":"vault/Notion/DB/DB Blog Post/torch.no_gard()의 역할.md","title":"torch.no_gard()의 역할","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\ntorch.no_gard()\n\n\n참조\npytorch.org/docs/stable/generated/torch.no_grad.html\nWeek 2\ntorch.no_gard()\n\nbackward를 사용하지 않을 때(inference 시점) 메모리 소비를 줄임\n\n모델 내 tensor 중 requred_grad = True인 경우에도 required_grad=False처리하여 계산\n\n\n"},"vault/Notion/DB/DB-Blog-Post/torch.tensor의-requires_grad-param의-기능":{"slug":"vault/Notion/DB/DB-Blog-Post/torch.tensor의-requires_grad-param의-기능","filePath":"vault/Notion/DB/DB Blog Post/torch.tensor의 requires_grad param의 기능.md","title":"torch.tensor의 requires_grad param의 기능","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\ntorch.tensor의 requires_grad param의 기능\n\n\n참조\nWeek 2\ntorch.tensor의 requires_grad param의 기능\n\nrequires_grad = True일 경우, 해당 tensor는 model이 backward 될 때 자동미분(auto grad)을 수행합니다.\n"},"vault/Notion/DB/DB-Blog-Post/venv-설치-및-사용-방법":{"slug":"vault/Notion/DB/DB-Blog-Post/venv-설치-및-사용-방법","filePath":"vault/Notion/DB/DB Blog Post/venv 설치 및 사용 방법.md","title":"venv 설치 및 사용 방법","links":[],"tags":[],"content":"\n개요\n설치\n사용\n\n\n개요\n여기서는 Python Virtual Environment을 사용하는 방법을 설명합니다. Virtual Environment은 프로젝트마다 독립적인 개발 환경을 제공합니다.\n설치\nvenv는 python에 내장되어 있는 기능으로, 별도의 설치가 필요하지 않습니다.\n사용\n\n\n프로젝트 디렉토리에서 python -m venv [가상환경이름]을 입력하여 가상환경을 생성합니다.\n\n\n가상 환경은 현재 directory에 입력한 가상환경이름으로 생성됩니다.\n\n\n관행적으로 아래와 같은 이름을 사용합니다\npython -m venv .venv\n\n\n\n\n생성된 가상환경을 활성화합니다. 각 OS마다 다른 명령이 필요합니다:\n\n\nWindows\n[가상환경이름]\\\\Scripts\\\\activate.bat\n\n\nmacOS/Linux\nsource [가상환경이름]/bin/activate\n\n\n\n\n(Optional)저장소가 git으로 제어되는 경우, 아래 명령어로 gitignore 목록에 추가합니다\necho [가상환경이름] &gt;&gt; .gitignore\n\n\n프로젝트가 끝나면 가상환경을 비활성화하고 나갑니다.\ndeactivate\n\n"},"vault/Notion/DB/DB-Blog-Post/가능도-함수(Likelihood-function,-likelihood,-우도-함수,-우도)":{"slug":"vault/Notion/DB/DB-Blog-Post/가능도-함수(Likelihood-function,-likelihood,-우도-함수,-우도)","filePath":"vault/Notion/DB/DB Blog Post/가능도 함수(Likelihood function, likelihood, 우도 함수, 우도).md","title":"가능도 함수(Likelihood function, likelihood, 우도 함수, 우도)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n가능도 함수(Likelihood function, likelihood, 우도 함수, 우도)\n\n\n참조\nWeek 1\nen.wikipedia.org/wiki/Likelihood_function\nWeek 1\n가능도 함수(Likelihood function, likelihood, 우도 함수, 우도)\n\n확률 분포의 모수가, 어던 확률변수의 표집값과 일관되는 정도를 나타내는 값\n선택한 parameter에 대한 statistical model에서 관측값들이 나올수 있는 확률, 즉 해당 statistical model과 관측 값들의 결합확률(Joint probability)을 의미\n그러므로 가능도는 parameter에 대한 함수임을 강조하기 위해 아래와 같이 표현\n\nL(\\theta | X) ( =P(X|\\theta) )\n\n\n"},"vault/Notion/DB/DB-Blog-Post/가상-메모리는-무엇일까/가상-메모리는-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/가상-메모리는-무엇일까/가상-메모리는-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/가상 메모리는 무엇일까/가상 메모리는 무엇일까.md","title":"가상 메모리는 무엇일까","links":[],"tags":[],"content":"참조\nko.wikipedia.org/wiki/가상_메모리\nrebro.kr/179\nbrownbears.tistory.com/47\n가상 메모리(virtual memory, virtual storage)란?\n\n메모리 관리 기법의 하나로, 기계에 실제로 이용 가능한 기억 자원을 이상적으로 추상화하여 사용자들에게 매우 큰 (주) 메모리로 보이게 만드는 것을 의미합니다.\n각 프로그램에 실제 메모리 주소가 아닌 가상의 메모리 주소를 주는 방식입니다.\n\n가상 주소(virtual address) or 논리 주소(logical address) : 가상적으로 주어진 주소\n물리 주소(physical address) or 실주소(real address) : 실제 메모리 상에서 유효한 주소\n메모리 관리 장치(MMU) : 가상 주소 공간에서 물리 주소로 변경해주는 장치\n\n등장 배경\n기존에는 프로세스가 실행되는 코드의 전체를 메모리에 로드해야 했고, 메모리 용량보다 더 큰 프로그램은 실행시킬 수 없었습니다.\n하지만 실제로는 코드의 일부에서만 대부분의 시간을 사용하고, 프로세스는 특정 순간에는 항상 작은 양의 주소 공간을 사용했기 때문에 이러한 방식은 매우 비효율적이었습니다.\n**가상 메모리(Virtual Memory)**는 이러한 물리적 메모리 크기의 한계를 극복하기 위해 나온 기술입니다. 프로세스를 실행할 때 실행에 필요한 일부만 메모리에 로드하고 나머지는 디스크에 두는 것입니다.\n이를 통해 프로세스 전체가 물리적 메모리에 있는 것 ‘처럼’ 수행되는, 즉 물리적 메모리가 훨씬 많이 있는 것처럼 보이게 됩니다.\n결과적으로 메모리에 작은 양의 주소 공간만 있으면 충분히 프로세스를 수행할 수 있고, 그에 따라 더 많은 프로그램을 동시에 실행할 수 있게 됩니다.\n\n이처럼 현재 필요한 page만 메모리에 올리는 것을 Demand Paging이라고 합니다.\n"},"vault/Notion/DB/DB-Blog-Post/가중치-초기화(Weight-Initialization)가-필요한-이유와-Xavier--and--He-초기화/가중치-초기화(Weight-Initialization)가-필요한-이유와-Xavier--and--He-초기화":{"slug":"vault/Notion/DB/DB-Blog-Post/가중치-초기화(Weight-Initialization)가-필요한-이유와-Xavier--and--He-초기화/가중치-초기화(Weight-Initialization)가-필요한-이유와-Xavier--and--He-초기화","filePath":"vault/Notion/DB/DB Blog Post/가중치 초기화(Weight Initialization)가 필요한 이유와 Xavier & He 초기화/가중치 초기화(Weight Initialization)가 필요한 이유와 Xavier & He 초기화.md","title":"가중치 초기화(Weight Initialization)가 필요한 이유와 Xavier & He 초기화","links":[],"tags":[],"content":"\n참조\n가중치 초기화 (Weight Initialization)\nXavier Initialization - 사비에르 초기화\nHe Initialization - 히 초기화\n\n\n참조\nyngie-c.github.io/deep learning/2020/03/17/parameter_init/\nsupermemi.tistory.com/entry/CNN-가중치-초기화-Weight-Initialization-PyTorch-Code\n가중치 초기화 (Weight Initialization)\n\n신경망의 목적 : 신경망 모델의 목적은 손실(Loss)을 최소화 = Parameter Optimization\n\n\nLoss를 최소화하기 위해서는 경사 하강법을 사용하여 Loss를 줄여나가야 합니다. 그러나 위 그림과 같이 시작점에 따라서 전혀 다른 방향으로 학습이 진행될 수도 있고 최저점을 찾기 위해 걸리는 속도가 다를 수도 있습니다.\n학습 시작 시, 적절하게 가중치를 초기화하는 것은 학습에 있어 중요합니다.\n\nZero Initialization\n\n모든 파라미터 값을 0으로 놓고 시작하면 되지 않을까?라고 생각할 수 있습니다.\n그러나 파라미터의 값이 모두 같다면 역전파(Back propagation)를 통해서 갱신하더라도 모두 같은 값으로 변하게됩니다.\n신경망 노드의 파라미터가 모두 동일하다면 여러 개의 노드로 신경망을 구성하는 의미가 사라집니다.\n\n\nRandom Initialization\n\n\n파라미터에 다른 값을 부여하기 위해서 가장 쉽게 생각해 볼 수 있는 방법은 확률분포를 사용하는 것입니다.\n\n\nStd1, Sigmoid를 사용한 경우\n\n\n0, 1에 가까운 값만 출력\n활성화 값이 0, 1에 가까울 때, Sigmoid 기준 기울기가 0에 가까워지므로 학습이 거의 일어나지 않게 되어 Gradient Vanishing 현상 발생\n\n\n\nStd 0.01, Sigmoid를 사용한 경우\n\n\n대부분의 출력 값이 0.5 주변에 위치하며, 따라서 Gradient Vanishing 현상을 방지할 수 있음\n출력 값이 비슷하면 노드를 여러개 구성하는 의미가 사라지게 됨\n\n\n\n\n\nXavier Initialization - 사비에르 초기화\n\n\n사비에르 글로로트(Xavier Glorot)가 제안\n\n\n고정된 표준편차를 사용하지 않음, 은닉층의 노드 수에 맞춰 표준편차를 선정\nn:이전\\ 은닉층\\ 노드의\\ 수 \\\\ m:현재\\ 은닉층\\ 노드의\\ 수 \\\\ \\sigma={2 \\over \\sqrt{n+m}}\n\n\n적용 후\n\n\n\nCode\n# TensorFlow\ntf.keras.initializers.GlorotNormal()\n \n# PyTorch\ntorch.nn.init.xavier_normal_()\n\n\nHe Initialization - 히 초기화\n\n\n카이밍 히(Kaiming He)가 제안\n\n\nReLU함수를 활성화 함수로 사용할 때 추천되는 초기화 방법\nn:이전\\ 은닉층\\ 노드의\\ 수 \\\\ \\sigma = \\sqrt{2 \\over n}\n\n\n적용 전\n\n\n\n적용 후\n\n\n\nCode\n# TensorFlow\ntf.keras.initializers.HeNormal()\n \n# PyTorch\ntorch.nn.init.kaiming_normal_()\n\n"},"vault/Notion/DB/DB-Blog-Post/교착상태":{"slug":"vault/Notion/DB/DB-Blog-Post/교착상태","filePath":"vault/Notion/DB/DB Blog Post/교착상태.md","title":"교착상태","links":[],"tags":[],"content":"\n참조\n교착상태란?\n발생 조건\n예방 방법\n\n\n참조\ncocoon1787.tistory.com/858\n교착상태란?\n\n두 개 이상의 프로세스가 자원을 점유한 상태에서 서로 다른 프로세스가 점유하고 있는 자원을 요구하며, 서로의 작업을 끝나기만을 기다리며 둘 다 영원히 끝나지 않는 상황\n\n발생 조건\n교착상태는 아래의 4가지 조건이 모두 만족되는 경우(필요충분조건)에 발생할 가능성이 있으며,\n하나라도 만족하지 않으면 교착상태가 발생하지 않습니다.\n\n상호 배제(Mutual Exclusion)\n\n한 번에 한 개의 프로세스만이 공유자원을 사용할 수 있음\n\n\n점유 대기(Hold and Wait)\n\n프로세스가 할당된 자원을 가진 상태에서 다른 자원을 기다림\n\n\n비선점(No Preemption)\n\n프로세스가 작업을 마친 후 자원을 자발적으로 반환할 때까지 기다림(이미 할당된 자원을 강제적으로 빼앗을 수 없음)\n\n\n순환 대기(Circular Wait)\n\n\n프로세스의 자원 점유 및 점유된 자원의 요구 관계가 원형을 이루면서 대기하는 조건.각 프로세스는 순환적으로 다음 프로세스가 요구하는 자원을 가지고 있음\n\n\n\n\n예방 방법\n교착상태 발생조건을 방지해서 데드락을 예방하는 방법은 시스템 처리량이나 자원 사용의 효율성을 떨어트리는 단점이 있습니다\n\n\n상호 배제 부정\n\n여러 개의 프로세스가 동시에 공유자원을 사용할 수 있음\n\n\n\n점유 대기 부정\n\n프로세스가 실행되기 전에 필요한 모든 자원을 할당하여 프로세스 대기를 없애거나, 자원이 점유되지 않은 상태에서만 자원 요청을 받도록 함\n\n\n\n비선점 부정\n\n모든 자원에 대한 선점을 허용\n\n\n\n순환 대기 부정\n\n자원을 선형으로 분류하여 고유 번호를 할당하고, 각 프로세스는 현재 점유한 자원의 고유번호보다 앞이나 뒤 한쪽 방향으로만 자원을 요구하도록 함\n\n\n"},"vault/Notion/DB/DB-Blog-Post/그래프와-트리(Tree-and-Graph)":{"slug":"vault/Notion/DB/DB-Blog-Post/그래프와-트리(Tree-and-Graph)","filePath":"vault/Notion/DB/DB Blog Post/그래프와 트리(Tree and Graph).md","title":"그래프와 트리(Tree and Graph)","links":[],"tags":[],"content":"참조\nko.wikipedia.org/wiki/그래프_(자료_구조)ko.wikipedia.org/wiki/트리_구조byjus.com/gate/difference-between-graph-and-tree/\nGraph\n그래프는 vertex와 edge로 구성된 비선형 자료구조이다. vertex는 정점, edge는 정점과 정점을 연결하는 간선이다. 최상위 노드(Root Node)가 존재하지 않는다. 여러 개의 edge를 가질 수 있다.\nTree\n트리는 그래프의 일종으로, 한 노드에서 시작해서 다른 정점들을 순회하여 자기 자신에게 돌아오는 순환이 없는 연결 그래프이다. 최상위 노드가 존재한다. 부모로 노트로 연결하는 edge와 자식 노트로 연결하는 edge를 가질 수 있다."},"vault/Notion/DB/DB-Blog-Post/그리디-알고리즘(Greedy-Algorithm)":{"slug":"vault/Notion/DB/DB-Blog-Post/그리디-알고리즘(Greedy-Algorithm)","filePath":"vault/Notion/DB/DB Blog Post/그리디 알고리즘(Greedy Algorithm).md","title":"그리디 알고리즘(Greedy Algorithm)","links":[],"tags":[],"content":"참조\nko.wikipedia.org/wiki/탐욕_알고리즘namu.wiki/w/그리디%20알고리즘janghw.tistory.com/entry/알고리즘-Greedy-Algorithm-탐욕-알고리즘\n그리디 알고리즘\n그리디 알고리즘은 최적해를 구하는 데에 사용되는 근사적인 방법으로, 여러 경우 중 하나를 결정해야 할 때마다 그 순간에 최적이라고 생각되는 것을 선택해 나가는 방식으로 진행하여 최종적인 해답에 도달한다.순간마다 하는 선택은 그 순간에 대해 지역적으로는 최적이지만, 그 선택들을 계속 수집하여 최종적(전역적)인 해답을 만들었다고 해서 그것이 최적해라는 보장은 없다.\n최적해가 보장되는 경우\n탐욕 선택 속성(greedy choice property)과 최적 부분 구조(optimal substructure) 특성을 가지는 문제들을 해결하는 데 강점이 있다.한번의 선택이 다음 선택에는 전혀 무관한 값이며 매 순간의 최적해가 문제에 대한 최적해라면 이 알고리즘을 사용하여 도출한 답은 최적해가 보장된다."},"vault/Notion/DB/DB-Blog-Post/그리디-알고리즘--최소-신장-트리(MST,-Minimum-Spanning-Tree)":{"slug":"vault/Notion/DB/DB-Blog-Post/그리디-알고리즘--최소-신장-트리(MST,-Minimum-Spanning-Tree)","filePath":"vault/Notion/DB/DB Blog Post/그리디 알고리즘- 최소 신장 트리(MST, Minimum Spanning Tree).md","title":"그리디 알고리즘- 최소 신장 트리(MST, Minimum Spanning Tree)","links":[],"tags":[],"content":"참조\nen.wikipedia.org/wiki/Minimum_spanning_treehttps://janghw.tistory.com/entry/알고리즘-Greedy-Algorithm-탐욕-알고리즘ko.wikipedia.org/wiki/크러스컬_알고리즘ko.wikipedia.org/wiki/프림_알고리즘\n최소 신장 트리\n\n어떤 그래프가 있을 때, 모든 정점을 최소 비용으로 연결하는 트리크루스컬 알고리즘(Kruskal’s algorithm)과 프림 알고리즘(Prim’s algorithm)이 있다.\n크루스컬 알고리즘\n크러스컬 알고리즘은 아래의 순서대로 작동한다.\n\n그래프에서 꼭짓점 또는 나무를 분리해 내지 않는 최대 가중치의 변을 제거한다.\n그래프에 n-1개의 변만 남을 때까지 1을 반복한다.\n그래프에 n-1개의 변이 남으면 최소 비용 생성나무이다.\n\n프림 알고리즘\n프림 알고리즘은 아래의 순서대로 작동한다:\n\n그래프에서 하나의 꼭짓점을 선택하여 트리를 만든다.\n그래프의 모든 변이 들어 있는 집합을 만든다.\n모든 꼭짓점이 트리에 포함되어 있지 않은 동안 트리와 연결된 변 가운데 트리 속의 두 꼭짓점을 연결하지 않는 가장 가중치가 작은 변을 트리에 추가한다.\n알고리즘이 종료됐을 때 만들어진 트리는 최소 비용 신장트리가 된다.\n"},"vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기":{"slug":"vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기","filePath":"vault/Notion/DB/DB Blog Post/글또 8기/글또 8기.md","title":"글또 8기","links":["vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기를-시작하며","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Boostcamp-AI-Tech-4기-후기/Boostcamp-AI-Tech-4기-후기","vault/Notion/DB/DB-Blog-Post/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)","vault/Notion/DB/DB-Blog-Post/Photometric-Stereo를-사용한-형상추출-알고리즘-개발/Photometric-Stereo를-사용한-형상추출-알고리즘-개발","vault/Notion/DB/DB-Blog-Post/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution","vault/Notion/DB/DB-Blog-Post/글또-8기/제-2차-커피드백---서로의-글-피드백-하기","vault/Notion/DB/DB-Blog-Post/ONNX(Open-Neural-Network-Exchange)란/ONNX(Open-Neural-Network-Exchange)란","vault/Notion/DB/DB-Blog-Post/ONNX-Runtime은-무엇일까/ONNX-Runtime은-무엇일까","vault/Notion/DB/DB-Blog-Post/AWS-ECS+S3로-MLFlow-구축해보기-1/AWS-ECS+S3로-MLFlow-구축해보기-1","vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기를-마치며/글또-8기를-마치며"],"tags":[],"content":"개요\n\n커뮤니티의 비전\n\n글을 작성하는 개발 직군분들이 모여서, 좋은 영향을 주고 서로 같이 자랄 수 있는 커뮤니티\n개발자들의 성장을 지원하는 커뮤니티\n각자의 직군에서 얻을 수 있는 내용을 토대로 글쓰기 진행\n\n\n부가적으로 삶의 철학, 여러 고민을 나누는 커뮤니티\n\n\n\n\n\n\n\n활동\n글또 8기를 시작하며\nBoostcamp AI Tech 4기 후기\nBasic Optimizer &amp; Adam(Adaptive Moment Esimation)\n🎫 Pass\nPhotometric Stereo를 사용한 형상추출 알고리즘 개발\nDBPN - Deep Back-Projection Networks for Single Image Super-resolution\n제 2차 커피드백 - 서로의 글 피드백 하기\n🎫 Pass\nONNX(Open Neural Network Exchange)란\nONNX-Runtime은 무엇일까\nAWS-ECS+S3로 MLFlow 구축해보기 1\n글또 8기를 마치며\n\n또봇 사용법 메모!\n본인의 코어 채널에서 다음의 명령어를 입력해주세요.\n/제출\n\n글 제출 모달을 불러옵니다.\n\n/패스\n\n글 패스 모달을 불러옵니다.\n\n/제출내역\n\n본인의 글 제출내역을 DM으로 불러옵니다. (편집됨)\n"},"vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기를-마치며/글또-8기를-마치며":{"slug":"vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기를-마치며/글또-8기를-마치며","filePath":"vault/Notion/DB/DB Blog Post/글또 8기/글또 8기를 마치며/글또 8기를 마치며.md","title":"글또 8기를 마치며","links":["vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기를-시작하며","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Boostcamp-AI-Tech-4기-후기/Boostcamp-AI-Tech-4기-후기","vault/Notion/DB/DB-Blog-Post/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)","vault/Notion/DB/DB-Blog-Post/Photometric-Stereo를-사용한-형상추출-알고리즘-개발/Photometric-Stereo를-사용한-형상추출-알고리즘-개발","vault/Notion/DB/DB-Blog-Post/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution","vault/Notion/DB/DB-Blog-Post/ONNX(Open-Neural-Network-Exchange)란/ONNX(Open-Neural-Network-Exchange)란","vault/Notion/DB/DB-Blog-Post/ONNX-Runtime은-무엇일까/ONNX-Runtime은-무엇일까","vault/Notion/DB/DB-Blog-Post/AWS-ECS+S3로-MLFlow-구축해보기-1/AWS-ECS+S3로-MLFlow-구축해보기-1"],"tags":[],"content":"\n개요\n그동안 쓴 글\n얼마나 목표를 이루었을까요?\n글또의 좋았던 점\n글또의 힘들었던 점\n마치며\n\n개요\n글또에 참가하고, 어느덧 마지막 제출일이 되었습니다.\n글또 8기의 마지막 글로 그동안의 활동에 대한 회고를 해보려 합니다.\n그동안 쓴 글\n이 글까지 포함하면 총 12번의 제출이 있었고, 그 중 이 글을 포함하여 총 9개의 글을 작성하였습니다. 글또 활동으로 쓴 글을 아래와 같습니다.\n\n\n글또 8기를 시작하며\n글또에 대한 소개와 참가 동기를 담은 글\n\n\nBoostcamp AI Tech 4기 후기\n부스트캠프 AI 테크 캠프 4기 수료 후기\n\n\nBasic Optimizer &amp; Adam(Adaptive Moment Esimation)\n딥러닝에서 최적화 함수 중 하나인 Adam의 개념과 사용 방법에 대한 글\n\n\nPhotometric Stereo를 사용한 형상추출 알고리즘 개발\n빛의 방향과 강도를 이용해 오브젝트의 형상을 추정하는 Photometric Stereo 알고리즘 개발 프로젝트 소개\n\n\nDBPN - Deep Back-Projection Networks for Single Image Super-resolution\n단일 이미지 초해상도 기술 중 하나인 DBPN 알고리즘에 대한 글\n\n\nONNX(Open Neural Network Exchange)란\n딥러닝 모델을 서로 다른 프레임워크 간에 변환하고 공유하기 위한 오픈소스 프로젝트인 ONNX에 대한 글\n\n\nONNX-Runtime은 무엇일까\nONNX-Runtime에 대한 설명 글\n\n\nAWS-ECS+S3로 MLFlow 구축해보기 1\nMLFlow를 AWS ECS와 S3를 이용해 구축하는 과정과 그 설명에 대한 글\n\n\n얼마나 목표를 이루었을까요?\n글또를 시작하면서 세웠던 목표는 아래와 같이 3개였습니다.\n\n\n글을 꾸준히 쓸 수 있는 습관을 기르는 것\n초반에는 열심히 글을 작성했지만, 취업 준비와 취업을 하면서 점차 글을 쓰는 것에 마음이 가지 않게 되었습니다. 글또 시작할 때, 12번 모두 제출하기로 내심 마음먹었지만 2회의 pass와 1회 미제출로 아쉬움이 남습니다.\n그러나 글또와는 별개로 노션에 ‘업무 일지’를 만들어서 회사 업무와 관련된 자료나 실험 결과들을 주기적으로 정리하고 있습니다. 나아가 정리한 자료들을 동료들과 공유하고 있으면서 이전보다 확실히 많은 글을 쓰고있다고 채감 할 수 있었습니다. 어떤 설명이 필요할 때, 그냥 말로 하기보다 글로 정리해서 공유하니 새롭게 일하게 된 직장의 동료분들도 좋은 반응을 보여주셔서 가끔씩 뿌듯함도 느껴집니다.\n\n\nML, DL, CV 등 관심분야에 대해 꾸준히 공부하고, 그것을 글로 남기는 것\n처음 목표는 위와 같이 ai, 혹은 cv에 관련된 내용을 쓰려고 했는데, 적다보니 지금 하고 있는 일의 영향 받게 되었습니다. 처음에는 알고리즘에 관련된 내용이다가 점차 mlops나 model serving에 대한 글을 다루게 되었습니다.\n\n\n남들과 공유하며 피드백을 받는 것\n이건 쓰면서도 어떻게 해야할까 고민했었는데, 글또에서 자체적으로 네트워킹할 수 있는 기회를 만들어 주었습니다. 4명이 한 조가 되어서 서로에 대한 글을 읽고 몇 가지 피드백을 주는 시간을 가졌습니다. 전혀 배경지식이 없는 사람들도 배려하며 글을 쓰시는 분들을 보면서 감탄하기도 했습니다.\n\n\n결론적으로, 글또 활동을 통해 목표한 바를 어느정도 이루었기 때문에 만족하고 있습니다.\n글또의 좋았던 점\n글또를 진행하면서 좋았던 점은 다음과 같습니다.\n\n저와 같은 분야에서 일 하는 분들과 교류할 수 있습니다.\n\n‘커피드백’과 같은 활동을 통해 다른 분들과 대면 or 비대면으로 비슷한 업무를 하고 있는 사람끼리 교류할 수 있는 기회가 주어집니다. 고민을 이야기할 수도 있고, 서로의 글을 피드백할 수도 있습니다. 개인적으로, 교류를 통해 다양한 분들을 만나보면서 다양한 것들(지식, 마음가짐, etc)을 배우게 되었습니다. 사람들과 교류하는게 정말 중요하다고 느끼게 되었습니다.\n\n\n저와 같은 분야에서 일 하는 분들이 쓰는 글을 읽어볼 수 있습니다.\n\n다른 사람들은 요즘 어떤 것에 관심을 갖고 있는지 알 수 있었습니다.\n글을 쓰다보면 어떤 내용을 어떻게 표현할지 고민할 때가 있습니다. 이 때, 다른 분들의 글을 읽어보면서 해결되는 경우가 있었습니다.\n저랑 비슷한 주제로 글을 작성했지만, 더 잘 쓴 글을 보면서 성장할 수 있습니다.\n\n\n글을 쓸 동기를 부여해 줍니다.\n\n예치금을 넣고, 글을 쓰지 않을 때 마다 예치금의 일부를 돌려받지 못하게 되기 때문에 동기를 부여 해줍니다.\n다른 분들과 교류하기 때문에 이를 위해서라도 글을 쓰게 됩니다.\n\n\n\n글또의 힘들었던 점\n글또를 진행하면서 힘들었던 점은 다음과 같습니다.\n\n(마음가짐의 문제지만) 여유가 없을 때, 글을 제출한다는 것이 스트레스가 됩니다.\n\n평소에는 상관 없었지만, 업무에 시달리다가 글또 제출일이 다가오면 위 좋았던 점의 ‘3번-동기부여’가 역으로 스트레스가 될 경우가 있었습니다.\n\n\n\n마치며\n비록 글또 활동을 100% 완료 하지는 못했지만 어느정도 목표한 바를 이루었기 때문에 굉장히 만족스럽습니다. 9기가 모집된다면 꼭 다시 참여하고 싶습니다.\n하지만 이번 기수는 제출하기 급했던 면이 조금 있어서, 만약 9기에 참여하게 되면 제출을 잘 못하더라도 양질의 글을 써보고 싶습니다.\n\n커피드백 1회, 2회를 같이 한 조원 분들\n\n마지막 커피드백을 같이 한 조원 분들"},"vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기를-시작하며":{"slug":"vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기를-시작하며","filePath":"vault/Notion/DB/DB Blog Post/글또 8기/글또 8기를 시작하며.md","title":"글또 8기를 시작하며","links":[],"tags":[],"content":"\n글또란?\n시작하게 된 이유는?\n무엇을 얻고 싶은지?\n무엇을 쓸 것인지?\n마치며\n\n\n글또란?\n글또는 6개월간 활동하며 2주에 한 번, 총 12개의 글을 써서 참가자들과 공유하는 커뮤니티 활동입니다. 글또의 비전은 아래와 같습니다.\n\n커뮤니티의 비전\n\n글을 작성하는 개발 직군분들이 모여서, 좋은 영향을 주고 서로 같이 자랄 수 있는 커뮤니티\n개발자들의 성장을 지원하는 커뮤니티\n각자의 직군에서 얻을 수 있는 내용을 토대로 글쓰기 진행\n\n부가적으로 삶의 철학, 여러 고민을 나누는 커뮤니티\n\n\n\n\n시작하게 된 이유는?\nBoostCamp AI Tech 4기에 참여하면서, 지속적으로 글을 쓰게 되었습니다.\n그리고 글이 조금씩 쌓여가면는 것과 지난 시간을 회고할 수 있는 것에 뿌듯함을 느꼈습니다.\n하지만 그와 반대로 지금까지 경험한 많은 일들을 그냥 지나쳐온 것에 대해 너무 아쉬움이 생겼습니다.\n또한 다른 분들의 글을 보며 나도 글을 더 잘 쓸 수 있을까? 라는 고민을 하게 되었고 BoostCamp의 멘토님께 글또라는 활동을 추천받았습니다.\n글또가 어떤 활동인지 찾아보았고, 글을 지속적으로 쓸 수 있는 습관을 기르고 싶은 저에게 알맞은 활동이라고 판단하여 시작하게 되었습니다.\n무엇을 얻고 싶은지?\n제가 글또를 통해 얻고 싶은 것은 다음과 같습니다.\n\n글을 꾸준히 쓸 수 있는 습관을 기르는 것\nML, DL, CV 등 관심분야에 대해 꾸준히 공부하고, 그것을 글로 남기는 것\n남들과 공유하며 피드백을 받는 것\n\n무엇을 쓸 것인지?\n현재 저는 AI 기반의 Computer Vision 기술에 많은 흥미를 갖고 있고, 해당 분야로 커리어를 쌓아가고 싶다고 생각하고 있습니다.\n따라서 저는 글또 8기를 통해 Computer Vision 관련 주제를 주하여 글을 쓰고자 합니다.\n마치며\n글또 활동을 통해 많은 것을 얻고 공유해 나갈 것이며, 모두 즐거운 글또 8기를 보내기를 기대합니다."},"vault/Notion/DB/DB-Blog-Post/글또-8기/제-2차-커피드백---서로의-글-피드백-하기":{"slug":"vault/Notion/DB/DB-Blog-Post/글또-8기/제-2차-커피드백---서로의-글-피드백-하기","filePath":"vault/Notion/DB/DB Blog Post/글또 8기/제 2차 커피드백 - 서로의 글 피드백 하기.md","title":"제 2차 커피드백 - 서로의 글 피드백 하기","links":["vault/Notion/DB/DB-Blog-Post/Photometric-Stereo를-사용한-형상추출-알고리즘-개발/Photometric-Stereo를-사용한-형상추출-알고리즘-개발","vault/Notion/DB/DB-Blog-Post/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution"],"tags":[],"content":"\n\n글또 피드백 가이드라인\n\n 글의 흐름은 자연스러운가?\n 아무것도 모르는 독자가 읽어도 쉽게 이해되는가?\n 맞춤법 혹은 띄어쓰기 오류가 있는가?\n 더 궁금한 내용이 있는가?\n 더 추가하면 좋을 내용이 있는가?\n 레이아웃에 대한 이야기(여백이 있으면 좋을 것 같아요, 너무 빽빽해요)\n 적절하게 이미지를 활용했는지\n\n\n\n정연님\n\nWASABI(curieuxjy.github.io/posts/paper/2023-03-12-wasabi.html)\n\n\n글에 대한 전개가 잘 이해가 안되는 부분이 있었는지(해당 분야의 연구자가 아닐때 느끼는 난이도 등)\n→ 우선 저는 강화학습에 대한 배경지식이 거의 없어서 reward, policy 등의 용어가 무엇을 뜻하는지 알 수 없었습니다. 다만 이런 글의 경우 보통 관련 전공자가 읽기 때문에, 쓰지 고민하지 않아도 될거같습니다.\n\n\n논문 리뷰글에서 중요하게 생각하는 부분들이 무엇인지\n→ 논문 리뷰 글은 논문을 읽는것보다 더 빠르고 간결하게 그 논문을 이해하고 핵심적인 내용(모델 구조, 기여 등)을 알 수 있게 해주는 것이라고 생각합니다.\n\n\n\nOrbit 설치하기(curieuxjy.github.io/posts/code/2023-04-04-insatll-orbit.html)\n\n\n실습과 코드 위주의 글에서 독자 입장에서 바라는 점이 있다면 어떤 점인지/해당 글은 그런 점들을 잘 만족하는지\n==→ 빠르고 쉽게 실습을 할 수 있고, Step별로 정리되어 있으며, 실습을 진행하면서 많이 실수하는 부분을 짚어줄 수 있는 것입니다.\n직접 해보지는 않았지만, 글이 어렵다고 느껴지지는 않았으며 Step별로 정리되어 있어 읽기 좋았습니다.\n다만 혹여 초심자들이 실수해서 오류를 발생시키는 포인트 들이 있다면 추가해주시면 좋을거같습니다.\n\n\n새로운 프로그램에 대한 흥미도를 끌어올릴 수 있는 글인지\n→ 이부분은… 잘 모르겠습니다 ㅠ\n\n\n\n\n\n\n현우님\n\n\n글또 8기를 시작하며 (www.sshowbiz.xyz/9d54a07e-e211-4c79-b29f-f72b53107b2f)\n\n\n글의 몰입감이 어떠한가요?\n→ 글또 7기를 수료한 현우님께서 아쉬웠던 점과 글쓰기를 더 하기 위해 8기를 시작했다는 것을 알 수 있었습니다. 필요한 만큼의 몰입도를 갖고 있다고 생각합니다.\n\n\n지루해서 중간에 그만 읽고 싶다면, 어떤 것을 개선하면 좋을까요?\n→ 현우님의 개인의 8기 참여 이유에 대한 서술이 주를 이루기 때문에 만약 다른 사람을 염두한다면 글 자체를 더 가볍고 간략하게 하는것은 어떨까 생각합니다.\n\n\n일상적인 글을 더 몰입감 있게 작성하려면 어떻게 하면 좋을까요?\n→ 음.. 이부분은 잘 모르겠습니다. 저는 개인적으로 일상적인 글이 몰입감이 있어야 한다고 생각하지 않습니다. ㅠㅠ\n\n\n이 글은 사실 개인적인 일기에 가까워서 명확한 독자 층을 설정하고 작성한 글은 아닙니다만, 저는 누군가의 일기 같은 후기 글도 재미있게 읽은 경험이 있고, 그것과 비슷하게 흉내내어 보려고 했습니다.이러한 일기 형태의 글을 누군가가 잘 읽을 수 있도록 할 수 있는 방법이 있을까요?\n→ 다른 사람의 입장에서 궁금해할 수 있는 정보가 포함되면 좋지 않을까 생각합니다. 예를 들어 글또가 비슷한 분야의 사람들끼리 네트워킹하는 기회를 제공한다는 정보도 다른 사람입장에서 궁금하고 알고싶은 내용이 아닐까 싶습니다.\n\n\n\n\nHyperDiffusion (www.sshowbiz.xyz/c94ff081-07c5-4b28-b381-13716bc799e9)\n\n저는 리뷰할 때 요새 Experiments 는 크게 공을 들여 서술하지는 않고 나열식인 경우가 많긴 한데,\n서술이 (입문자의 입장에서 논문의 이해, 해당 분야 종사자의 입장에서 디테일에) 도움이 되는 것 같은지\n해당 부분 설명이 지금도 괜찮은지 혹은 어떻게 바꾸면 더 나아질 것 같은지\n전체적인 글의 가시성에 개선할 점이 있다면 어떤 것이 있을까요?\n읽는데 거슬렸던 부분이 있을까요?\n댓글 허용, 목차 정리 등의 기능을 도입하신 분이 있다면 후기가 궁금합니다!\n이 외에 추가로 넣으면 좋을 기능이 있을까요?\n\n==→ 기존 논문의 구성을 그대로 따르면서, 한글로 이해하기 쉽게 풀어 주셔서 좋다고 생각합니다. 입문자의 입장에서는 읽기 힘든 글이지만, 해당 분야 종사자의 입장에서는 가치 있는 글이라고 생각합니다.\n실험의 경우, 현우님이 크게 중요하지 않다고 생각한다면 생략해도 상관 없다고 생각합니다. 너무 많은 내용이 되려 이해하기 힘든 장벽이 되는 경우도 있었기 때문입니다.\n\n\n\n\n유진님\n\nATSS 논문 리뷰 (velog.io/@dust_potato/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection-CVPR-2020)\n\n\n최대한 백그라운드를 담아보려 했는데 글의 이해에 도움이 되는지\n→ 이건 제 매우매우 개인적인, 취향에 가까운 생각인데.. 저는 글에서 이해에 필요한 배경지식 다뤄도 솔직히 이해가 잘 되지않습니다(다른 분들의 글도) 따라서 저는 개인적으로 이해해야 할 배경지식들이 있다면 그것을 잘 정리한 글의 링크를 상단에 서술해주시는 것을 좋아합니다.\n\n\n부족하다면 어떤 키워드에 대한 백그라운드를 더 담아야 할지\n→ ATSS에 대한 python 구현 코드 등이 있다면 좋을거같습니다.(개인 취향입니다)\n\n\n논문 설명 중 논리적이지 않은 부분은 없는지\n→ 없었습니다!\n\n\n특히 Exp1 - Inconsistency Removal 부분 명확하게 이해 되시는지\n==→ 제가 이해한 내용으로는, anchor based model들에 대한 비교 실험을 진행하였고, 목적은 “The number of anchors tiled per location”의 차이가 검출 성능에 중요한 역할을 하는지를 검증하기 위함이었습니다. 결과, FCOS에 사용한 기법을 RetinaNet에 적용해보았고, FCOS와의 성능 차이가 AP 0.8까지 좁혀진 결과를 얻었습니다. 때문에 “The number of anchors tiled per location”이 모델 성능에 중요한 역할을 하지 않는 다는 것을 실험적으로 증명했습니다.\n다만 이것이 “anchor-based 와 anchor-free methods의 핵심 차이점이 아닌 것을 의미합니다.”라는 결과로 이어지는 것은 조금 이상하다고 느꼈습니다.\n\n제가 이해를 잘못했군요 ㅠ 죄송합니다\n\n\n\n\nYolo5~8 요약 리뷰 (velog.io/@dust_potato/Yolo-v1Yolo-v8-%EC%9A%94%EC%95%BD-%EB%A6%AC%EB%B7%B0)\n\n\n요약 리뷰이긴 한데 독자가 이해는 해야 하니 백그라운드를 최대한 넣었는데 이 부분이 이해에 도움/방해가 되는지 여부와 그 이유?\n→ 버전별로 필요한 백그라운드가 존재하며, 이를 이해하면서 오히려 Yolo 5~8에 대한 몰입도가 떨어짐을 느꼈습니다. 백그라운드를 잘 이해할 수 있는 링크로 대체하는 것이 좋다고 느껴졌습니다.\n\n\n비어캔 치킨 같은 고집으로 글을 나누지 않고 하나에 적었는데 양은 어떤지\n→ 각 버전별 구성 자체가 개별 글로 빼도 부족하지 않을 구성 및 분량이라고 느껴집니다. 오히려 개별로 작성하고 핵심을 요약해주시면 좋지 않을까 생각됩니다.\n\n\n\n\n\n\n내 글\n\n\nPhotometric Stereo를 사용한 형상추출 알고리즘 개발\n\n재직 중 진행했던 프로젝트를 포트폴리오로 만들고 싶어서 쓴 글 입니다. 다만, 영업비밀을 다룰 수는 없어서 일반적인 내용만을 기술했습니다. 다른 분들이 보시기에 포트폴리오로 삼을만한 가치가 있어보이는지 궁금합니다.\n이해하기 읽기 불편한 부분이 있는지 궁금합니다.\n\n기업한테 어떤 문제를 어떻게 해결했는지 필요한 일인지 공감가고 정량적인 성과가 있어서 좋았음\nCamera Device에서 모든 처리를 하고 보내주기 때문에 오래 걸림, 여러 종류(Albedo, Gradient, etc)의 이미지를 사용해야 하는데 차례로 한 장씩 전송됨 → 모든 처리가 어떤 의미인지?\ns와 n이 무엇인지 이해에 도움이 되는 그림 예시가 있으면 좋겠음\n더 잘 기술할 수 있었을까?\n용어들이 친숙하지 않고 약어들에 대한 풀어가 한번은 되어 있었으면 좋겠다.\n전체적으로는 흐름이 이해가 가긴 한다. 그러나 수식적인 부분이 너무 나열되있어 아쉬웠다.\ngithub, notion에 필요성과 본인이 한것을 짧게 요약한 문단이 있었으면 좋겠다. ppt같은 템포로 그림이 추가되면 좋을 것 같다(이해를 위해)\nLUT 무엇인지 풀어쓰기…\n\n\nDBPN - Deep Back-Projection Networks for Single Image Super-resolution\n\n원문의 구성을 그대로 따르지만, 실제 사용 및 이해에 필요한 부분만을 리뷰하였습니다.이 글을 통해 이 논문이 가진 특징 혹은 기여는 무엇인지, 모델은 구조는 어떤지, 훈련에 사용할 때 파라미터를 어떻게 할지 등을 원문을 읽지 않아도 알 수 있게 하는 것이 목적입니다.(나중에 까먹으면 제가 보려고..)\n다른 분들이 보시기에 이해에 필요한 설명이 부족하지는 않은지\n문맥상 어색한 내용이 존재하는지(필요 없다고 생각한 부분을 다 잘라서…) 궁금합니다.\n\n관련연구, 배경, 용어 설명, 약어가 친절하지 않음\nPSRN, SSIM에 대한 설명\n피드백 구조 해결된 문제인건가\nDense 구조 추가하고 파라미터가 증가했는데 중요하지 않은건지?\n디테일이 잘 이해가 되지 않음(Feed-Back이 뭔지 처음 보는 사람이 잘 알 수 있게)이 뭔지…\n기여가 무엇인지 확신할 수 없었다.(이해를 기반으로 기여를 작성해야 함)\n“전방향 및 역방향 투영” 이해가 안갈 수 있음 이해가 안가는 것들이 초반에 있으면 몰입도가 떨어짐.\n지금 몰라도 되는 티를 내거나, 이해할 수 있게 풀어주는 것이 좋을 것 같음.\n개요는 쉽게 풀어서 쓰는 것이 좋을 것 같습니다.\n\n\n코드 구현이 따로 있는데, 깃허브 정리가 끝나는 대로 링크를 추가할 예정입니다.\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/기대값":{"slug":"vault/Notion/DB/DB-Blog-Post/기대값","filePath":"vault/Notion/DB/DB Blog Post/기대값.md","title":"기대값","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n기대값\n\n\n참조\nWeek 1\n기대값\n\n데이터를 대표하는 통계량\n확률분포를 통해 다른 통계적 범함수를 계산하는데 사용\n\n분산, 첨도, 공분산 등 여러 통계량 계산에 사용\n\n\n"},"vault/Notion/DB/DB-Blog-Post/기댓값(expectation)":{"slug":"vault/Notion/DB/DB-Blog-Post/기댓값(expectation)","filePath":"vault/Notion/DB/DB Blog Post/기댓값(expectation).md","title":"기댓값(expectation)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n기댓값(expectation)\n\n\n참조\nnamu.wiki/w/기댓값\nWeek 1\n기댓값(expectation)\n\n어떤 확률 과정을 무한히 반복했을 때, 얻을 수 있는 값의 평균으로써 기대할 수 있는 값\n"},"vault/Notion/DB/DB-Blog-Post/깊이-우선-탐색(DFS,-Depth-First-Search)":{"slug":"vault/Notion/DB/DB-Blog-Post/깊이-우선-탐색(DFS,-Depth-First-Search)","filePath":"vault/Notion/DB/DB Blog Post/깊이 우선 탐색(DFS, Depth-First Search).md","title":"깊이 우선 탐색(DFS, Depth-First Search)","links":[],"tags":[],"content":"참조\nko.wikipedia.org/wiki/깊이_우선_탐색gmlwjd9405.github.io/2018/08/14/algorithm-dfs.html\nDFS\n\n임의의 정점(vertax)에서 시작해서 다음 분기(branch)로 넘어가기 전에 해당 분기를 완벽하게 탐색하는 방법자기 자신을 호출하는 순환 알고리즘의 형태 를 가지고 있다."},"vault/Notion/DB/DB-Blog-Post/마르코프-연쇄-몬테카를로(MCMC,-Markov-Chain-Monte-Carlo)":{"slug":"vault/Notion/DB/DB-Blog-Post/마르코프-연쇄-몬테카를로(MCMC,-Markov-Chain-Monte-Carlo)","filePath":"vault/Notion/DB/DB Blog Post/마르코프 연쇄 몬테카를로(MCMC, Markov Chain Monte Carlo).md","title":"마르코프 연쇄 몬테카를로(MCMC, Markov Chain Monte Carlo)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n마르코프 연쇄 몬테카를로(MCMC, Markov Chain Monte Carlo)\n\n\n참조\nko.wikipedia.org/wiki/마르코프_연쇄_몬테카를로\nwww.secmem.org/blog/2019/01/11/mcmc/\nko.wikipedia.org/wiki/마르코프_연쇄\nWeek 1\n마르코프 연쇄 몬테카를로(MCMC, Markov Chain Monte Carlo)\n\n마르코프 연쇄의 구성에 기반한 확률 분포로부터 원하는 분포의 정적분포를 갖는 표본을 추출하는 알고리즘의 한 부류\n어떤 함수 f(x) 를 계산할 수 있고 확률분포 p(x)를 샘플링 할 수 있을 때, 아래와 같이 적분 결과를 근사시킬 수 있다.\n\n{\\int f(x)p(x)dx} \\approx {\\sum_{i=1}^{n}{f(X_i) \\over N}}\n\np(x)가 없어진 이유\n\nX_i는 표본이며, 표본은 확률 분포를 반영한다. 그러므로 p(x)를 곱한 것과 같은 효과가 나타난다.\n\n\nMarkov Chain\n\nMCMC는 표본만을 이용하지만 필요한 만큼 많이 뽑게 된다면 현재까지 뽑은 전체 표본이 확률 분포를 거의 모방한다. 다만, 이 결과를 보장하기 위해 Markov Chain을 사용\n확률론에서 마르코프 연쇄는 이산 시간 확률 과정을 의미\n시간에 따른 계의 상태의 변화를 나타냄\n마르코프 성질을 가지고 있음\n\n과거와 현재의 상태가 주어졌을 때, 미래 상태의 조건부 확률 분포가 과거 상태와는 독립적으로 현재 상태에 의해서만 결정됨\n\n\n\n\n"},"vault/Notion/DB/DB-Blog-Post/마스크,-성별,-나이-분류기/마스크,-성별,-나이-분류기":{"slug":"vault/Notion/DB/DB-Blog-Post/마스크,-성별,-나이-분류기/마스크,-성별,-나이-분류기","filePath":"vault/Notion/DB/DB Blog Post/마스크, 성별, 나이 분류기/마스크, 성별, 나이 분류기.md","title":"마스크, 성별, 나이 분류기","links":["vault/Notion/DB/DB-Blog-Post/NAVER-Connect-마스크-착용-상태-분류/NAVER-Connect-마스크-착용-상태-분류"],"tags":[],"content":"\n\n                  \n                  GitHub - 404Vector/App.Streamlit.Mask-Gender-Age.Classifier: ViT 기반 Mask, Gender, Age 예측 WebService Repos \n                  \n                \n\n\npip install -r requirements.\ngithub.com/404Vector/App.Streamlit.Mask-Gender-Age.Classifier\n\n\n\n\n개요\n설치 및 실행\n결과\n\n\n개요\n\n목적\n\n이 프로젝트는 사용자가 업로드한 이미지에서 마스크, 성별, 나이를 예측해주는 Web 기반 Service를 개발하는 것이 목적입니다.\n\n\nModel, Weight\n\n\n마스크, 성별, 나이의 예측은 기존에 대회를 통해 훈련했던 ViT모델(ViT_B_16)과 가중치를 불러와 사용했습니다.\n→ NAVER Connect 마스크 착용 상태 분류\n\n\n\nWeb\n\n\nStreamlit을 사용하여 웹 기반 서비스로 구현합니다.\n\n\n\n\n\n설치 및 실행\n\n\n설치\n\n주의\n\nPython 3.8, curl, git-lfs가 필요합니다.\n\n\n\n# (optional)curl이 없는 경우\napt-get install curl\n \n# (optional)git-lfs가 없는 경우\ncurl -s packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\napt install git-lfs\n \n# git repos clone &amp; install package\ngit clone github.com/404Vector/App.Streamlit.Mask-Gender-Age.Classifier.git\ncd App.Streamlit.Mask-Gender-Age.Classifier\npip install -r requirements.txt\n\n\n실행\nstreamlit run app.py --server.port {PORT} --server.fileWatcherType none\n \n# example\nstreamlit run app.py --server.port 30001 --server.fileWatcherType none\n\n\n실행화면 - 로그인\n기본 비밀번호 : 공백 1개\n\n\n\n실행화면 - 파일 업로드\n\n\n\n실행화면 3 - 실행 결과\n\n\n\n\n\n\n결과\n\nStreamlit을 사용하여 훈련한 모델로 결과를 얻을 수 있었다.\nStreamlit은 Page Update마다 script를 다시 다 읽기 때문에 cache 처리를 통해 모델을 로드하게 해야 했다.\n서비스를 다시 시작해야 하는 수정사항이 있을 때 마다, model을 다시 load하는 것이 불편하게 느껴졌다. Inference 파트는 별도의 inference server를 구성해 backend로 빼는 것이 디버깅에 더 편리하겠다고 느껴졌다.\n"},"vault/Notion/DB/DB-Blog-Post/메모리-관리-전략은-무엇일까":{"slug":"vault/Notion/DB/DB-Blog-Post/메모리-관리-전략은-무엇일까","filePath":"vault/Notion/DB/DB Blog Post/메모리 관리 전략은 무엇일까.md","title":"메모리 관리 전략은 무엇일까","links":[],"tags":[],"content":"참조\ndheldh77.tistory.com/entry/운영체제메모리-관리-전략Memory-Management-Strategy\n메모리 관리 전략이란?\n메모리 용량이 증가함에 따라 프로그램의 크기 또한 계속 증가하고 있기 떄문에 메모리는 언제나 부족합니다.\n메모리 관리 전략은 제한된 물리 메모리의 효율적인 사용과 메모리 참조 방식을 제공하기 위한 전략입니다.\n1) 연속 메모리 할당\n\n프로세스를 메모리에 연속적으로 할당하는 기법\n할당과 제거를 반복하다보면 Scattered Holes가 생겨나고 이로 인한 외부 단편화가 발생\n연속 메모리 할당에서 외부 단편화를 줄이기 위한 할당 방식\n\n최초 적합(First fit)\n\n가장 처음 만나는 빈 메모리 공간에 프로세스를 할당\n빠름\n\n\n최적 적합(Best fit)\n\n빈 메모리 공간의 크기와 프로세스의 크기 차이가 가장 적은 곳에 프로세스를 할당\n\n\n최악 적합(Worst fit)\n\n빈 메모리 공간의 크기와 프로세스의 크기 차이가 가장 큰 곳에 프로세스를 할당\n이렇게 생긴 빈 메모리 공간에 또 다른 프로세스를 할당할 수 있을 거라는 가정에 기인\n\n\n\n\n\n2) 페이징(Paging)\n\n메모리 공간이 연속적으로 할당되어야 한다는 제약조건을 없애는 메모리 관리 전략\n논리 메모리는 고정크기의 페이지, 물리메모리는 고정크기의 프레임 블록으로 나누어 관리\n프로세스가 사용하는 공간을 논리 메모리에서 여러 개의 페이지로 나누어 관리하고, 개별 페이지는 순서에 상관없이 물리 메모리에 있는 프레임에 매핑되어 저장\nMMU(Memory Management Unit)의 재배치 레지스터 방식을 활용해 CPU가 마치 프로세스가 연속된 메모리에 할당된 것처럼 인식하도록 함\n내부 단편화 발생\n\n3) 세그멘테이션(Segmentation)\n\n페이징 기법과 반대로 논리 메모리와 물리 메모리를 같은 크기의 블록이 아닌, 서로 다른 크기의 논리적 단위인 세그먼트로 분할\n외부 단편화 발생\n\n4) 세그멘테이션 페이징 혼용 기법\n\n페이징과 세그멘테이션도 각각 내부 단편화와 외부 단편화가 발생\n페이징과 세그멘테이션을 혼용해 이러한 단편화를 최대한 줄이는 전략\n프로세스를 세그먼트(논리적 기능 단위)로 나눈 다음 세그먼트를 다시 페이지 단위로 나누어 관리\n매핑 테이블을 두 번 거쳐야하므로 속도가 느려짐\n\n효과적인 메모리 사용\n(1) 동적 적재(Dynamic Loading)\n\n프로그램 실행에 반드시 필요한 루틴과 데이터만 적재하는 기법\n모든 루틴(ex. 오류처리)과 데이터(ex. 배열)는 항상 사용하지 않고, 실행 시 필요하다면 그때 해당 부분을 메모리에 적재\n\n(2) 동적 연결(Dynamic Linking)\n\n라이브러리 루틴연결을 컴파일 시점에 하는 것이 아닌 실행 시점까지 미루는 기법\n\n(3) 스와핑(Swapping)\n\nCPU에서 실행중이지 않는 프로세스는 저장장치의 Swap 영역으로 이동(Swap in/Swap out)해 메모리를 확보\n문맥 교환으로 인한 오버헤드가 발생할 수 있고 속도가 느려지지만, 메모리 공간 확보에는 효율적\n"},"vault/Notion/DB/DB-Blog-Post/모-평균(population-mean,-μ)":{"slug":"vault/Notion/DB/DB-Blog-Post/모-평균(population-mean,-μ)","filePath":"vault/Notion/DB/DB Blog Post/모 평균(population mean, μ).md","title":"모 평균(population mean, μ)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n모 평균(population mean, μ)\n\n\n참조\nnamu.wiki/w/통계학?from=모집단#s-2\nWeek 1\n모 평균(population mean, μ)\n\n모 집단의 평균, 확률 변수의 기댓값\n"},"vault/Notion/DB/DB-Blog-Post/모수":{"slug":"vault/Notion/DB/DB-Blog-Post/모수","filePath":"vault/Notion/DB/DB Blog Post/모수.md","title":"모수","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n모수\n\n\n참조\nnamu.wiki/w/모수\nWeek 1\n모수\n\n모집단(population)의 수\n통계학에서 모 평균, 모 표준편차, 모 분산 등 모집단의 데이터를 의미\n통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표\n그러나 유한한 개수의 데이터로 정확한 분포를 구할 수는 없음\n따라서 근사적으로 확률분포 추정 필요\n\nex) 정규분포, 베르누이분포,… etc\n\n\n"},"vault/Notion/DB/DB-Blog-Post/모수적(parametric)-방법론":{"slug":"vault/Notion/DB/DB-Blog-Post/모수적(parametric)-방법론","filePath":"vault/Notion/DB/DB Blog Post/모수적(parametric) 방법론.md","title":"모수적(parametric) 방법론","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n모수적(parametric) 방법론\n\n\n참조\nWeek 1\n모수적(parametric) 방법론\n\n유한한 개수의 데이터를 특정 확률 분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법론\n"},"vault/Notion/DB/DB-Blog-Post/모집단(population)":{"slug":"vault/Notion/DB/DB-Blog-Post/모집단(population)","filePath":"vault/Notion/DB/DB Blog Post/모집단(population).md","title":"모집단(population)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n모집단(population)\n\n\n참조\nnamu.wiki/w/통계학?from=모집단#s-2\nWeek 1\n모집단(population)\n\n관측 대상이 되는 전체 집단\n"},"vault/Notion/DB/DB-Blog-Post/몬테카를로-방법/몬테카를로-방법":{"slug":"vault/Notion/DB/DB-Blog-Post/몬테카를로-방법/몬테카를로-방법","filePath":"vault/Notion/DB/DB Blog Post/몬테카를로 방법/몬테카를로 방법.md","title":"몬테카를로 방법","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n몬테카를로 방법\n\n\n참조\nko.wikipedia.org/wiki/몬테카를로_방법\nWeek 1\n몬테카를로 방법\n\n반복된 무작위 추출(repeated random sampling)을 이용하여 함수의 값을 수리적으로 근사하는 알고리즘\n대체적으로, 몬테카를로 방법은 확률론적 해석을 가진 문제를 해결하기 위해 사용될 수 있음\n대수의 법칙에 의해 어떤 확률 변수의 기댓값으로 설명되는 적분은 랜덤표본(ramdom sample)의 표본 평균을 취함으로써 근사치를 구할 수 있음\n\n"},"vault/Notion/DB/DB-Blog-Post/범함수(functional)":{"slug":"vault/Notion/DB/DB-Blog-Post/범함수(functional)","filePath":"vault/Notion/DB/DB Blog Post/범함수(functional).md","title":"범함수(functional)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n범함수(functional)\n\n\n참조\nnamu.wiki/w/범함수\nWeek 1\n범함수(functional)\n\n함수를 입력받아 스칼라(즉 수 하나)를 내어놓는 함수\n"},"vault/Notion/DB/DB-Blog-Post/샘플링(Samiling)":{"slug":"vault/Notion/DB/DB-Blog-Post/샘플링(Samiling)","filePath":"vault/Notion/DB/DB Blog Post/샘플링(Samiling).md","title":"샘플링(Samiling)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n샘플링(Samiling)\n\n\n참조\nko.wikipedia.org/wiki/샘플링\nWeek 1\n샘플링(Samiling)\n\n어떤 자료에서 일부 값을 추출하는 것\n"},"vault/Notion/DB/DB-Blog-Post/선형회귀의-목적식":{"slug":"vault/Notion/DB/DB-Blog-Post/선형회귀의-목적식","filePath":"vault/Notion/DB/DB Blog Post/선형회귀의 목적식.md","title":"선형회귀의 목적식","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n선형회귀의 목적식\n\n\n참조\nWeek 1\n선형회귀의 목적식\n입력 데이터 X = [x_1,x_2,..,x_n]과 가중치 B=[b_1,..., b_n] 그리고 출력 데이터 Y=[y_1, ..., y_n]가 있을 때, y_t \\simeq x_t*b_t가 되도록 행렬 B를 최적화해보자.\n예측값 r_t=x_tb_t일 때, 실제 값과의 차이는 아래와 같다.\nL = \\sqrt{{1\\over n}\\sum_{t=1}^{n}||y_t - r_t||_2}\nL(B) =L(b_1,b_2,...,b_n)= \\sqrt{{1\\over n}\\sum_{t=1}^{n}||y_t - x_tb_t||_2}\n{L}={({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))^{0.5}}\nL(loss)를 기준으로 얼마나 가중치모델 B를 통해 도출한 예측 값 R이 실제 값 Y에 근접하는지의 척도를 알 수 있다.\nL은 R에 대한 함수이므로 위 함수에 B를 대입하면 함수 L에 대한 결과 값도 변화한다.\n따라서 L 대하여 편미분 예측 값 R로 편미분을 해서 나온 기울기를 통해 L값이 낮아지는 방향으로 B를 업데이트 할 수 있다.\n우선 R로 L을 편미분 해보자.\n{\\delta L \\over \\delta R}={1 \\over 2}({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))\\prime{({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))^{-0.5}}\\\\={1 \\over 2}{({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))\\prime \\over \\sqrt{({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))}}\\\\={1 \\over {2L}}{({1 \\over n} ((y_1-r_1)^2+...+(y_n-r_n)^2))}\\prime\\\\={1 \\over {2nL}}{(-2(y_1-r_1)+...+-2(y_n-r_n))}\\\\={-{1 \\over {nL}}}{((y_1-r_1)+...+(y_n-r_n))}\nR로 먼저 편미분을 한 이유는 B는 R의 함수이고 R은 L의 함수이기 때문이다.\n{\\delta r_t \\over \\delta b_t} = x_t\n{\\delta l_t \\over \\delta b_t}={\\delta l_t \\over \\delta r_t}{\\delta r_t \\over \\delta b_t}\n{\\delta L \\over \\delta B} = {\\delta L \\over \\delta R}{\\delta R \\over \\delta B}\\\\={-{1 \\over nL}}{((y_1-r_1)x_1+...+(y_n-r_n)x_n)}"},"vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation-사용법":{"slug":"vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation-사용법","filePath":"vault/Notion/DB/DB Blog Post/순열(Permutation)- C++ next_permutation 사용법.md","title":"순열(Permutation)- C++ next_permutation 사용법","links":[],"tags":[],"content":"참조\n\nen.cppreference.com/w/cpp/algorithm/next_permutation\nen.cppreference.com/w/cpp/algorithm/prev_permutation\n\nHeader\n\\#include&lt;algorithm&gt;\nUse\ntemplate&lt; class BidirIt, class Compare &gt;\nbool next_permutation( BidirIt first, BidirIt last );\n \n          or\n \ntemplate&lt; class BidirIt, class Compare &gt;\nbool next_permutation( BidirIt first, BidirIt last, Compare comp );\n\n기본적으로 배열의 iterator를 param으로 받는다.입력 배열의 내부 원소 위치만 바꾸기(swap) 때문에 메모리가 새롭게 생성되지 않는다.\n따라서 모든 경우의 수를 배열로 얻어야 한다면 loop 내에서 계속 호출하며 별도의 컨테이너에 복사해야 한다.\n배열을 오름차순으로 정렬하고 사용해야 한다.변경 전 배열을 A, 변경 후 배열을 B라고 할 때 사전식(lexicographically) 순서로 비교하여 B가 A보다 크다면 true를 반환하고 그렇지 않다면 false를 반환한다.\n리턴 값(bool)이 false라면 새로운 순열이 더 이상 없다고 간주하고 loop를 종료한다.\n\nExample\n\\#include &lt;algorithm&gt;\n\\#include &lt;string&gt;\n\\#include &lt;iostream&gt;\n \nint main()\n{\n    std::string s = &quot;aba&quot;;\n    std::sort(s.begin(), s.end());\n    do {\n        std::cout &lt;&lt; s &lt;&lt; &#039;\\n&#039;;\n    } while(std::next_permutation(s.begin(), s.end()));\n}"},"vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation--nPr-순열-구하기":{"slug":"vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation--nPr-순열-구하기","filePath":"vault/Notion/DB/DB Blog Post/순열(Permutation)- C++ next_permutation- nPr 순열 구하기.md","title":"순열(Permutation)- C++ next_permutation- nPr 순열 구하기","links":["vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation-사용법"],"tags":[],"content":"참조\n순열(Permutation)- C++ next_permutation 사용법\n설명\n기본적으로 next_premutation을 사용하여 순열을 만들 경우, n개의 원소 중 n개를 뽑게 된다.(nPn)하지만 약간의 코드가 추가된다면 nPr연산이 가능하다.배열 A = {1,2,3,4,5} 일 때, next_permutation을 사용하여 순열을 만든다면 아래와 같이 진행된다.\n\nloop 0: 12345loop 1: 12354loop 2: 12435loop 3: 12453loop 4: 12534loop 5: 12543loop 6: 13245loop 7: 13254loop 8: 13425loop 9: 13452…(생략)\n\n하지만 r+1번째 부터 ~ n번째 까지의 원소를 뒤집어 버린다면 어떻게 될까?\n\nloop 0: 12345 → 12354loop 1: 12435 → 12453loop 2: 12534 → 12543…(생략)\n\nr+1번째 부터 n번째 까지의 원소를 뒤집어 버림으로써 첫 번째부터 r번째의 원소가 업데이트 될 수 밖에 없어진다.\nheader\n\\#include &lt;algorithm&gt;\nexample\n\\#include &lt;iostream&gt;\n\\#include &lt;vector&gt;\n\\#include &lt;algorithm&gt;\n \nusing namespace std;\n \nint main(){\n \n\tvector&lt;int&gt; v = {3, 1, 2, 4, 5};\n \n\tsort(v.begin(), v.end()); // v : {1,2,3,4,5}\n \n\tdo\n\t{\n\t\tfor(int e : v) cout &lt;&lt; e &lt;&lt; &quot; &quot;;\n\t\tcout &lt;&lt; &#039;\\n&#039;;\n\t\treverse(v.begin() + r, v.end());\n\t}while(next_permutation(v.begin(),v.end()));\n\treturn 0;\n}"},"vault/Notion/DB/DB-Blog-Post/순열(Permutation)":{"slug":"vault/Notion/DB/DB-Blog-Post/순열(Permutation)","filePath":"vault/Notion/DB/DB Blog Post/순열(Permutation).md","title":"순열(Permutation)","links":[],"tags":[],"content":"정의\n순열 또는 치환은 순서가 부여된 임의의 집합을 다른 순서로 뒤섞는 연산이다.\n참조\nko.wikipedia.org/wiki/순열\nnPr\nn개의 원소 중 r개를 뽑아 만드는 permutation\n(r &lt;= n, 0! = 1) // 중요 조건\nnPr에 대한 모든 경우의 수\n기본 : (n-0)*(n-1)*(n-2)...*(n-r+1)\n중복순열 : n * n * n ... n = n^r\n코드 구현\n\\#include &lt;string&gt;\n\\#include &lt;vector&gt;\n\\#include &lt;iostream&gt;\nusing namespace std;\n \n    template&lt;typename T&gt;\n    void Swap(T &amp; a, T &amp; b)\n    {\n        T temp = a;\n        a = b;\n        b = temp;\n    }\n \n    template&lt;typename T&gt;\n    void Permutation(vector&lt;T&gt; &amp;data, vector&lt; vector&lt;T&gt; &gt; &amp;result, const int &amp;n, const int &amp;r, int depth)\n    {\n        if (depth == r) // escape condition\n        {\n            vector&lt;T&gt; resultElement(data.begin(), data.begin()+r);\n            result.push_back(resultElement);\n            return;\n        }\n \n        for(int i = depth; i &lt; n; i++)\n        {\n            swap(data[i], data[depth]);\n            Permutation&lt;T&gt;(data, result, n, r, depth + 1);\n            swap(data[i], data[depth]);\n        }\n    }\n \n    template&lt;typename T&gt;\n    vector&lt; vector&lt;T&gt; &gt; CreatePermuation(vector&lt;T&gt; &amp;data, int &amp;n, int &amp;r){\n        vector&lt; vector&lt;T&gt; &gt; rs;\n        Permutation(data, rs, n, r, 0);\n        return rs;\n    }"},"vault/Notion/DB/DB-Blog-Post/유클리드-호제법(Euclidean-algorithm)":{"slug":"vault/Notion/DB/DB-Blog-Post/유클리드-호제법(Euclidean-algorithm)","filePath":"vault/Notion/DB/DB Blog Post/유클리드 호제법(Euclidean algorithm).md","title":"유클리드 호제법(Euclidean algorithm)","links":[],"tags":[],"content":"참조\nko.wikipedia.org/wiki/유클리드_호제법\n유클리드 호제법\n유클리드 알고리즘은 두 자연수의 최대공약수를 구하는 알고리즘의 하나이다.호제법이란 말은 두 수가 서로 상대방 수를 나누어서 결국 원하는 수를 얻는 알고리즘을 의마한다.2개의 자연수 a, b에 대해서 a를 b로 나눈 나머지를 r이라 하면(단, a&gt;b), a와 b의 최대공약수는 b와 r의 최대공약수와 같다.a : kxb : kyk : 최대공약수r : a%b = kx%ky = x%y\na = bq + rkx = kyq + r→ x과 yq는 서로소\nr = k(x - yq) = kp→ 나머지인 r 또한 k의 배수\nkx = k(yq + p)x = yq + pp = x - yq→ x와 yq는 서로소이므로 1 이외에 공약수가 존재하지 않는다.\n따라서 나머지 r은 x와 yq에 대하여 서로소이다.\n그러므로, a와 b의 최대공약수는 k인 것과 같이 b와 r의 최대 공약수도 k이다."},"vault/Notion/DB/DB-Blog-Post/이산형-확률변수와-연속형-확률변수":{"slug":"vault/Notion/DB/DB-Blog-Post/이산형-확률변수와-연속형-확률변수","filePath":"vault/Notion/DB/DB Blog Post/이산형 확률변수와 연속형 확률변수.md","title":"이산형 확률변수와 연속형 확률변수","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n이산형 확률변수와 연속형 확률변수\n\n\n참조\nWeek 1\n이산형 확률변수와 연속형 확률변수\n\nP(X = x) : 확률변수가 x 값을 가질 확률\n이산형 확률변수(discrete random variable)\n\n확률 변수 X가 취할 수 있는 모든 값을 x1, x2, x3, … 처럼 셀 수 있을 때 X를 이산확률변수라고 한다.\n\n\n\n{\\sum_{x \\in A}}P(X = x)$$\n\n- 연속형 확률변수(continuous random variable)\n    - 적절한 구간 내의 모든 값을 취하는 확률 변수이다.\n    - 연속적인 범위의 값을 지니는 확률변수. 예를 들어, &#039;핸드폰으로 나무위키를 보는 사람의 수&#039;는 셀 수 있으므로 이산확률변수이나, &#039;핸드폰으로 나무위키를 보는 사람이 일요일에 나무위키를 본 시간&#039;은 셀 수 없으므로 연속확률변수이다.\n\n$$P(X \\in A) =  \n{\\int_{A}}P(x)dx$$"},"vault/Notion/DB/DB-Blog-Post/자동-미분(Autograd)":{"slug":"vault/Notion/DB/DB-Blog-Post/자동-미분(Autograd)","filePath":"vault/Notion/DB/DB Blog Post/자동 미분(Autograd).md","title":"자동 미분(Autograd)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-2"],"tags":[],"content":"\n참조\n자동 미분(Autograd)\nPytorch\n\n\n참조\nwikidocs.net/60754\npytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\nWeek 2\n자동 미분(Autograd)\n\n역전파를 위한 자동 미분기능\n\nPytorch\n\nPytorch는 Autograd 기능이 엔진에 내장되어 있습니다.\n\nimport torch\nfrom torchvision.models import resnet18, ResNet18_Weights\n \nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\ndata = torch.rand(1, 3, 64, 64)\nlabels = torch.rand(1, 1000)\n \n# Forward Pass\nprediction = model(data)\n \n# Calc loss\nloss = (prediction - labels).sum()\n \n# Backward Pass\n# Autograd를 사용하여 자동으로 미분 수행, 결과 값은 각 param의 .grad에 저장\nloss.backward()\n \n# Gradient descent\n# model.parameters()로 얻은 각 param의 .grad를 사용하여 해당 param을 업데이트\noptim.step()"},"vault/Notion/DB/DB-Blog-Post/조건부-확률":{"slug":"vault/Notion/DB/DB-Blog-Post/조건부-확률","filePath":"vault/Notion/DB/DB Blog Post/조건부 확률.md","title":"조건부 확률","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n조건부 확률\n\n\n참조\nWeek 1\n조건부 확률\n\n입력변수 x에 대해 정답이 y일 확률\n"},"vault/Notion/DB/DB-Blog-Post/주변-분포(Marginal-Probability-Distribution)":{"slug":"vault/Notion/DB/DB-Blog-Post/주변-분포(Marginal-Probability-Distribution)","filePath":"vault/Notion/DB/DB Blog Post/주변 분포(Marginal Probability Distribution).md","title":"주변 분포(Marginal Probability Distribution)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n주변 분포(Marginal Probability Distribution)\n\n\n참조\nko.wikipedia.org/wiki/주변_분포\ndeep-learning-study.tistory.com/59\nWeek 1\n주변 분포(Marginal Probability Distribution)\n\n확률론과 통계학에서 확률 변수들의 부분 집합의 주변 분포(周邊分布)란 그 부분 집합에 속한 확률 변수들의 확률 분포를 의미\n주변분포는 두 이산확률변수 X와 Y의 결합확률질량함수가 f(x,y)일 때, X의 확률질량함수는 모든 y의 결합확률질량함수를 더하여 구할 수 있는 것을 의미합니다.\n\n\n\n이 표에서 주변에 있는 확률의 합은 각 확률변수가 가질 수 있는 값에 대한 확률로 Y�의 주변확률질량함수는 다음과 같습니다.\nfy(1)=f(0,1)+f(1,1)+f(2,1)+f(3,1)=34"},"vault/Notion/DB/DB-Blog-Post/초보-운전자를-위한-안전-주행-보조-시스템/초보-운전자를-위한-안전-주행-보조-시스템":{"slug":"vault/Notion/DB/DB-Blog-Post/초보-운전자를-위한-안전-주행-보조-시스템/초보-운전자를-위한-안전-주행-보조-시스템","filePath":"vault/Notion/DB/DB Blog Post/초보 운전자를 위한 안전 주행 보조 시스템/초보 운전자를 위한 안전 주행 보조 시스템.md","title":"초보 운전자를 위한 안전 주행 보조 시스템","links":[],"tags":[],"content":"\n\n                  \n                  GitHub - 404Vector/App.DrivingSafetyAssistanceSystem: Monocular 3D Object Detection을 사용하여 초보 운전자가 혼자 도로를 주행할 때 끼어들기나 전방 차량과의 거리에 대한 위험을 알려주는 시스템 \n                  \n                \n\n\nMonocular 3D Object Detection을 사용하여 초보 운전자가 혼자 도로를 주행할 때 끼어들기나 전방 차량과의 거리에 대한 위험을 알려주는 시스템 - GitHub - 404Vector/App.\ngithub.com/404Vector/App.DrivingSafetyAssistanceSystem\n\n\n\n\nTeam Member\n\nMy Contribute\n외부 기여\n\n\n프로젝트 개요\n\n기획 배경\n문제에 대한 수요 설문\n문제 정의\n서비스 시나리오\n\n\n기술 선정\n\nWhat we do?\nWhy 3D Object Detection?\nWhy monocular?\nHow to define Rule base Danger Level\n\n\n수행 절차 및 방법\n\n프로젝트 진행 Pipeline\nKITTI - Pretraining Dataset\n강건한(Robust) 융합 센서 객체 인식 자율주행 데이터 - Finetuning Dataset\nModel 선정\nSMOKE (Single-Stage Monocular 3D Object Detection via Keypoint Estimation)\nMetric 정의 - DDS(Danger Detection Score)\nModel Serving\n\n\n결과\n\n실험 및 결과\nInference Time Check\nModel Deploy\n\nWeb Demo(A100)\nEdge Device(Jetson Xavier)\n\n\n\n\n결론 및 토의\n\n프로젝트 의의\n프로젝트의 한계점\n향후 목표\n\n\n\n\nTeam Member\n\n김형석 - [github.com/404Vector]\n이동훈 - [github.com/teedihuni]\n전지용 - [github.com/Jiyong-Jeon]\n정원국 - [github.com/jungwonguk]\n한상준 - [github.com/jphan32]\n\nMy Contribute\n\n팀 내 최대 Code Contributor\n자율주행 Data Analysis\nLidar to Camera Coordinate Converting\nVisualization(3D-2D Projection, BirdeyeView)\nInference Engine 개발\nStreamlit&amp;FastAPI를 사용한 Model Deploy(Web Demo)\nModel Train &amp; Inference\nBuilding a development environment(Server)\n\n외부 기여\n==✨OpenSource - MMdet3D의 local_visualizer.py의 버그를 수정 및 PR Merge [====Link====]==\n프로젝트 개요\n기획 배경\n운전 초보인 내겐 운전이 너무 힘들다! → 어떻게 하면 도움을 줄 수 있을까?\n\n브레이크 지금 밟아…?\n끼어들기 어떻게 대처하지?\n갑자기 사람이 튀어나오면 어쩌지..\n\n문제에 대한 수요 설문\n설문조사 참여자 : boostcamp AI Tech 캠퍼 80명 / 서울 소재 대학 재학생 60명\n\n\n\n\n초보 운전자의 운전 중 애로사항 중 급정거 및 끼어들기 대응이 약 60% 를 차지\n안전 주행 시스템의 이용 의향이 약 85% 로 수요가 높을 것으로 조사 됨\n\n문제 정의\n초보운전자의 사고 위험 향상 원인은?\n\n좁은 시야로 인한 상황 판단 미숙\n\n초보 운전자의 시야, 경력 운전자의 20% 수준\n좌우 탐색 시간은 경력 운전자의 25%에 불과\n\n\n차량의 거리 판단의 어려움\n\n전방차량과의 안전거리 확보 미숙\n\n\n\n서비스 시나리오\n\n초보 운전자가 차량에 탑승\n블랙박스를 주행 안전 모드로 설정\n주행 시작\n위험 상황 발생\n안전 시스템에 의해 단계별 경고 안내\n운전자는 경고를 확인하고 상황 판단\n\n\n기술 선정\nWhat we do?\n차량 검출\n\n거리 &amp; 방향 검출\n\n위험도 판정\n\n경고 전달\n\nWhy 3D Object Detection?\n거리, 방향 검출이 필요\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2D Detection3D Detection거리 탐지XO방향 탐지XOInference 속도빠름중간Task 난이도쉬움어려움\n\n2D Detection\n\n거리와 방향 같은 추가적인 정보\n\nDepth Estimation이나 장면 인식 등 추가 적인 Task 필요\n\n\n사용의 편의성을 떨어뜨리는 여러 조건 필요\n\n\n3D Detection\n\nObject의 좌표계 예측\n\nObject의 거리와 방향까지 함께 판단\n\n\nTask 난이도\n\n관련 공개 자료가 너무 적음\n기술들의 난이도가 높음\nTask에 대한 도메인 지식을 학습을 하는데 어려움\n\n\n\n\n\n\n2D Object detection\n\n3D Object detection\nWhy monocular?\n\nlidar센서 포함 Fusion 방식 vs Only Camera, Monocular 방식\n선정 Key Point\n\n서비스 Target (초보자)\n비용\n일정 수준의 성능\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLidarMonocularTarget 적합도낮음높음비용약 700만원약 30만원성능매우 높음중간Task 난이도높음높음\nHow to define Rule base Danger Level\n\n위험 상황의 정의 (모든 상황을 파악하기 어렵기 때문에 특정 상황으로 한정)\n\n옆 차선에서 끼어드는 경우\n앞 차와의 거리 조절이 안되는 경우\n\n\n위험 수준의 정의\n\n안전(Safe)\n조심(Warning)\n위험 (Danger)\n\n\n\n\n\n위험 측정 방식\n\n시속 60km 기준 : 제동거리 44m\nCase 1 : 옆 차선에서 끼어드는 경우\n\n50M 내 Yaw가 내 차선방향(±7º~30º)으로 들어올 때 → Warning\n25M 내 Yaw가 내 차선방향(±7º~30º)으로 들어올 때 → Danger\n\n\nCase 2 : 앞 차와의 거리 조절이 안되는 경우\n\n전방의 차량 간격이 50M 내로 줄어들 때 → Warning\n전방의 차량 간격이 25M 내로 줄어들 때 → Danger\n\n\n\n\n\n\n수행 절차 및 방법\n프로젝트 진행 Pipeline\n\nKITTI - Pretraining Dataset\n\n이미지 개수 : 14999장\n이미지 크기 : 1242 x 375\n데이터 구성 : streo, optical flow, visual odometry, 3D object Detection, 3D tracking\n데이터 특징 : 맑은 날씨\n클래스 구성 : car, pedestrian, cyclist, van, truck, tram, misc, person_sitting\n\n\n\nAnnotation 정보\n\nlabel (str) : 차, 보행자 등의 라벨 정보 문자열 (‘Car’, ‘Pedestrian’, …)\ntruncation (float) : 이미지상에서 물체가 잘려 있는 정도\nocclusion (int) : 폐섹 수준(camera 시야 기준으로 추측 됨), 물체가 가려진 정도\nalpha (float) : 관측각, 관측자(자율주행자동차) 기준 물체가 위치한 각도\nxmin (float) : image 상에서 물체를 감싸는 2d bbox의 left-x\nymin (float) : image 상에서 물체를 감싸는 2d bbox의 top-y\nxmax (float) : image 상에서 물체를 감싸는 2d bbox의 right-x\nymax (float) : image 상에서 물체를 감싸는 2d bbox의 bottom-y\nHeight (float) (y) : Camera 좌표계 상에서 물체의 높이(in meters)\nWidth (float) (z) : Camera 좌표계 상에서 물체의 너비(in meters)\nLength (float) (x) : Camera 좌표계 상에서 물체의 길이(in meters)\ntx (float) : Camera 좌표계 상에서 물체의 x(in meters)\nty (float) : Camera 좌표계 상에서 물체의 y(in meters)\ntz (float) : Camera 좌표계 상에서 물체의 z(in meters)\nry (float) : Camera 좌표계 상에서 물체의 yaw (pi:좌측 ~ 0:정면 ~ pi:우측)\n\n\n\n강건한(Robust) 융합 센서 객체 인식 자율주행 데이터 - Finetuning Dataset\n\n이미지 개수 : 360,000개\n이미지 크기 : 1920 X 1200\n데이터 구성 : Image , 2D 바운딩 박스, 2D 세그멘 테이션, 3D 바운딩 박스\n데이터 특징 : 2가지 촬영 시간대, 2가지 날씨, 5가지 도로 유형으로 매우 다양한 상황에서 촬영됨\n클래스 구성 : bicycle, bus, car, motorcycle, other vehicles, pedestrian, truck\n\n\n\nKitti Format 으로 Finetuning Dataset 변환\n\nAi-hub 좌표축을 KITTI 좌표축으로 변환\n시각화를 위한 Carlibration 수치 보정\nYaw(방향) 값 기준 통일\nClass 병합 (car, pedstrian, cyclist)\n\n\n최종 Camera 기준 Kitti format label Data 작성\n\n\nModel 선정\n\n선정 기준\n\nCamera Only로 사용 가능해야 함 → Monocular Model\nMMDetection 3D에서 사용 가능 → Smoke &amp; PGD\nReal Time Inference가 가능해야 함→ Inference Time 0.2s\n적정 수준의 성능\n\n\n\n\n→ SMOKE Model 최종 선택\nSMOKE (Single-Stage Monocular 3D Object Detection via Keypoint Estimation)\n2D bbox 예측하고 이를 바탕으로 3D bbox를 예측하는 기존의 방식과는 다르게 바로 3D bbox 예측\n\nOne-Stage architecture 제안\nmulti-step disentanglement approach 제안\n\n중심/크기/각도로 분리하여 3D bbox 예측\n\n\n\n\n\nMetric 정의 - DDS(Danger Detection Score)\n\n\n위험 기준 설정\n\n\n정면 차간 거리 (25m~50m)\n\n\n끼어들기 차량의 각도 (7도~30도)\n\n\n옆 차선의 차량과의 거리 (1.5m)\n\n\n\n\n경고 레벨에 따른 confusion matrix\n\n\n\nModel Serving\n\n\nWeb\n\n개발 목적 : Inference Engine 개발, 협업, 데모\n\n개발도중 Edge device 환경 구성에 문제 발생, Edge device 없이 Inference Engine 개발 필요\n원활한 협업을 위해 기존 개발 환경(ai-stages server)에서 바로 동작하고, 각자의 작업물을 병합하여 결과를 볼 수 있는 도구 필요\n다른 사람들에게 손쉽게 보여줄 수 있는 방법이 필요\n\n\n\n\nWeb Demo Flow\n\n\nInference Engine\n\nInference Engine Structure\n\nSet Engine Sequence\n\nRun Engine Sequence\n\n\nEdge Device (APP)\n\n개발 목적 : 프로젝트 실현 가능성 검증 , 향후 실험\n\n프로젝트의 실현 가능성을 확인하기 위해서는 Edge Device에서의 사용 가능 여부 검증 필요\n짧은 일정으로 실제 목표인 블랙박스에 Serving하지 못하는 상황, 그러나 가능성이 있는지 확인\n\n\n\n\n\n\nModel serving flow for edge divide(Jetson Xavier)\n\nInference flow in Jetson Xavier\n\n결과\n실험 및 결과\n\n→ Fine Tuning + Augmentation을 사용하여 프로젝트 목표에 맞게 결과를 도출\n\nNo1. Inference Image\n\nNo2. Inference Image\n\n\n\n\nInference Time Check\n\n\nCUDA 코어 개수 차이 10:1 / 소비 전력 차이 8:1\n\n→ 큰 차이에도 속도는 2배 정도 밖에 안 느려지지 않았고, 경량화 가능성 확인하였음\nModel Deploy\nWeb Demo(A100)\n\nLeft : only KITTI dataset / Right : Our Model(KITTI + Finetuning)\nEdge Device(Jetson Xavier)\n\n결론 및 토의\n프로젝트 의의\n\n자율주행 데이터셋을 MMDetection3D에 맞게 변환하여 모델 훈련\n정의한 Metric에 대한 성능 향상 실현\nInference Engine 개발 및 Web을 사용한 동작 확인\nEdge Device에서 Inference 수행 및 사용 가능성 확인\n\n프로젝트의 한계점\n\n새롭게 수집되는 data들을 사용한 지속적인 모델 개선 방법은 고려하지 못함\n너무 가깝거나 바로 옆에 있는 자동차의 경우는 잘 탐지하지 못하는 경우도 존재\nMonocular 모델 자체의 성능 개선 한계가 존재\nEdge Device에서 Inference하기 위해서는 경량화가 필요하지만 경량화 시도 및 그에 따른 성능 변화를 고려하지 못함\n\n향후 목표\n\nActive learning 또는 Self-supervised Learning을 통해 Data 관련 문제를 보완\n새롭게 정의한 Metric을 전문가의 피드백을 받은 후에 경고 자체를 학습하도록 구조 개선\n거리에 따라서 서로 다른 가중치를 주어 위험도를 학습하여 가까운 물체를 더 잘 탐지하도록 함\nKnowledge distillation, pseudo labeling 사용하여 모델의 성능 향상\n"},"vault/Notion/DB/DB-Blog-Post/최대가능도-추정법(MLE,-Maximum-likelihood-estimation)":{"slug":"vault/Notion/DB/DB-Blog-Post/최대가능도-추정법(MLE,-Maximum-likelihood-estimation)","filePath":"vault/Notion/DB/DB Blog Post/최대가능도 추정법(MLE, Maximum likelihood estimation).md","title":"최대가능도 추정법(MLE, Maximum likelihood estimation)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n최대가능도 추정법(MLE, Maximum likelihood estimation)\n\n\n참조\nWeek 1\n최대가능도 추정법(MLE, Maximum likelihood estimation)\n\n최대가능도 추정법(MLE, Maximum likelihood estimation)은 모수 theta가 주어졌을 때 데이터 X에 대한 가능도 함수(likelihood function)의 최대값을 찾는 것을 말한다.\n따라서 최대가능도 추정법은 확률 분포의 모수 theta를 추정하는 기법으로, 관측된 데이터의 가능도가 최대가 되는 모수값을 찾는 것이다.\n즉, MLE는 관찰된 데이터가 주어졌을 때, 모수 theta가 가장 잘 관찰된 데이터를 나타내는 값이 되는 값을 찾는 것이다.\n"},"vault/Notion/DB/DB-Blog-Post/표본(sample)":{"slug":"vault/Notion/DB/DB-Blog-Post/표본(sample)","filePath":"vault/Notion/DB/DB Blog Post/표본(sample).md","title":"표본(sample)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n표본(sample)\n\n\n참조\nnamu.wiki/w/통계학?from=모집단#s-2\nWeek 1\n표본(sample)\n\n모 집단에서 일부만 조사한 것\n"},"vault/Notion/DB/DB-Blog-Post/표본분산(sample-variance)":{"slug":"vault/Notion/DB/DB-Blog-Post/표본분산(sample-variance)","filePath":"vault/Notion/DB/DB Blog Post/표본분산(sample variance).md","title":"표본분산(sample variance)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n표본분산(sample variance)\n\n\n참조\nWeek 1\nWeek 1\n표본분산(sample variance)\n\n표본(sample)의 분산\n관측값에서 표본 평균을 빼고 제곱한 값을 모두 더한 것을 n-1로 나눈 값\n\ns^2 ={1 \\over {N-1}}{\\sum_{i=1}^{N}(X_i-\\bar X)}"},"vault/Notion/DB/DB-Blog-Post/표본편차(sample-standard-deviation-)":{"slug":"vault/Notion/DB/DB-Blog-Post/표본편차(sample-standard-deviation-)","filePath":"vault/Notion/DB/DB Blog Post/표본편차(sample standard deviation ).md","title":"표본편차(sample standard deviation )","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n표본편차(sample standard deviation )\n\n\n참조\nko.wikipedia.org/wiki/표준_편차\nWeek 1\n표본편차(sample standard deviation )\n\n표본의 표준 편차\n\ns =\\sqrt{{1 \\over {N-1}}{\\sum_{i=1}^{N}(X_i-\\bar X)}}"},"vault/Notion/DB/DB-Blog-Post/표본평균(Sample-mean)":{"slug":"vault/Notion/DB/DB-Blog-Post/표본평균(Sample-mean)","filePath":"vault/Notion/DB/DB Blog Post/표본평균(Sample mean).md","title":"표본평균(Sample mean)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n표본평균(Sample mean)\n\n\n참조\nen.wikipedia.org/wiki/Sample_mean_and_covariance\nWeek 1\n표본평균(Sample mean)\n\n표본 평균은 모집단에서 가져온 표본의 평균 값\n\nX \\in \\Omega \\\\ \\bar{X} = {1 \\over N}{\\sum_{i=1}^{N}X}"},"vault/Notion/DB/DB-Blog-Post/하이퍼볼릭-탄젠트(Hyperbolic-Tangent)":{"slug":"vault/Notion/DB/DB-Blog-Post/하이퍼볼릭-탄젠트(Hyperbolic-Tangent)","filePath":"vault/Notion/DB/DB Blog Post/하이퍼볼릭 탄젠트(Hyperbolic Tangent).md","title":"하이퍼볼릭 탄젠트(Hyperbolic Tangent)","links":["vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-1"],"tags":[],"content":"\n참조\n하이퍼볼릭 탄젠트(Hyperbolic Tangent)\n\n\n참조\ntaewan.kim/post/tanh_diff/\nWeek 1\n하이퍼볼릭 탄젠트(Hyperbolic Tangent)\n\nSigmoid의 대체제로 사용될 수 있는 활성화 함수\n출력 범위는 -1에서 1사이\n출력 범위가 더 넓고 경사면이 큰 범위가 더 크기 때문에 더 빠르게 수렴하여 학습\nSigmoid의 치명적인 단점인 Vanishing gradient problem 문제를 그대로 갖고 있음\n\ntanh(x)={{e^{x}-e^{-x}} \\over {e^{x}+e^{-x}}}"},"vault/Notion/HyeongSeok-Kim’s-Notion/About":{"slug":"vault/Notion/HyeongSeok-Kim’s-Notion/About","filePath":"vault/Notion/HyeongSeok Kim’s Notion/About.md","title":"About","links":[],"tags":[],"content":"Contact.\nEmail. tiryul@gmail.com\nIntroduce.\n안녕하세요, 저는 2년 차 머신러닝 엔지니어이자 5년 차 소프트웨어 엔지니어인 김형석입니다.\n저는 Machine Vision 분야에서 이미지 처리 기반 검사 알고리즘을 개발하며 소프트웨어 엔지니어로 활동하다가 AI 검사 솔루션을 접하면서 AI 기술에 관심을 갖게 되었습니다. 그 결과 네이버 커넥트 재단의 Boostcamp AI Tech 프로그램을 수료하고, 현재 영상 화질 개선 AI를 연구하는 팀에서 머신러닝 엔지니어로 활동하고 있습니다.\nSkill.\nPrograming Language\n\n\nPython\n\n\nC#\n\n\nC++\n\n\nML\n\nPyTorch\nTensorRT, Tirton, ONNX-Runtime\nAWS Inferentia, Furiosa Warboy\n\nMulti-Processing\n\n\nAsyncIO\n\n\nRay\n\n\ngRPC\n\n\nMLOps, CI/CD\n\nMLFlow\nDocker &amp; Docker-Compose\nGithub &amp; Github Action\n\nComputer Vision\n\n\nOpenCV\n\n\nMIL\n\n\nCognex\n\n\nect.\n\n\nPoetry, pipenv, pyenv\n\n\nFastAPI\n\n\nWPF, Streamlit\n\n\nWork Experience.\n4by4\nPixell Lab Manager / Video Enhance AI\nML Engineer\n==2023.04-현재\n(1년 2개월)\n\nPython / Asyncio / TensorRT, ONNX-Runtime, Furiosa, Inferentia /\nDocker, Docker-Compose / MLFlow / Github Action\n==Pixell Management System v3.0 Project(진행중)==\n\nML\n\n자사 SR 모델 변환 및 배포(torch → onnx → trt)\n\n\nMLOps\n\nLocal Cluster 구성\n\n사내 Server, IDC Server등을 통합하여 Ray Cluster 구성\nPrometheus 및 Grafana 연동\n\n\nInference Service의 각 모듈 패키지화, unittest 및 PyPI 배포 자동화\n\npms-inference-engine\npms-tensorrt, pms-nvidia-processor, pms-furiosa-processor\npms-model-manager, pms-ray-cluster\n\n\n모델 변환 및 배포 자동화\n\nCluster에 등록된 gpu type별로 서비스하는 모든 모델을 TensorRT 모델로 변환하고, 자사 mlflow model registry에 upload 하도록 자동화\n\n\n\n\n\n==Pixell Management System v2.0 Project==\n\nML\n\n==Inference Pipeline 재설계==\n\n기존 PMS의 문제점인 I/O 부하를 개선하기 위해 Inference Pipeline 전체를 다시 설계\nImage에 대한 I/O(528MBps)를 모두 제거하여 성능 개선\n30fps FHD 비디오 기준 필요 분당 Disk 용량을 373 MBps 단축\n\n\n==Inference Engine 개발==\n\n기존 Pipe방식의 통신 방식을 Ray기반 RPC로 변경하여 개발 편의성 향상\nInference Processor에 대한 가상화로 개발 확장성 증대\nAsync IO를 사용하여 비동기 처리를 수행할 수 있도록 하여 자원 사용 최적화 \n\n\nModel Manager 개발\n\nWorker node가 MLFlow와 연동하여 항상 최신버전의 모델을 사용 할 수 있도록 자동화, 배포 시간 단축\n\n\n\n\nMLOps\n\n==AWS Auto Scaling Cluster 구성==\n\nRay를 기반으로 사용 요청 시 동적으로 AWS GPU Instance 생성, 사용이 없을 시 자동으로 중지하여 비용 최적화\nRuntime 환경이 구성된 Docker Image 배포, Cluster Worker Node 생성 시간 단축\n\n\n\n\n\nAWS-Inferentia2 평가 Project\n\nML\n\n자사 Denoise 모델 변환\nInference Pipeling 구성, NPU Utilization 100% 기준 성능 평가\n\n\n\n==PMS On-Premise ver Project==\n\nMLOps\n\n기존 PMS 관련 모든 Service들을 병합하기 위해 개별 Docker Image 생성\nDocker Compose를 사용하여 8개의 Micro Service를 Integration\n\n\nBusiness\n\nVideo Enhance AI를 On-Premise로 사용하길 고객이 원함, 고객과 소통을 통해 SaaS형 솔루션인 PMS를 On-Premise로 사용하기를 제안\n미국 고객사 Server에 배포하는 방식으로 납품 완료\n\n\n\n==Pixell Management System v1.0 Project==\n\nML\n\nDPIR, AnimeSR 모델 코드 해석 및 변환(torch → onnx → trt)\nGPU용 Denoise &amp; Super Resolution Inference Pipeline 개발\n==실시간 처리를 위한 속도 최적화==\n\n총 I/O가 약 528 MBps로 너무 높아 Disk Interface가 변경 되야 함을 제시, 검증, 적용\nPython의 GIL로 인해 Multi-Processing으로 Inference Engine의 Architecture가 변경되야 함을 제시, 검증, 적용 \nHeap Memory Allocation을 통해 DPIR 기준 56ms의 전/후처리 시간을 21ms로 줄여 64%의 성능 개선\nGIL의 문제를 회피하기 위해 GPU 당 Process를 2로 증가시켜 DPIR 기준 44% 성능 개선\nTensorRT를 추가로 적용하여 DPIR 기준 Inference Time 770% 가속\n\n\n\n\nMLOps\n\nGPU, NPU용 개별 Docker Runtime Image 생성, 배포 시간 단축\nInference Service 배포(IDC Server, Local NPU Server, Kakao Cloud NPU Server)\n\n\n\n==AI Image Editor 개발 Project==\n\nCooperation\n\n사내 타 부서 서비스 팀과 협업, AI Image Editor 기획\n인력 구성, 스케줄 등을 고려해 Segment Anything 및 LaMa 모델 선정\n\n\nML\n\n==Object Selection을 위해 Meta AI의 Segment Anything 구조 해석==\n==Object Remove를 위해 LaMa 구조 해석==\n\n\nSW\n\nFastAPI로 Inference를 위한 Backend 개발\n\n\n\n==AWS MLFlow Service 구축==\n\nMLOps\n\nMLFlow를 이용하여 Metric tracking 및 버전 관리\ngithub의 source가 update 되었을 시, AWS에 배포한 시스템이 자동으로 update 되도록 자동화\n\n\n\nFST\n연구소 Inspection 1팀 대리 / 반도체 검사장비 R&amp;D\nAlgorithm &amp; AI Engineer\n==2022.04-2022.06\n(3개월)\n협의된 업무와 실제 업무가 달라 퇴사\n\n.NET / WPF / teleik\nWafer Inspection Algorithm 및 설비 운용 SW 개발\n\nSW\n\nImage Display Control Interface 정의\nSingle Image Display, Tile Image Display 등 기본 Control 개발\n가상화 및 동적 로딩을 통한 초 고해상도 Wafer Image(18GB)의 Display Control 개발\n\n\nInfra\n\nOn-premise gitlab 서버 구성, 팀 내 SCM 환경 제공\n\n\n\nPIE\nSW 개발팀 대리 / 이차전지 검사모듈 R&amp;D\nAlgorithm &amp; SW Engineer\n==2021.05-2022.03\n(11개월)\n커리어 전환(AI)을 위해 이직\n\n.NET / Image Processing / Cognex / OpenCV / WPF\nPhotometric Stereo 기법을 이용한 형상 추출 알고리즘 개발\n\nAlgorithm\n\n4 Directions 광학계를 이용한 형상 추출 알고리즘 개발 완료 및 실제 상용 제품(LumiTrax)과 비교 및 검증 완료\n3 Directions 및 6 Directions 광학계를 이용한 형상 추출 알고리즘 개발 완료\n\n\nSW\n\nC# 기반 연산결과 Mapping을 통한 프로세싱 시간 50% 단축 (2MB 기준 400ms → 200ms)\nC++ 기반 알고리즘 구현 및 최적화를 통한 프로세싱 시간 80% 단축 (2MB 기준 200ms → 40ms)\nHW적 결함으로 인한 Camera 위치 차이를 보정하기 위한 Calibration Program 개발\n\n\nHW\n\n3색광을 사용한 광학계 구성을 위한 지그 설계 및 제작(3D CAD &amp; 3D Printer)\n\n\n\n차세대 C# Framework 개발\n\nSW\n\nC# WPF 기반 검사 Application 기본 패턴 정의(MVVM)\nImage Display Control 등 기본 Control 개발\nGrabber 공용 Interface 정의 및 Virtual Grabber 개발\n\n\n\nWashing Top 검사기 개발(원통형 2차전지 상부 검사)\n\nAlgrithm\n\nScratch, Dent, Contamination 검사 알고리즘 개발\n기존 검토에 사용된 Cognex 라이브러리를 OpenCV로 대체, 원가절감\n\n\nSW\n\nMain Application 개발(WPF, .NET Framework 4.8)\nInspection Result Viewer 개발 및 Main Application과 IPC 기능 개발\nResult Data Viewer 개발 및 DB 연동\n\n\nBusiness\n\nUI 수정 및 신규 기능 추가, 검사 대상 변경, 검사 정확도 증명, Result Viewer 추가 개발 등 고객 요구 대응\n고객사 납품 및 양산화\n\n\n\nJelly Roll Bottom 검사기 개발(원통형 2차전지 젤리롤 하부 검사)\n\nAlgorithm\n\nDent, Short 검사 알고리즘 개발\n\n\nSW\n\nMain Application 개발(WPF, .NET Framework 4.8)\nResult Data Viewer 개발 및 DB 연동\n\n\nBusiness\n\nUI 수정 및 신규 기능 추가, 기존에 없던 신규 검사 항목 추가 등 고객 요구 대응\n고객사 납품 및 양산화\n\n\n\nHYC Korea\nInspection 팀 사원 / 반도체&amp;디스플레이 검사장비 R&amp;D\nAlgorithm &amp; SW Engineer\n==2019.08-2021.05\n(1년 10개월)\n회사 경영 악화로 이직\n\nc++ / Image Processing / MIL / OpenCV\nOLED Display Camera Hole 검사기\n\nAlgorithm\n\nCamera hole edge defect 검사 알고리즘 개발\n\n\nSW\n\nArea Camera 및 Frame Grabber 제어 기능 개발\n\n\nBusiness\n\n검사 기술서 정의를 위해 이미지 상 결함 추정 부위들을 정리 및 분류, 회사 동료들과의 협업을 통해 필요 검사 항목 도출\n결함에 대한 검사 알고리즘 제안 및 고객사 납품 승인\n고객사 납품 및 양산 대응\n\n\n\nPattern Wafer 검사기 개발\n\nAlgorithm\n\nDie to Die surface defect 검사 알고리즘 개발\nEdge profile defect 검사 알고리즘 개발\n\n\nSW\n\nTDI LineScan Camera &amp; Grabber 제어 기능 개발\n검사 시뮬레이션 SW 개발\n\n\nBusiness\n\n\n고객사 납품\n\n\nStage &amp; Optic Spec 검토\n\n\n\n\nEducation.\n\n\n2017.03 ~ 2019.02 강원대학교 일반대학원, 기계융합공학과 - 공학 석사\n\n디지털 판서(태블릿, 전자 칠판 등을 사용한 쓰기)에 거부감을 갖는 사람들에게 아날로그적인 판서 방법으로 디지털 판서를 수행할 수 있는 방법을 연구\nArUco Maker Detection을 이용한 Camea 내 Display 영역 검출 및 Homography 계산\nPC 영상과 동일해지도록 정합하여 차영상을 계산하여 판서 요소만을 추출하고 PC영상에 합성하여 아날로그적인 판서방법(보드마카)를 사용해 디지털 판서를 가능하게 하는 시스템을 개발\n\nInteractive whiteboard 판서를 위한 차영상 기반의 영상처리 시스템 개발\n\n\n2013.03 ~ 2017.02 강원대학교,기계메카트로닉스공학과 - 공학 학사\n\n\nOther Experience.\n2024.04-2024.06\n2024.01-2024.02\n2018.10-2018.10\n2017.05-2018.02\n2016.12-2017.02\n2016.09-2016.12\n2015.03-2016.02\n\n\n네이버 커넥트재단 부스트클래스 AI 엔지니어 기초 다지기 2기 코치\n\n\n코드 리뷰 및 멘토링 진행\n\n\n\n\n네이버 커넥트재단 부스트클래스 AI 엔지니어 기초 다지기 1기 코치\n\n\n코드 리뷰 및 멘토링 진행\n\n\n\n\n강원중학교 자율학기 강사\n\n\n전자회로 이론 및 실습 강의를 진행\n\n\n\n\n(주)새울의 영상처리 기반 전자교탁 설계과제 참여\n\nOpenCV(Linux C++, Raspberry Pi)기반 영상처리 프로그램 개발\n특허등록 : 쌍안시 영상처리를 이용한 전자 교탁 시스템\n\n\n등록번호 : 1020073240000\n\n\n\n\n\n\n강원대학교 창업동아리 Hard Core 활동\n\n\nSmart Study Care Desk를 설계 및 제작\n\n\n\n\n(주)BT의 제세동 훈련장치 시제품 설계과제 참여\n\n\n제세동 훈련을 위한 안드로이드 App 개발\n\n\n\n\n강원대학교 레고전문인력양성 사업단 참여\n\nEV3를 이용한 다양한 로봇 설계 및 제작\n레고야 놀자 성과회 개최\n특허등록 : 거북선 트랜스포머\n\n등록번호 : 3008642950000\n\n\n\n\n"},"vault/Notion/HyeongSeok-Kim’s-Notion/HyeongSeok-Kim’s-Notion":{"slug":"vault/Notion/HyeongSeok-Kim’s-Notion/HyeongSeok-Kim’s-Notion","filePath":"vault/Notion/HyeongSeok Kim’s Notion/HyeongSeok Kim’s Notion.md","title":"HyeongSeok Kim’s Notion","links":["vault/Notion/DB/DB-Blog-Post/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding","vault/Notion/DB/DB-Blog-Post/OpenEXR-Python","vault/Notion/DB/DB-Blog-Post/Video2Video","vault/Notion/DB/DB-Blog-Post/2023년-회고-ML-Engineer로서의-첫걸음","vault/Notion/DB/DB-Blog-Post/AWS-ECS+S3로-MLFlow-구축해보기-1/AWS-ECS+S3로-MLFlow-구축해보기-1","vault/Notion/DB/DB-Blog-Post/Rust-Study","vault/Notion/DB/DB-Blog-Post/ONNX-Runtime은-무엇일까/ONNX-Runtime은-무엇일까","vault/Notion/DB/DB-Blog-Post/ONNX(Open-Neural-Network-Exchange)란/ONNX(Open-Neural-Network-Exchange)란","vault/Notion/DB/DB-Blog-Post/DPIR---Plug-and-Play-Image-Restoration-with-Deep-Denoiser-Prior(작성중)","vault/Notion/DB/DB-Blog-Post/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution","vault/Notion/DB/DB-Blog-Post/Photometric-Stereo를-사용한-형상추출-알고리즘-개발/Photometric-Stereo를-사용한-형상추출-알고리즘-개발","vault/Notion/DB/DB-Blog-Post/Programmers---공원-산책","vault/Notion/DB/DB-Blog-Post/DockerERROR--Unexpected-bus-error-encountered-in-worker.-This-might-be-caused-by-insufficient-shared-memory-(shm)","vault/Notion/DB/DB-Blog-Post/Programmers---숫자의-표현","vault/Notion/DB/DB-Blog-Post/Programming-language-Dart-2-OOP","vault/Notion/DB/DB-Blog-Post/Programming-language-Dart-1-기본기/Programming-language-Dart-1-기본기","vault/Notion/DB/DB-Blog-Post/Programmers---햄버거-만들기","vault/Notion/DB/DB-Blog-Post/Programmers---옹알이(2)","vault/Notion/DB/DB-Blog-Post/Programmers---숫자-짝궁","vault/Notion/DB/DB-Blog-Post/Programmers---가장-가까운-글자","vault/Notion/DB/DB-Blog-Post/Programmers---크기가-작은-부분-문자열","vault/Notion/DB/DB-Blog-Post/Programmers---개인정보-수집-유효기간","vault/Notion/DB/DB-Blog-Post/Programmers---둘만의-암호","vault/Notion/DB/DB-Blog-Post/Programmers---카드뭉치","vault/Notion/DB/DB-Blog-Post/Programmers---콜라-문제","vault/Notion/DB/DB-Blog-Post/Programmers---대충-만든-자판","vault/Notion/DB/DB-Blog-Post/Programmers---덧칠하기","vault/Notion/DB/DB-Blog-Post/2D-Object-Detection/2D-Object-Detection","vault/Notion/DB/DB-Blog-Post/Programmers---리코쳇-로봇","vault/Notion/DB/DB-Blog-Post/Programmers---당구-연습","vault/Notion/DB/DB-Blog-Post/Machine-Learning-Notation(Shan-Hung-Wu)-번역","vault/Notion/DB/DB-Blog-Post/Kaggle---Intro-to-Machine-Learning-번역","vault/Notion/DB/DB-Blog-Post/Programmers---바탕화면-정리","vault/Notion/DB/DB-Blog-Post/Decision-Tree/Decision-Tree","vault/Notion/DB/DB-Blog-Post/Programmers---단어-변환","vault/Notion/DB/DB-Blog-Post/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)","vault/Notion/DB/DB-Blog-Post/가중치-초기화(Weight-Initialization)가-필요한-이유와-Xavier--and--He-초기화/가중치-초기화(Weight-Initialization)가-필요한-이유와-Xavier--and--He-초기화","vault/Notion/DB/DB-Blog-Post/StyleGAN--A-Style-Based-Generator-Architecture-for-GANs/StyleGAN--A-Style-Based-Generator-Architecture-for-GANs","vault/Notion/DB/DB-Blog-Post/RetinaNet/RetinaNet","vault/Notion/DB/DB-Blog-Post/SSD(Single-Shot-Multibox-Detector)/SSD(Single-Shot-Multibox-Detector)","vault/Notion/DB/DB-Blog-Post/Yolo-v1(You-Only-Look-Once)/Yolo-v1(You-Only-Look-Once)","vault/Notion/DB/DB-Blog-Post/BiFPN(Neck,-EfficientDet)/BiFPN(Neck,-EfficientDet)","vault/Notion/DB/DB-Blog-Post/DetectroRS/DetectroRS","vault/Notion/DB/DB-Blog-Post/PANet(Path-Aggregation-Network)/PANet(Path-Aggregation-Network)","vault/Notion/DB/DB-Blog-Post/FPN(Feature-Pyramid-Network)/FPN(Feature-Pyramid-Network)","vault/Notion/DB/DB-Blog-Post/Neck/Neck","vault/Notion/DB/DB-Blog-Post/NMS(Non-Maximum-Suppression)","vault/Notion/DB/DB-Blog-Post/RPN(Region-Proposal-Network)/RPN(Region-Proposal-Network)","vault/Notion/DB/DB-Blog-Post/Object-Detection-History(2001~2022)/Object-Detection-History(2001~2022)","vault/Notion/DB/DB-Blog-Post/GAN(Vanilla-GAN)/GAN(Vanilla-GAN)","vault/Notion/DB/DB-Blog-Post/MLOps란-무엇일까/MLOps란-무엇일까","vault/Notion/DB/DB-Blog-Post/Window-10-OpenSSH-서버-활성화-및-실행하기","vault/Notion/DB/DB-Blog-Post/EDA(Exploratory-Data-Analysis,-탐색적-데이터-분석)는-무엇일까","vault/Notion/DB/DB-Blog-Post/Simple-ML-Flow-vs-Competition-Flow","vault/Notion/DB/DB-Blog-Post/Super-resolution-GAN/Super-resolution-GAN","vault/Notion/DB/DB-Blog-Post/Conditional-GAN(cGAN)/Conditional-GAN(cGAN)","vault/Notion/DB/DB-Blog-Post/Analysis-of-model-behaviors/Analysis-of-model-behaviors","vault/Notion/DB/DB-Blog-Post/Two-stage-detector-vs-One-stage-detector/Two-stage-detector-vs-One-stage-detector","vault/Notion/DB/DB-Blog-Post/Receptive-Field(수용필드,-수용장)/Receptive-Field(수용필드,-수용장)","vault/Notion/DB/DB-Blog-Post/Swin-Transformer/Swin-Transformer","vault/Notion/DB/DB-Blog-Post/GoogLeNet(Inception-v1)/GoogLeNet(Inception-v1)","vault/Notion/DB/DB-Blog-Post/Knowledge-distillation/Knowledge-distillation","vault/Notion/DB/DB-Blog-Post/Regularization---Batch-normalization/Regularization---Batch-normalization","vault/Notion/DB/DB-Blog-Post/Regularization---Dropout/Regularization---Dropout","vault/Notion/DB/DB-Blog-Post/Regularization---Label-Smoothing","vault/Notion/DB/DB-Blog-Post/Regularization---Data-Augmentation","vault/Notion/DB/DB-Blog-Post/Regularization---Early-Stopping","vault/Notion/DB/DB-Blog-Post/Optimization에서-중요한-것들/Optimization에서-중요한-것들","vault/Notion/DB/DB-Blog-Post/DeepLearning에서-가장-중요한-아이디어들(denny-britz,-2020-07-29)","vault/Notion/DB/DB-Blog-Post/Loss-Function-for-task","vault/Notion/DB/DB-Blog-Post/torch.no_gard()의-역할","vault/Notion/DB/DB-Blog-Post/BCELoss(Binary-Cross-Entroby-Loss)","vault/Notion/DB/DB-Blog-Post/Cross-Entropy-Loss","vault/Notion/DB/DB-Blog-Post/Metric(in-machine-learning)","vault/Notion/DB/DB-Blog-Post/Transfer-Learning/Transfer-Learning","vault/Notion/DB/DB-Blog-Post/Pytorch-Performance-Tuning-Practices/Pytorch-Performance-Tuning-Practices","vault/Notion/DB/DB-Blog-Post/PyTorch-딥러닝-학습의-기본-순서","vault/Notion/DB/DB-Blog-Post/optimizer.zero_grad()은-무엇일까","vault/Notion/DB/DB-Blog-Post/torch.nn.Parameter은-무엇일까","vault/Notion/DB/DB-Blog-Post/torch.nn.Module은-무엇일까","vault/Notion/DB/DB-Blog-Post/torch.tensor의-requires_grad-param의-기능","vault/Notion/DB/DB-Blog-Post/torch.mm-vs-torch.matmul-차이점/torch.mm-vs-torch.matmul-차이점","vault/Notion/DB/DB-Blog-Post/numpy.ndarray.view와-numpy.ndarray.reshape의-차이","vault/Notion/DB/DB-Blog-Post/자동-미분(Autograd)","vault/Notion/DB/DB-Blog-Post/Computational-Graph/Computational-Graph","vault/Notion/DB/DB-Blog-Post/교착상태","vault/Notion/DB/DB-Blog-Post/최대가능도-추정법(MLE,-Maximum-likelihood-estimation)","vault/Notion/DB/DB-Blog-Post/가능도-함수(Likelihood-function,-likelihood,-우도-함수,-우도)","vault/Notion/DB/DB-Blog-Post/표본편차(sample-standard-deviation-)","vault/Notion/DB/DB-Blog-Post/표본분산(sample-variance)","vault/Notion/DB/DB-Blog-Post/표본평균(Sample-mean)","vault/Notion/DB/DB-Blog-Post/하이퍼볼릭-탄젠트(Hyperbolic-Tangent)","vault/Notion/DB/DB-Blog-Post/RNN(Recurrent-Neural-Networks)/RNN(Recurrent-Neural-Networks)","vault/Notion/DB/DB-Blog-Post/주변-분포(Marginal-Probability-Distribution)","vault/Notion/DB/DB-Blog-Post/RNN-내부에서-sigmoid-또는-tanh를-쓰는-이유","vault/Notion/DB/DB-Blog-Post/Sample-Distribution-vs-Sampling-Distribution","vault/Notion/DB/DB-Blog-Post/모수적(parametric)-방법론","vault/Notion/DB/DB-Blog-Post/모수","vault/Notion/DB/DB-Blog-Post/기댓값(expectation)","vault/Notion/DB/DB-Blog-Post/모-평균(population-mean,-μ)","vault/Notion/DB/DB-Blog-Post/표본(sample)","vault/Notion/DB/DB-Blog-Post/모집단(population)","vault/Notion/DB/DB-Blog-Post/마르코프-연쇄-몬테카를로(MCMC,-Markov-Chain-Monte-Carlo)","vault/Notion/DB/DB-Blog-Post/몬테카를로-방법/몬테카를로-방법","vault/Notion/DB/DB-Blog-Post/샘플링(Samiling)","vault/Notion/DB/DB-Blog-Post/범함수(functional)","vault/Notion/DB/DB-Blog-Post/기대값","vault/Notion/DB/DB-Blog-Post/조건부-확률","vault/Notion/DB/DB-Blog-Post/이산형-확률변수와-연속형-확률변수","vault/Notion/DB/DB-Blog-Post/선형회귀의-목적식","vault/Notion/DB/DB-Blog-Post/Norm,-L1-Norm,-L2-Norm","vault/Notion/DB/DB-Blog-Post/Quick-Sort/Quick-Sort","vault/Notion/DB/DB-Blog-Post/ViT/ViT","vault/Notion/DB/DB-Blog-Post/ResNet/ResNet","vault/Notion/DB/DB-Blog-Post/Pix2Pix/Pix2Pix","vault/Notion/DB/DB-Blog-Post/메모리-관리-전략은-무엇일까","vault/Notion/DB/DB-Blog-Post/가상-메모리는-무엇일까/가상-메모리는-무엇일까","vault/Notion/DB/DB-Blog-Post/Multi-process-vs-Multi-thread","vault/Notion/DB/DB-Blog-Post/Program-vs-Process-vs-Thread/Program-vs-Process-vs-Thread","vault/Notion/DB/DB-Blog-Post/Heap이란-무엇일까/Heap이란-무엇일까","vault/Notion/DB/DB-Blog-Post/Binary-Tree란-무엇일까/Binary-Tree란-무엇일까","vault/Notion/DB/DB-Blog-Post/Tree란-무엇일까","vault/Notion/DB/DB-Blog-Post/Graph란-무엇일까","vault/Notion/DB/DB-Blog-Post/CycleGAN/CycleGAN","vault/Notion/DB/DB-Blog-Post/Open-Source-Contiribution","vault/Notion/DB/DB-Blog-Post/VGGNet/VGGNet","vault/Notion/DB/DB-Blog-Post/AlexNet/AlexNet","vault/Notion/DB/DB-Blog-Post/Monocular-3D-Object-Detection을-위한-KITTI-Dataset의-이해(작성중)/Monocular-3D-Object-Detection을-위한-KITTI-Dataset의-이해(작성중)","vault/Notion/DB/DB-Blog-Post/초보-운전자를-위한-안전-주행-보조-시스템/초보-운전자를-위한-안전-주행-보조-시스템","vault/Notion/DB/DB-Blog-Post/Text-Region-검출기/Text-Region-검출기","vault/Notion/DB/DB-Blog-Post/마스크,-성별,-나이-분류기/마스크,-성별,-나이-분류기","vault/Notion/DB/DB-Blog-Post/mIoU(Mean-Intersection-over-Union)","vault/Notion/DB/DB-Blog-Post/NAVER-Connect-재활용-품목-분류를-위한-Semantic-Segmentation/NAVER-Connect-재활용-품목-분류를-위한-Semantic-Segmentation","vault/Notion/DB/DB-Blog-Post/DetEval","vault/Notion/DB/DB-Blog-Post/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선","vault/Notion/DB/DB-Blog-Post/NAVER-Connect-재활용-품목-분류를-위한-Object-Detection/NAVER-Connect-재활용-품목-분류를-위한-Object-Detection","vault/Notion/DB/DB-Blog-Post/NAVER-Connect-마스크-착용-상태-분류/NAVER-Connect-마스크-착용-상태-분류","vault/Notion/DB/DB-Blog-Post/Github-잔디-Notion에-Embed하기","vault/Notion/DB/DB-Blog-Post/pipenv-설치-및-사용","vault/Notion/DB/DB-Blog-Post/venv-설치-및-사용-방법","vault/Notion/DB/DB-Blog-Post/pyenv-설치-및-사용-방법","vault/Notion/DB/DB-Blog-Post/Linux-Command-Collection","vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기","vault/Notion/DB/DB-Blog-Post/ML-Flow---Tracking-Server-Docker-Image-만들기","vault/Notion/DB/DB-Blog-Post/ML-Flow---기본-사용/ML-Flow---기본-사용","vault/Notion/DB/DB-Blog-Post/DACON-예술작품-화가-분류-AI-경진대회/DACON-예술작품-화가-분류-AI-경진대회","vault/Notion/DB/DB-Blog-Post/Tmux-설치-및-사용법","vault/Notion/DB/DB-Blog-Post/유클리드-호제법(Euclidean-algorithm)","vault/Notion/DB/DB-Blog-Post/그리디-알고리즘--최소-신장-트리(MST,-Minimum-Spanning-Tree)","vault/Notion/DB/DB-Blog-Post/그리디-알고리즘(Greedy-Algorithm)","vault/Notion/DB/DB-Blog-Post/깊이-우선-탐색(DFS,-Depth-First-Search)","vault/Notion/DB/DB-Blog-Post/그래프와-트리(Tree-and-Graph)","vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation--nPr-순열-구하기","vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation-사용법","vault/Notion/DB/DB-Blog-Post/순열(Permutation)","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Naver-Connect---Boostcamp-AI-Tech-4기","vault/Notion/HyeongSeok-Kim’s-Notion/About"],"tags":[],"content":"DB.Blog.Post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이름TypeLast EditTagTags담당자상태생성일LayoutLM- Pre-training of Text and Layout for Document Image UnderstandingProject2024년 10월 25일 오전 10:44Document Classification, Information ExtractionML &amp; MLOpsKim HyeongSeok진행 중2024년 6월 28일 오전 11:03OpenEXR-PythonProject2024년 10월 25일 오전 10:44Python Package시작 전2024년 6월 28일 오전 10:34Video2VideoProject2024년 10월 25일 오전 10:44Python Package시작 전2024년 6월 28일 오전 10:182023년 회고-ML Engineer로서의 첫걸음Post2024년 10월 25일 오전 10:44etc시작 전2024년 1월 7일 오후 10:55AWS-ECS+S3로 MLFlow 구축해보기 1Post2024년 10월 25일 오전 10:44ML&amp;MLOps Engineering시작 전2023년 7월 2일 오후 10:32Rust StudyPost2024년 10월 25일 오전 10:44etc시작 전2023년 6월 26일 오후 12:40ONNX-Runtime은 무엇일까Post2024년 10월 25일 오전 10:44ML&amp;MLOps Engineering시작 전2023년 6월 18일 오후 2:22ONNX(Open Neural Network Exchange)란Post2024년 10월 25일 오전 10:44ML&amp;MLOps Engineering시작 전2023년 6월 4일 오후 8:43DPIR - Plug-and-Play Image Restoration with Deep Denoiser Prior(작성중)Post2024년 10월 25일 오전 10:44Image Denoise시작 전2023년 5월 4일 오전 10:30DBPN - Deep Back-Projection Networks for Single Image Super-resolutionPost2024년 10월 25일 오전 10:44Image Super Resolution시작 전2023년 4월 28일 오전 10:13Photometric Stereo를 사용한 형상추출 알고리즘 개발Project2024년 10월 25일 오전 10:443D Surface Normal Estimation시작 전2023년 3월 28일 오후 9:24Programmers - 공원 산책Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 24일 오후 7:28DockerERROR- Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm)Post2024년 10월 25일 오전 10:44ML&amp;MLOps Engineering시작 전2023년 3월 23일 오후 9:37Programmers - 숫자의 표현Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 22일 오후 5:50Programming language Dart 2 OOPPost2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 7:25Programming language Dart 1 기본기Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 6:21Programmers - 햄버거 만들기Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 1:18Programmers - 옹알이(2)Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 12:57Programmers - 숫자 짝궁Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 12:45Programmers - 가장 가까운 글자Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:42Programmers - 크기가 작은 부분 문자열Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:38Programmers - 개인정보 수집 유효기간Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:35Programmers - 둘만의 암호Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:15Programmers - 카드뭉치Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:03Programmers - 콜라 문제Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 8:59Programmers - 대충 만든 자판Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 5:25Programmers - 덧칠하기Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 2:092D Object DetectionPost2024년 10월 25일 오전 10:44Classification시작 전2023년 3월 17일 오후 9:09Programmers - 리코쳇 로봇Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 17일 오후 9:02Programmers - 당구 연습Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 17일 오후 7:46Machine Learning Notation(Shan-Hung Wu) 번역Post2024년 10월 25일 오전 10:44ML&amp;DL Common시작 전2023년 3월 13일 오후 8:50Kaggle - Intro to Machine Learning 번역Post2024년 10월 25일 오전 10:44ML&amp;DL Common시작 전2023년 3월 11일 오전 11:43Programmers - 바탕화면 정리Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 10일 오후 10:23Decision TreePost2024년 10월 25일 오전 10:44Computer Science시작 전2023년 3월 10일 오후 6:50Programmers - 단어 변환Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 10일 오후 3:35Basic Optimizer &amp; Adam(Adaptive Moment Esimation)Post2024년 10월 25일 오전 10:44ML&amp;DL Common시작 전2023년 3월 6일 오후 10:23가중치 초기화(Weight Initialization)가 필요한 이유와 Xavier &amp; He 초기화Post2024년 10월 25일 오전 10:44ML&amp;DL Common시작 전2023년 3월 5일 오후 2:44StyleGAN- A Style-Based Generator Architecture for GANsPost2024년 10월 25일 오전 10:44GAN시작 전2023년 2월 27일 오후 2:19RetinaNetPost2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 26일 오후 9:49SSD(Single Shot Multibox Detector)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 26일 오후 9:16Yolo v1(You Only Look Once)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 25일 오전 11:22BiFPN(Neck, EfficientDet)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 25일 오전 11:06DetectroRSPost2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 25일 오전 10:23PANet(Path Aggregation Network)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 25일 오전 12:10FPN(Feature Pyramid Network)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 11:50NeckPost2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 11:43NMS(Non-Maximum Suppression)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 11:37RPN(Region Proposal Network)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 11:14Object Detection History(2001~2022)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 8:48GAN(Vanilla GAN)Post2024년 10월 25일 오전 10:44GAN시작 전2023년 2월 24일 오후 12:58MLOps란 무엇일까Post2024년 6월 27일 오후 3:39ML&amp;MLOps Engineering시작 전2023년 2월 23일 오전 10:40Window 10 OpenSSH 서버 활성화 및 실행하기Post2024년 6월 27일 오후 3:37etc시작 전2023년 2월 23일 오전 10:38EDA(Exploratory Data Analysis, 탐색적 데이터 분석)는 무엇일까Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 23일 오전 10:36Simple ML Flow vs Competition FlowPost2024년 6월 27일 오후 3:39ML&amp;MLOps Engineering시작 전2023년 2월 23일 오전 10:33Super resolution GANPost2024년 6월 27일 오후 3:26GAN시작 전2023년 2월 23일 오전 10:23Conditional GAN(cGAN)Post2024년 6월 27일 오후 3:26GAN시작 전2023년 2월 23일 오전 10:22Analysis of model behaviorsPost2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 23일 오전 10:16Two-stage detector vs One-stage detectorPost2024년 6월 27일 오후 3:252D Object Detection시작 전2023년 2월 23일 오전 10:14Receptive Field(수용필드, 수용장)Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 23일 오전 10:11Swin TransformerPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 23일 오전 10:10GoogLeNet(Inception v1)Post2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 22일 오후 7:36Knowledge distillationPost2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 3:31Regularization - Batch normalizationPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:57Regularization - DropoutPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:54Regularization - Label SmoothingPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:38Regularization - Data AugmentationPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:35Regularization - Early StoppingPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:30Optimization에서 중요한 것들Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 2:20DeepLearning에서 가장 중요한 아이디어들(denny britz, 2020-07-29)Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 2:19Loss Function for taskPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:18torch.no_gard()의 역할Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 2:16BCELoss(Binary Cross Entroby Loss)Post2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:15Cross Entropy LossPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:14Metric(in machine learning)Post2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:12Transfer LearningPost2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 2:11Pytorch Performance Tuning PracticesPost2024년 6월 27일 오후 3:39ML&amp;MLOps Engineering시작 전2023년 2월 22일 오후 1:51PyTorch 딥러닝 학습의 기본 순서Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:47optimizer.zero_grad()은 무엇일까Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:44torch.nn.Parameter은 무엇일까Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:43torch.nn.Module은 무엇일까Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:41torch.tensor의 requires_grad param의 기능Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:40torch.mm vs torch.matmul 차이점Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:35numpy.ndarray.view와 numpy.ndarray.reshape의 차이Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:34자동 미분(Autograd)Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:24Computational GraphPost2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:22교착상태Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 22일 오후 1:08최대가능도 추정법(MLE, Maximum likelihood estimation)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:06가능도 함수(Likelihood function, likelihood, 우도 함수, 우도)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:04표본편차(sample standard deviation )Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:04표본분산(sample variance)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:03표본평균(Sample mean)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:01하이퍼볼릭 탄젠트(Hyperbolic Tangent)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:00RNN(Recurrent Neural Networks)Post2024년 6월 27일 오후 3:26NLP시작 전2023년 2월 22일 오전 11:53주변 분포(Marginal Probability Distribution)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:47RNN 내부에서 sigmoid 또는 tanh를 쓰는 이유Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:45Sample Distribution vs Sampling DistributionPost2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:42모수적(parametric) 방법론Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:41모수Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:40기댓값(expectation)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:39모 평균(population mean, μ)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:38표본(sample)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:38모집단(population)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:37마르코프 연쇄 몬테카를로(MCMC, Markov Chain Monte Carlo)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:34몬테카를로 방법Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:33샘플링(Samiling)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:32범함수(functional)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:31기대값Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:30조건부 확률Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:29이산형 확률변수와 연속형 확률변수Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:25선형회귀의 목적식Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:15Norm, L1 Norm, L2 NormPost2024년 6월 27일 오후 3:33ML&amp;DL Common시작 전2023년 2월 21일 오후 9:54Quick SortPost2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 21일 오전 10:54ViTPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 18일 오전 12:32ResNetPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 17일 오후 11:18Pix2PixPost2024년 6월 27일 오후 3:26GAN시작 전2023년 2월 17일 오후 10:57메모리 관리 전략은 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 17일 오후 1:09가상 메모리는 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 17일 오후 12:51Multi-process vs Multi-threadPost2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 17일 오후 12:50Program vs Process vs ThreadPost2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 17일 오후 12:50Heap이란 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 16일 오전 11:55Binary Tree란 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 16일 오전 11:47Tree란 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 16일 오전 11:35Graph란 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 16일 오전 11:30CycleGANPost2024년 6월 27일 오후 3:26GAN시작 전2023년 2월 15일 오후 2:54Open Source ContiributionActivity2024년 6월 27일 오후 3:36etc시작 전2023년 2월 14일 오후 12:59VGGNetPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 13일 오후 11:05AlexNetPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 13일 오후 10:38Monocular 3D Object Detection을 위한 KITTI Dataset의 이해(작성중)Post2024년 6월 27일 오후 3:263D Object Detection시작 전2023년 2월 3일 오전 1:17초보 운전자를 위한 안전 주행 보조 시스템Project2024년 6월 28일 오전 10:543D Object Detection, Monocular시작 전2023년 2월 1일 오후 11:03Text Region 검출기Project2024년 6월 28일 오전 10:552D Object Detection시작 전2023년 2월 1일 오후 3:50마스크, 성별, 나이 분류기Project2024년 6월 28일 오전 10:55Classification시작 전2023년 2월 1일 오전 12:53mIoU(Mean Intersection over Union)Post2024년 6월 27일 오후 3:362D Object Detection시작 전2023년 1월 31일 오후 11:08NAVER Connect 재활용 품목 분류를 위한 Semantic SegmentationCompetition2024년 6월 28일 오전 10:55Semantic Segmentation시작 전2023년 1월 31일 오후 10:22DetEvalPost2024년 6월 27일 오후 3:362D Object Detection시작 전2023년 1월 31일 오후 6:57NAVER Connect 학습 데이터 추가 및 수정을 통한 이미지 속 글자 검출 성능 개선Competition2024년 6월 28일 오전 10:552D Object Detection시작 전2023년 1월 31일 오후 6:37NAVER Connect 재활용 품목 분류를 위한 Object DetectionCompetition2024년 6월 28일 오전 10:562D Object Detection시작 전2023년 1월 31일 오후 3:09NAVER Connect 마스크 착용 상태 분류Competition2024년 6월 28일 오전 10:57Classification시작 전2023년 1월 31일 오후 1:21Github 잔디 Notion에 Embed하기Post2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:41pipenv 설치 및 사용Post2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:26venv 설치 및 사용 방법Post2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:18pyenv 설치 및 사용 방법Post2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:13Linux Command CollectionPost2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:10글또 8기Activity2024년 6월 27일 오후 3:39etc시작 전2023년 1월 29일 오후 10:01ML Flow - Tracking Server Docker Image 만들기Post2024년 6월 27일 오후 3:38ML&amp;MLOps Engineering시작 전2023년 1월 5일 오후 7:33ML Flow - 기본 사용Post2024년 6월 27일 오후 3:38ML&amp;MLOps Engineering시작 전2023년 1월 2일 오후 2:40DACON 예술작품 화가 분류 AI 경진대회Competition2024년 6월 28일 오전 10:57Classification시작 전2022년 12월 29일 오후 3:29Tmux 설치 및 사용법Post2024년 6월 27일 오후 3:37etc시작 전2022년 12월 19일 오후 6:54유클리드 호제법(Euclidean algorithm)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:41그리디 알고리즘- 최소 신장 트리(MST, Minimum Spanning Tree)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:40그리디 알고리즘(Greedy Algorithm)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:36깊이 우선 탐색(DFS, Depth-First Search)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:35그래프와 트리(Tree and Graph)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:35순열(Permutation)- C++ next_permutation- nPr 순열 구하기Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:34순열(Permutation)- C++ next_permutation 사용법Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:33순열(Permutation)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:32Naver Connect - Boostcamp AI Tech 4기Activity2024년 6월 27일 오후 3:39ML&amp;DL Common시작 전2022년 12월 19일 오전 11:17\n\nAbout\n📎 Github Profile\n==📎 Programmers Profile==\nindify.co/widgets/live/weather/TqXgD5VuAGNBohZWupYM\n\n"},"vault/index":{"slug":"vault/index","filePath":"vault/index.md","title":"Main","links":["vault/Notion/DB/DB-Blog-Post/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding/LayoutLM--Pre-training-of-Text-and-Layout-for-Document-Image-Understanding","vault/Notion/DB/DB-Blog-Post/OpenEXR-Python","vault/Notion/DB/DB-Blog-Post/Video2Video","vault/Notion/DB/DB-Blog-Post/2023년-회고-ML-Engineer로서의-첫걸음","vault/Notion/DB/DB-Blog-Post/AWS-ECS+S3로-MLFlow-구축해보기-1/AWS-ECS+S3로-MLFlow-구축해보기-1","vault/Notion/DB/DB-Blog-Post/Rust-Study","vault/Notion/DB/DB-Blog-Post/ONNX-Runtime은-무엇일까/ONNX-Runtime은-무엇일까","vault/Notion/DB/DB-Blog-Post/ONNX(Open-Neural-Network-Exchange)란/ONNX(Open-Neural-Network-Exchange)란","vault/Notion/DB/DB-Blog-Post/DPIR---Plug-and-Play-Image-Restoration-with-Deep-Denoiser-Prior(작성중)","vault/Notion/DB/DB-Blog-Post/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution/DBPN---Deep-Back-Projection-Networks-for-Single-Image-Super-resolution","vault/Notion/DB/DB-Blog-Post/Photometric-Stereo를-사용한-형상추출-알고리즘-개발/Photometric-Stereo를-사용한-형상추출-알고리즘-개발","vault/Notion/DB/DB-Blog-Post/Programmers---공원-산책","vault/Notion/DB/DB-Blog-Post/DockerERROR--Unexpected-bus-error-encountered-in-worker.-This-might-be-caused-by-insufficient-shared-memory-(shm)","vault/Notion/DB/DB-Blog-Post/Programmers---숫자의-표현","vault/Notion/DB/DB-Blog-Post/Programming-language-Dart-2-OOP","vault/Notion/DB/DB-Blog-Post/Programming-language-Dart-1-기본기/Programming-language-Dart-1-기본기","vault/Notion/DB/DB-Blog-Post/Programmers---햄버거-만들기","vault/Notion/DB/DB-Blog-Post/Programmers---옹알이(2)","vault/Notion/DB/DB-Blog-Post/Programmers---숫자-짝궁","vault/Notion/DB/DB-Blog-Post/Programmers---가장-가까운-글자","vault/Notion/DB/DB-Blog-Post/Programmers---크기가-작은-부분-문자열","vault/Notion/DB/DB-Blog-Post/Programmers---개인정보-수집-유효기간","vault/Notion/DB/DB-Blog-Post/Programmers---둘만의-암호","vault/Notion/DB/DB-Blog-Post/Programmers---카드뭉치","vault/Notion/DB/DB-Blog-Post/Programmers---콜라-문제","vault/Notion/DB/DB-Blog-Post/Programmers---대충-만든-자판","vault/Notion/DB/DB-Blog-Post/Programmers---덧칠하기","vault/Notion/DB/DB-Blog-Post/2D-Object-Detection/2D-Object-Detection","vault/Notion/DB/DB-Blog-Post/Programmers---리코쳇-로봇","vault/Notion/DB/DB-Blog-Post/Programmers---당구-연습","vault/Notion/DB/DB-Blog-Post/Machine-Learning-Notation(Shan-Hung-Wu)-번역","vault/Notion/DB/DB-Blog-Post/Kaggle---Intro-to-Machine-Learning-번역","vault/Notion/DB/DB-Blog-Post/Programmers---바탕화면-정리","vault/Notion/DB/DB-Blog-Post/Decision-Tree/Decision-Tree","vault/Notion/DB/DB-Blog-Post/Programmers---단어-변환","vault/Notion/DB/DB-Blog-Post/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)/Basic-Optimizer--and--Adam(Adaptive-Moment-Esimation)","vault/Notion/DB/DB-Blog-Post/가중치-초기화(Weight-Initialization)가-필요한-이유와-Xavier--and--He-초기화/가중치-초기화(Weight-Initialization)가-필요한-이유와-Xavier--and--He-초기화","vault/Notion/DB/DB-Blog-Post/StyleGAN--A-Style-Based-Generator-Architecture-for-GANs/StyleGAN--A-Style-Based-Generator-Architecture-for-GANs","vault/Notion/DB/DB-Blog-Post/RetinaNet/RetinaNet","vault/Notion/DB/DB-Blog-Post/SSD(Single-Shot-Multibox-Detector)/SSD(Single-Shot-Multibox-Detector)","vault/Notion/DB/DB-Blog-Post/Yolo-v1(You-Only-Look-Once)/Yolo-v1(You-Only-Look-Once)","vault/Notion/DB/DB-Blog-Post/BiFPN(Neck,-EfficientDet)/BiFPN(Neck,-EfficientDet)","vault/Notion/DB/DB-Blog-Post/DetectroRS/DetectroRS","vault/Notion/DB/DB-Blog-Post/PANet(Path-Aggregation-Network)/PANet(Path-Aggregation-Network)","vault/Notion/DB/DB-Blog-Post/FPN(Feature-Pyramid-Network)/FPN(Feature-Pyramid-Network)","vault/Notion/DB/DB-Blog-Post/Neck/Neck","vault/Notion/DB/DB-Blog-Post/NMS(Non-Maximum-Suppression)","vault/Notion/DB/DB-Blog-Post/RPN(Region-Proposal-Network)/RPN(Region-Proposal-Network)","vault/Notion/DB/DB-Blog-Post/Object-Detection-History(2001~2022)/Object-Detection-History(2001~2022)","vault/Notion/DB/DB-Blog-Post/GAN(Vanilla-GAN)/GAN(Vanilla-GAN)","vault/Notion/DB/DB-Blog-Post/MLOps란-무엇일까/MLOps란-무엇일까","vault/Notion/DB/DB-Blog-Post/Window-10-OpenSSH-서버-활성화-및-실행하기","vault/Notion/DB/DB-Blog-Post/EDA(Exploratory-Data-Analysis,-탐색적-데이터-분석)는-무엇일까","vault/Notion/DB/DB-Blog-Post/Simple-ML-Flow-vs-Competition-Flow","vault/Notion/DB/DB-Blog-Post/Super-resolution-GAN/Super-resolution-GAN","vault/Notion/DB/DB-Blog-Post/Conditional-GAN(cGAN)/Conditional-GAN(cGAN)","vault/Notion/DB/DB-Blog-Post/Analysis-of-model-behaviors/Analysis-of-model-behaviors","vault/Notion/DB/DB-Blog-Post/Two-stage-detector-vs-One-stage-detector/Two-stage-detector-vs-One-stage-detector","vault/Notion/DB/DB-Blog-Post/Receptive-Field(수용필드,-수용장)/Receptive-Field(수용필드,-수용장)","vault/Notion/DB/DB-Blog-Post/Swin-Transformer/Swin-Transformer","vault/Notion/DB/DB-Blog-Post/GoogLeNet(Inception-v1)/GoogLeNet(Inception-v1)","vault/Notion/DB/DB-Blog-Post/Knowledge-distillation/Knowledge-distillation","vault/Notion/DB/DB-Blog-Post/Regularization---Batch-normalization/Regularization---Batch-normalization","vault/Notion/DB/DB-Blog-Post/Regularization---Dropout/Regularization---Dropout","vault/Notion/DB/DB-Blog-Post/Regularization---Label-Smoothing","vault/Notion/DB/DB-Blog-Post/Regularization---Data-Augmentation","vault/Notion/DB/DB-Blog-Post/Regularization---Early-Stopping","vault/Notion/DB/DB-Blog-Post/Optimization에서-중요한-것들/Optimization에서-중요한-것들","vault/Notion/DB/DB-Blog-Post/DeepLearning에서-가장-중요한-아이디어들(denny-britz,-2020-07-29)","vault/Notion/DB/DB-Blog-Post/Loss-Function-for-task","vault/Notion/DB/DB-Blog-Post/torch.no_gard()의-역할","vault/Notion/DB/DB-Blog-Post/BCELoss(Binary-Cross-Entroby-Loss)","vault/Notion/DB/DB-Blog-Post/Cross-Entropy-Loss","vault/Notion/DB/DB-Blog-Post/Metric(in-machine-learning)","vault/Notion/DB/DB-Blog-Post/Transfer-Learning/Transfer-Learning","vault/Notion/DB/DB-Blog-Post/Pytorch-Performance-Tuning-Practices/Pytorch-Performance-Tuning-Practices","vault/Notion/DB/DB-Blog-Post/PyTorch-딥러닝-학습의-기본-순서","vault/Notion/DB/DB-Blog-Post/optimizer.zero_grad()은-무엇일까","vault/Notion/DB/DB-Blog-Post/torch.nn.Parameter은-무엇일까","vault/Notion/DB/DB-Blog-Post/torch.nn.Module은-무엇일까","vault/Notion/DB/DB-Blog-Post/torch.tensor의-requires_grad-param의-기능","vault/Notion/DB/DB-Blog-Post/torch.mm-vs-torch.matmul-차이점/torch.mm-vs-torch.matmul-차이점","vault/Notion/DB/DB-Blog-Post/numpy.ndarray.view와-numpy.ndarray.reshape의-차이","vault/Notion/DB/DB-Blog-Post/자동-미분(Autograd)","vault/Notion/DB/DB-Blog-Post/Computational-Graph/Computational-Graph","vault/Notion/DB/DB-Blog-Post/교착상태","vault/Notion/DB/DB-Blog-Post/최대가능도-추정법(MLE,-Maximum-likelihood-estimation)","vault/Notion/DB/DB-Blog-Post/가능도-함수(Likelihood-function,-likelihood,-우도-함수,-우도)","vault/Notion/DB/DB-Blog-Post/표본편차(sample-standard-deviation-)","vault/Notion/DB/DB-Blog-Post/표본분산(sample-variance)","vault/Notion/DB/DB-Blog-Post/표본평균(Sample-mean)","vault/Notion/DB/DB-Blog-Post/하이퍼볼릭-탄젠트(Hyperbolic-Tangent)","vault/Notion/DB/DB-Blog-Post/RNN(Recurrent-Neural-Networks)/RNN(Recurrent-Neural-Networks)","vault/Notion/DB/DB-Blog-Post/주변-분포(Marginal-Probability-Distribution)","vault/Notion/DB/DB-Blog-Post/RNN-내부에서-sigmoid-또는-tanh를-쓰는-이유","vault/Notion/DB/DB-Blog-Post/Sample-Distribution-vs-Sampling-Distribution","vault/Notion/DB/DB-Blog-Post/모수적(parametric)-방법론","vault/Notion/DB/DB-Blog-Post/모수","vault/Notion/DB/DB-Blog-Post/기댓값(expectation)","vault/Notion/DB/DB-Blog-Post/모-평균(population-mean,-μ)","vault/Notion/DB/DB-Blog-Post/표본(sample)","vault/Notion/DB/DB-Blog-Post/모집단(population)","vault/Notion/DB/DB-Blog-Post/마르코프-연쇄-몬테카를로(MCMC,-Markov-Chain-Monte-Carlo)","vault/Notion/DB/DB-Blog-Post/몬테카를로-방법/몬테카를로-방법","vault/Notion/DB/DB-Blog-Post/샘플링(Samiling)","vault/Notion/DB/DB-Blog-Post/범함수(functional)","vault/Notion/DB/DB-Blog-Post/기대값","vault/Notion/DB/DB-Blog-Post/조건부-확률","vault/Notion/DB/DB-Blog-Post/이산형-확률변수와-연속형-확률변수","vault/Notion/DB/DB-Blog-Post/선형회귀의-목적식","vault/Notion/DB/DB-Blog-Post/Norm,-L1-Norm,-L2-Norm","vault/Notion/DB/DB-Blog-Post/Quick-Sort/Quick-Sort","vault/Notion/DB/DB-Blog-Post/ViT/ViT","vault/Notion/DB/DB-Blog-Post/ResNet/ResNet","vault/Notion/DB/DB-Blog-Post/Pix2Pix/Pix2Pix","vault/Notion/DB/DB-Blog-Post/메모리-관리-전략은-무엇일까","vault/Notion/DB/DB-Blog-Post/가상-메모리는-무엇일까/가상-메모리는-무엇일까","vault/Notion/DB/DB-Blog-Post/Multi-process-vs-Multi-thread","vault/Notion/DB/DB-Blog-Post/Program-vs-Process-vs-Thread/Program-vs-Process-vs-Thread","vault/Notion/DB/DB-Blog-Post/Heap이란-무엇일까/Heap이란-무엇일까","vault/Notion/DB/DB-Blog-Post/Binary-Tree란-무엇일까/Binary-Tree란-무엇일까","vault/Notion/DB/DB-Blog-Post/Tree란-무엇일까","vault/Notion/DB/DB-Blog-Post/Graph란-무엇일까","vault/Notion/DB/DB-Blog-Post/CycleGAN/CycleGAN","vault/Notion/DB/DB-Blog-Post/Open-Source-Contiribution","vault/Notion/DB/DB-Blog-Post/VGGNet/VGGNet","vault/Notion/DB/DB-Blog-Post/AlexNet/AlexNet","vault/Notion/DB/DB-Blog-Post/Monocular-3D-Object-Detection을-위한-KITTI-Dataset의-이해(작성중)/Monocular-3D-Object-Detection을-위한-KITTI-Dataset의-이해(작성중)","vault/Notion/DB/DB-Blog-Post/초보-운전자를-위한-안전-주행-보조-시스템/초보-운전자를-위한-안전-주행-보조-시스템","vault/Notion/DB/DB-Blog-Post/Text-Region-검출기/Text-Region-검출기","vault/Notion/DB/DB-Blog-Post/마스크,-성별,-나이-분류기/마스크,-성별,-나이-분류기","vault/Notion/DB/DB-Blog-Post/mIoU(Mean-Intersection-over-Union)","vault/Notion/DB/DB-Blog-Post/NAVER-Connect-재활용-품목-분류를-위한-Semantic-Segmentation/NAVER-Connect-재활용-품목-분류를-위한-Semantic-Segmentation","vault/Notion/DB/DB-Blog-Post/DetEval","vault/Notion/DB/DB-Blog-Post/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선/NAVER-Connect-학습-데이터-추가-및-수정을-통한-이미지-속-글자-검출-성능-개선","vault/Notion/DB/DB-Blog-Post/NAVER-Connect-재활용-품목-분류를-위한-Object-Detection/NAVER-Connect-재활용-품목-분류를-위한-Object-Detection","vault/Notion/DB/DB-Blog-Post/NAVER-Connect-마스크-착용-상태-분류/NAVER-Connect-마스크-착용-상태-분류","vault/Notion/DB/DB-Blog-Post/Github-잔디-Notion에-Embed하기","vault/Notion/DB/DB-Blog-Post/pipenv-설치-및-사용","vault/Notion/DB/DB-Blog-Post/venv-설치-및-사용-방법","vault/Notion/DB/DB-Blog-Post/pyenv-설치-및-사용-방법","vault/Notion/DB/DB-Blog-Post/Linux-Command-Collection","vault/Notion/DB/DB-Blog-Post/글또-8기/글또-8기","vault/Notion/DB/DB-Blog-Post/ML-Flow---Tracking-Server-Docker-Image-만들기","vault/Notion/DB/DB-Blog-Post/ML-Flow---기본-사용/ML-Flow---기본-사용","vault/Notion/DB/DB-Blog-Post/DACON-예술작품-화가-분류-AI-경진대회/DACON-예술작품-화가-분류-AI-경진대회","vault/Notion/DB/DB-Blog-Post/Tmux-설치-및-사용법","vault/Notion/DB/DB-Blog-Post/유클리드-호제법(Euclidean-algorithm)","vault/Notion/DB/DB-Blog-Post/그리디-알고리즘--최소-신장-트리(MST,-Minimum-Spanning-Tree)","vault/Notion/DB/DB-Blog-Post/그리디-알고리즘(Greedy-Algorithm)","vault/Notion/DB/DB-Blog-Post/깊이-우선-탐색(DFS,-Depth-First-Search)","vault/Notion/DB/DB-Blog-Post/그래프와-트리(Tree-and-Graph)","vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation--nPr-순열-구하기","vault/Notion/DB/DB-Blog-Post/순열(Permutation)--C++-next_permutation-사용법","vault/Notion/DB/DB-Blog-Post/순열(Permutation)","vault/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Naver-Connect---Boostcamp-AI-Tech-4기","vault/Notion/HyeongSeok-Kim’s-Notion/About"],"tags":[],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이름TypeLast EditTagTags담당자상태생성일LayoutLM- Pre-training of Text and Layout for Document Image UnderstandingProject2024년 10월 25일 오전 10:44Document Classification, Information ExtractionML &amp; MLOpsKim HyeongSeok진행 중2024년 6월 28일 오전 11:03OpenEXR-PythonProject2024년 10월 25일 오전 10:44Python Package시작 전2024년 6월 28일 오전 10:34Video2VideoProject2024년 10월 25일 오전 10:44Python Package시작 전2024년 6월 28일 오전 10:182023년 회고-ML Engineer로서의 첫걸음Post2024년 10월 25일 오전 10:44etc시작 전2024년 1월 7일 오후 10:55AWS-ECS+S3로 MLFlow 구축해보기 1Post2024년 10월 25일 오전 10:44ML&amp;MLOps Engineering시작 전2023년 7월 2일 오후 10:32Rust StudyPost2024년 10월 25일 오전 10:44etc시작 전2023년 6월 26일 오후 12:40ONNX-Runtime은 무엇일까Post2024년 10월 25일 오전 10:44ML&amp;MLOps Engineering시작 전2023년 6월 18일 오후 2:22ONNX(Open Neural Network Exchange)란Post2024년 10월 25일 오전 10:44ML&amp;MLOps Engineering시작 전2023년 6월 4일 오후 8:43DPIR - Plug-and-Play Image Restoration with Deep Denoiser Prior(작성중)Post2024년 10월 25일 오전 10:44Image Denoise시작 전2023년 5월 4일 오전 10:30DBPN - Deep Back-Projection Networks for Single Image Super-resolutionPost2024년 10월 25일 오전 10:44Image Super Resolution시작 전2023년 4월 28일 오전 10:13Photometric Stereo를 사용한 형상추출 알고리즘 개발Project2024년 10월 25일 오전 10:443D Surface Normal Estimation시작 전2023년 3월 28일 오후 9:24Programmers - 공원 산책Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 24일 오후 7:28DockerERROR- Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm)Post2024년 10월 25일 오전 10:44ML&amp;MLOps Engineering시작 전2023년 3월 23일 오후 9:37Programmers - 숫자의 표현Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 22일 오후 5:50Programming language Dart 2 OOPPost2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 7:25Programming language Dart 1 기본기Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 6:21Programmers - 햄버거 만들기Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 1:18Programmers - 옹알이(2)Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 12:57Programmers - 숫자 짝궁Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 21일 오후 12:45Programmers - 가장 가까운 글자Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:42Programmers - 크기가 작은 부분 문자열Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:38Programmers - 개인정보 수집 유효기간Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:35Programmers - 둘만의 암호Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:15Programmers - 카드뭉치Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 9:03Programmers - 콜라 문제Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 8:59Programmers - 대충 만든 자판Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 5:25Programmers - 덧칠하기Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 20일 오후 2:092D Object DetectionPost2024년 10월 25일 오전 10:44Classification시작 전2023년 3월 17일 오후 9:09Programmers - 리코쳇 로봇Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 17일 오후 9:02Programmers - 당구 연습Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 17일 오후 7:46Machine Learning Notation(Shan-Hung Wu) 번역Post2024년 10월 25일 오전 10:44ML&amp;DL Common시작 전2023년 3월 13일 오후 8:50Kaggle - Intro to Machine Learning 번역Post2024년 10월 25일 오전 10:44ML&amp;DL Common시작 전2023년 3월 11일 오전 11:43Programmers - 바탕화면 정리Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 10일 오후 10:23Decision TreePost2024년 10월 25일 오전 10:44Computer Science시작 전2023년 3월 10일 오후 6:50Programmers - 단어 변환Post2024년 10월 25일 오전 10:44etc시작 전2023년 3월 10일 오후 3:35Basic Optimizer &amp; Adam(Adaptive Moment Esimation)Post2024년 10월 25일 오전 10:44ML&amp;DL Common시작 전2023년 3월 6일 오후 10:23가중치 초기화(Weight Initialization)가 필요한 이유와 Xavier &amp; He 초기화Post2024년 10월 25일 오전 10:44ML&amp;DL Common시작 전2023년 3월 5일 오후 2:44StyleGAN- A Style-Based Generator Architecture for GANsPost2024년 10월 25일 오전 10:44GAN시작 전2023년 2월 27일 오후 2:19RetinaNetPost2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 26일 오후 9:49SSD(Single Shot Multibox Detector)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 26일 오후 9:16Yolo v1(You Only Look Once)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 25일 오전 11:22BiFPN(Neck, EfficientDet)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 25일 오전 11:06DetectroRSPost2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 25일 오전 10:23PANet(Path Aggregation Network)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 25일 오전 12:10FPN(Feature Pyramid Network)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 11:50NeckPost2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 11:43NMS(Non-Maximum Suppression)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 11:37RPN(Region Proposal Network)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 11:14Object Detection History(2001~2022)Post2024년 10월 25일 오전 10:442D Object Detection시작 전2023년 2월 24일 오후 8:48GAN(Vanilla GAN)Post2024년 10월 25일 오전 10:44GAN시작 전2023년 2월 24일 오후 12:58MLOps란 무엇일까Post2024년 6월 27일 오후 3:39ML&amp;MLOps Engineering시작 전2023년 2월 23일 오전 10:40Window 10 OpenSSH 서버 활성화 및 실행하기Post2024년 6월 27일 오후 3:37etc시작 전2023년 2월 23일 오전 10:38EDA(Exploratory Data Analysis, 탐색적 데이터 분석)는 무엇일까Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 23일 오전 10:36Simple ML Flow vs Competition FlowPost2024년 6월 27일 오후 3:39ML&amp;MLOps Engineering시작 전2023년 2월 23일 오전 10:33Super resolution GANPost2024년 6월 27일 오후 3:26GAN시작 전2023년 2월 23일 오전 10:23Conditional GAN(cGAN)Post2024년 6월 27일 오후 3:26GAN시작 전2023년 2월 23일 오전 10:22Analysis of model behaviorsPost2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 23일 오전 10:16Two-stage detector vs One-stage detectorPost2024년 6월 27일 오후 3:252D Object Detection시작 전2023년 2월 23일 오전 10:14Receptive Field(수용필드, 수용장)Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 23일 오전 10:11Swin TransformerPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 23일 오전 10:10GoogLeNet(Inception v1)Post2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 22일 오후 7:36Knowledge distillationPost2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 3:31Regularization - Batch normalizationPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:57Regularization - DropoutPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:54Regularization - Label SmoothingPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:38Regularization - Data AugmentationPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:35Regularization - Early StoppingPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:30Optimization에서 중요한 것들Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 2:20DeepLearning에서 가장 중요한 아이디어들(denny britz, 2020-07-29)Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 2:19Loss Function for taskPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:18torch.no_gard()의 역할Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 2:16BCELoss(Binary Cross Entroby Loss)Post2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:15Cross Entropy LossPost2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:14Metric(in machine learning)Post2024년 6월 27일 오후 3:24ML&amp;DL Common시작 전2023년 2월 22일 오후 2:12Transfer LearningPost2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 2:11Pytorch Performance Tuning PracticesPost2024년 6월 27일 오후 3:39ML&amp;MLOps Engineering시작 전2023년 2월 22일 오후 1:51PyTorch 딥러닝 학습의 기본 순서Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:47optimizer.zero_grad()은 무엇일까Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:44torch.nn.Parameter은 무엇일까Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:43torch.nn.Module은 무엇일까Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:41torch.tensor의 requires_grad param의 기능Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:40torch.mm vs torch.matmul 차이점Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:35numpy.ndarray.view와 numpy.ndarray.reshape의 차이Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:34자동 미분(Autograd)Post2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:24Computational GraphPost2024년 6월 27일 오후 3:14ML&amp;DL Common시작 전2023년 2월 22일 오후 1:22교착상태Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 22일 오후 1:08최대가능도 추정법(MLE, Maximum likelihood estimation)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:06가능도 함수(Likelihood function, likelihood, 우도 함수, 우도)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:04표본편차(sample standard deviation )Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:04표본분산(sample variance)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:03표본평균(Sample mean)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:01하이퍼볼릭 탄젠트(Hyperbolic Tangent)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오후 12:00RNN(Recurrent Neural Networks)Post2024년 6월 27일 오후 3:26NLP시작 전2023년 2월 22일 오전 11:53주변 분포(Marginal Probability Distribution)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:47RNN 내부에서 sigmoid 또는 tanh를 쓰는 이유Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:45Sample Distribution vs Sampling DistributionPost2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:42모수적(parametric) 방법론Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:41모수Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:40기댓값(expectation)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:39모 평균(population mean, μ)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:38표본(sample)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:38모집단(population)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:37마르코프 연쇄 몬테카를로(MCMC, Markov Chain Monte Carlo)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:34몬테카를로 방법Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:33샘플링(Samiling)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:32범함수(functional)Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:31기대값Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:30조건부 확률Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:29이산형 확률변수와 연속형 확률변수Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:25선형회귀의 목적식Post2024년 6월 27일 오후 3:34ML&amp;DL Common시작 전2023년 2월 22일 오전 11:15Norm, L1 Norm, L2 NormPost2024년 6월 27일 오후 3:33ML&amp;DL Common시작 전2023년 2월 21일 오후 9:54Quick SortPost2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 21일 오전 10:54ViTPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 18일 오전 12:32ResNetPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 17일 오후 11:18Pix2PixPost2024년 6월 27일 오후 3:26GAN시작 전2023년 2월 17일 오후 10:57메모리 관리 전략은 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 17일 오후 1:09가상 메모리는 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 17일 오후 12:51Multi-process vs Multi-threadPost2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 17일 오후 12:50Program vs Process vs ThreadPost2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 17일 오후 12:50Heap이란 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 16일 오전 11:55Binary Tree란 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 16일 오전 11:47Tree란 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 16일 오전 11:35Graph란 무엇일까Post2024년 6월 27일 오후 3:34Computer Science시작 전2023년 2월 16일 오전 11:30CycleGANPost2024년 6월 27일 오후 3:26GAN시작 전2023년 2월 15일 오후 2:54Open Source ContiributionActivity2024년 6월 27일 오후 3:36etc시작 전2023년 2월 14일 오후 12:59VGGNetPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 13일 오후 11:05AlexNetPost2024년 6월 27일 오후 3:24Classification시작 전2023년 2월 13일 오후 10:38Monocular 3D Object Detection을 위한 KITTI Dataset의 이해(작성중)Post2024년 6월 27일 오후 3:263D Object Detection시작 전2023년 2월 3일 오전 1:17초보 운전자를 위한 안전 주행 보조 시스템Project2024년 6월 28일 오전 10:543D Object Detection, Monocular시작 전2023년 2월 1일 오후 11:03Text Region 검출기Project2024년 6월 28일 오전 10:552D Object Detection시작 전2023년 2월 1일 오후 3:50마스크, 성별, 나이 분류기Project2024년 6월 28일 오전 10:55Classification시작 전2023년 2월 1일 오전 12:53mIoU(Mean Intersection over Union)Post2024년 6월 27일 오후 3:362D Object Detection시작 전2023년 1월 31일 오후 11:08NAVER Connect 재활용 품목 분류를 위한 Semantic SegmentationCompetition2024년 6월 28일 오전 10:55Semantic Segmentation시작 전2023년 1월 31일 오후 10:22DetEvalPost2024년 6월 27일 오후 3:362D Object Detection시작 전2023년 1월 31일 오후 6:57NAVER Connect 학습 데이터 추가 및 수정을 통한 이미지 속 글자 검출 성능 개선Competition2024년 6월 28일 오전 10:552D Object Detection시작 전2023년 1월 31일 오후 6:37NAVER Connect 재활용 품목 분류를 위한 Object DetectionCompetition2024년 6월 28일 오전 10:562D Object Detection시작 전2023년 1월 31일 오후 3:09NAVER Connect 마스크 착용 상태 분류Competition2024년 6월 28일 오전 10:57Classification시작 전2023년 1월 31일 오후 1:21Github 잔디 Notion에 Embed하기Post2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:41pipenv 설치 및 사용Post2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:26venv 설치 및 사용 방법Post2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:18pyenv 설치 및 사용 방법Post2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:13Linux Command CollectionPost2024년 6월 27일 오후 3:37etc시작 전2023년 1월 31일 오후 12:10글또 8기Activity2024년 6월 27일 오후 3:39etc시작 전2023년 1월 29일 오후 10:01ML Flow - Tracking Server Docker Image 만들기Post2024년 6월 27일 오후 3:38ML&amp;MLOps Engineering시작 전2023년 1월 5일 오후 7:33ML Flow - 기본 사용Post2024년 6월 27일 오후 3:38ML&amp;MLOps Engineering시작 전2023년 1월 2일 오후 2:40DACON 예술작품 화가 분류 AI 경진대회Competition2024년 6월 28일 오전 10:57Classification시작 전2022년 12월 29일 오후 3:29Tmux 설치 및 사용법Post2024년 6월 27일 오후 3:37etc시작 전2022년 12월 19일 오후 6:54유클리드 호제법(Euclidean algorithm)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:41그리디 알고리즘- 최소 신장 트리(MST, Minimum Spanning Tree)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:40그리디 알고리즘(Greedy Algorithm)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:36깊이 우선 탐색(DFS, Depth-First Search)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:35그래프와 트리(Tree and Graph)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:35순열(Permutation)- C++ next_permutation- nPr 순열 구하기Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:34순열(Permutation)- C++ next_permutation 사용법Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:33순열(Permutation)Post2024년 6월 27일 오후 3:34Computer Science시작 전2022년 12월 19일 오후 6:32Naver Connect - Boostcamp AI Tech 4기Activity2024년 6월 27일 오후 3:39ML&amp;DL Common시작 전2022년 12월 19일 오전 11:17\n\nAbout\n📎 Github Profile\n==📎 Programmers Profile==\nindify.co/widgets/live/weather/TqXgD5VuAGNBohZWupYM\n\n"}}