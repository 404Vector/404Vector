<!DOCTYPE html>
<html lang="en"><head><title>ViT</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;family=IBM Plex Mono:wght@400;600&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="HyeongSeok Kim's Vault"/><meta property="og:title" content="ViT"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="ViT"/><meta name="twitter:description" content="참조 ViT란 무엇인가? ViT의 특징 ViT Inference 과정 Class Token이 있는 이유는? Layer Normalization ViT의 결과 Pytorch를 이용한 ViT 구현 참조 arxiv.org/abs/2010.11929 visionhong.tistory.com/25 kmhana.tistory.com/27 deep-learning-study.tistory.com/716 jalammar.github.io/illustrated-transformer/ daebaq27.tistory.com/108 gaussian37..."/><meta property="og:description" content="참조 ViT란 무엇인가? ViT의 특징 ViT Inference 과정 Class Token이 있는 이유는? Layer Normalization ViT의 결과 Pytorch를 이용한 ViT 구현 참조 arxiv.org/abs/2010.11929 visionhong.tistory.com/25 kmhana.tistory.com/27 deep-learning-study.tistory.com/716 jalammar.github.io/illustrated-transformer/ daebaq27.tistory.com/108 gaussian37..."/><meta property="og:image:alt" content="참조 ViT란 무엇인가? ViT의 특징 ViT Inference 과정 Class Token이 있는 이유는? Layer Normalization ViT의 결과 Pytorch를 이용한 ViT 구현 참조 arxiv.org/abs/2010.11929 visionhong.tistory.com/25 kmhana.tistory.com/27 deep-learning-study.tistory.com/716 jalammar.github.io/illustrated-transformer/ daebaq27.tistory.com/108 gaussian37..."/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https://quartz.jzhao.xyz/vault/Notion/DB/DB-Blog-Post/ViT/ViT"/><meta property="twitter:url" content="https://quartz.jzhao.xyz/vault/Notion/DB/DB-Blog-Post/ViT/ViT"/><link rel="icon" href="../../../../../static/icon.png"/><meta name="description" content="참조 ViT란 무엇인가? ViT의 특징 ViT Inference 과정 Class Token이 있는 이유는? Layer Normalization ViT의 결과 Pytorch를 이용한 ViT 구현 참조 arxiv.org/abs/2010.11929 visionhong.tistory.com/25 kmhana.tistory.com/27 deep-learning-study.tistory.com/716 jalammar.github.io/illustrated-transformer/ daebaq27.tistory.com/108 gaussian37..."/><meta name="generator" content="Quartz"/><link href="../../../../../index.css" rel="stylesheet" type="text/css" spa-preserve/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  padding: 2rem;
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,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 */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../../../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../../../../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://quartz.jzhao.xyz/index.xml"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image" content="https://quartz.jzhao.xyz/vault/Notion/DB/DB-Blog-Post/ViT/ViT-og-image.webp"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/vault/Notion/DB/DB-Blog-Post/ViT/ViT-og-image.webp"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/vault/Notion/DB/DB-Blog-Post/ViT/ViT-og-image.webp"/><meta property="og:image:type" content="image/.webp"/></head><body data-slug="vault/Notion/DB/DB-Blog-Post/ViT/ViT"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../../../../..">HyeongSeok Kim's Vault</a></h2><div class="spacer mobile-only"></div><div class="flex-component" style="flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search"><button class="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div class="search-layout" data-preview="true"></div></div></div></div></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="readermode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="readerIcon" fill="currentColor" stroke="currentColor" stroke-width="0.2" stroke-linecap="round" stroke-linejoin="round" width="64px" height="64px" viewBox="0 0 24 24" aria-label="Reader mode"><title>Reader mode</title><g transform="translate(-1.8, -1.8) scale(1.15, 1.2)"><path d="M8.9891247,2.5 C10.1384702,2.5 11.2209868,2.96705384 12.0049645,3.76669482 C12.7883914,2.96705384 13.8709081,2.5 15.0202536,2.5 L18.7549359,2.5 C19.1691495,2.5 19.5049359,2.83578644 19.5049359,3.25 L19.5046891,4.004 L21.2546891,4.00457396 C21.6343849,4.00457396 21.9481801,4.28672784 21.9978425,4.6528034 L22.0046891,4.75457396 L22.0046891,20.25 C22.0046891,20.6296958 21.7225353,20.943491 21.3564597,20.9931534 L21.2546891,21 L2.75468914,21 C2.37499337,21 2.06119817,20.7178461 2.01153575,20.3517706 L2.00468914,20.25 L2.00468914,4.75457396 C2.00468914,4.37487819 2.28684302,4.061083 2.65291858,4.01142057 L2.75468914,4.00457396 L4.50368914,4.004 L4.50444233,3.25 C4.50444233,2.87030423 4.78659621,2.55650904 5.15267177,2.50684662 L5.25444233,2.5 L8.9891247,2.5 Z M4.50368914,5.504 L3.50468914,5.504 L3.50468914,19.5 L10.9478955,19.4998273 C10.4513189,18.9207296 9.73864328,18.5588115 8.96709342,18.5065584 L8.77307039,18.5 L5.25444233,18.5 C4.87474657,18.5 4.56095137,18.2178461 4.51128895,17.8517706 L4.50444233,17.75 L4.50368914,5.504 Z M19.5049359,17.75 C19.5049359,18.1642136 19.1691495,18.5 18.7549359,18.5 L15.2363079,18.5 C14.3910149,18.5 13.5994408,18.8724714 13.0614828,19.4998273 L20.5046891,19.5 L20.5046891,5.504 L19.5046891,5.504 L19.5049359,17.75 Z M18.0059359,3.999 L15.0202536,4 L14.8259077,4.00692283 C13.9889509,4.06666544 13.2254227,4.50975805 12.7549359,5.212 L12.7549359,17.777 L12.7782651,17.7601316 C13.4923805,17.2719483 14.3447024,17 15.2363079,17 L18.0059359,16.999 L18.0056891,4.798 L18.0033792,4.75457396 L18.0056891,4.71 L18.0059359,3.999 Z M8.9891247,4 L6.00368914,3.999 L6.00599909,4.75457396 L6.00599909,4.75457396 L6.00368914,4.783 L6.00368914,16.999 L8.77307039,17 C9.57551536,17 10.3461406,17.2202781 11.0128313,17.6202194 L11.2536891,17.776 L11.2536891,5.211 C10.8200889,4.56369974 10.1361548,4.13636104 9.37521067,4.02745763 L9.18347055,4.00692283 L8.9891247,4 Z"></path></g></svg></button></div></div><div class="explorer" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>node&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-content"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div class="explorer-content" aria-expanded="false"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../../../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../vault/">Main</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../vault/Notion/">Notion</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../vault/Notion/DB/">DB</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../vault/Notion/DB/DB-Blog-Post/">DB Blog Post</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../vault/Notion/DB/DB-Blog-Post/ViT/">ViT</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>ViT</a></div></nav><h1 class="article-title">ViT</h1><p show-comma="true" class="content-meta"><time datetime="2025-06-16T10:23:10.362Z">Jun 16, 2025</time><span>12 min read</span></p></div></div><article class="popover-hint"><ul>
<li><a href="#%EC%B0%B8%EC%A1%B0" class="internal alias">참조</a></li>
<li><a href="#vit%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80" class="internal alias">ViT란 무엇인가?</a></li>
<li><a href="#vit%EC%9D%98-%ED%8A%B9%EC%A7%95" class="internal alias">ViT의 특징</a></li>
<li><a href="#vit-inference-%EA%B3%BC%EC%A0%95" class="internal alias">ViT Inference 과정</a></li>
<li><a href="#class-token%EC%9D%B4-%EC%9E%88%EB%8A%94-%EC%9D%B4%EC%9C%A0%EB%8A%94" class="internal alias">Class Token이 있는 이유는?</a></li>
<li><a href="#layer-normalization" class="internal alias">Layer Normalization</a></li>
<li><a href="#vit%EC%9D%98-%EA%B2%B0%EA%B3%BC" class="internal alias">ViT의 결과</a></li>
<li><a href="#pytorch%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-vit-%EA%B5%AC%ED%98%84" class="internal alias">Pytorch를 이용한 ViT 구현</a></li>
</ul>
<h2 id="참조">참조<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#참조" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p><a href="https://arxiv.org/abs/2010.11929" class="external">https://arxiv.org/abs/2010.11929<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><a href="https://visionhong.tistory.com/25" class="external">https://visionhong.tistory.com/25<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><a href="https://kmhana.tistory.com/27" class="external">https://kmhana.tistory.com/27<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><a href="https://deep-learning-study.tistory.com/716" class="external">https://deep-learning-study.tistory.com/716<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><a href="https://jalammar.github.io/illustrated-transformer/" class="external">https://jalammar.github.io/illustrated-transformer/<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><a href="https://daebaq27.tistory.com/108" class="external">https://daebaq27.tistory.com/108<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><a href="https://gaussian37.github.io/dl-concept-vit/" class="external">https://gaussian37.github.io/dl-concept-vit/<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><a href="https://datascience.stackexchange.com/questions/90649/class-token-in-vit-and-bert" class="external">https://datascience.stackexchange.com/questions/90649/class-token-in-vit-and-bert<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><a href="https://learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/" class="external">https://learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<h2 id="vit란-무엇인가">ViT란 무엇인가?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#vit란-무엇인가" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Transformer를 image patch의 sequence에 적용하여 classification을 수행하는 모델입니다.</p>
<p>ViT는 일반적인 CNN과 다르게 공간에 대한, “Inductive bias”이 없습니다. 따라서 더 많은 데이터를 통해, 원초적인 관계를 Robust 하게 학습시켜야 하기 때문에 매우 많은 데이터가 필요합니다.</p>
<p><img src="../../../../../vault/resources/Untitled-22.png" width="auto" height="auto" alt="Untitled 22.png"/></p>
<ul>
<li><code>Vision Transformer</code>는 Transformer의 전체 아키텍쳐를 크게 변경하지 않은 상태에서 이미지 처리를 위한 용도로 사용되는데 의의가 있습니다.</li>
<li>기존의 이미지 분야에서 <code>attention</code> 기법을 사용할 경우 대부분 CNN과 함께 사용되거나 전체 CNN 구조를 유지하면서 CNN의 특정 구성 요소를 대체하는 데 사용되어 왔습니다.</li>
<li>또는 <code>attention</code> 만을 이용한 모델도 있었지만 기존의 CNN을 기반으로 하는 모델의 성능을 넘지는 못하였습니다.</li>
<li>하지만 <code>Vision Transformer</code>에서는 CNN에 의존하지 않고 <code>이미지 패치의 시퀀스</code>를 입력값으로 사용하는 transformer를 적용하여 CNN 기반의 모델의 성능을 넘는 성능을 보여주었습니다. 이미지를 이미지 패치의 시퀀스로 입력하는 방법은 뒤에서 다루겠습니다.</li>
</ul>
<h2 id="vit의-특징">ViT의 특징<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#vit의-특징" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>매우 많은 데이터가 필요합니다. ImageNet와 같은 Mid-sized 데이터셋으로 학습 시, ResNet보다 낮은 성능을 보입니다.</li>
<li>ViT는 이미지를 patch로 분할하고, 이 patch들을 linear embedding에 전달합니다. 그리고 이것을 transformer의 입력값으로 사용합니다. 즉, 하나 하나의 patch를 NLP의 token으로 간주합니다.</li>
<li>Embedding은 NLP의 transformer와 동일하게 learnable positional embedding과 element-wise sum으로 결합된 embedding을 사용하며 0번째 patch + position embedding에는 class를 부여합니다. 그리고 이것을 patch embedding이라고 부릅니다.</li>
<li>Transformer encoder 출력값에 하나의 hidden layer를 가진 MLP를 사용하여 pre-train 합니다. pine-tunning 시에는 랜덤 초기화된 하나의 linear layer를 사용합니다.</li>
</ul>
<h2 id="vit-inference-과정"><strong>ViT Inference 과정</strong><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#vit-inference-과정" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p><img src="../../../../../vault/resources/Untitled-1-17.png" width="auto" height="auto" alt="Untitled 1 17.png"/></p>
<ol>
<li>
<p>==<strong>Before Input</strong>==</p>
<p><img src="../../../../../vault/resources/Untitled-2-12.png" width="auto" height="auto" alt="Untitled 2 12.png"/></p>
<ol>
<li>(C,H,W) 크기의 이미지를 크기가 (P,P)인 패치 N개로 자른 후, N개의 1-D 벡터 (P^2⋅c차원)로 flatten</li>
<li>N=HW/P^2로 계산되며, P는 하이퍼 파라미터.
<ol>
<li>예시는 참고용이며, 실제 실험에서는 모델 크기에 따라 P=14,16,32 등 다양</li>
</ol>
</li>
<li>이후, Linear projection을 수행하여 크기가 D인 벡터의 시퀀스로 차원 변경
<ol>
<li>이때, D는 모든 레이어 전체에 고정된 값.</li>
</ol>
</li>
</ol>
</li>
<li>
<p>==<strong>Linear Projection</strong>==</p>
<ol>
<li>xp 를 Embedding 하기 위하여 행렬 E 와 곱 연산 수행
<ol>
<li>E 의 shape : (P^2 x C, D)</li>
<li>D : Embedding dimension, P^2 x C 크기의 벡터를 D 로 변경하겠다는 의미</li>
</ol>
</li>
<li><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>
<ol>
<li>xp shape : ( N,P^2 x C )</li>
<li>E shape : ( P^2 x C,D )</li>
</ol>
</li>
</ol>
</li>
<li>
<p>==<strong>Embedding (P=16, H=W=224, N=196, D=768)</strong>==</p>
<p><img src="../../../../../vault/resources/Untitled-3-11.png" width="auto" height="auto" alt="Untitled 3 11.png"/></p>
<ol>
<li>Class token 추가
<ol>
<li>가장 첫번째 패치 임베딩 앞에 학습 가능한 임베딩 벡터를 추가</li>
<li>추후 이미지 전체에 대한 표현을 나타내게 됨</li>
</ol>
</li>
<li>Posotional Embeding
<ol>
<li>N+1개의 학습 가능한 1D 포지션 임베딩 벡터 (이미지 패치의 위치를 특정하기 위함)를 생성</li>
<li>기존 임베딩 벡터와 합 연산</li>
</ol>
</li>
<li>만들어진 임베딩 패치를 Transformer Encoder에 입력</li>
</ol>
</li>
<li>
<p>==<strong>Transformer Encoder</strong>==</p>
<p><img src="../../../../../vault/resources/Untitled-4-8.png" width="auto" height="auto" alt="Untitled 4 8.png"/></p>
<ol>
<li>Transformer의 Encoder는 L 번 반복하기 위해 입력과 출력의 크기가 같도록 유지</li>
<li>입력값 z0 에서 시작하여 L 번 반복하여 얻은 zL 이 최종 Encoder의 출력</li>
<li>Encoder 출력에서 Class Token 부분만 Classification에 사용, 마지막에 추가적으로 MLP Head를 붙여 분류 수행</li>
</ol>
</li>
</ol>
<h2 id="class-token이-있는-이유는">Class Token이 있는 이유는?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#class-token이-있는-이유는" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Class Token은 BERT에서 제시된 아이디어이며 학습 가능한 임베딩이 있는 입력으로, 입력 패치 임베딩이 앞에 추가되고 MSA를 사용하여 모든 패치에서 정보를 수집합니다. </p>
<p>트랜스포머는 기본적으로 시퀀스-시퀀스 네트워크입니다.<br/>
ViT에 디코더 레이어가 없기 때문에 입력 시퀀스(패치 수)의 길이는 출력 시퀀스의 길이와 같습니다.<br/>
따라서 목표가 분류인 경우 두 가지 선택지가 있습니다.</p>
<ol>
<li>model 위에 완전 연결된 계층을 적용
<ol>
<li>입력 이미지 해상도를 고정해야 하기 때문에 좋은 생각이 아닙니다</li>
</ol>
</li>
<li>출력 시퀀스의 하나의 항목에 분류 계층을 적용
<ol>
<li>해당 패치에 대하여 선호도가 생겨버립니다</li>
</ol>
</li>
</ol>
<p>따라서 최선의 해결책은 더미 입력인 클래스 토큰을 추가하고 해당 출력 항목에 분류 계층을 적용하는 것입니다</p>
<h2 id="layer-normalization">Layer Normalization<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#layer-normalization" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>전설적인 Geoffrey Hinton 교수의 연구실에서 처음 제안한 레이어 정규화는 배치 정규화의 약간 다른 버전입니다. 우리는 모두 컴퓨터 비전에서 배치 정규화에 익숙합니다. 그러나 배치 정규화는 반복 아키텍처에 직접 적용할 수 없습니다. </p>
<p>또한 배치 정규화의 평균(μ) 및 표준 편차(σ) 통계량이 미니 배치에 대해 계산되기 때문에 결과는 배치 크기에 따라 달라집니다. 미니 배치의 각 샘플은 서로 다른 μ, σ를 갖지만 평균 및 표준 편차는 레이어의 모든 뉴런에 대해 동일합니다.</p>
<p>일반적인 모델 크기의 경우 레이어 정규화가 배치 표준보다 느리다는 점에 유의해야 합니다. </p>
<p>따라서 DEST(속도를 위해 설계되었지만 여기서는 소개하지 않음)와 같은 일부 아키텍처는 엔지니어링 트릭을 사용하여 훈련을 안정적으로 유지하면서 배치 정규화를 사용합니다. </p>
<p>그러나 가장 널리 사용되는 ViT의 경우 레이어 정규화가 사용되며 성능에 매우 중요합니다.</p>
<h2 id="vit의-결과">ViT의 결과<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#vit의-결과" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p><img src="../../../../../vault/resources/Untitled-5-7.png" width="auto" height="auto" alt="Untitled 5 7.png"/></p>
<ul>
<li>가장 왼쪽의 <strong>RGB embedding filter 그림의 시사점은 ViT 또한 CNN과 같은 형태로 학습이 되었다는 점</strong>
<ul>
<li>RGB embedding filter : Transformer Encoder에 입력되기 전 Embedding을 할 때 사용한 filter</li>
<li>filter의 일부분을 가져와서 시각화 하였을 때, 위 그림과 같은 형태가 나타남</li>
<li>핵심은 CNN에서의 low level layer에서와 유사한 형태의 결과가 시각화로 나타난다는 점</li>
<li>즉, CNN 처럼 학습이 잘 되었다는 것을 의미</li>
</ul>
</li>
<li>중간의 Positional Embedding Similarity 그림의 시사점은 <strong>Positional Embedding이 데이터의 위치를 잘 의미하도록 학습이 잘 되었다는 점</strong>
<ul>
<li>Positional Embedding 에는 각 패치 마다 대응되는 Embedding 벡터가 존재</li>
<li>모든 패치 p(i,pj) 에 대하여 cosine similarity를 구하였을 때 각 row, col에 해당하는 부분의 패치가 similarity가 높은 것을 통해 Position이 의미가 있도록 학습이 잘 되었다는 것을 확인</li>
</ul>
</li>
<li>가장 오른쪽의 그래프는 attention이 관심을 두는 위치의 편차를 나타냄
<ul>
<li><strong>low level layer : 가까운 곳에서부터 먼 곳까지 모두 살펴 보고 있음</strong></li>
<li><strong>high level layer : 전체적으로 보고 있음</strong>
<ul>
<li>y축의 distance의 의미는 어떤 query 위치를 기준으로 의미있는 영역까지의 평균 거리</li>
<li>이 거리가 짧을수록 가까운 영역에 대하여 attention</li>
<li>이 거리가 길수록 먼 영역에 대하여 attention</li>
</ul>
</li>
</ul>
</li>
<li>CNN 또한 convolution 연산의 특성상 layer가 깊어질수록 점점 더 큰 영역을 보게되는데 Vision Transformer 또한 그러한 성질을 가지는 것을 확인할 수 있었습니다.</li>
</ul>
<h2 id="pytorch를-이용한-vit-구현">Pytorch를 이용한 ViT 구현<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#pytorch를-이용한-vit-구현" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>import torch</span></span>
<span data-line><span>import torch.nn as nn</span></span>
<span data-line> </span>
<span data-line><span>class LinearProjection(nn.Module):</span></span>
<span data-line> </span>
<span data-line><span>    def __init__(self, patch_vec_size, num_patches, latent_vec_dim, drop_rate):</span></span>
<span data-line><span>        super().__init__()</span></span>
<span data-line><span>        self.linear_proj = nn.Linear(patch_vec_size, latent_vec_dim)</span></span>
<span data-line><span>        self.cls_token = nn.Parameter(torch.randn(1, latent_vec_dim))</span></span>
<span data-line><span>        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, latent_vec_dim))</span></span>
<span data-line><span>        self.dropout = nn.Dropout(drop_rate)</span></span>
<span data-line> </span>
<span data-line><span>    def forward(self, x):</span></span>
<span data-line><span>        batch_size = x.size(0)</span></span>
<span data-line><span>        x = torch.cat([self.cls_token.repeat(batch_size, 1, 1), self.linear_proj(x)], dim=1)</span></span>
<span data-line><span>        x += self.pos_embedding</span></span>
<span data-line><span>        x = self.dropout(x)</span></span>
<span data-line><span>        return x</span></span>
<span data-line> </span>
<span data-line><span>class MultiheadedSelfAttention(nn.Module):</span></span>
<span data-line><span>    def __init__(self, latent_vec_dim, num_heads, drop_rate):</span></span>
<span data-line><span>        super().__init__()</span></span>
<span data-line><span>        device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span></span>
<span data-line><span>        self.num_heads = num_heads</span></span>
<span data-line><span>        self.latent_vec_dim = latent_vec_dim</span></span>
<span data-line><span>        self.head_dim = int(latent_vec_dim / num_heads)</span></span>
<span data-line><span>        self.query = nn.Linear(latent_vec_dim, latent_vec_dim)</span></span>
<span data-line><span>        self.key = nn.Linear(latent_vec_dim, latent_vec_dim)</span></span>
<span data-line><span>        self.value = nn.Linear(latent_vec_dim, latent_vec_dim)</span></span>
<span data-line><span>        self.scale = torch.sqrt(latent_vec_dim*torch.ones(1)).to(device)</span></span>
<span data-line><span>        self.dropout = nn.Dropout(drop_rate)</span></span>
<span data-line> </span>
<span data-line><span>    def forward(self, x):</span></span>
<span data-line><span>        batch_size = x.size(0)</span></span>
<span data-line><span>        q = self.query(x)</span></span>
<span data-line><span>        k = self.key(x)</span></span>
<span data-line><span>        v = self.value(x)</span></span>
<span data-line><span>        q = q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)</span></span>
<span data-line><span>        k = k.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,3,1) # k.t</span></span>
<span data-line><span>        v = v.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)</span></span>
<span data-line><span>        attention = torch.softmax(q @ k / self.scale, dim=-1)</span></span>
<span data-line><span>        x = self.dropout(attention) @ v</span></span>
<span data-line><span>        x = x.permute(0,2,1,3).reshape(batch_size, -1, self.latent_vec_dim)</span></span>
<span data-line> </span>
<span data-line><span>        return x, attention</span></span>
<span data-line> </span>
<span data-line><span>class TFencoderLayer(nn.Module):</span></span>
<span data-line><span>    def __init__(self, latent_vec_dim, num_heads, mlp_hidden_dim, drop_rate):</span></span>
<span data-line><span>        super().__init__()</span></span>
<span data-line><span>        self.ln1 = nn.LayerNorm(latent_vec_dim)</span></span>
<span data-line><span>        self.ln2 = nn.LayerNorm(latent_vec_dim)</span></span>
<span data-line><span>        self.msa = MultiheadedSelfAttention(latent_vec_dim=latent_vec_dim, num_heads=num_heads, drop_rate=drop_rate)</span></span>
<span data-line><span>        self.dropout = nn.Dropout(drop_rate)</span></span>
<span data-line><span>        self.mlp = nn.Sequential(nn.Linear(latent_vec_dim, mlp_hidden_dim),</span></span>
<span data-line><span>                                 nn.GELU(), nn.Dropout(drop_rate),</span></span>
<span data-line><span>                                 nn.Linear(mlp_hidden_dim, latent_vec_dim),</span></span>
<span data-line><span>                                 nn.Dropout(drop_rate))</span></span>
<span data-line> </span>
<span data-line><span>    def forward(self, x):</span></span>
<span data-line><span>        z = self.ln1(x)</span></span>
<span data-line><span>        z, att = self.msa(z)</span></span>
<span data-line><span>        z = self.dropout(z)</span></span>
<span data-line><span>        x = x + z</span></span>
<span data-line><span>        z = self.ln2(x)</span></span>
<span data-line><span>        z = self.mlp(z)</span></span>
<span data-line><span>        x = x + z</span></span>
<span data-line> </span>
<span data-line><span>        return x, att</span></span>
<span data-line> </span>
<span data-line><span>class VisionTransformer(nn.Module):</span></span>
<span data-line><span>    def __init__(self, patch_vec_size, num_patches, latent_vec_dim, num_heads, mlp_hidden_dim, drop_rate, num_layers, num_classes):</span></span>
<span data-line><span>        super().__init__()</span></span>
<span data-line><span>        self.patchembedding = LinearProjection(patch_vec_size=patch_vec_size, num_patches=num_patches,</span></span>
<span data-line><span>                                               latent_vec_dim=latent_vec_dim, drop_rate=drop_rate)</span></span>
<span data-line><span>        self.transformer = nn.ModuleList([TFencoderLayer(latent_vec_dim=latent_vec_dim, num_heads=num_heads,</span></span>
<span data-line><span>                                                         mlp_hidden_dim=mlp_hidden_dim, drop_rate=drop_rate)</span></span>
<span data-line><span>                                          for _ in range(num_layers)])</span></span>
<span data-line> </span>
<span data-line><span>        self.mlp_head = nn.Sequential(nn.LayerNorm(latent_vec_dim), nn.Linear(latent_vec_dim, num_classes))</span></span>
<span data-line> </span>
<span data-line><span>    def forward(self, x):</span></span>
<span data-line><span>        att_list = []</span></span>
<span data-line><span>        x = self.patchembedding(x)</span></span>
<span data-line><span>        for layer in self.transformer:</span></span>
<span data-line><span>            x, att = layer(x)</span></span>
<span data-line><span>            att_list.append(att)</span></span>
<span data-line><span>        x = self.mlp_head(x[:,0])</span></span>
<span data-line> </span>
<span data-line><span>        return x, att_list</span></span></code></pre></figure></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div class="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false,&quot;enableRadial&quot;:false}"></div><button class="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div class="global-graph-outer"><div class="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.2,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true,&quot;enableRadial&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul class="toc-content overflow" id="list-1"><li class="depth-0"><a href="#참조" data-for="참조">참조</a></li><li class="depth-0"><a href="#vit란-무엇인가" data-for="vit란-무엇인가">ViT란 무엇인가?</a></li><li class="depth-0"><a href="#vit의-특징" data-for="vit의-특징">ViT의 특징</a></li><li class="depth-0"><a href="#vit-inference-과정" data-for="vit-inference-과정">ViT Inference 과정</a></li><li class="depth-0"><a href="#class-token이-있는-이유는" data-for="class-token이-있는-이유는">Class Token이 있는 이유는?</a></li><li class="depth-0"><a href="#layer-normalization" data-for="layer-normalization">Layer Normalization</a></li><li class="depth-0"><a href="#vit의-결과" data-for="vit의-결과">ViT의 결과</a></li><li class="depth-0"><a href="#pytorch를-이용한-vit-구현" data-for="pytorch를-이용한-vit-구현">Pytorch를 이용한 ViT 구현</a></li><li class="overflow-end"></li></ul></div><div class="backlinks"><h3>Backlinks</h3><ul id="list-2" class="overflow"><li><a href="../../../../../vault/Notion/HyeongSeok-Kim’s-Notion/HyeongSeok-Kim’s-Notion" class="internal">HyeongSeok Kim’s Notion</a></li><li><a href="../../../../../vault/" class="internal">Main</a></li><li class="overflow-end"></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.5.1</a> © 2025</p><ul><li><a href="https://github.com/404Vector/404Vector">GitHub</a></li></ul></footer></div></div></body><script type="application/javascript">function n(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.getElementsByClassName("callout-content")[0];if(!e)return;let l=t.classList.contains("is-collapsed");e.style.gridTemplateRows=l?"0fr":"1fr"}function c(){let t=document.getElementsByClassName("callout is-collapsible");for(let e of t){let l=e.getElementsByClassName("callout-title")[0],s=e.getElementsByClassName("callout-content")[0];if(!l||!s)continue;l.addEventListener("click",n),window.addCleanup(()=>l.removeEventListener("click",n));let o=e.classList.contains("is-collapsed");s.style.gridTemplateRows=o?"0fr":"1fr"}}document.addEventListener("nav",c);
</script><script type="module">function f(i,e){if(!i)return;function r(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function t(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}i?.addEventListener("click",r),window.addCleanup(()=>i?.removeEventListener("click",r)),document.addEventListener("keydown",t),window.addCleanup(()=>document.removeEventListener("keydown",t))}function y(i){for(;i.firstChild;)i.removeChild(i.firstChild)}var h=class{constructor(e,r){this.container=e;this.content=r;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),r=this.onMouseMove.bind(this),t=this.onMouseUp.bind(this),o=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",r),document.addEventListener("mouseup",t),window.addEventListener("resize",o),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",r),()=>document.removeEventListener("mouseup",t),()=>window.removeEventListener("resize",o))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let r=this.createButton("+",()=>this.zoom(.1)),t=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(t),e.appendChild(o),e.appendChild(r),this.container.appendChild(e)}createButton(e,r){let t=document.createElement("button");return t.textContent=e,t.className="mermaid-control-button",t.addEventListener("click",r),window.addCleanup(()=>t.removeEventListener("click",r)),t}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}zoom(e){let r=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),t=this.content.getBoundingClientRect(),o=t.width/2,n=t.height/2,c=r-this.scale;this.currentPan.x-=o*c,this.currentPan.y-=n*c,this.scale=r,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){this.scale=1;let e=this.content.querySelector("svg");this.currentPan={x:e.getBoundingClientRect().width/2,y:e.getBoundingClientRect().height/2},this.updateTransform()}},C=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],E;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;E||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let r=E.default,t=new WeakMap;for(let n of e)t.set(n,n.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let a=t.get(s);a&&(s.innerHTML=a)}let n=C.reduce((s,a)=>(s[a]=window.getComputedStyle(document.documentElement).getPropertyValue(a),s),{}),c=document.documentElement.getAttribute("saved-theme")==="dark";r.initialize({startOnLoad:!1,securityLevel:"loose",theme:c?"dark":"base",themeVariables:{fontFamily:n["--codeFont"],primaryColor:n["--light"],primaryTextColor:n["--darkgray"],primaryBorderColor:n["--tertiary"],lineColor:n["--darkgray"],secondaryColor:n["--secondary"],tertiaryColor:n["--tertiary"],clusterBkg:n["--light"],edgeLabelBackground:n["--highlight"]}}),await r.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let n=0;n<e.length;n++){let v=function(){let g=l.querySelector("#mermaid-space"),m=l.querySelector(".mermaid-content");if(!m)return;y(m);let w=c.querySelector("svg").cloneNode(!0);m.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new h(g,m)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},c=e[n],s=c.parentElement,a=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(a),L=a.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),f(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../../../../../postscript.js" type="module"></script></html>