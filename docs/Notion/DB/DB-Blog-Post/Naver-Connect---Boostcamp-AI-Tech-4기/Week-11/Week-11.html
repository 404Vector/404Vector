<!DOCTYPE html>
<html lang="en"><head><title>Week 11</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;family=IBM Plex Mono:wght@400;600&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="HyeongSeok Kim's Vault"/><meta property="og:title" content="Week 11"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Week 11"/><meta name="twitter:description" content="Advanced Object Detection Casecade RCNN Contribution Method Deformable Convolution Networks(DCN) Contribution Method ViT End to End Object Detection with Transformer Contibution Architecture Swin Transformer VIT의 문제점 Contibution Architecture YOLO v4 Overview Related work Selection of architecture A..."/><meta property="og:description" content="Advanced Object Detection Casecade RCNN Contribution Method Deformable Convolution Networks(DCN) Contribution Method ViT End to End Object Detection with Transformer Contibution Architecture Swin Transformer VIT의 문제점 Contibution Architecture YOLO v4 Overview Related work Selection of architecture A..."/><meta property="og:image:alt" content="Advanced Object Detection Casecade RCNN Contribution Method Deformable Convolution Networks(DCN) Contribution Method ViT End to End Object Detection with Transformer Contibution Architecture Swin Transformer VIT의 문제점 Contibution Architecture YOLO v4 Overview Related work Selection of architecture A..."/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11"/><meta property="twitter:url" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11"/><link rel="icon" href="../../../../../static/icon.png"/><meta name="description" content="Advanced Object Detection Casecade RCNN Contribution Method Deformable Convolution Networks(DCN) Contribution Method ViT End to End Object Detection with Transformer Contibution Architecture Swin Transformer VIT의 문제점 Contibution Architecture YOLO v4 Overview Related work Selection of architecture A..."/><meta name="generator" content="Quartz"/><link href="../../../../../index.css" rel="stylesheet" type="text/css" spa-preserve/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  padding: 2rem;
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,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 */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../../../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../../../../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://quartz.jzhao.xyz/index.xml"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11-og-image.webp"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11-og-image.webp"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11-og-image.webp"/><meta property="og:image:type" content="image/.webp"/></head><body data-slug="Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/Week-11"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../../../../..">HyeongSeok Kim's Vault</a></h2><div class="spacer mobile-only"></div><div class="flex-component" style="flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search"><button class="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div class="search-layout" data-preview="true"></div></div></div></div></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="readermode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="readerIcon" fill="currentColor" stroke="currentColor" stroke-width="0.2" stroke-linecap="round" stroke-linejoin="round" width="64px" height="64px" viewBox="0 0 24 24" aria-label="Reader mode"><title>Reader mode</title><g transform="translate(-1.8, -1.8) scale(1.15, 1.2)"><path d="M8.9891247,2.5 C10.1384702,2.5 11.2209868,2.96705384 12.0049645,3.76669482 C12.7883914,2.96705384 13.8709081,2.5 15.0202536,2.5 L18.7549359,2.5 C19.1691495,2.5 19.5049359,2.83578644 19.5049359,3.25 L19.5046891,4.004 L21.2546891,4.00457396 C21.6343849,4.00457396 21.9481801,4.28672784 21.9978425,4.6528034 L22.0046891,4.75457396 L22.0046891,20.25 C22.0046891,20.6296958 21.7225353,20.943491 21.3564597,20.9931534 L21.2546891,21 L2.75468914,21 C2.37499337,21 2.06119817,20.7178461 2.01153575,20.3517706 L2.00468914,20.25 L2.00468914,4.75457396 C2.00468914,4.37487819 2.28684302,4.061083 2.65291858,4.01142057 L2.75468914,4.00457396 L4.50368914,4.004 L4.50444233,3.25 C4.50444233,2.87030423 4.78659621,2.55650904 5.15267177,2.50684662 L5.25444233,2.5 L8.9891247,2.5 Z M4.50368914,5.504 L3.50468914,5.504 L3.50468914,19.5 L10.9478955,19.4998273 C10.4513189,18.9207296 9.73864328,18.5588115 8.96709342,18.5065584 L8.77307039,18.5 L5.25444233,18.5 C4.87474657,18.5 4.56095137,18.2178461 4.51128895,17.8517706 L4.50444233,17.75 L4.50368914,5.504 Z M19.5049359,17.75 C19.5049359,18.1642136 19.1691495,18.5 18.7549359,18.5 L15.2363079,18.5 C14.3910149,18.5 13.5994408,18.8724714 13.0614828,19.4998273 L20.5046891,19.5 L20.5046891,5.504 L19.5046891,5.504 L19.5049359,17.75 Z M18.0059359,3.999 L15.0202536,4 L14.8259077,4.00692283 C13.9889509,4.06666544 13.2254227,4.50975805 12.7549359,5.212 L12.7549359,17.777 L12.7782651,17.7601316 C13.4923805,17.2719483 14.3447024,17 15.2363079,17 L18.0059359,16.999 L18.0056891,4.798 L18.0033792,4.75457396 L18.0056891,4.71 L18.0059359,3.999 Z M8.9891247,4 L6.00368914,3.999 L6.00599909,4.75457396 L6.00599909,4.75457396 L6.00368914,4.783 L6.00368914,16.999 L8.77307039,17 C9.57551536,17 10.3461406,17.2202781 11.0128313,17.6202194 L11.2536891,17.776 L11.2536891,5.211 C10.8200889,4.56369974 10.1361548,4.13636104 9.37521067,4.02745763 L9.18347055,4.00692283 L8.9891247,4 Z"></path></g></svg></button></div></div><div class="explorer" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>node&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-content"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div class="explorer-content" aria-expanded="false"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../../../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/">Notion</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/DB/">DB</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/DB/DB-Blog-Post/">DB Blog Post</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/">Naver Connect   Boostcamp AI Tech 4기</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-11/">Week 11</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Week 11</a></div></nav><h1 class="article-title">Week 11</h1><p show-comma="true" class="content-meta"><time datetime="2025-06-17T02:24:26.043Z">Jun 17, 2025</time><span>14 min read</span></p></div></div><article class="popover-hint"><ul>
<li><a href="#advanced-object-detection" class="internal alias">Advanced Object Detection</a>
<ul>
<li><a href="#casecade-rcnn" class="internal alias">Casecade RCNN</a>
<ul>
<li><a href="#contribution" class="internal alias">Contribution</a></li>
<li><a href="#method" class="internal alias">Method</a></li>
</ul>
</li>
<li><a href="#deformable-convolution-networksdcn" class="internal alias">Deformable Convolution Networks(DCN)</a>
<ul>
<li><a href="#contribution" class="internal alias">Contribution</a></li>
<li><a href="#method" class="internal alias">Method</a></li>
</ul>
</li>
<li><a href="#vit" class="internal alias">ViT</a></li>
<li><a href="#end-to-end-object-detection-with-transformer" class="internal alias">End to End Object Detection with Transformer</a>
<ul>
<li><a href="#contibution" class="internal alias">Contibution</a></li>
<li><a href="#architecture" class="internal alias">Architecture</a></li>
</ul>
</li>
<li><a href="#swin-transformer" class="internal alias">Swin Transformer</a>
<ul>
<li><a href="#vit%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90" class="internal alias">VIT의 문제점</a></li>
<li><a href="#contibution" class="internal alias">Contibution</a></li>
<li><a href="#architecture" class="internal alias">Architecture</a></li>
</ul>
</li>
<li><a href="#yolo-v4" class="internal alias">YOLO v4</a>
<ul>
<li><a href="#overview" class="internal alias">Overview</a></li>
<li><a href="#related-work" class="internal alias">Related work</a></li>
<li><a href="#selection-of-architecture" class="internal alias">Selection of architecture</a></li>
<li><a href="#additional-improvements" class="internal alias">Additional Improvements</a></li>
</ul>
</li>
<li><a href="#m2det" class="internal alias">M2Det</a>
<ul>
<li><a href="#overview" class="internal alias">Overview</a></li>
<li><a href="#architecture" class="internal alias">Architecture</a></li>
</ul>
</li>
<li><a href="#cornernet" class="internal alias">CornerNet</a>
<ul>
<li><a href="#overview" class="internal alias">Overview</a></li>
<li><a href="#architecture" class="internal alias">Architecture</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr/>
<h1 id="advanced-object-detection">Advanced Object Detection<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#advanced-object-detection" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="casecade-rcnn">Casecade RCNN<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#casecade-rcnn" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="contribution">Contribution<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#contribution" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Faster RCNN</p>
<p>RPN을 사용한 최초의 End to End network</p>
<p>IoU 0.7 이상을 positive, 0.3 이하를 negative로 훈련시켰다</p>
<p>그 후, 0.5를 기준으로 True / False를 판별하여 성능을 측정했다</p>
<p>→ 왜 0.5로 했는가? 0.6이면 어떻게 변화하는가? 에 대한 고찰이 없었다</p>
<p>Casecade RCNN</p>
<p>Faster RCNN에서 하지 않았던 고찰들을 실험을 통해 명확히 하였음<br/>
또한 실험 결과를 기반으로 새로운 모델을 제시, 성능 향상을 이뤄냄</p>
<h3 id="method">Method<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#method" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Cascade R-CNN: Delving into High Quality Object Detection </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>In object detection, an intersection over union (IoU) threshold is required to define positives and negatives.<br/>
<a href="https://arxiv.org/abs/1712.00726" class="external">https://arxiv.org/abs/1712.00726<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../../../../resources/Untitled-75.png" width="auto" height="auto" alt="Untitled 75.png"/></p>
<p>input iou : RPN의 결과 값으로 그린 bbox와 GT 간의 IOU</p>
<p>output iou : box-head를 통과한 최종 bbox와 GT 간의 IOU</p>
<p>훈련에서 사용한 iou 기준이 높을수록, rpn이 bbox를 잘 예측했을 때(input iou가 클수록) box-head를 통과한 최종 bbox 예측 성능(output iou)도 좋아짐</p>
<p>그러나, rpn이 iou를 잘 예측하지 못한다면, 낮은 iou threshold로 훈련하는 것이 더 좋은 성능을 보여준다는 것을 알 수 있다</p>
<p>나아가서 detection 성능 또한 0.5로 훈련시킨 것이 전반적으로 좋은 map를 얻을 수 있는 것을 실험적으로 증명했다<br/>
→ high quality detection이 필요하다면, 높은 iou 기준으로 훈련 및 높은 iou threshold로 분류(map 기준에서 성능 하락)<br/>
→ low quality detection이 필요하다면, 낮은 iou 기준으로 훈련 및 낮은 Iou threshold로 분류</p>
<p><img src="../../../../../resources/Untitled-1-56.png" width="auto" height="auto" alt="Untitled 1 56.png"/></p>
<p>선행연구(interactive bbox at inference, integral loss)는 약간의 성능 향상이 있었음</p>
<p><img src="../../../../../resources/Untitled-2-40.png" width="auto" height="auto" alt="Untitled 2 40.png"/></p>
<p>그러나 이를 조합한다면(Casecade R-CNN), 더 큰 성능 향상을 얻을 수 있었음</p>
<h2 id="deformable-convolution-networksdcn">Deformable Convolution Networks(DCN)<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#deformable-convolution-networksdcn" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="contribution-1">Contribution<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#contribution-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>CNN의 문제</p>
<p>CNN은 Geometric Transformation들에 한계를 지님</p>
<p>기존 해결방법</p>
<p>Geometric augmentation을 수행</p>
<p>하지만, 그러므로 휴리스틱하게 사람이 넣어준 augmentation에 대해서만 학습을 수행하므로 사람이 잘 넣어줘야 함</p>
<p>Deformable Convolution</p>
<p>task에 따라 CNN을 변형시켜서 학습</p>
<h3 id="method-1">Method<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#method-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Deformable Convolutional Networks </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules.<br/>
<a href="https://arxiv.org/abs/1703.06211" class="external">https://arxiv.org/abs/1703.06211<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>DCN </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>(Deformable Convolutional Networks) Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the ﬁxed geometric structures in their building modules.<br/>
<a href="https://jjeamin.github.io/posts/DCN/" class="external">https://jjeamin.github.io/posts/DCN/<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>Deformable Convolution</p>
<p><img src="../../../../../resources/Untitled-3-29.png" width="auto" height="auto" alt="Untitled 3 29.png"/></p>
<p>기존 kernel size를 유지한 체, 각 sampling position에 대한 offset을 파라미터에 추가한다</p>
<p><img src="../../../../../resources/Untitled-4-21.png" width="auto" height="auto" alt="Untitled 4 21.png"/></p>
<p><img src="../../../../../resources/Untitled-5-17.png" width="auto" height="auto" alt="Untitled 5 17.png"/></p>
<p>CNN을 통해 Offset Field R을 생성하고, R을 참조하여 Conv 시 Sampling Position을 결정한다 offset은 아주 작은 값이기 때문에 소수점이 될 수 있고, 이 경우 bilinear 보간을 통해 값을 얻는다</p>
<p>offset filed는 왜 2N일까?</p>
<p>이미지 공간은 2차원이으로 offset을 표현하기 위해서는 dx, dy가 필요하다 즉, Offset Filed R의 크기가 N이라면, 사실 2N의 크기가 필요하다</p>
<p><img src="../../../../../resources/Untitled-6-14.png" width="auto" height="auto" alt="Untitled 6 14.png"/></p>
<p><img src="../../../../../resources/Untitled-7-10.png" width="auto" height="auto" alt="Untitled 7 10.png"/></p>
<p>Faster R-CNN등에서는 큰 성능 향상이 없었다 그러나 DeepLab 등의 segmantation task에서는 큰 성능향상이 있다</p>
<p>Object detection, Segmentation에서 좋은 효과를 보인다</p>
<h2 id="vit">ViT<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#vit" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited.<br/>
<a href="https://arxiv.org/abs/2010.11929" class="external">https://arxiv.org/abs/2010.11929<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>NLP에서 long range dependanc를 해결</p>
<p>vision에 적용한 case가 ViT</p>
<p>이전에 학습한 내용과 크게 상이하지 않으므로 생략</p>
<h2 id="end-to-end-object-detection-with-transformer">End to End Object Detection with Transformer<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#end-to-end-object-detection-with-transformer" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>End-to-End Object Detection with Transformers </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>We present a new method that views object detection as a direct set prediction problem.<br/>
<a href="https://arxiv.org/abs/2005.12872" class="external">https://arxiv.org/abs/2005.12872<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<h3 id="contibution">Contibution<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#contibution" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Trainsformer를 처음으로 Object Detection에 적용</p>
<p>기존의 Object Detection의 Hand-Crafted post process 단계(ex: NMS)를 Transformer를 사용해 없앰</p>
<h3 id="architecture">Architecture<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#architecture" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../../../../../resources/Untitled-8-8.png" width="auto" height="auto" alt="Untitled 8 8.png"/></p>
<p>CNN을 backbone으로 feature map 추출, 지정한 개수 만큼 transformer를 사용해서 bbox 생성</p>
<p><img src="../../../../../resources/Untitled-9-7.png" width="auto" height="auto" alt="Untitled 9 7.png"/></p>
<p>Transformer 특성상 많은 연산이 필요하기 때문에, Highest level feature map을 사용해서 입력을 감소시킴</p>
<p>decoder를 통과한 결과 값을 Feed forward network를 통과시켜 bbox를 얻음</p>
<p>decoder 1개당 1개의 ffn과 1개의 bbox 및 class 예측을 수행하기 때문에 이미지에 존재하는 오브젝트의 총 개수보다 디코더 개수가 많아야 함</p>
<p><img src="../../../../../resources/Untitled-10-6.png" width="auto" height="auto" alt="Untitled 10 6.png"/></p>
<h2 id="swin-transformer">Swin Transformer<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#swin-transformer" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision.<br/>
<a href="https://arxiv.org/abs/2103.14030" class="external">https://arxiv.org/abs/2103.14030<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<h3 id="vit의-문제점">VIT의 문제점<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#vit의-문제점" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>많은 양의 data 학습 필요</p>
<p>transformer 특성상 computational const가 큼</p>
<p>일반적인 backbone으로 사용이 어려움</p>
<h3 id="contibution-1">Contibution<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#contibution-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>CNN과 유사 구조로 설계</p>
<p>Window라는 개념을 활용하여 cost 감소</p>
<h3 id="architecture-1">Architecture<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#architecture-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../../../../../resources/Untitled-11-6.png" width="auto" height="auto" alt="Untitled 11 6.png"/></p>
<ol>
<li>
<p>Patch Partitioning(이미지를 patch 크기로 나눔)</p>
</li>
<li>
<p>Linear Embeding(Detection task를 목적으로 하기 때문에 CLS Token 없음)</p>
</li>
<li>
<p>Swin Transformer Block</p>
<ol>
<li>
<p>Window Multi-head self attention</p>
<p><img src="../../../../../resources/Untitled-12-5.png" width="auto" height="auto" alt="Untitled 12 5.png"/></p>
</li>
<li>
<p>Shifted window Multi-head attention</p>
</li>
</ol>
<p><img src="../../../../../resources/Untitled-13-5.png" width="auto" height="auto" alt="Untitled 13 5.png"/></p>
<p><img src="../../../../../resources/Untitled-14-4.png" width="auto" height="auto" alt="Untitled 14 4.png"/></p>
</li>
<li>
<p>Patch Merging</p>
</li>
</ol>
<p>매우 큰 성능 향상</p>
<p>적은 data에도 학습이 잘됨</p>
<p>window단위로 computation cost 감소</p>
<p>cnn과 비슷한 구조로 object detction backbone으로 사용 가능</p>
<h2 id="yolo-v4">YOLO v4<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#yolo-v4" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="overview">Overview<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#overview" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Contribution</p>
<p>하나의 GPU에서 훈련할 수 있는 빠르고 정확한 Object detector</p>
<p>BOF(Bag of Freebies), BOS방법들을 실험을 통해서 증명하고 조합을 찾음</p>
<ul>
<li>BOF : Inference 비용을 늘리지 안호 정확도를 향상시키는 방법</li>
<li>BOS : Inference 비용을 조금 높이지만 정확도가 크게 향상되는 방법</li>
</ul>
<p>GPU 학습에 더 효율적이고 적합하도록 방법들을 변형</p>
<h3 id="related-work">Related work<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#related-work" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Bag of freebies</p>
<ul>
<li>
<p>Data augmentation</p>
<ul>
<li>Photometric Distortions</li>
<li>Geometric Distortions</li>
<li>CutOut</li>
<li>Random Erase</li>
<li>MixUp</li>
<li>CutMix</li>
<li>GAN</li>
</ul>
</li>
<li>
<p>Semantic Distribution Bias(Dataset 불균형을 해결하기 위한 방법)</p>
<ul>
<li>Hard Negative Mining
<ul>
<li>어려운 배경을 강제로 batch에 많이 포함시킴</li>
</ul>
</li>
<li>OHEM</li>
<li>Focal Loss</li>
<li>Label Smoothing
<ul>
<li>0→0.1 , 1→ 0.9로 smoothing</li>
<li>잘못된 label이 많을 경우, 효과적</li>
<li>overfitting을 막아주고 regularization 효과 기대</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Bounding Box Regression</p>
<ul>
<li>
<p>MSE</p>
<ul>
<li>GT 와 bbox의 거리에 대한 loss, 허나 iou 오차를 대변하지 못하기 때문에 iou관련 loss를 추가로 사용</li>
</ul>
</li>
<li>
<p>GIOU</p>
</li>
<li>
<p>DIoU</p>
</li>
<li>
<p>CIoU</p>
</li>
</ul>
</li>
</ul>
<p>Bag of specials</p>
<ul>
<li>Enhancement of Receptive field
<ul>
<li>SPP(Spatial Pyramid Pooling)</li>
<li>ASPP(Atrous SPP)</li>
<li>RFB(Receptive field block)</li>
</ul>
</li>
<li>Attension mudle
<ul>
<li>SE(Squeeze and excitation)</li>
<li>SAM(spatial attension module)</li>
</ul>
</li>
<li>Feature integration(=Neck)
<ul>
<li>FPN(Feature Pyramid network)</li>
<li>SFPM(Scale-wise feature aggregation module)</li>
<li>ASFF(Adaptively spatial feature)</li>
<li>BiFPN</li>
</ul>
</li>
<li>Activation Function
<ul>
<li>ReLU</li>
<li>Leaky ReLU</li>
<li>Parametric ReLU</li>
<li>ReLU6</li>
<li>Swish / Mish
<ul>
<li>약간의 음수를 허용하기 때문에 ReLU의 zero bound 보다 gradient흐름에 좋은 영향</li>
<li>모든 구간 미분 가능</li>
</ul>
</li>
</ul>
</li>
<li>Post-processing method
<ul>
<li>NMS(Non maximum suppression)</li>
<li>Soft NMS</li>
<li>DIoU NMS</li>
</ul>
</li>
</ul>
<h3 id="selection-of-architecture">Selection of architecture<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#selection-of-architecture" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>YOLOv4: Optimal Speed and Accuracy of Object Detection </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy.<br/>
<a href="https://arxiv.org/abs/2004.10934" class="external">https://arxiv.org/abs/2004.10934<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../../../../resources/Untitled-15-4.png" width="auto" height="auto" alt="Untitled 15 4.png"/></p>
<ul>
<li>
<p>기존 Yolo Architecture에 CSPNet을 추가</p>
</li>
</ul>
<p>CSPNet(Cross Stage Partial Network)</p>
<ul>
<li>
<p>정확도 유지 + 경량화</p>
</li>
<li>
<p>메모리 cost 감소</p>
</li>
<li>
<p>다양한 backbone에서 사용 가능</p>
</li>
<li>
<p>연산 bottleneck 제거</p>
</li>
<li>
<p>기존 DenseNet의 문제</p>
<ul>
<li>
<p>가중치 업데이트 시, gradient 정보를 재사용하게 됨</p>
</li>
</ul>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>CSPNet: A New Backbone that can Enhance Learning Capability of CNN <br/>
Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection.<br/>
<a href="https://arxiv.org/abs/1911.11929" class="external">https://arxiv.org/abs/1911.11929<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p></div>
                  
                </div>
</blockquote>
<p><img src="../../../../../resources/Untitled-16-4.png" width="auto" height="auto" alt="Untitled 16 4.png"/></p>
</li>
<li>
<p>모든 feature map을 사용하는 방식에서 일부 feature map을 사용하는 방식으로 변경</p>
<p><img src="../../../../../resources/Untitled-17-4.png" width="auto" height="auto" alt="Untitled 17 4.png"/></p>
</li>
</ul>
<h3 id="additional-improvements">Additional Improvements<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#additional-improvements" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../../../../../resources/Untitled-18-4.png" width="auto" height="auto" alt="Untitled 18 4.png"/></p>
<ul>
<li>New data augmentation
<ul>
<li>Mosaic
<ul>
<li>4장의 이미지를 합처버림</li>
</ul>
</li>
<li>SAT(Self-Adversarial Training)
<ul>
<li>모델의 오검출을 유도</li>
</ul>
</li>
</ul>
</li>
<li>기존 방법 변형
<ul>
<li>modified SAM</li>
<li>modified PAN</li>
<li>CmBN(Cross mini-batch normalization)</li>
</ul>
</li>
</ul>
<h2 id="m2det">M2Det<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#m2det" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Feature pyramids are widely exploited by both the state-of-the-art one-stage object detectors (e.<br/>
<a href="https://arxiv.org/abs/1811.04533" class="external">https://arxiv.org/abs/1811.04533<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<h3 id="overview-1">Overview<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#overview-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Background</p>
<p>물체에 대한 scale 변화는 object detection의 과제</p>
<ul>
<li>
<p>Image Pyramid</p>
</li>
<li>
<p>Feature Pyramid(Neck의 기본 구조)</p>
</li>
</ul>
<p>Feature Pyramid의 한계점</p>
<p><img src="../../../../../resources/Untitled-19-4.png" width="auto" height="auto" alt="Untitled 19 4.png"/></p>
<ul>
<li>backbone으로부터 feature pyramid 구성</li>
<li>Classification task를 위해 설계된 backbone은 object detection task를 수행하기에 충분하지 않음</li>
<li>backbone network는 single-level layer로, singe-level 정보만을 나타냄</li>
<li>일반적으로, low-level feature는 간단한 외형을, high-level feature는 복잡한 외형을 나타내는 것에 적합</li>
</ul>
<h3 id="architecture-2">Architecture<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#architecture-2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>MLFPN(Multi-level, multi-scale feature pyramid) 제안</li>
<li>SSD에 합쳐서 M2Det이라는 One Stage Detector 제안</li>
</ul>
<p>Overall archtecture</p>
<p><img src="../../../../../resources/Untitled-20-4.png" width="auto" height="auto" alt="Untitled 20 4.png"/></p>
<p>Unet 구조의 TUM을 통과시키고 나온 Feature Map에서 가장 큰 FeatureMap(마지막 Featrue map)을 가져와 FFMv2 연산으로 concat하는 것을 반복</p>
<p>이후, Level별 Multi-scale Feature Map을 SFAM을 사용해 scale 별로 concat</p>
<p>마지막으로 Prediction Layers를 통과</p>
<p>FFM(Feature Fusion Module)</p>
<p><img src="../../../../../resources/Untitled-21-4.png" width="auto" height="auto" alt="Untitled 21 4.png"/></p>
<p>서로 다른 scale의 2 feature map</p>
<p>FFMv1 : 서로 다른 scale의 두 feature map을 channel wise로 conca해서 Base feature 생성(서로 다른 scale의 2 feature map을 합쳐 semantic 정보가 풍부)</p>
<p>FFMv2 : FFMv1 결과에 추가로 TUM의 가장 큰 feature map을 concat</p>
<p>TUM(Thinned U-shape Module)</p>
<p>Encoder-decoder 구조</p>
<p>구조상, 가장 마지막 feature map(디코더 출력)은 U Shape의 구조상 다른 scale의 feature map들의 정보가 모두 반영되어 있으므로 Multi-scale feature map으로 볼 수 있다</p>
<p>SFAM(Scale-wise Feature Aggregation Module)</p>
<p><img src="../../../../../resources/Untitled-22-4.png" width="auto" height="auto" alt="Untitled 22 4.png"/></p>
<p>동일 크기를 가진 feature map끼리 concat</p>
<p>각각의 scale의 feature들은 multi-level 정보를 포함</p>
<p>Scale-wise로 concat된 블록들은 SE block을 통해 추가적으로 가중치를 조절하고(채널 별 중요도 조정) SSD에 입력(이미 multi scale feature map이 있기 때문에, SSD Header 연산만을 사용)</p>
<p><img src="../../../../../resources/Untitled-23-4.png" width="auto" height="auto" alt="Untitled 23 4.png"/></p>
<h2 id="cornernet">CornerNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cornernet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>CornerNet: Detecting Objects as Paired Keypoints </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network.<br/>
<a href="https://arxiv.org/abs/1808.01244" class="external">https://arxiv.org/abs/1808.01244<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<h3 id="overview-2">Overview<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#overview-2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Anchor box의 단점</p>
<p>Anchor box의 수가 너무 많음(feature map의 각 pixel마다 n개의 anchor box가 생성됨)</p>
<ul>
<li>Positive Sample(객체)가 적고, 대부분이 Negative Sampe(배경)임
<ul>
<li>Class Imbalance가 존재</li>
</ul>
</li>
</ul>
<p>Anchor box를 사용할 때, 하이퍼 파라미터를 고려해야 함</p>
<ul>
<li>Anchor box 개수, 사이즈, 비율</li>
</ul>
<p>CornerNet</p>
<p>Anchor box가 없는 1stage detector</p>
<p>Center가 아닌 Corner를 사용하는 이유</p>
<ul>
<li>중심점을 잡게 되면 4개의 면을 모두 고려해야 하는 반면, corner를 사용하면 2개만 고려할 수 있음</li>
</ul>
<h3 id="architecture-3">Architecture<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#architecture-3" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../../../../../resources/Untitled-24-4.png" width="auto" height="auto" alt="Untitled 24 4.png"/></p>
<p>Hourglass를 사용해서 feature map을 생성</p>
<p>Hourgalss</p>
<p>Human pose estimation task에서 사용</p>
<p>Global, Local 정보 모두 추출 가능</p>
<p>Encoder(Feature 추출)-Decoder(Reconstruct) 구조</p>
<p>Prediction Module</p>
<p>2개의 Prediction Module을 사용</p>
<ul>
<li>Top-Left 담당 모듈</li>
<li>Bottom-Right 담당 모듈</li>
</ul>
<p>결과</p>
<ul>
<li>Heatmaps</li>
<li>Embeddings</li>
<li>Offsets</li>
</ul>
<p>두 heatmap을 통해서 예측</p>
<p>H x W x C(number of category)로 구성</p>
<p>Corner loss의 경우, focal loss를 변형해서 사용</p>
<p>정답에 가까울 수록 적은 loss 적용</p>
<p>Detecting Corner</p>
<ul>
<li>Conv를 통과하면서 heatmap에 floating point loss 발생</li>
<li>Heatmap에서 이미지로 위치를 다시 mapping 시킬 때 차이 발생</li>
<li>Offset을 사용하여 예측한 위치를 조정</li>
<li>Smooth L1 Loss 사용</li>
</ul>
<p>Grouping Corner</p>
<p>한 이미지 안에서 동일 클래스 오브젝트가 존재할 수 있음</p>
<ul>
<li>Top-Left 코너와 Bottom-Right 코너의 짝을 맞춰주는 과정</li>
<li>Top-Left 코너와 Bottom-Right 코너의 임베딩값 차이에 따라서 그룹을 지어줌
<ul>
<li>Embedding 값 사이의 거리가 작으면 같은 물체의 bbox에 속한다고 가정</li>
</ul>
</li>
</ul>
<p>Corner Pooling</p>
<p><img src="../../../../../resources/Untitled-25-4.png" width="auto" height="auto" alt="Untitled 25 4.png"/></p>
<p>코너에는 특징적인 부분이 없음</p>
<p>코너를 결정하기 위해서 feature map에 corner pooling 수행</p>
<ul>
<li>Horizontal : Right to left, Horizontal max pooling</li>
<li>Vertical : bottom to top Vertical max pooling</li>
<li>이후 embedding, offset 계산에도 사용</li>
</ul>
<p><img src="../../../../../resources/Untitled-26-4.png" width="auto" height="auto" alt="Untitled 26 4.png"/></p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div class="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false,&quot;enableRadial&quot;:false}"></div><button class="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div class="global-graph-outer"><div class="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.2,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true,&quot;enableRadial&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul class="toc-content overflow" id="list-1"><li class="depth-0"><a href="#advanced-object-detection" data-for="advanced-object-detection">Advanced Object Detection</a></li><li class="depth-1"><a href="#casecade-rcnn" data-for="casecade-rcnn">Casecade RCNN</a></li><li class="depth-2"><a href="#contribution" data-for="contribution">Contribution</a></li><li class="depth-2"><a href="#method" data-for="method">Method</a></li><li class="depth-1"><a href="#deformable-convolution-networksdcn" data-for="deformable-convolution-networksdcn">Deformable Convolution Networks(DCN)</a></li><li class="depth-2"><a href="#contribution-1" data-for="contribution-1">Contribution</a></li><li class="depth-2"><a href="#method-1" data-for="method-1">Method</a></li><li class="depth-1"><a href="#vit" data-for="vit">ViT</a></li><li class="depth-1"><a href="#end-to-end-object-detection-with-transformer" data-for="end-to-end-object-detection-with-transformer">End to End Object Detection with Transformer</a></li><li class="depth-2"><a href="#contibution" data-for="contibution">Contibution</a></li><li class="depth-2"><a href="#architecture" data-for="architecture">Architecture</a></li><li class="depth-1"><a href="#swin-transformer" data-for="swin-transformer">Swin Transformer</a></li><li class="depth-2"><a href="#vit의-문제점" data-for="vit의-문제점">VIT의 문제점</a></li><li class="depth-2"><a href="#contibution-1" data-for="contibution-1">Contibution</a></li><li class="depth-2"><a href="#architecture-1" data-for="architecture-1">Architecture</a></li><li class="depth-1"><a href="#yolo-v4" data-for="yolo-v4">YOLO v4</a></li><li class="depth-2"><a href="#overview" data-for="overview">Overview</a></li><li class="depth-2"><a href="#related-work" data-for="related-work">Related work</a></li><li class="depth-2"><a href="#selection-of-architecture" data-for="selection-of-architecture">Selection of architecture</a></li><li class="depth-2"><a href="#additional-improvements" data-for="additional-improvements">Additional Improvements</a></li><li class="depth-1"><a href="#m2det" data-for="m2det">M2Det</a></li><li class="depth-2"><a href="#overview-1" data-for="overview-1">Overview</a></li><li class="depth-2"><a href="#architecture-2" data-for="architecture-2">Architecture</a></li><li class="depth-1"><a href="#cornernet" data-for="cornernet">CornerNet</a></li><li class="depth-2"><a href="#overview-2" data-for="overview-2">Overview</a></li><li class="depth-2"><a href="#architecture-3" data-for="architecture-3">Architecture</a></li><li class="overflow-end"></li></ul></div><div class="backlinks"><h3>Backlinks</h3><ul id="list-2" class="overflow"><li><a href="../../../../../Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Naver-Connect---Boostcamp-AI-Tech-4기" class="internal">Naver Connect - Boostcamp AI Tech 4기</a></li><li class="overflow-end"></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.5.1</a> © 2025</p><ul><li><a href="https://github.com/404Vector/404Vector">GitHub</a></li></ul></footer></div></div></body><script type="application/javascript">function n(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.getElementsByClassName("callout-content")[0];if(!e)return;let l=t.classList.contains("is-collapsed");e.style.gridTemplateRows=l?"0fr":"1fr"}function c(){let t=document.getElementsByClassName("callout is-collapsible");for(let e of t){let l=e.getElementsByClassName("callout-title")[0],s=e.getElementsByClassName("callout-content")[0];if(!l||!s)continue;l.addEventListener("click",n),window.addCleanup(()=>l.removeEventListener("click",n));let o=e.classList.contains("is-collapsed");s.style.gridTemplateRows=o?"0fr":"1fr"}}document.addEventListener("nav",c);
</script><script type="module">function f(i,e){if(!i)return;function r(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function t(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}i?.addEventListener("click",r),window.addCleanup(()=>i?.removeEventListener("click",r)),document.addEventListener("keydown",t),window.addCleanup(()=>document.removeEventListener("keydown",t))}function y(i){for(;i.firstChild;)i.removeChild(i.firstChild)}var h=class{constructor(e,r){this.container=e;this.content=r;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),r=this.onMouseMove.bind(this),t=this.onMouseUp.bind(this),o=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",r),document.addEventListener("mouseup",t),window.addEventListener("resize",o),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",r),()=>document.removeEventListener("mouseup",t),()=>window.removeEventListener("resize",o))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let r=this.createButton("+",()=>this.zoom(.1)),t=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(t),e.appendChild(o),e.appendChild(r),this.container.appendChild(e)}createButton(e,r){let t=document.createElement("button");return t.textContent=e,t.className="mermaid-control-button",t.addEventListener("click",r),window.addCleanup(()=>t.removeEventListener("click",r)),t}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}zoom(e){let r=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),t=this.content.getBoundingClientRect(),o=t.width/2,n=t.height/2,c=r-this.scale;this.currentPan.x-=o*c,this.currentPan.y-=n*c,this.scale=r,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){this.scale=1;let e=this.content.querySelector("svg");this.currentPan={x:e.getBoundingClientRect().width/2,y:e.getBoundingClientRect().height/2},this.updateTransform()}},C=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],E;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;E||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let r=E.default,t=new WeakMap;for(let n of e)t.set(n,n.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let a=t.get(s);a&&(s.innerHTML=a)}let n=C.reduce((s,a)=>(s[a]=window.getComputedStyle(document.documentElement).getPropertyValue(a),s),{}),c=document.documentElement.getAttribute("saved-theme")==="dark";r.initialize({startOnLoad:!1,securityLevel:"loose",theme:c?"dark":"base",themeVariables:{fontFamily:n["--codeFont"],primaryColor:n["--light"],primaryTextColor:n["--darkgray"],primaryBorderColor:n["--tertiary"],lineColor:n["--darkgray"],secondaryColor:n["--secondary"],tertiaryColor:n["--tertiary"],clusterBkg:n["--light"],edgeLabelBackground:n["--highlight"]}}),await r.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let n=0;n<e.length;n++){let v=function(){let g=l.querySelector("#mermaid-space"),m=l.querySelector(".mermaid-content");if(!m)return;y(m);let w=c.querySelector("svg").cloneNode(!0);m.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new h(g,m)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},c=e[n],s=c.parentElement,a=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(a),L=a.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),f(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../../../../../postscript.js" type="module"></script></html>