<!DOCTYPE html>
<html lang="en"><head><title>Week 7</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;family=IBM Plex Mono:wght@400;600&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="HyeongSeok Kim's Vault"/><meta property="og:title" content="Week 7"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Week 7"/><meta name="twitter:description" content="AI Stages - 마스크 착용 상태 분류 대회 Age-Gender-MaskStatus 한번에 예측하기 #1 기존에 Age, Gender, Mask Train을 위해 만든 자원을 활용, 다시 코드를 짜서 train을 시켜보려고 했다."/><meta property="og:description" content="AI Stages - 마스크 착용 상태 분류 대회 Age-Gender-MaskStatus 한번에 예측하기 #1 기존에 Age, Gender, Mask Train을 위해 만든 자원을 활용, 다시 코드를 짜서 train을 시켜보려고 했다."/><meta property="og:image:alt" content="AI Stages - 마스크 착용 상태 분류 대회 Age-Gender-MaskStatus 한번에 예측하기 #1 기존에 Age, Gender, Mask Train을 위해 만든 자원을 활용, 다시 코드를 짜서 train을 시켜보려고 했다."/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7"/><meta property="twitter:url" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7"/><link rel="icon" href="../../../../../static/icon.png"/><meta name="description" content="AI Stages - 마스크 착용 상태 분류 대회 Age-Gender-MaskStatus 한번에 예측하기 #1 기존에 Age, Gender, Mask Train을 위해 만든 자원을 활용, 다시 코드를 짜서 train을 시켜보려고 했다."/><meta name="generator" content="Quartz"/><link href="../../../../../index.css" rel="stylesheet" type="text/css" spa-preserve/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  padding: 2rem;
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,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 */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../../../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../../../../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://quartz.jzhao.xyz/index.xml"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7-og-image.webp"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7-og-image.webp"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7-og-image.webp"/><meta property="og:image:type" content="image/.webp"/></head><body data-slug="Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/Week-7"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../../../../..">HyeongSeok Kim's Vault</a></h2><div class="spacer mobile-only"></div><div class="flex-component" style="flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search"><button class="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div class="search-layout" data-preview="true"></div></div></div></div></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="readermode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="readerIcon" fill="currentColor" stroke="currentColor" stroke-width="0.2" stroke-linecap="round" stroke-linejoin="round" width="64px" height="64px" viewBox="0 0 24 24" aria-label="Reader mode"><title>Reader mode</title><g transform="translate(-1.8, -1.8) scale(1.15, 1.2)"><path d="M8.9891247,2.5 C10.1384702,2.5 11.2209868,2.96705384 12.0049645,3.76669482 C12.7883914,2.96705384 13.8709081,2.5 15.0202536,2.5 L18.7549359,2.5 C19.1691495,2.5 19.5049359,2.83578644 19.5049359,3.25 L19.5046891,4.004 L21.2546891,4.00457396 C21.6343849,4.00457396 21.9481801,4.28672784 21.9978425,4.6528034 L22.0046891,4.75457396 L22.0046891,20.25 C22.0046891,20.6296958 21.7225353,20.943491 21.3564597,20.9931534 L21.2546891,21 L2.75468914,21 C2.37499337,21 2.06119817,20.7178461 2.01153575,20.3517706 L2.00468914,20.25 L2.00468914,4.75457396 C2.00468914,4.37487819 2.28684302,4.061083 2.65291858,4.01142057 L2.75468914,4.00457396 L4.50368914,4.004 L4.50444233,3.25 C4.50444233,2.87030423 4.78659621,2.55650904 5.15267177,2.50684662 L5.25444233,2.5 L8.9891247,2.5 Z M4.50368914,5.504 L3.50468914,5.504 L3.50468914,19.5 L10.9478955,19.4998273 C10.4513189,18.9207296 9.73864328,18.5588115 8.96709342,18.5065584 L8.77307039,18.5 L5.25444233,18.5 C4.87474657,18.5 4.56095137,18.2178461 4.51128895,17.8517706 L4.50444233,17.75 L4.50368914,5.504 Z M19.5049359,17.75 C19.5049359,18.1642136 19.1691495,18.5 18.7549359,18.5 L15.2363079,18.5 C14.3910149,18.5 13.5994408,18.8724714 13.0614828,19.4998273 L20.5046891,19.5 L20.5046891,5.504 L19.5046891,5.504 L19.5049359,17.75 Z M18.0059359,3.999 L15.0202536,4 L14.8259077,4.00692283 C13.9889509,4.06666544 13.2254227,4.50975805 12.7549359,5.212 L12.7549359,17.777 L12.7782651,17.7601316 C13.4923805,17.2719483 14.3447024,17 15.2363079,17 L18.0059359,16.999 L18.0056891,4.798 L18.0033792,4.75457396 L18.0056891,4.71 L18.0059359,3.999 Z M8.9891247,4 L6.00368914,3.999 L6.00599909,4.75457396 L6.00599909,4.75457396 L6.00368914,4.783 L6.00368914,16.999 L8.77307039,17 C9.57551536,17 10.3461406,17.2202781 11.0128313,17.6202194 L11.2536891,17.776 L11.2536891,5.211 C10.8200889,4.56369974 10.1361548,4.13636104 9.37521067,4.02745763 L9.18347055,4.00692283 L8.9891247,4 Z"></path></g></svg></button></div></div><div class="explorer" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>node&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-content"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div class="explorer-content" aria-expanded="false"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../../../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/">Notion</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/DB/">DB</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/DB/DB-Blog-Post/">DB Blog Post</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/">Naver Connect   Boostcamp AI Tech 4기</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../../Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Week-7/">Week 7</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Week 7</a></div></nav><h1 class="article-title">Week 7</h1><p show-comma="true" class="content-meta"><time datetime="2025-06-17T02:23:14.293Z">Jun 17, 2025</time><span>97 min read</span></p></div></div><article class="popover-hint"><h2 id="ai-stages---마스크-착용-상태-분류-대회">AI Stages - <strong>마스크 착용 상태 분류 대회</strong><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#ai-stages---마스크-착용-상태-분류-대회" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="age-gender-maskstatus-한번에-예측하기-1">Age-Gender-MaskStatus 한번에 예측하기 #1<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#age-gender-maskstatus-한번에-예측하기-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>기존에 Age, Gender, Mask Train을 위해 만든 자원을 활용, 다시 코드를 짜서 train을 시켜보려고 했다.</p>
<p>Basemodel은 다른 대외에서 준수한 성능을 보여주었단 EfficientNet B4를 사용했다.</p>
<ul>
<li>
<p>Output</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>Namespace(batch_size=32, csv_path='../train_i.csv', device='cuda', epochs=20, img_size=380, lr=0.0004, save_name='weights_all_efb4_v0.tar', save_path='../models/checkpoint/', seed=41, step_gamma=1.0, step_size=10, target_model='EfficientnetB4()', validation_ratio=0.2)</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:20&lt;00:00,  1.06s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.08it/s]</span></span>
<span data-line><span>Epoch [1] / Train Loss : [0.51088] / Val Loss : [4.52802] / F1 : [0.16130]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.16130]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:17&lt;00:00,  1.05s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.14it/s]</span></span>
<span data-line><span>Epoch [2] / Train Loss : [0.23640] / Val Loss : [4.87189] / F1 : [0.16493]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.16493]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:11&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.16it/s]</span></span>
<span data-line><span>Epoch [3] / Train Loss : [0.15332] / Val Loss : [5.03418] / F1 : [0.17144]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.17144]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.05it/s]</span></span>
<span data-line><span>Epoch [4] / Train Loss : [0.12163] / Val Loss : [5.47039] / F1 : [0.16459]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]</span></span>
<span data-line><span>Epoch [5] / Train Loss : [0.09997] / Val Loss : [6.38236] / F1 : [0.16500]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.19it/s]</span></span>
<span data-line><span>Epoch [6] / Train Loss : [0.09601] / Val Loss : [6.03461] / F1 : [0.17007]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:09&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.13it/s]</span></span>
<span data-line><span>Epoch [7] / Train Loss : [0.08823] / Val Loss : [6.99676] / F1 : [0.16531]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:13&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]</span></span>
<span data-line><span>Epoch [8] / Train Loss : [0.06689] / Val Loss : [7.06806] / F1 : [0.17427]</span></span>
<span data-line><span> * New Best Model -> Epoch [8] / best_score : [0.17427]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:14&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.06it/s]</span></span>
<span data-line><span>Epoch [9] / Train Loss : [0.06878] / Val Loss : [7.18693] / F1 : [0.17101]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:11&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.11it/s]</span></span>
<span data-line><span>Epoch [10] / Train Loss : [0.06150] / Val Loss : [7.87434] / F1 : [0.16583]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.11it/s]</span></span>
<span data-line><span>Epoch [11] / Train Loss : [0.06174] / Val Loss : [7.73951] / F1 : [0.17413]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.12it/s]</span></span>
<span data-line><span>Epoch [12] / Train Loss : [0.05296] / Val Loss : [7.74434] / F1 : [0.17096]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.14it/s]</span></span>
<span data-line><span>Epoch [13] / Train Loss : [0.05569] / Val Loss : [7.90913] / F1 : [0.17755]</span></span>
<span data-line><span> * New Best Model -> Epoch [13] / best_score : [0.17755]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/weights_all_efb4_v0.tar</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:13&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.13it/s]</span></span>
<span data-line><span>Epoch [14] / Train Loss : [0.05386] / Val Loss : [7.90242] / F1 : [0.16470]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.20it/s]</span></span>
<span data-line><span>Epoch [15] / Train Loss : [0.05038] / Val Loss : [8.74257] / F1 : [0.17196]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:09&lt;00:00,  1.03s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.16it/s]</span></span>
<span data-line><span>Epoch [16] / Train Loss : [0.05482] / Val Loss : [8.96333] / F1 : [0.16495]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]</span></span>
<span data-line><span>Epoch [17] / Train Loss : [0.04385] / Val Loss : [8.56864] / F1 : [0.17062]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:10&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]</span></span>
<span data-line><span>Epoch [18] / Train Loss : [0.04033] / Val Loss : [8.51767] / F1 : [0.16381]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:11&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.08it/s]</span></span>
<span data-line><span>Epoch [19] / Train Loss : [0.03861] / Val Loss : [9.03492] / F1 : [0.16623]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 473/473 [08:12&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|███████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.15it/s]</span></span>
<span data-line><span>Epoch [20] / Train Loss : [0.04452] / Val Loss : [9.04841] / F1 : [0.16245]</span></span></code></pre></figure>
</li>
</ul>
<p>결과, 성능이 나오질 않았다.</p>
<p>Train Loss는 지속적으로 감소했지만, Validation Loss는 증가했다.<br/>
Learning Rate의 문제라고 생각하여 1e-2 ~1e-5까지 0.1배로 테스트를 진행했으나 달라진 점이 보이지 않았다.</p>
<p><strong>원인 분석</strong></p>
<ul>
<li>
<p>Learning Rate를 잘못 설정해서 훈련 자체가 안되는 것일까?</p>
<ul>
<li>우선, TrainLoss는 감소하므로 훈련이 아에 안되는 상황은 아닐 것이다.</li>
</ul>
</li>
<li>
<p>Data Argmentation을 너무 과하게 한 것인가?</p>
<ul>
<li>
<p>결과</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>train_transform = A.Compose([</span></span>
<span data-line><span>                                # A.Resize(args.img_size, args.img_size),</span></span>
<span data-line><span>                                A.RandomResizedCrop(</span></span>
<span data-line><span>                                    args.img_size, args.img_size, scale=(0.8, 1.0)),</span></span>
<span data-line><span>                                A.RandomBrightnessContrast(p=0.3),</span></span>
<span data-line><span>                                A.RandomGamma(p=0.3),</span></span>
<span data-line><span>                                A.RandomFog(),</span></span>
<span data-line><span>                                A.RandomToneCurve(),</span></span>
<span data-line><span>                                A.HorizontalFlip(p=0.5),</span></span>
<span data-line><span>                                A.Normalize(mean=(0.485, 0.456, 0.406), std=(</span></span>
<span data-line><span>                                    0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),</span></span>
<span data-line><span>                                ToTensorV2()</span></span>
<span data-line><span>                                ])</span></span>
<span data-line> </span>
<span data-line><span>    test_transform = A.Compose([</span></span>
<span data-line><span>        A.Resize(args.img_size, args.img_size),</span></span>
<span data-line><span>        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),</span></span>
<span data-line><span>                    max_pixel_value=255.0, always_apply=False, p=1.0),</span></span>
<span data-line><span>        ToTensorV2()</span></span>
<span data-line><span>    ])</span></span></code></pre></figure>
</li>
<li>
<p>Gender 분류 모델에 넣은 Argmentation을 그대로 사용했기 때문에 가능성이 있다.</p>
</li>
<li>
<p>원복 및 재실행 결과</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>Namespace(batch_size=32, csv_path='../train_i.csv', device='cuda', epochs=20, img_size=380, lr=0.0004, save_name='MGAWeight_EfficientnetB4.tar', save_path='../models/checkpoint/', seed=41, step_gamma=1.0, step_size=10, target_model='EfficientnetB4()', validation_ratio=0.2)</span></span>
<span data-line><span>100%|███████████████████████████████████| 473/473 [06:37&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  2.99it/s]</span></span>
<span data-line><span>Epoch [1] / Train Loss : [0.41027] / Val Loss : [4.48446] / F1 : [0.16489]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.16489]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/MGAWeight_EfficientnetB4.tar</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:34&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.04it/s]</span></span>
<span data-line><span>Epoch [2] / Train Loss : [0.12687] / Val Loss : [5.51019] / F1 : [0.17274]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.17274]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/MGAWeight_EfficientnetB4.tar</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:33&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.05it/s]</span></span>
<span data-line><span>Epoch [3] / Train Loss : [0.10025] / Val Loss : [5.66333] / F1 : [0.17255]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:35&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.04it/s]</span></span>
<span data-line><span>Epoch [4] / Train Loss : [0.06157] / Val Loss : [6.57190] / F1 : [0.16740]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:34&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.03it/s]</span></span>
<span data-line><span>Epoch [5] / Train Loss : [0.07978] / Val Loss : [6.40279] / F1 : [0.17934]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.17934]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/MGAWeight_EfficientnetB4.tar</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:35&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.04it/s]</span></span>
<span data-line><span>Epoch [6] / Train Loss : [0.04555] / Val Loss : [6.41356] / F1 : [0.17498]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:33&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.09it/s]</span></span>
<span data-line><span>Epoch [7] / Train Loss : [0.03668] / Val Loss : [7.69871] / F1 : [0.16752]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:36&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.07it/s]</span></span>
<span data-line><span>Epoch [8] / Train Loss : [0.03775] / Val Loss : [7.32922] / F1 : [0.16793]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:35&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:39&lt;00:00,  3.04it/s]</span></span>
<span data-line><span>Epoch [9] / Train Loss : [0.05067] / Val Loss : [7.42311] / F1 : [0.16720]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:32&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.08it/s]</span></span>
<span data-line><span>Epoch [10] / Train Loss : [0.05210] / Val Loss : [7.89588] / F1 : [0.17300]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.13it/s]</span></span>
<span data-line><span>Epoch [11] / Train Loss : [0.02483] / Val Loss : [7.87729] / F1 : [0.16588]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.09it/s]</span></span>
<span data-line><span>Epoch [12] / Train Loss : [0.04351] / Val Loss : [8.21418] / F1 : [0.16275]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.12it/s]</span></span>
<span data-line><span>Epoch [13] / Train Loss : [0.02155] / Val Loss : [8.29829] / F1 : [0.16369]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.19it/s]</span></span>
<span data-line><span>Epoch [14] / Train Loss : [0.02002] / Val Loss : [8.14832] / F1 : [0.15482]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.09it/s]</span></span>
<span data-line><span>Epoch [15] / Train Loss : [0.03869] / Val Loss : [9.19703] / F1 : [0.16264]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:29&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.09it/s]</span></span>
<span data-line><span>Epoch [16] / Train Loss : [0.02364] / Val Loss : [8.64021] / F1 : [0.16931]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.08it/s]</span></span>
<span data-line><span>Epoch [17] / Train Loss : [0.02183] / Val Loss : [9.18431] / F1 : [0.17084]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:40&lt;00:00,  2.95it/s]</span></span>
<span data-line><span>Epoch [18] / Train Loss : [0.03817] / Val Loss : [8.90696] / F1 : [0.16667]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:38&lt;00:00,  3.10it/s]</span></span>
<span data-line><span>Epoch [19] / Train Loss : [0.03505] / Val Loss : [9.06653] / F1 : [0.16512]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:30&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:37&lt;00:00,  3.14it/s]</span></span>
<span data-line><span>Epoch [20] / Train Loss : [0.01673] / Val Loss : [8.40525] / F1 : [0.16786]</span></span></code></pre></figure>
</li>
</ul>
</li>
</ul>
<p><strong>팀원들과 논의</strong></p>
<p>방향을 잡기 어려워 팀원들에게 도움을 요청했다. 리더보드에서 가장 높은 성적은 거둔 팀원은 timm의 vit를 사용했다고 한다.</p>
<p>==<strong>ViT로 Classification 하기</strong>==</p>
<p><strong>ViT를 사용해서 Classification을 하려면 어떻게 해야 할까?</strong></p>
<p>인터넷의 <a href="https://velog.io/@gtpgg1013/pytorch-Image-Classification-Using-ViT" class="external">블로그<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>에서 예제코드를 찾았다.</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>import timm</span></span>
<span data-line> </span>
<span data-line><span>num_classes = 120</span></span>
<span data-line><span>model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)</span></span></code></pre></figure>
<p>timm이라는 library를 사용해서 모델을 로드하고 파라미터로 num_classes를 넘겨준다<br/>
하지만 이래서는 blackbox여서 내부 구조를 알 수 없다<br/>
print를 사용해 내부를 확인해보았다.</p>
<ul>
<li>
<p>기본 구조(for 1000 classes)</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>VisionTransformer(</span></span>
<span data-line><span>  (patch_embed): PatchEmbed(</span></span>
<span data-line><span>    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))</span></span>
<span data-line><span>    (norm): Identity()</span></span>
<span data-line><span>  )</span></span>
<span data-line><span>  (pos_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>  (norm_pre): Identity()</span></span>
<span data-line><span>  (blocks): Sequential(</span></span>
<span data-line><span>    (0): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (1): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (2): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (3): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (4): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (5): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (6): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (7): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (8): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (9): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (10): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (11): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>  )</span></span>
<span data-line><span>  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>  (fc_norm): Identity()</span></span>
<span data-line><span>  (head): Linear(in_features=768, out_features=1000, bias=True)</span></span>
<span data-line><span>)</span></span></code></pre></figure>
</li>
<li>
<p>변경 구조(for 18 classes)</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>VisionTransformer(</span></span>
<span data-line><span>  (patch_embed): PatchEmbed(</span></span>
<span data-line><span>    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))</span></span>
<span data-line><span>    (norm): Identity()</span></span>
<span data-line><span>  )</span></span>
<span data-line><span>  (pos_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>  (norm_pre): Identity()</span></span>
<span data-line><span>  (blocks): Sequential(</span></span>
<span data-line><span>    (0): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (1): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (2): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (3): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (4): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (5): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (6): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (7): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (8): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (9): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (10): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (11): Block(</span></span>
<span data-line><span>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (attn): Attention(</span></span>
<span data-line><span>        (qkv): Linear(in_features=768, out_features=2304, bias=True)</span></span>
<span data-line><span>        (attn_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (proj): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        (proj_drop): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls1): Identity()</span></span>
<span data-line><span>      (drop_path1): Identity()</span></span>
<span data-line><span>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>      (mlp): Mlp(</span></span>
<span data-line><span>        (fc1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>        (act): GELU(approximate=none)</span></span>
<span data-line><span>        (drop1): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (fc2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>        (drop2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (ls2): Identity()</span></span>
<span data-line><span>      (drop_path2): Identity()</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>  )</span></span>
<span data-line><span>  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>  (fc_norm): Identity()</span></span>
<span data-line><span>  (head): Linear(in_features=768, out_features=18, bias=True)</span></span>
<span data-line><span>)</span></span></code></pre></figure>
</li>
</ul>
<p>기본적으로 patch_embed → encoder block → head 구조로 되어있다<br/>
class parameter 변경 시, head 부분의 out_features가 변경된다<br/>
torchvision.models에도 vit가 존재한다</p>
<ul>
<li>
<p>torchvision.models</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>VisionTransformer(</span></span>
<span data-line><span>  (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))</span></span>
<span data-line><span>  (encoder): Encoder(</span></span>
<span data-line><span>    (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>    (layers): Sequential(</span></span>
<span data-line><span>      (encoder_layer_0): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_1): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_2): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_3): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_4): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_5): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_6): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_7): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_8): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_9): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_10): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>      (encoder_layer_11): EncoderBlock(</span></span>
<span data-line><span>        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (self_attention): MultiheadAttention(</span></span>
<span data-line><span>          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>        (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>        (mlp): MLPBlock(</span></span>
<span data-line><span>          (0): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span data-line><span>          (1): GELU(approximate=none)</span></span>
<span data-line><span>          (2): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>          (3): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span data-line><span>          (4): Dropout(p=0.0, inplace=False)</span></span>
<span data-line><span>        )</span></span>
<span data-line><span>      )</span></span>
<span data-line><span>    )</span></span>
<span data-line><span>    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span></span>
<span data-line><span>  )</span></span>
<span data-line><span>  (heads): Sequential(</span></span>
<span data-line><span>    (head): Linear(in_features=768, out_features=1000, bias=True)</span></span>
<span data-line><span>  )</span></span>
<span data-line><span>)</span></span></code></pre></figure>
</li>
</ul>
<p>MultiheadAttention Block 내부가 print로 노출되지 않는 점을 제외하고는 동일한 구조를 가지고 있으므로 TorchVision의 ViT를 사용해서 모델을 구성했다.</p>
<ul>
<li>
<p>Model Code</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>import torch</span></span>
<span data-line><span>import torch.nn as nn</span></span>
<span data-line><span>import torch.optim as optim</span></span>
<span data-line><span>import torch.nn.functional as F</span></span>
<span data-line><span>import torchvision.models as models</span></span>
<span data-line> </span>
<span data-line> </span>
<span data-line><span>class VIT_V0_KHS(nn.Module):</span></span>
<span data-line><span>    def __init__(self, is_freeze:bool = True):</span></span>
<span data-line><span>        super(VIT_V0_KHS, self).__init__()</span></span>
<span data-line><span>        self.backborn = models.vit_b_16(weights = models.ViT_B_16_Weights.IMAGENET1K_V1)</span></span>
<span data-line><span>        if(is_freeze == True):</span></span>
<span data-line><span>            for p in self.backborn.parameters():</span></span>
<span data-line><span>                p.requires_grad = False</span></span>
<span data-line><span>        self.backborn.heads = nn.Sequential(nn.Linear(768, 18))</span></span>
<span data-line> </span>
<span data-line><span>    def forward(self, x):</span></span>
<span data-line><span>        x = self.backborn(x)</span></span>
<span data-line><span>        return x</span></span></code></pre></figure>
</li>
</ul>
<p><span class="text-highlight">결과, 마찬가지로 훈련이 되지 않았다</span></p>
<ul>
<li>
<p>출력(도중에 취소)</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>(lv1_imageclassification_cv02) (base) root@536ffaebb1ec:~/repos/lv1_imageclassification_cv02/train# python trainALL.py </span></span>
<span data-line><span>Namespace(batch_size=32, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V0_KHS.tar', save_path='../models/checkpoint/', seed=41, step_gamma=1.0, step_size=10, target_model='VIT_V0_KHS()', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [01:57&lt;00:00,  4.04it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:28&lt;00:00,  4.22it/s]</span></span>
<span data-line><span>Epoch [1] / Train Loss : [0.90020] / Val Loss : [2.79555] / F1 : [0.16190]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.16190]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [01:53&lt;00:00,  4.19it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:26&lt;00:00,  4.48it/s]</span></span>
<span data-line><span>Epoch [2] / Train Loss : [0.54853] / Val Loss : [3.16076] / F1 : [0.16314]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.16314]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [01:54&lt;00:00,  4.12it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:26&lt;00:00,  4.45it/s]</span></span>
<span data-line><span>Epoch [3] / Train Loss : [0.46461] / Val Loss : [3.36511] / F1 : [0.16870]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.16870]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [01:55&lt;00:00,  4.08it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:27&lt;00:00,  4.39it/s]</span></span>
<span data-line><span>Epoch [4] / Train Loss : [0.41091] / Val Loss : [3.66533] / F1 : [0.16694]</span></span></code></pre></figure>
</li>
</ul>
<p><strong>내가 직접 짠 코드의 코드의 문제일까?(train, validation 오류 or loss의 잘못된 사용, … etc)</strong></p>
<p>baseline code를 가져와 다시 세팅을 시작했다</p>
<p>baselinecode에 vit를 돌려본 결과, 된다…</p>
<ul>
<li>
<p>출력(required_grad = true)</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>Namespace(augmentation='BaseAugmentation', batch_size=64, criterion='cross_entropy', data_dir='/opt/ml/repos/lv1_imageclassification_cv02/data/train/images', dataset='MaskBaseDataset', epochs=10, log_interval=20, lr=0.001, lr_decay_step=20, model='MyModel', model_dir='/opt/ml/repos/lv1_imageclassification_cv02/local/v2/model', name='exp', optimizer='SGD', resize=[224, 224], seed=42, val_ratio=0.2, valid_batch_size=1000)</span></span>
<span data-line><span>save_dir is  /opt/ml/repos/lv1_imageclassification_cv02/local/v2/model/exp</span></span>
<span data-line><span>/opt/ml/.local/share/virtualenvs/lv1_imageclassification_cv02-bp8_CroY/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.</span></span>
<span data-line><span>  warnings.warn(</span></span>
<span data-line><span>Epoch[0/10](20/236) || training loss 2.663 || training accuracy 16.64% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](40/236) || training loss 2.311 || training accuracy 28.20% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](60/236) || training loss 2.185 || training accuracy 33.75% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](80/236) || training loss 2.066 || training accuracy 34.45% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](100/236) || training loss 1.981 || training accuracy 37.73% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](120/236) || training loss 1.88 || training accuracy 42.34% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](140/236) || training loss 1.826 || training accuracy 45.47% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](160/236) || training loss 1.77 || training accuracy 48.91% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](180/236) || training loss 1.716 || training accuracy 50.00% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](200/236) || training loss 1.68 || training accuracy 50.62% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](220/236) || training loss 1.571 || training accuracy 54.53% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 46.69%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 46.69%, loss:  1.5 || best acc : 46.69%, best loss:  1.5</span></span>
<span data-line> </span>
<span data-line><span>Epoch[1/10](20/236) || training loss 1.464 || training accuracy 59.14% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](40/236) || training loss 1.425 || training accuracy 58.52% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](60/236) || training loss 1.434 || training accuracy 58.75% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](80/236) || training loss 1.39 || training accuracy 59.84% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](100/236) || training loss 1.33 || training accuracy 63.44% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](120/236) || training loss 1.243 || training accuracy 65.47% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](140/236) || training loss 1.243 || training accuracy 65.47% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](160/236) || training loss 1.204 || training accuracy 66.41% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](180/236) || training loss 1.173 || training accuracy 67.03% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](200/236) || training loss 1.165 || training accuracy 68.59% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](220/236) || training loss 1.145 || training accuracy 68.05% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 54.92%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 54.92%, loss:  1.1 || best acc : 54.92%, best loss:  1.1</span></span>
<span data-line> </span>
<span data-line><span>Epoch[2/10](20/236) || training loss 1.083 || training accuracy 69.69% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](40/236) || training loss 0.9979 || training accuracy 72.42% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](60/236) || training loss 1.025 || training accuracy 71.17% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](80/236) || training loss 0.9999 || training accuracy 72.42% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](100/236) || training loss 0.9508 || training accuracy 73.59% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](120/236) || training loss 0.9475 || training accuracy 74.38% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](140/236) || training loss 0.858 || training accuracy 77.81% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](160/236) || training loss 0.8918 || training accuracy 75.78% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](180/236) || training loss 0.8423 || training accuracy 77.03% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](200/236) || training loss 0.8633 || training accuracy 75.08% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](220/236) || training loss 0.7923 || training accuracy 78.98% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 61.35%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 61.35%, loss: 0.83 || best acc : 61.35%, best loss: 0.83</span></span>
<span data-line> </span>
<span data-line><span>Epoch[3/10](20/236) || training loss 0.7954 || training accuracy 78.12% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](40/236) || training loss 0.7668 || training accuracy 80.16% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](60/236) || training loss 0.7573 || training accuracy 79.84% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](80/236) || training loss 0.7483 || training accuracy 79.84% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](100/236) || training loss 0.7233 || training accuracy 80.94% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](120/236) || training loss 0.7215 || training accuracy 80.55% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](140/236) || training loss 0.6908 || training accuracy 80.78% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](160/236) || training loss 0.6719 || training accuracy 81.25% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](180/236) || training loss 0.6898 || training accuracy 82.66% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](200/236) || training loss 0.6436 || training accuracy 82.73% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](220/236) || training loss 0.6389 || training accuracy 84.22% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 64.71%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 64.71%, loss: 0.67 || best acc : 64.71%, best loss: 0.67</span></span>
<span data-line> </span>
<span data-line><span>Epoch[4/10](20/236) || training loss 0.583 || training accuracy 86.02% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](40/236) || training loss 0.5989 || training accuracy 84.06% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](60/236) || training loss 0.5982 || training accuracy 84.30% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](80/236) || training loss 0.5952 || training accuracy 83.44% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](100/236) || training loss 0.594 || training accuracy 84.69% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](120/236) || training loss 0.5895 || training accuracy 83.91% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](140/236) || training loss 0.5601 || training accuracy 84.92% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](160/236) || training loss 0.5484 || training accuracy 84.69% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](180/236) || training loss 0.5619 || training accuracy 84.06% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](200/236) || training loss 0.5694 || training accuracy 84.14% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](220/236) || training loss 0.5262 || training accuracy 85.70% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 66.30%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 66.30%, loss: 0.57 || best acc : 66.30%, best loss: 0.57</span></span>
<span data-line> </span>
<span data-line><span>Epoch[5/10](20/236) || training loss 0.4774 || training accuracy 87.50% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](40/236) || training loss 0.534 || training accuracy 84.38% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](60/236) || training loss 0.4892 || training accuracy 86.64% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](80/236) || training loss 0.5108 || training accuracy 86.33% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](100/236) || training loss 0.4871 || training accuracy 86.09% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](120/236) || training loss 0.5371 || training accuracy 84.61% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](140/236) || training loss 0.501 || training accuracy 86.48% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](160/236) || training loss 0.464 || training accuracy 87.66% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](180/236) || training loss 0.4492 || training accuracy 87.89% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](200/236) || training loss 0.4843 || training accuracy 87.42% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](220/236) || training loss 0.4314 || training accuracy 87.58% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 68.20%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 68.20%, loss:  0.5 || best acc : 68.20%, best loss:  0.5</span></span>
<span data-line> </span>
<span data-line><span>Epoch[6/10](20/236) || training loss 0.4387 || training accuracy 87.81% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](40/236) || training loss 0.4259 || training accuracy 88.52% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](60/236) || training loss 0.4104 || training accuracy 89.53% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](80/236) || training loss 0.4057 || training accuracy 87.73% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](100/236) || training loss 0.4395 || training accuracy 87.42% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](120/236) || training loss 0.4188 || training accuracy 88.20% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](140/236) || training loss 0.3998 || training accuracy 89.22% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](160/236) || training loss 0.4294 || training accuracy 88.12% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](180/236) || training loss 0.3973 || training accuracy 90.08% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](200/236) || training loss 0.379 || training accuracy 90.23% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](220/236) || training loss 0.4163 || training accuracy 87.27% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 68.84%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 68.84%, loss: 0.45 || best acc : 68.84%, best loss: 0.45</span></span>
<span data-line> </span>
<span data-line><span>Epoch[7/10](20/236) || training loss 0.343 || training accuracy 91.17% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](40/236) || training loss 0.3674 || training accuracy 90.00% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](60/236) || training loss 0.3756 || training accuracy 90.39% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](80/236) || training loss 0.3744 || training accuracy 88.91% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](100/236) || training loss 0.3935 || training accuracy 89.14% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](120/236) || training loss 0.3587 || training accuracy 90.62% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](140/236) || training loss 0.3971 || training accuracy 88.52% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](160/236) || training loss 0.3804 || training accuracy 90.00% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](180/236) || training loss 0.3774 || training accuracy 89.06% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](200/236) || training loss 0.3152 || training accuracy 91.09% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](220/236) || training loss 0.3563 || training accuracy 89.61% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 69.05%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 69.05%, loss: 0.42 || best acc : 69.05%, best loss: 0.42</span></span>
<span data-line> </span>
<span data-line><span>Epoch[8/10](20/236) || training loss 0.3367 || training accuracy 90.55% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](40/236) || training loss 0.3338 || training accuracy 91.09% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](60/236) || training loss 0.3573 || training accuracy 89.38% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](80/236) || training loss 0.3324 || training accuracy 90.62% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](100/236) || training loss 0.33 || training accuracy 90.78% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](120/236) || training loss 0.3303 || training accuracy 91.09% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](140/236) || training loss 0.294 || training accuracy 92.34% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](160/236) || training loss 0.3216 || training accuracy 91.09% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](180/236) || training loss 0.3058 || training accuracy 91.80% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](200/236) || training loss 0.3355 || training accuracy 91.02% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](220/236) || training loss 0.3109 || training accuracy 90.94% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 69.44%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 69.44%, loss: 0.38 || best acc : 69.44%, best loss: 0.38</span></span>
<span data-line> </span>
<span data-line><span>Epoch[9/10](20/236) || training loss 0.29 || training accuracy 92.66% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](40/236) || training loss 0.3052 || training accuracy 91.09% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](60/236) || training loss 0.2971 || training accuracy 91.02% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](80/236) || training loss 0.317 || training accuracy 91.02% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](100/236) || training loss 0.3074 || training accuracy 91.41% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](120/236) || training loss 0.2781 || training accuracy 91.72% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](140/236) || training loss 0.2847 || training accuracy 91.80% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](160/236) || training loss 0.2853 || training accuracy 92.19% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](180/236) || training loss 0.3097 || training accuracy 91.17% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](200/236) || training loss 0.2889 || training accuracy 91.95% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](220/236) || training loss 0.275 || training accuracy 93.59% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 69.63%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 69.63%, loss: 0.37 || best acc : 69.63%, best loss: 0.37</span></span></code></pre></figure>
</li>
<li>
<p>출력(required_grad = false)</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>Namespace(augmentation='BaseAugmentation', batch_size=64, criterion='cross_entropy', data_dir='/opt/ml/repos/lv1_imageclassification_cv02/data/train/images', dataset='MaskBaseDataset', epochs=10, log_interval=20, lr=0.001, lr_decay_step=20, model='MyModel', model_dir='/opt/ml/repos/lv1_imageclassification_cv02/local/v2/model', name='exp', optimizer='SGD', resize=[224, 224], seed=42, val_ratio=0.2, valid_batch_size=1000)</span></span>
<span data-line><span>save_dir is  /opt/ml/repos/lv1_imageclassification_cv02/local/v2/model/exp2</span></span>
<span data-line><span>/opt/ml/.local/share/virtualenvs/lv1_imageclassification_cv02-bp8_CroY/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.</span></span>
<span data-line><span>  warnings.warn(</span></span>
<span data-line><span>Epoch[0/10](20/236) || training loss 2.801 || training accuracy 10.16% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](40/236) || training loss 2.536 || training accuracy 20.62% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](60/236) || training loss 2.401 || training accuracy 28.12% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](80/236) || training loss 2.286 || training accuracy 29.53% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](100/236) || training loss 2.212 || training accuracy 30.94% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](120/236) || training loss 2.129 || training accuracy 35.94% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](140/236) || training loss 2.098 || training accuracy 36.02% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](160/236) || training loss 2.072 || training accuracy 37.19% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](180/236) || training loss 2.04 || training accuracy 37.66% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](200/236) || training loss 2.022 || training accuracy 37.66% || lr 0.001</span></span>
<span data-line><span>Epoch[0/10](220/236) || training loss 1.961 || training accuracy 41.09% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 34.97%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 34.97%, loss:  1.9 || best acc : 34.97%, best loss:  1.9</span></span>
<span data-line> </span>
<span data-line><span>Epoch[1/10](20/236) || training loss 1.885 || training accuracy 43.75% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](40/236) || training loss 1.889 || training accuracy 43.05% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](60/236) || training loss 1.901 || training accuracy 42.73% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](80/236) || training loss 1.878 || training accuracy 43.59% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](100/236) || training loss 1.86 || training accuracy 45.16% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](120/236) || training loss 1.762 || training accuracy 47.66% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](140/236) || training loss 1.784 || training accuracy 48.12% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](160/236) || training loss 1.774 || training accuracy 48.05% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](180/236) || training loss 1.73 || training accuracy 49.77% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](200/236) || training loss 1.752 || training accuracy 49.84% || lr 0.001</span></span>
<span data-line><span>Epoch[1/10](220/236) || training loss 1.744 || training accuracy 49.92% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 40.48%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 40.48%, loss:  1.7 || best acc : 40.48%, best loss:  1.7</span></span>
<span data-line> </span>
<span data-line><span>Epoch[2/10](20/236) || training loss 1.722 || training accuracy 50.31% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](40/236) || training loss 1.644 || training accuracy 54.14% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](60/236) || training loss 1.68 || training accuracy 52.03% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](80/236) || training loss 1.654 || training accuracy 52.03% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](100/236) || training loss 1.615 || training accuracy 52.81% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](120/236) || training loss 1.667 || training accuracy 52.50% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](140/236) || training loss 1.582 || training accuracy 54.77% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](160/236) || training loss 1.608 || training accuracy 54.61% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](180/236) || training loss 1.552 || training accuracy 54.77% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](200/236) || training loss 1.576 || training accuracy 53.83% || lr 0.001</span></span>
<span data-line><span>Epoch[2/10](220/236) || training loss 1.522 || training accuracy 57.19% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 44.60%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 44.60%, loss:  1.5 || best acc : 44.60%, best loss:  1.5</span></span>
<span data-line> </span>
<span data-line><span>Epoch[3/10](20/236) || training loss 1.537 || training accuracy 55.31% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](40/236) || training loss 1.521 || training accuracy 55.86% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](60/236) || training loss 1.524 || training accuracy 55.31% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](80/236) || training loss 1.526 || training accuracy 54.77% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](100/236) || training loss 1.482 || training accuracy 55.55% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](120/236) || training loss 1.479 || training accuracy 58.44% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](140/236) || training loss 1.46 || training accuracy 59.14% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](160/236) || training loss 1.474 || training accuracy 58.20% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](180/236) || training loss 1.459 || training accuracy 58.67% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](200/236) || training loss 1.466 || training accuracy 58.05% || lr 0.001</span></span>
<span data-line><span>Epoch[3/10](220/236) || training loss 1.433 || training accuracy 58.98% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 47.17%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 47.17%, loss:  1.4 || best acc : 47.17%, best loss:  1.4</span></span>
<span data-line> </span>
<span data-line><span>Epoch[4/10](20/236) || training loss 1.435 || training accuracy 57.58% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](40/236) || training loss 1.414 || training accuracy 58.36% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](60/236) || training loss 1.418 || training accuracy 57.89% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](80/236) || training loss 1.42 || training accuracy 59.22% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](100/236) || training loss 1.413 || training accuracy 60.16% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](120/236) || training loss 1.401 || training accuracy 58.20% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](140/236) || training loss 1.369 || training accuracy 62.11% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](160/236) || training loss 1.343 || training accuracy 62.03% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](180/236) || training loss 1.359 || training accuracy 60.94% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](200/236) || training loss 1.368 || training accuracy 60.94% || lr 0.001</span></span>
<span data-line><span>Epoch[4/10](220/236) || training loss 1.384 || training accuracy 59.77% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 48.94%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 48.94%, loss:  1.3 || best acc : 48.94%, best loss:  1.3</span></span>
<span data-line> </span>
<span data-line><span>Epoch[5/10](20/236) || training loss 1.316 || training accuracy 61.48% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](40/236) || training loss 1.362 || training accuracy 60.62% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](60/236) || training loss 1.33 || training accuracy 63.28% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](80/236) || training loss 1.322 || training accuracy 61.64% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](100/236) || training loss 1.329 || training accuracy 61.17% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](120/236) || training loss 1.341 || training accuracy 61.09% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](140/236) || training loss 1.354 || training accuracy 59.61% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](160/236) || training loss 1.306 || training accuracy 62.03% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](180/236) || training loss 1.276 || training accuracy 64.61% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](200/236) || training loss 1.312 || training accuracy 62.19% || lr 0.001</span></span>
<span data-line><span>Epoch[5/10](220/236) || training loss 1.255 || training accuracy 62.81% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 50.45%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 50.45%, loss:  1.3 || best acc : 50.45%, best loss:  1.3</span></span>
<span data-line> </span>
<span data-line><span>Epoch[6/10](20/236) || training loss 1.293 || training accuracy 62.42% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](40/236) || training loss 1.268 || training accuracy 64.22% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](60/236) || training loss 1.232 || training accuracy 65.00% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](80/236) || training loss 1.264 || training accuracy 63.36% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](100/236) || training loss 1.278 || training accuracy 62.97% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](120/236) || training loss 1.261 || training accuracy 63.20% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](140/236) || training loss 1.266 || training accuracy 61.88% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](160/236) || training loss 1.279 || training accuracy 62.50% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](180/236) || training loss 1.215 || training accuracy 63.52% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](200/236) || training loss 1.195 || training accuracy 66.80% || lr 0.001</span></span>
<span data-line><span>Epoch[6/10](220/236) || training loss 1.266 || training accuracy 62.27% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 51.48%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 51.48%, loss:  1.2 || best acc : 51.48%, best loss:  1.2</span></span>
<span data-line> </span>
<span data-line><span>Epoch[7/10](20/236) || training loss 1.198 || training accuracy 65.70% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](40/236) || training loss 1.226 || training accuracy 63.98% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](60/236) || training loss 1.218 || training accuracy 64.22% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](80/236) || training loss 1.229 || training accuracy 65.31% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](100/236) || training loss 1.241 || training accuracy 62.19% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](120/236) || training loss 1.198 || training accuracy 64.77% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](140/236) || training loss 1.225 || training accuracy 63.12% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](160/236) || training loss 1.253 || training accuracy 62.81% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](180/236) || training loss 1.201 || training accuracy 65.23% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](200/236) || training loss 1.137 || training accuracy 67.58% || lr 0.001</span></span>
<span data-line><span>Epoch[7/10](220/236) || training loss 1.186 || training accuracy 64.22% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 52.38%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 52.38%, loss:  1.2 || best acc : 52.38%, best loss:  1.2</span></span>
<span data-line> </span>
<span data-line><span>Epoch[8/10](20/236) || training loss 1.186 || training accuracy 65.94% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](40/236) || training loss 1.166 || training accuracy 66.17% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](60/236) || training loss 1.208 || training accuracy 64.61% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](80/236) || training loss 1.166 || training accuracy 65.47% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](100/236) || training loss 1.192 || training accuracy 64.14% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](120/236) || training loss 1.149 || training accuracy 66.17% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](140/236) || training loss 1.128 || training accuracy 68.12% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](160/236) || training loss 1.147 || training accuracy 64.92% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](180/236) || training loss 1.14 || training accuracy 67.97% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](200/236) || training loss 1.197 || training accuracy 63.59% || lr 0.001</span></span>
<span data-line><span>Epoch[8/10](220/236) || training loss 1.171 || training accuracy 64.69% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 52.75%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 52.75%, loss:  1.2 || best acc : 52.75%, best loss:  1.2</span></span>
<span data-line> </span>
<span data-line><span>Epoch[9/10](20/236) || training loss 1.165 || training accuracy 65.55% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](40/236) || training loss 1.14 || training accuracy 66.80% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](60/236) || training loss 1.127 || training accuracy 65.86% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](80/236) || training loss 1.182 || training accuracy 64.30% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](100/236) || training loss 1.122 || training accuracy 67.58% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](120/236) || training loss 1.141 || training accuracy 67.11% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](140/236) || training loss 1.107 || training accuracy 67.73% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](160/236) || training loss 1.127 || training accuracy 66.25% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](180/236) || training loss 1.162 || training accuracy 64.06% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](200/236) || training loss 1.087 || training accuracy 69.45% || lr 0.001</span></span>
<span data-line><span>Epoch[9/10](220/236) || training loss 1.099 || training accuracy 68.05% || lr 0.001</span></span>
<span data-line><span>Calculating validation results...</span></span>
<span data-line><span>New best model for val accuracy : 53.52%! saving the best model..</span></span>
<span data-line><span>[Val] acc : 53.52%, loss:  1.1 || best acc : 53.52%, best loss:  1.1</span></span></code></pre></figure>
</li>
</ul>
<p>Train, Validation 메소드를 baseline code에서 가져와 원래 프로젝트에 맞게 변경</p>
<p>→ 그래도 안된다…</p>
<p><strong>원인발견</strong></p>
<p>vailidation을 위한 image loader 시, val_df가 아닌 train_df을 넣어버렸다…</p>
<ul>
<li>
<p>문제의 원본</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>train_img_paths = train_df['path'].values</span></span>
<span data-line><span>    train_labels = [U.convertAgeGenderMaskToLabel(m, g, a) for m, g, a in zip(train_df['mask_class'].values, train_df['gender_class'].values, train_df['age_class'].values)]</span></span>
<span data-line><span>    val_img_paths = val_df['path'].values</span></span>
<span data-line><span>    val_labels = [U.convertAgeGenderMaskToLabel(m, g, a) for m, g, a in zip(train_df['mask_class'].values, train_df['gender_class'].values, train_df['age_class'].values)]</span></span></code></pre></figure>
</li>
<li>
<p>변경</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>train_img_paths = train_df['path'].values</span></span>
<span data-line><span>    train_labels = [U.convertAgeGenderMaskToLabel(m, g, a) for m, g, a in zip(train_df['mask_class'].values, train_df['gender_class'].values, train_df['age_class'].values)]</span></span>
<span data-line><span>    val_img_paths = val_df['path'].values</span></span>
<span data-line><span>    val_labels = [U.convertAgeGenderMaskToLabel(m, g, a) for m, g, a in zip(val_df['mask_class'].values, val_df['gender_class'].values, val_df['age_class'].values)]</span></span></code></pre></figure>
</li>
</ul>
<p><strong>원인 발견 후, 훈련 및 결과</strong></p>
<ul>
<li>
<p>출력 backbone not freeze, lr 0.001, batch 64, image size 224<br/>
Best : V.Acc[0.67969] / F1[0.55520]</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --save_name=Weight_VIT_V1.KHS.tar --target_model=&quot;VIT_V0_KHS(False)&quot;</span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V1.KHS.tar', save_path='../models/checkpoint/', seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model='VIT_V0_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:43&lt;00:00,  2.21s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.41s/it]</span></span>
<span data-line><span>Ep[1] / T.Loss[2.35598] / T.Acc[0.22917] / V.Loss[2.28715] / V.Acc[0.25859] / F1[0.03890]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.25859]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:06&lt;00:00,  2.56s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:28&lt;00:00,  1.47s/it]</span></span>
<span data-line><span>Ep[2] / T.Loss[2.09842] / T.Acc[0.30505] / V.Loss[1.99231] / V.Acc[0.34063] / F1[0.10350]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.34063]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:04&lt;00:00,  2.55s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:16&lt;00:00,  1.27s/it]</span></span>
<span data-line><span>Ep[3] / T.Loss[1.73236] / T.Acc[0.41851] / V.Loss[1.65402] / V.Acc[0.44896] / F1[0.19274]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.44896]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:12&lt;00:00,  2.58s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:20&lt;00:00,  1.35s/it]</span></span>
<span data-line><span>Ep[4] / T.Loss[1.35878] / T.Acc[0.54252] / V.Loss[1.29845] / V.Acc[0.55651] / F1[0.36214]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.55651]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:02&lt;00:00,  2.54s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.33s/it]</span></span>
<span data-line><span>Ep[5] / T.Loss[1.12199] / T.Acc[0.61617] / V.Loss[1.11763] / V.Acc[0.60781] / F1[0.39866]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.60781]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [09:59&lt;00:00,  2.53s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.96528] / T.Acc[0.66667] / V.Loss[1.11791] / V.Acc[0.61146] / F1[0.38909]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.61146]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:08&lt;00:00,  2.57s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:20&lt;00:00,  1.34s/it]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.85278] / T.Acc[0.69818] / V.Loss[1.05862] / V.Acc[0.61797] / F1[0.46073]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.61797]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:11&lt;00:00,  2.58s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:18&lt;00:00,  1.31s/it]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.76820] / T.Acc[0.72752] / V.Loss[1.04059] / V.Acc[0.63333] / F1[0.48444]</span></span>
<span data-line><span> * New Best Model -> Epoch [8] / best_score : [0.63333]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [09:57&lt;00:00,  2.52s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:25&lt;00:00,  1.43s/it]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.68204] / T.Acc[0.76035] / V.Loss[1.04405] / V.Acc[0.66224] / F1[0.49360]</span></span>
<span data-line><span> * New Best Model -> Epoch [9] / best_score : [0.66224]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:14&lt;00:00,  2.59s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.41s/it]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.62996] / T.Acc[0.77696] / V.Loss[1.05925] / V.Acc[0.65104] / F1[0.49769]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:09&lt;00:00,  2.57s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:22&lt;00:00,  1.37s/it]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.56258] / T.Acc[0.79971] / V.Loss[1.01786] / V.Acc[0.65026] / F1[0.49967]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [10:08&lt;00:00,  2.57s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.50302] / T.Acc[0.82021] / V.Loss[1.00199] / V.Acc[0.66615] / F1[0.55401]</span></span>
<span data-line><span> * New Best Model -> Epoch [12] / best_score : [0.66615]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:59&lt;00:00,  2.02s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.46993] / T.Acc[0.82964] / V.Loss[1.07661] / V.Acc[0.65964] / F1[0.49023]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:22&lt;00:00,  1.61s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.42177] / T.Acc[0.84751] / V.Loss[1.18239] / V.Acc[0.66276] / F1[0.49722]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:29&lt;00:00,  1.64s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.38900] / T.Acc[0.85687] / V.Loss[1.06578] / V.Acc[0.67760] / F1[0.52162]</span></span>
<span data-line><span> * New Best Model -> Epoch [15] / best_score : [0.67760]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:25&lt;00:00,  1.63s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.35508] / T.Acc[0.86841] / V.Loss[1.10693] / V.Acc[0.67969] / F1[0.55520]</span></span>
<span data-line><span> * New Best Model -> Epoch [16] / best_score : [0.67969]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:27&lt;00:00,  1.64s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.32741] / T.Acc[0.88205] / V.Loss[1.15547] / V.Acc[0.67161] / F1[0.53029]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:21&lt;00:00,  1.61s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.30776] / T.Acc[0.88252] / V.Loss[1.12714] / V.Acc[0.67682] / F1[0.53797]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:27&lt;00:00,  1.64s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.27441] / T.Acc[0.89511] / V.Loss[1.23778] / V.Acc[0.64922] / F1[0.48549]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:29&lt;00:00,  1.64s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.25071] / T.Acc[0.90388] / V.Loss[1.17095] / V.Acc[0.67526] / F1[0.53311]</span></span></code></pre></figure>
</li>
<li>
<p>출력 backbone freeze, lr 0.001, batch 64, image size 224<br/>
==<strong>Best : V.Acc[0.80156] / F1[0.68465]</strong>==</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --save_name=Weight_VIT_V0.KHS.tar --target_model=&quot;VIT_V0_KHS(True)&quot;</span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V0.KHS.tar', save_path='../models/checkpoint/', seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model='VIT_V0_KHS(True)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:42&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.35s/it]</span></span>
<span data-line><span>Ep[1] / T.Loss[1.01287] / T.Acc[0.68176] / V.Loss[0.76897] / V.Acc[0.74687] / F1[0.59991]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.74687]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:35&lt;00:00,  1.42s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.60664] / T.Acc[0.80729] / V.Loss[0.66516] / V.Acc[0.77005] / F1[0.63105]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.77005]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:39&lt;00:00,  1.43s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.36s/it]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.51408] / T.Acc[0.83511] / V.Loss[0.62337] / V.Acc[0.78281] / F1[0.65041]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.78281]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:38&lt;00:00,  1.43s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:25&lt;00:00,  1.42s/it]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.45651] / T.Acc[0.85476] / V.Loss[0.60633] / V.Acc[0.78281] / F1[0.64349]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:31&lt;00:00,  1.40s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.41884] / T.Acc[0.86702] / V.Loss[0.58609] / V.Acc[0.79505] / F1[0.68581]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.79505]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:31&lt;00:00,  1.40s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.38804] / T.Acc[0.88014] / V.Loss[0.58559] / V.Acc[0.79245] / F1[0.67278]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:39&lt;00:00,  1.43s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:28&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.36484] / T.Acc[0.88443] / V.Loss[0.57582] / V.Acc[0.79375] / F1[0.67049]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:44&lt;00:00,  1.45s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.40s/it]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.34282] / T.Acc[0.89280] / V.Loss[0.58755] / V.Acc[0.79063] / F1[0.66752]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:31&lt;00:00,  1.40s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.32651] / T.Acc[0.89695] / V.Loss[0.56441] / V.Acc[0.80156] / F1[0.68465]</span></span>
<span data-line><span> * New Best Model -> Epoch [9] / best_score : [0.80156]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:35&lt;00:00,  1.42s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.37s/it]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.31110] / T.Acc[0.90157] / V.Loss[0.56519] / V.Acc[0.79688] / F1[0.67979]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:38&lt;00:00,  1.43s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.41s/it]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.29579] / T.Acc[0.90763] / V.Loss[0.57807] / V.Acc[0.79010] / F1[0.67123]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:37&lt;00:00,  1.43s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.40s/it]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.28497] / T.Acc[0.91390] / V.Loss[0.58366] / V.Acc[0.79036] / F1[0.67093]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:40&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.27343] / T.Acc[0.91700] / V.Loss[0.56615] / V.Acc[0.79505] / F1[0.66275]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:35&lt;00:00,  1.42s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:26&lt;00:00,  1.45s/it]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.26345] / T.Acc[0.92102] / V.Loss[0.56966] / V.Acc[0.79661] / F1[0.67525]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:35&lt;00:00,  1.42s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:23&lt;00:00,  1.39s/it]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.25552] / T.Acc[0.92240] / V.Loss[0.57085] / V.Acc[0.79766] / F1[0.67867]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:40&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:19&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.24716] / T.Acc[0.92491] / V.Loss[0.57247] / V.Acc[0.79609] / F1[0.67629]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:42&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:24&lt;00:00,  1.41s/it]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.23834] / T.Acc[0.92807] / V.Loss[0.57362] / V.Acc[0.79505] / F1[0.67354]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:37&lt;00:00,  1.43s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:20&lt;00:00,  1.35s/it]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.23141] / T.Acc[0.93163] / V.Loss[0.58015] / V.Acc[0.79505] / F1[0.67538]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:32&lt;00:00,  1.40s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.35s/it]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.22470] / T.Acc[0.93493] / V.Loss[0.59474] / V.Acc[0.78958] / F1[0.66586]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:40&lt;00:00,  1.44s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:21&lt;00:00,  1.35s/it]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.21834] / T.Acc[0.93440] / V.Loss[0.57225] / V.Acc[0.79818] / F1[0.68122]</span></span></code></pre></figure>
</li>
<li>
<p>출력 backbone not freeze, lr 0.001, batch 32, image size 224<br/>
Best : V.Acc[0.68803] / F1[0.53817]</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --save_name=Weight_VIT_V3_KHS.tar --target_model=&quot;VIT_V0_KHS(False)&quot; --batch_size=32</span></span>
<span data-line><span>Namespace(batch_size=32, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V3_KHS.tar', save_path='../models/checkpoint/', seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model='VIT_V0_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:18&lt;00:00,  1.31s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:20&lt;00:00,  1.48it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[2.35882] / T.Acc[0.23077] / V.Loss[2.24234] / V.Acc[0.30935] / F1[0.06268]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.30935]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:26&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.47it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[2.05232] / T.Acc[0.31917] / V.Loss[2.04367] / V.Acc[0.33456] / F1[0.08747]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.33456]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:22&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.46it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[1.70743] / T.Acc[0.43036] / V.Loss[1.55862] / V.Acc[0.46507] / F1[0.24850]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.46507]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:16&lt;00:00,  1.30s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.46it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[1.37479] / T.Acc[0.53621] / V.Loss[1.38156] / V.Acc[0.52048] / F1[0.35487]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.52048]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:26&lt;00:00,  1.33s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:17&lt;00:00,  1.53it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[1.15002] / T.Acc[0.60610] / V.Loss[1.19543] / V.Acc[0.59611] / F1[0.40237]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.59611]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:28&lt;00:00,  1.33s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:19&lt;00:00,  1.50it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.97343] / T.Acc[0.66246] / V.Loss[1.17950] / V.Acc[0.58613] / F1[0.40528]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:21&lt;00:00,  1.31s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:20&lt;00:00,  1.48it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.86760] / T.Acc[0.70085] / V.Loss[1.09320] / V.Acc[0.63813] / F1[0.48825]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.63813]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:25&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:25&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.78374] / T.Acc[0.72774] / V.Loss[0.99804] / V.Acc[0.65678] / F1[0.52388]</span></span>
<span data-line><span> * New Best Model -> Epoch [8] / best_score : [0.65678]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:19&lt;00:00,  1.31s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:19&lt;00:00,  1.49it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.70054] / T.Acc[0.75251] / V.Loss[1.04469] / V.Acc[0.65389] / F1[0.49099]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:22&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:22&lt;00:00,  1.45it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.61854] / T.Acc[0.78165] / V.Loss[1.01717] / V.Acc[0.66518] / F1[0.53997]</span></span>
<span data-line><span> * New Best Model -> Epoch [10] / best_score : [0.66518]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:27&lt;00:00,  1.33s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:20&lt;00:00,  1.47it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.56697] / T.Acc[0.79982] / V.Loss[0.95836] / V.Acc[0.67516] / F1[0.53046]</span></span>
<span data-line><span> * New Best Model -> Epoch [11] / best_score : [0.67516]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:18&lt;00:00,  1.31s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:00&lt;00:00,  1.97it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.50977] / T.Acc[0.81488] / V.Loss[1.09090] / V.Acc[0.67201] / F1[0.53584]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:52&lt;00:00,  2.27it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.46537] / T.Acc[0.83278] / V.Loss[1.00580] / V.Acc[0.68251] / F1[0.54040]</span></span>
<span data-line><span> * New Best Model -> Epoch [13] / best_score : [0.68251]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:52&lt;00:00,  2.26it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.39941] / T.Acc[0.85485] / V.Loss[1.15871] / V.Acc[0.67647] / F1[0.52933]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:25&lt;00:00,  1.23it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:51&lt;00:00,  2.31it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.38542] / T.Acc[0.86000] / V.Loss[1.15737] / V.Acc[0.68251] / F1[0.54623]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:32&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:49&lt;00:00,  2.40it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.35436] / T.Acc[0.87018] / V.Loss[1.22028] / V.Acc[0.65546] / F1[0.49839]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:32&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:51&lt;00:00,  2.31it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.32017] / T.Acc[0.88293] / V.Loss[1.31025] / V.Acc[0.68803] / F1[0.53817]</span></span>
<span data-line><span> * New Best Model -> Epoch [17] / best_score : [0.68803]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V3_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:29&lt;00:00,  1.22it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:52&lt;00:00,  2.26it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.28085] / T.Acc[0.89984] / V.Loss[1.26609] / V.Acc[0.68277] / F1[0.56806]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [06:31&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:45&lt;00:00,  2.63it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.25701] / T.Acc[0.90684] / V.Loss[1.28518] / V.Acc[0.68409] / F1[0.50869]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:06&lt;00:00,  1.54it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:28&lt;00:00,  4.12it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.23210] / T.Acc[0.91788] / V.Loss[1.37192] / V.Acc[0.66833] / F1[0.52610]</span></span></code></pre></figure>
</li>
<li>
<p>출력 backbone freeze, lr 0.001, batch 32, image size 224<br/>
Best : V.Acc[0.80672] / F1[0.67891]</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --save_name=Weight_VIT_V4_KHS.tar --target_model=&quot;VIT_V0_KHS(True)&quot; --batch_size=32</span></span>
<span data-line><span>Namespace(batch_size=32, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V4_KHS.tar', save_path='../models/checkpoint/', seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model='VIT_V0_KHS(True)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:43&lt;00:00,  1.38it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:22&lt;00:00,  1.45it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.90020] / T.Acc[0.71545] / V.Loss[0.70784] / V.Acc[0.76628] / F1[0.61103]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.76628]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:40&lt;00:00,  1.39it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.42it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.54853] / T.Acc[0.82426] / V.Loss[0.63471] / V.Acc[0.78755] / F1[0.64693]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.78755]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:39&lt;00:00,  1.39it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.46it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.46461] / T.Acc[0.85016] / V.Loss[0.60170] / V.Acc[0.79123] / F1[0.64586]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.79123]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:36&lt;00:00,  1.41it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.41it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.41091] / T.Acc[0.86853] / V.Loss[0.60359] / V.Acc[0.78493] / F1[0.65325]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:38&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.47it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.37491] / T.Acc[0.88062] / V.Loss[0.58578] / V.Acc[0.79412] / F1[0.66977]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.79412]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:34&lt;00:00,  1.41it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.34709] / T.Acc[0.89072] / V.Loss[0.58770] / V.Acc[0.80042] / F1[0.67648]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.80042]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:31&lt;00:00,  1.42it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.32333] / T.Acc[0.89859] / V.Loss[0.58049] / V.Acc[0.80173] / F1[0.67949]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.80173]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:39&lt;00:00,  1.39it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.45it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.30371] / T.Acc[0.90817] / V.Loss[0.59278] / V.Acc[0.79779] / F1[0.66623]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:35&lt;00:00,  1.41it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.43it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.28532] / T.Acc[0.91385] / V.Loss[0.58300] / V.Acc[0.80593] / F1[0.68009]</span></span>
<span data-line><span> * New Best Model -> Epoch [9] / best_score : [0.80593]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:37&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:21&lt;00:00,  1.47it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.27151] / T.Acc[0.91801] / V.Loss[0.57586] / V.Acc[0.80095] / F1[0.67977]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:43&lt;00:00,  1.38it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.43it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.25854] / T.Acc[0.92065] / V.Loss[0.58382] / V.Acc[0.79701] / F1[0.67650]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:32&lt;00:00,  1.42it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.43it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.24652] / T.Acc[0.92548] / V.Loss[0.60004] / V.Acc[0.79727] / F1[0.66746]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:40&lt;00:00,  1.39it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.41it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.23740] / T.Acc[0.92845] / V.Loss[0.58483] / V.Acc[0.80042] / F1[0.67031]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:36&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:25&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.22740] / T.Acc[0.93281] / V.Loss[0.59447] / V.Acc[0.79727] / F1[0.66578]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:34&lt;00:00,  1.41it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.41it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.21846] / T.Acc[0.93605] / V.Loss[0.59572] / V.Acc[0.79727] / F1[0.67786]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:43&lt;00:00,  1.38it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:25&lt;00:00,  1.39it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.20956] / T.Acc[0.93856] / V.Loss[0.58916] / V.Acc[0.80672] / F1[0.67891]</span></span>
<span data-line><span> * New Best Model -> Epoch [16] / best_score : [0.80672]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V4_KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:38&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.42it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.20336] / T.Acc[0.94146] / V.Loss[0.61937] / V.Acc[0.79254] / F1[0.66284]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:40&lt;00:00,  1.39it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:23&lt;00:00,  1.43it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.19630] / T.Acc[0.94279] / V.Loss[0.60255] / V.Acc[0.80226] / F1[0.68113]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:39&lt;00:00,  1.39it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:24&lt;00:00,  1.40it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.19012] / T.Acc[0.94391] / V.Loss[0.62428] / V.Acc[0.79858] / F1[0.66664]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [05:35&lt;00:00,  1.41it/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:17&lt;00:00,  1.53it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.18292] / T.Acc[0.94642] / V.Loss[0.59904] / V.Acc[0.80252] / F1[0.68419]</span></span></code></pre></figure>
</li>
</ul>
<p>기본적으로 backbone을 freeze한 것이 성능이 더 좋다<br/>
다음으로는 Best Case를 도출한 Hyper Parameter 기준으로 Argmentation 적용해보았다</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>기존 : A.Resize -> A.Normalize -> ToTensorV2</span></span>
<span data-line><span>변경 : A.RandomResizedCrop -> A.RandomBrightnessContrast(p=0.3)</span></span>
<span data-line><span>			-> A.RandomGamma(p=0.3) -> A.RandomToneCurve()</span></span>
<span data-line><span>			-> A.HorizontalFlip(p=0.5) -> A.Normalize</span></span>
<span data-line><span>			-> ToTensorV2</span></span></code></pre></figure>
<ul>
<li>
<p>출력 backbone freeze, lr 0.001, batch 64, image size 224 <strong><span class="text-highlight">+ Argmentation</span></strong></p>
<p><strong>Best :</strong> V.Acc[0.79870] / F1[0.68489]</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --save_name=Weight_VIT_V5.KHS.tar --target_model=&quot;VIT_V0_KHS(True)&quot;</span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V5.KHS.tar', save_path='../models/checkpoint/', seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model='VIT_V0_KHS(True)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [02:12&lt;00:00,  1.79it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[1.01179] / T.Acc[0.68348] / V.Loss[0.77707] / V.Acc[0.73932] / F1[0.59065]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.73932]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:10&lt;00:00,  1.25it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.61161] / T.Acc[0.80406] / V.Loss[0.66892] / V.Acc[0.76745] / F1[0.63043]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.76745]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.52576] / T.Acc[0.82555] / V.Loss[0.63488] / V.Acc[0.78151] / F1[0.64378]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.78151]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:40&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.47068] / T.Acc[0.84909] / V.Loss[0.61181] / V.Acc[0.77656] / F1[0.64096]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:38&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.43788] / T.Acc[0.85278] / V.Loss[0.58862] / V.Acc[0.79349] / F1[0.67686]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.79349]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.41271] / T.Acc[0.86313] / V.Loss[0.58636] / V.Acc[0.79141] / F1[0.66837]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:44&lt;00:00,  1.05it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.38895] / T.Acc[0.87118] / V.Loss[0.58156] / V.Acc[0.78906] / F1[0.66444]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:37&lt;00:00,  1.09it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.36667] / T.Acc[0.87658] / V.Loss[0.57438] / V.Acc[0.79115] / F1[0.65924]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.35887] / T.Acc[0.87836] / V.Loss[0.56909] / V.Acc[0.79844] / F1[0.68349]</span></span>
<span data-line><span> * New Best Model -> Epoch [9] / best_score : [0.79844]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.35002] / T.Acc[0.87915] / V.Loss[0.56809] / V.Acc[0.78984] / F1[0.67526]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.33657] / T.Acc[0.88726] / V.Loss[0.56813] / V.Acc[0.78776] / F1[0.67163]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.22it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.32434] / T.Acc[0.89016] / V.Loss[0.57840] / V.Acc[0.78594] / F1[0.67011]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:43&lt;00:00,  1.06it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.31700] / T.Acc[0.89300] / V.Loss[0.56339] / V.Acc[0.79557] / F1[0.68140]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.12it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.30919] / T.Acc[0.89623] / V.Loss[0.56173] / V.Acc[0.79870] / F1[0.68489]</span></span>
<span data-line><span> * New Best Model -> Epoch [14] / best_score : [0.79870]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V5.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:44&lt;00:00,  1.06it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.30300] / T.Acc[0.89768] / V.Loss[0.56118] / V.Acc[0.79297] / F1[0.67191]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:44&lt;00:00,  1.06it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.29778] / T.Acc[0.89827] / V.Loss[0.55687] / V.Acc[0.79583] / F1[0.68419]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:41&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.29262] / T.Acc[0.90045] / V.Loss[0.55521] / V.Acc[0.79818] / F1[0.68579]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.06it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.28258] / T.Acc[0.90691] / V.Loss[0.55916] / V.Acc[0.79609] / F1[0.68550]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.28178] / T.Acc[0.90493] / V.Loss[0.58251] / V.Acc[0.78750] / F1[0.67331]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:47&lt;00:00,  1.04it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.11it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.27821] / T.Acc[0.90520] / V.Loss[0.55727] / V.Acc[0.79792] / F1[0.68210]</span></span></code></pre></figure>
</li>
</ul>
<p>생각보다 성능이 별로다 model의 complexity가 부족한걸까?<br/>
더 큰 image size를 사용하는 weight를 적용해보았고, 매우 큰 성능 향상이 일어났다</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>기존 : IMAGENET1K_V1 (input size : 224x224)</span></span>
<span data-line><span>변경 : IMAGENET1K_SWAG_E2E_V1 (input size : 384x384)</span></span></code></pre></figure>
<ul>
<li>
<p>출력 backbone freeze, lr 0.001, batch 64, image size <span class="text-highlight">384</span> <strong><span class="text-highlight">+ Argmentation</span></strong></p>
<p><strong>Best :</strong> V.Acc[0.84792] / F1[0.76518]</p>
<p><code>==제출 : Finished f1 : 0.6365 / acc: 69.1746==</code> </p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --save_name=Weight_VIT_V6.KHS.tar --target_model=&quot;VIT_V1_KHS(True)&quot; --img_size=384</span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=384, lr=0.001, save_name='Weight_VIT_V6.KHS.tar', save_path='../models/checkpoint/', seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model='VIT_V1_KHS(True)', validation_ratio=0.2)</span></span>
<span data-line><span>Downloading: &quot;https://download.pytorch.org/models/vit_b_16_swag-9ac1b537.pth&quot; to /opt/ml/.cache/torch/hub/checkpoints/vit_b_16_swag-9ac1b537.pth</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 331M/331M [00:02&lt;00:00, 132MB/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:55&lt;00:00,  1.75s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:36&lt;00:00,  1.60s/it]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.75290] / T.Acc[0.76490] / V.Loss[0.49472] / V.Acc[0.82266] / F1[0.71996]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.82266]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:55&lt;00:00,  1.75s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:37&lt;00:00,  1.62s/it]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.42801] / T.Acc[0.85581] / V.Loss[0.45157] / V.Acc[0.83151] / F1[0.72053]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.83151]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:59&lt;00:00,  1.77s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.64s/it]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.36666] / T.Acc[0.87355] / V.Loss[0.44778] / V.Acc[0.83255] / F1[0.70651]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.83255]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:55&lt;00:00,  1.75s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:37&lt;00:00,  1.63s/it]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.32939] / T.Acc[0.88641] / V.Loss[0.42136] / V.Acc[0.84219] / F1[0.75506]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.84219]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:54&lt;00:00,  1.75s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:36&lt;00:00,  1.61s/it]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.30715] / T.Acc[0.89089] / V.Loss[0.41402] / V.Acc[0.84115] / F1[0.72825]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:56&lt;00:00,  1.76s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:37&lt;00:00,  1.63s/it]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.28671] / T.Acc[0.90005] / V.Loss[0.42921] / V.Acc[0.83047] / F1[0.73401]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:57&lt;00:00,  1.76s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.65s/it]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.26758] / T.Acc[0.90447] / V.Loss[0.40914] / V.Acc[0.84792] / F1[0.76518]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.84792]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V6.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:59&lt;00:00,  1.77s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:36&lt;00:00,  1.61s/it]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.25894] / T.Acc[0.90665] / V.Loss[0.40105] / V.Acc[0.84427] / F1[0.74449]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:00&lt;00:00,  1.77s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:37&lt;00:00,  1.62s/it]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.25316] / T.Acc[0.90803] / V.Loss[0.39724] / V.Acc[0.84531] / F1[0.75330]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:58&lt;00:00,  1.76s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:25&lt;00:00,  1.43s/it]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.23876] / T.Acc[0.91568] / V.Loss[0.42472] / V.Acc[0.83359] / F1[0.73190]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:46&lt;00:00,  1.21s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:04&lt;00:00,  1.08s/it]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.23479] / T.Acc[0.91653] / V.Loss[0.42066] / V.Acc[0.83724] / F1[0.74352]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:46&lt;00:00,  1.21s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:06&lt;00:00,  1.11s/it]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.23051] / T.Acc[0.91634] / V.Loss[0.41568] / V.Acc[0.84401] / F1[0.76051]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:48&lt;00:00,  1.22s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:06&lt;00:00,  1.11s/it]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.22216] / T.Acc[0.91983] / V.Loss[0.41315] / V.Acc[0.84323] / F1[0.74415]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:46&lt;00:00,  1.21s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:06&lt;00:00,  1.10s/it]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.22216] / T.Acc[0.92023] / V.Loss[0.41681] / V.Acc[0.84167] / F1[0.74784]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:46&lt;00:00,  1.21s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:08&lt;00:00,  1.14s/it]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.21173] / T.Acc[0.92412] / V.Loss[0.41928] / V.Acc[0.84609] / F1[0.73804]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:07&lt;00:00,  1.80s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:46&lt;00:00,  1.77s/it]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.20711] / T.Acc[0.92616] / V.Loss[0.45866] / V.Acc[0.83203] / F1[0.72702]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:52&lt;00:00,  2.00s/it]</span></span>
<span data-line><span> 40%|█████████████████████████████████████████▌                                                                                     42%|███████████████████████████████████████████▎                                                                                   43%|█████████████████████████████████████████████                                                                                  45%|██████████████████████████████████████████████▊                                                         | 27/60 [00:48&lt;01:01,  47%|████████████████████████████████████████████████▌                                                       | 28/60 [00:51&lt;01:01,  48%|██████████████████████████████████████████████████▎                                                     | 29/60 [00:52&lt;00:57,  50%|████████████████████████████████████████████████████                                                    | 30/60 [00:54&lt;00:52,  52%|█████████████████████████████████████████████████████▋                                                  | 31/60 [00:56&lt;00:51,  53%|███████████████████████████████████████████████████████▍                                                | 32/60 [00:58&lt;00:52,  55%|█████████████████████████████████████████████████████████▏                                              | 33/60 [01:00&lt;00:54,  57%|██████████████████████████████████████████████████████████▉                                             | 34/60 [01:02&lt;00:53,  58%|████████████████████████████████████████████████████████████▋                                           | 35/60 [01:04&lt;00:48,  60%|██████████████████████████████████████████████████████████████▍                                         | 36/60 [01:05&lt;00:43,  62%|████████████████████████████████████████████████████████████████▏                                       | 37/60 [01:07&lt;00:40,  63%|█████████████████████████████████████████████████████████████████▊                                      | 38/60 [01:09&lt;00:39,  65%|███████████████████████████████████████████████████████████████████▌                                    | 39/60 [01:11&lt;00:41,  67%|█████████████████████████████████████████████████████████████████████▎                                  | 40/60 [01:13&lt;00:39,  68%|███████████████████████████████████████████████████████████████████████                                 | 41/60 [01:15&lt;00:37,  70%|████████████████████████████████████████████████████████████████████████▊                               | 42/60 [01:17&lt;00:33,  72%|██████████████████████████████████████████████████████████████████████████▌                             | 43/60 [01:18&lt;00:30,  73%|████████████████████████████████████████████████████████████████████████████▎                           | 44/60 [01:20&lt;00:28,  75%|██████████████████████████████████████████████████████████████████████████████                          | 45/60 [01:22&lt;00:25,  77%|███████████████████████████████████████████████████████████████████████████████▋                        | 46/60 [01:24&lt;00:25,  78%|█████████████████████████████████████████████████████████████████████████████████▍                      | 47/60 [01:26&lt;00:24,  80%|███████████████████████████████████████████████████████████████████████████████████▏                    | 48/60 [01:28&lt;00:24,  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 49/60 [01:30&lt;00:22,  83%|██████████████████████████████████████████████████████████████████████████████████████▋                 | 50/60 [01:32&lt;00:20,  85%|████████████████████████████████████████████████████████████████████████████████████████▍               | 51/60 [01:35&lt;00:18,  87%|██████████████████████████████████████████████████████████████████████████████████████████▏             | 52/60 [01:36&lt;00:15,  88%|███████████████████████████████████████████████████████████████████████████████████████████▊            | 53/60 [01:38&lt;00:12,  90%|█████████████████████████████████████████████████████████████████████████████████████████████▌          | 54/60 [01:39&lt;00:10,  92%|███████████████████████████████████████████████████████████████████████████████████████████████▎        | 55/60 [01:42&lt;00:09,  93%|█████████████████████████████████████████████████████████████████████████████████████████████████       | 56/60 [01:44&lt;00:08,  95%|██████████████████████████████████████████████████████████████████████████████████████████████████▊     | 57/60 [01:46&lt;00:06,  97%|████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 58/60 [01:48&lt;00:03,  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 59/60 [01:49&lt;00:01, 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:49&lt;00:00,  1.83s/it]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.20190] / T.Acc[0.92570] / V.Loss[0.40630] / V.Acc[0.84141] / F1[0.73352]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:50&lt;00:00,  1.99s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.92s/it]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.20074] / T.Acc[0.92675] / V.Loss[0.40801] / V.Acc[0.84583] / F1[0.75911]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:52&lt;00:00,  1.99s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:48&lt;00:00,  1.81s/it]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.19480] / T.Acc[0.92820] / V.Loss[0.42851] / V.Acc[0.84531] / F1[0.72476]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [07:56&lt;00:00,  2.01s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:48&lt;00:00,  1.81s/it]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.19190] / T.Acc[0.93183] / V.Loss[0.41594] / V.Acc[0.84557] / F1[0.76164]</span></span></code></pre></figure>
</li>
</ul>
<p>그렇다면 같은 작은 입력크기지만 parameter가 더 큰 타입을 써보면 어떨까?</p>
<ul>
<li>
<p>출력backbone freeze, lr 0.001, batch 64, image size <span class="text-highlight">224</span> <strong><span class="text-highlight">+ Argmentation</span></strong></p>
<p><strong>Best :</strong> V.Acc[0.85026] / F1[0.73974]</p>
<p><code>==제출 : Finished f1 : 0.6365 / acc: 71.8889==</code> </p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --save_name=Weight_VIT_V7.KHS.tar --target_model=&quot;VIT_V2_KHS(True)&quot; --img_size=224</span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V7.KHS.tar', save_path='../models/checkpoint/', seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model='VIT_V2_KHS(True)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:54&lt;00:00,  1.75s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:36&lt;00:00,  1.60s/it]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.77953] / T.Acc[0.75514] / V.Loss[0.57383] / V.Acc[0.79896] / F1[0.61356]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.79896]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:51&lt;00:00,  1.74s/it]</span></span>
<span data-line><span> 35%|████████████████████████████████████▍                                                                                          37%|██████████████████████████████████████▏                                                                                        38%|███████████████████████████████████████▊                                                                                       40%|█████████████████████████████████████████▌                                                              | 24/60 [00:38&lt;00:54,  42%|███████████████████████████████████████████▎                                                            | 25/60 [00:40&lt;00:56,  43%|█████████████████████████████████████████████                                                           | 26/60 [00:42&lt;00:57,  45%|██████████████████████████████████████████████▊                                                         | 27/60 [00:43&lt;00:52,  47%|████████████████████████████████████████████████▌                                                       | 28/60 [00:44&lt;00:49,  48%|██████████████████████████████████████████████████▎                                                     | 29/60 [00:46&lt;00:45,  50%|████████████████████████████████████████████████████                                                    | 30/60 [00:47&lt;00:43,  52%|█████████████████████████████████████████████████████▋                                                  | 31/60 [00:49&lt;00:46,  53%|███████████████████████████████████████████████████████▍                                                | 32/60 [00:51&lt;00:49,  55%|█████████████████████████████████████████████████████████▏                                              | 33/60 [00:53&lt;00:50,  57%|██████████████████████████████████████████████████████████▉                                             | 34/60 [00:55&lt;00:44,  58%|████████████████████████████████████████████████████████████▋                                           | 35/60 [00:56&lt;00:39,  60%|██████████████████████████████████████████████████████████████▍                                         | 36/60 [00:57&lt;00:35,  62%|████████████████████████████████████████████████████████████████▏                                       | 37/60 [00:59&lt;00:33,  63%|█████████████████████████████████████████████████████████████████▊                                      | 38/60 [01:00&lt;00:33,  65%|███████████████████████████████████████████████████████████████████▌                                    | 39/60 [01:02&lt;00:35,  67%|█████████████████████████████████████████████████████████████████████▎                                  | 40/60 [01:04&lt;00:34,  68%|███████████████████████████████████████████████████████████████████████                                 | 41/60 [01:06&lt;00:33,  70%|████████████████████████████████████████████████████████████████████████▊                               | 42/60 [01:07&lt;00:28,  72%|██████████████████████████████████████████████████████████████████████████▌                             | 43/60 [01:09&lt;00:25,  73%|████████████████████████████████████████████████████████████████████████████▎                           | 44/60 [01:10&lt;00:23,  75%|██████████████████████████████████████████████████████████████████████████████                          | 45/60 [01:12&lt;00:23,  77%|███████████████████████████████████████████████████████████████████████████████▋                        | 46/60 [01:13&lt;00:21,  78%|█████████████████████████████████████████████████████████████████████████████████▍                      | 47/60 [01:15&lt;00:21,  80%|███████████████████████████████████████████████████████████████████████████████████▏                    | 48/60 [01:17&lt;00:21,  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 49/60 [01:19&lt;00:21,  83%|██████████████████████████████████████████████████████████████████████████████████████▋                 | 50/60 [01:22&lt;00:19,  85%|████████████████████████████████████████████████████████████████████████████████████████▍               | 51/60 [01:23&lt;00:17,  87%|██████████████████████████████████████████████████████████████████████████████████████████▏             | 52/60 [01:25&lt;00:15,  88%|███████████████████████████████████████████████████████████████████████████████████████████▊            | 53/60 [01:27&lt;00:12,  90%|█████████████████████████████████████████████████████████████████████████████████████████████▌          | 54/60 [01:28&lt;00:10,  92%|███████████████████████████████████████████████████████████████████████████████████████████████▎        | 55/60 [01:30&lt;00:07,  93%|█████████████████████████████████████████████████████████████████████████████████████████████████       | 56/60 [01:31&lt;00:06,  95%|██████████████████████████████████████████████████████████████████████████████████████████████████▊     | 57/60 [01:33&lt;00:04,  97%|████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 58/60 [01:35&lt;00:03,  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 59/60 [01:37&lt;00:01, 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00, 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.63s/it]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.47517] / T.Acc[0.84171] / V.Loss[0.49324] / V.Acc[0.81979] / F1[0.65554]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.81979]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:54&lt;00:00,  1.75s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.64s/it]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.41909] / T.Acc[0.85522] / V.Loss[0.47600] / V.Acc[0.81953] / F1[0.68702]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:59&lt;00:00,  1.77s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:38&lt;00:00,  1.63s/it]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.36425] / T.Acc[0.87599] / V.Loss[0.44738] / V.Acc[0.83672] / F1[0.71814]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.83672]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:55&lt;00:00,  1.75s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:35&lt;00:00,  1.59s/it]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.34096] / T.Acc[0.87922] / V.Loss[0.45111] / V.Acc[0.83307] / F1[0.70400]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:25&lt;00:00,  1.37s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:56&lt;00:00,  1.06it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.32696] / T.Acc[0.88423] / V.Loss[0.45945] / V.Acc[0.82734] / F1[0.69541]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:02&lt;00:00,  1.02s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:56&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.30602] / T.Acc[0.89260] / V.Loss[0.45552] / V.Acc[0.83437] / F1[0.68316]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:05&lt;00:00,  1.04s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:54&lt;00:00,  1.10it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.28945] / T.Acc[0.89913] / V.Loss[0.45325] / V.Acc[0.83698] / F1[0.70789]</span></span>
<span data-line><span> * New Best Model -> Epoch [8] / best_score : [0.83698]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [04:53&lt;00:00,  1.24s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.28267] / T.Acc[0.90282] / V.Loss[0.42905] / V.Acc[0.84297] / F1[0.73575]</span></span>
<span data-line><span> * New Best Model -> Epoch [9] / best_score : [0.84297]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:08&lt;00:00,  2.06s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.27318] / T.Acc[0.90381] / V.Loss[0.43089] / V.Acc[0.83802] / F1[0.71885]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:13&lt;00:00,  2.08s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.26326] / T.Acc[0.90454] / V.Loss[0.41700] / V.Acc[0.84870] / F1[0.73673]</span></span>
<span data-line><span> * New Best Model -> Epoch [11] / best_score : [0.84870]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:05&lt;00:00,  2.05s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.25318] / T.Acc[0.91001] / V.Loss[0.44166] / V.Acc[0.83750] / F1[0.72551]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:06&lt;00:00,  2.05s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:54&lt;00:00,  1.90s/it]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.25543] / T.Acc[0.90816] / V.Loss[0.42322] / V.Acc[0.84844] / F1[0.73197]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:06&lt;00:00,  2.05s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:56&lt;00:00,  1.95s/it]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.24292] / T.Acc[0.91357] / V.Loss[0.42808] / V.Acc[0.84505] / F1[0.74298]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:11&lt;00:00,  2.07s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:56&lt;00:00,  1.94s/it]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.24305] / T.Acc[0.91304] / V.Loss[0.41805] / V.Acc[0.85026] / F1[0.73974]</span></span>
<span data-line><span> * New Best Model -> Epoch [15] / best_score : [0.85026]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V7.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:13&lt;00:00,  2.08s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.93s/it]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.23123] / T.Acc[0.91851] / V.Loss[0.43814] / V.Acc[0.84635] / F1[0.73798]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:10&lt;00:00,  2.07s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:56&lt;00:00,  1.93s/it]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.22616] / T.Acc[0.91957] / V.Loss[0.45303] / V.Acc[0.83307] / F1[0.71838]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:02&lt;00:00,  2.03s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:55&lt;00:00,  1.92s/it]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.22559] / T.Acc[0.91733] / V.Loss[0.44956] / V.Acc[0.84089] / F1[0.72816]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:08&lt;00:00,  2.06s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:57&lt;00:00,  1.95s/it]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.22509] / T.Acc[0.91805] / V.Loss[0.44156] / V.Acc[0.83698] / F1[0.72255]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [08:11&lt;00:00,  2.08s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [01:56&lt;00:00,  1.95s/it]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.21347] / T.Acc[0.92418] / V.Loss[0.42967] / V.Acc[0.84349] / F1[0.74568]</span></span></code></pre></figure>
</li>
</ul>
<p>정확도가 조금 더 올라갔지만, F1 Score는 떨어졌다<br/>
모델의 Complexity가 단순히 크다고 성능이 올라가는 것이 아니었다<br/>
또한 제출결과, 두 모델의 f1 score는 같았다</p>
<p>그렇다면 더 큰 입력을 받는 vit 모델을 사용해보면 어떨까?</p>
<ul>
<li>
<p>출력backbone freeze, lr 0.001, batch 64, image size <span class="text-highlight">512</span> <strong><span class="text-highlight">+ Argmentation</span></strong></p>
<p><strong>Best :</strong> V.Acc[0.85625] / F1[0.77284] (시간이 너무 오래걸려 중도 취소)</p>
<p><code>==제출 : Finished f1 : 0.6996 / acc: 77.2698==</code> </p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --save_name=Weight_VIT_V8.KHS.tar --target_model=&quot;VIT_V3_KHS(True)&quot; --img_size=512</span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=512, lr=0.001, save_name='Weight_VIT_V8.KHS.tar', save_path='../models/checkpoint/', seed=41, step_enable=False, step_gamma=1.0, step_size=10, target_model='VIT_V3_KHS(True)', validation_ratio=0.2)</span></span>
<span data-line><span>Downloading: &quot;https://download.pytorch.org/models/vit_l_16_swag-4f3808c9.pth&quot; to /opt/ml/.cache/torch/hub/checkpoints/vit_l_16_swag-4f3808c9.pth</span></span>
<span data-line><span>100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.14G/1.14G [01:02&lt;00:00, 19.6MB/s]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [35:14&lt;00:00,  8.92s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [08:13&lt;00:00,  8.23s/it]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.80442] / T.Acc[0.75738] / V.Loss[0.51355] / V.Acc[0.81380] / F1[0.60947]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.81380]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [35:16&lt;00:00,  8.93s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [08:15&lt;00:00,  8.26s/it]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.44198] / T.Acc[0.86208] / V.Loss[0.42709] / V.Acc[0.84115] / F1[0.72250]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.84115]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [32:11&lt;00:00,  8.15s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:49&lt;00:00,  4.83s/it]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.36728] / T.Acc[0.88001] / V.Loss[0.39352] / V.Acc[0.84896] / F1[0.72509]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.84896]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [26:13&lt;00:00,  6.64s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:52&lt;00:00,  4.87s/it]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.33172] / T.Acc[0.88957] / V.Loss[0.38783] / V.Acc[0.84974] / F1[0.74419]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.84974]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [23:24&lt;00:00,  5.92s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:52&lt;00:00,  4.87s/it]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.30828] / T.Acc[0.89616] / V.Loss[0.36456] / V.Acc[0.85625] / F1[0.77284]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.85625]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V8.KHS.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [20:40&lt;00:00,  5.23s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:51&lt;00:00,  4.85s/it]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.29063] / T.Acc[0.90289] / V.Loss[0.39297] / V.Acc[0.84297] / F1[0.72794]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [20:33&lt;00:00,  5.20s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [04:49&lt;00:00,  4.83s/it]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.27253] / T.Acc[0.90638] / V.Loss[0.37759] / V.Acc[0.84870] / F1[0.72398]</span></span>
<span data-line><span> 22%|██████████████████████▏</span></span></code></pre></figure>
</li>
</ul>
<p>피어세션 시간에 팀원들과 다시 상의해본 결과, 가장 결과가 잘 나온 팀원은 Baseline 코드를 사용했는데, 내 코드와의 차이점은 Adam대신 SGD를 사용하고, Scheduler를 사용했다는 점이다.<br/>
Optimizer와 Scheduler를 동일하게 해서 다시 돌려보았다<br/>
<span class="text-highlight">기존(V.Acc[0.67969] / F1[0.55520])대비 향상된 결과를 얻었다</span><br/>
그러나 향상된 결과의 f1 score도 기존 case와 값과 크게 다르지 않다</p>
<ul>
<li>
<p>출력<span class="text-highlight">backbone not freeze</span>, lr 0.001, batch 64, image size <span class="text-highlight">224</span> <strong><span class="text-highlight">+ Argmentation</span></strong></p>
<p><strong>Best :</strong> V.Acc[0.86016] / F1[0.74706]</p>
<p><code>==제출 : Finished f1 : 0.6275 / acc: 72.0476==</code></p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py </span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V0_KHS_SGD.tar', save_path='../models/checkpoint/', seed=41, step_enable=True, step_gamma=1.0, step_size=10, target_model='VIT_V0_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:50&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.21it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[1.93595] / T.Acc[0.41377] / V.Loss[1.52286] / V.Acc[0.56536] / F1[0.27258]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.27258]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:50&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29&lt;00:00,  2.06it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[1.22550] / T.Acc[0.66172] / V.Loss[1.07307] / V.Acc[0.68802] / F1[0.40309]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.40309]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:47&lt;00:00,  1.04it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.20it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.88808] / T.Acc[0.75461] / V.Loss[0.84355] / V.Acc[0.75365] / F1[0.49735]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.49735]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:48&lt;00:00,  1.04it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.15it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.69466] / T.Acc[0.80861] / V.Loss[0.69515] / V.Acc[0.79453] / F1[0.54780]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.54780]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:53&lt;00:00,  1.01it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.10it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.57230] / T.Acc[0.83966] / V.Loss[0.61124] / V.Acc[0.80859] / F1[0.56861]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.56861]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29&lt;00:00,  2.05it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.49565] / T.Acc[0.85746] / V.Loss[0.55649] / V.Acc[0.81641] / F1[0.58890]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.58890]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.43692] / T.Acc[0.87144] / V.Loss[0.56076] / V.Acc[0.82005] / F1[0.60620]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.60620]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:48&lt;00:00,  1.04it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.09it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.39887] / T.Acc[0.87816] / V.Loss[0.48795] / V.Acc[0.82786] / F1[0.61825]</span></span>
<span data-line><span> * New Best Model -> Epoch [8] / best_score : [0.61825]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:51&lt;00:00,  1.02it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.08it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.36532] / T.Acc[0.88832] / V.Loss[0.46459] / V.Acc[0.83542] / F1[0.63643]</span></span>
<span data-line><span> * New Best Model -> Epoch [9] / best_score : [0.63643]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:51&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.18it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.34100] / T.Acc[0.89326] / V.Loss[0.47769] / V.Acc[0.83307] / F1[0.66169]</span></span>
<span data-line><span> * New Best Model -> Epoch [10] / best_score : [0.66169]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:50&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.31711] / T.Acc[0.90144] / V.Loss[0.44010] / V.Acc[0.83880] / F1[0.65670]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:51&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.19it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.29949] / T.Acc[0.90447] / V.Loss[0.42319] / V.Acc[0.84688] / F1[0.68023]</span></span>
<span data-line><span> * New Best Model -> Epoch [12] / best_score : [0.68023]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:52&lt;00:00,  1.02it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.28249] / T.Acc[0.90843] / V.Loss[0.43525] / V.Acc[0.84089] / F1[0.66333]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.17it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.26582] / T.Acc[0.91535] / V.Loss[0.41765] / V.Acc[0.84583] / F1[0.68537]</span></span>
<span data-line><span> * New Best Model -> Epoch [14] / best_score : [0.68537]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:52&lt;00:00,  1.02it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.11it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.25331] / T.Acc[0.91924] / V.Loss[0.47107] / V.Acc[0.83203] / F1[0.69194]</span></span>
<span data-line><span> * New Best Model -> Epoch [15] / best_score : [0.69194]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:50&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.12it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.23658] / T.Acc[0.92346] / V.Loss[0.53544] / V.Acc[0.80990] / F1[0.70046]</span></span>
<span data-line><span> * New Best Model -> Epoch [16] / best_score : [0.70046]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:48&lt;00:00,  1.04it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.14it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.22975] / T.Acc[0.92517] / V.Loss[0.44945] / V.Acc[0.84036] / F1[0.73061]</span></span>
<span data-line><span> * New Best Model -> Epoch [17] / best_score : [0.73061]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.19it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.21803] / T.Acc[0.92985] / V.Loss[0.38858] / V.Acc[0.85495] / F1[0.74456]</span></span>
<span data-line><span> * New Best Model -> Epoch [18] / best_score : [0.74456]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:49&lt;00:00,  1.03it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.20869] / T.Acc[0.93440] / V.Loss[0.39791] / V.Acc[0.85651] / F1[0.73926]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:51&lt;00:00,  1.02it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.12it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.19833] / T.Acc[0.93625] / V.Loss[0.38119] / V.Acc[0.86016] / F1[0.74706]</span></span>
<span data-line><span> * New Best Model -> Epoch [20] / best_score : [0.74706]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD.tar</span></span></code></pre></figure>
</li>
</ul>
<p>고민해본 결과, 224 x 224로 이미지를 압축하는데, 이는 이미지의 feature를 담기에 충분하지 않은 것이 아닐까 추측됬다<br/>
원본 이미지의 경우, 사람의 얼굴만 담겨있는 것이 아니라 상체, 배경 등이 모두 찍혀있다<br/>
따라서 입력 이미지의 크기를 키워주거나 불필요한 배경 영역을 crop을 통해 줄여줄 수 있다면 정확도를 더 올릴 수 있을 것이라도 추측했다</p>
<p>입력 이미지 크기를 키우고, SGD를 적용, 더 향상된 결과를 얻었다</p>
<ul>
<li>
<p>출력<span class="text-highlight">backbone not freeze</span>, lr 0.001, batch 64, image size 384</p>
<p><strong>Best :</strong> V.Acc[0.90152] / F1[0.82868]</p>
<p><code>==제출 : Finished f1 : 0.7156 / acc: 78.3333==</code></p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py </span></span>
<span data-line><span>Namespace(batch_size=32, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=384, lr=0.001, save_name='Weight_VIT_V1_KHS_SGD.tar', save_path='../models/checkpoint/', seed=41, step_enable=True, step_gamma=0.5, step_size=10, target_model='VIT_V1_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:06&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:02&lt;00:00,  1.89it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.48437] / T.Acc[0.85267] / V.Loss[0.30058] / V.Acc[0.88944] / F1[0.77559]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.77559]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:01&lt;00:00,  1.27s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.16489] / T.Acc[0.94364] / V.Loss[0.31616] / V.Acc[0.88997] / F1[0.75934]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:02&lt;00:00,  1.27s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.84it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.08276] / T.Acc[0.97298] / V.Loss[0.32128] / V.Acc[0.88524] / F1[0.80838]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.80838]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:06&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.03709] / T.Acc[0.99009] / V.Loss[0.36563] / V.Acc[0.90021] / F1[0.81112]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.81112]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.01758] / T.Acc[0.99584] / V.Loss[0.37122] / V.Acc[0.89942] / F1[0.82090]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.82090]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.00972] / T.Acc[0.99782] / V.Loss[0.39228] / V.Acc[0.89233] / F1[0.80634]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:18&lt;00:00,  1.31s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:05&lt;00:00,  1.81it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.00583] / T.Acc[0.99848] / V.Loss[0.40954] / V.Acc[0.90310] / F1[0.82603]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.82603]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:22&lt;00:00,  1.32s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.84it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.00394] / T.Acc[0.99874] / V.Loss[0.41472] / V.Acc[0.90047] / F1[0.82537]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:14&lt;00:00,  1.30s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.89it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.00314] / T.Acc[0.99868] / V.Loss[0.42522] / V.Acc[0.89916] / F1[0.82081]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.00240] / T.Acc[0.99894] / V.Loss[0.42924] / V.Acc[0.90152] / F1[0.82868]</span></span>
<span data-line><span> * New Best Model -> Epoch [10] / best_score : [0.82868]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD.tar</span></span></code></pre></figure>
</li>
</ul>
<h3 id="class-별-image-analysis">Class 별 Image Analysis<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#class-별-image-analysis" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>작은 input_size를 유지하면서 최대한의 feature를 전달하기 위해서는 최대한 얼굴 영역만 넣어주는 것이 필요하다<br/>
버려도 되는 영역이 있는지 확인하기 위해 train image에 있는 모든 이미지를 불러와 평균 이미지를 만들었다</p>
<p>Class별 분포</p>
<p><img src="../../../../../resources/Untitled-79.png" width="auto" height="auto" alt="Untitled 79.png"/></p>
<p>Class별 평균 이미지</p>
<p><img src="../../../../../resources/Untitled-1-58.png" width="auto" height="auto" alt="Untitled 1 58.png"/></p>
<p>전체 평균 이미지</p>
<p><img src="../../../../../resources/Untitled-2-42.png" width="auto" height="auto" alt="Untitled 2 42.png"/></p>
<p>Crop Area 정의 및 시각화</p>
<p>눈으로 보았을 때, y축의 경우 0<del>50, 400</del>500 구간은 불필요해보였다<br/>
x축의 경우 0<del>50, 334</del>384 구간은 불필요해보였다</p>
<p><img src="../../../../../resources/Untitled-3-31.png" width="auto" height="auto" alt="Untitled 3 31.png"/></p>
<p><img src="../../../../../resources/Untitled-4-23.png" width="auto" height="auto" alt="Untitled 4 23.png"/></p>
<p>기존 이미지의 크기(HxW)는 512x384 = 196608이다<br/>
Crop 후 이미지의 크기(HxW)는 350x284=99400이다<br/>
input image는 보통 이보다 더 작기 때문에 down sizing이 필요하지만 feature의 정보를 잃게 한다<br/>
그러나 Crop한 이미지는 기존 이미지보다 약 50% 감소한 넓이를 가지고 있기 때문에<br/>
down sizing 시 더 적게 feature 정보 잃어버릴 것이라고 기대할 수 있다</p>
<h3 id="age-gender-maskstatus-한번에-예측하기-2">Age-Gender-MaskStatus 한번에 예측하기 #2<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#age-gender-maskstatus-한번에-예측하기-2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Crop 후 다시 Train 진행</p>
<ul>
<li>
<p>출력<span class="text-highlight">backbone not freeze</span>, lr 0.001, batch 64, image size 384</p>
<p><strong>Best :</strong> V.Acc[0.90100] / F1[0.82616]</p>
<p><code>==제출 : Finished f1 : 0.74706 / acc: 79.74706==</code></p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL_temp.py </span></span>
<span data-line><span>Namespace(batch_size=32, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=384, lr=0.001, save_name='Weight_VIT_V1_KHS_SGD_C.tar', save_path='../models/checkpoint/', seed=41, step_enable=True, step_gamma=0.5, step_size=5, target_model='VIT_V1_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:59&lt;00:00,  2.15s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:41&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.47756] / T.Acc[0.85690] / V.Loss[0.32220] / V.Acc[0.87684] / F1[0.78062]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.78062]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [17:03&lt;00:00,  2.16s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:50&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.16756] / T.Acc[0.94285] / V.Loss[0.31923] / V.Acc[0.88577] / F1[0.75986]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:58&lt;00:00,  2.15s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:49&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.08937] / T.Acc[0.97139] / V.Loss[0.30343] / V.Acc[0.89601] / F1[0.81500]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.81500]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [17:01&lt;00:00,  2.16s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:42&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.04302] / T.Acc[0.98837] / V.Loss[0.44436] / V.Acc[0.86843] / F1[0.79183]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [17:00&lt;00:00,  2.16s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:48&lt;00:00,  1.10it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.02137] / T.Acc[0.99491] / V.Loss[0.35330] / V.Acc[0.90100] / F1[0.80714]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:53&lt;00:00,  2.14s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:50&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.01106] / T.Acc[0.99769] / V.Loss[0.36034] / V.Acc[0.89653] / F1[0.81579]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.81579]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:58&lt;00:00,  2.15s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:43&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.00811] / T.Acc[0.99835] / V.Loss[0.36717] / V.Acc[0.90047] / F1[0.82211]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.82211]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [16:55&lt;00:00,  2.15s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:49&lt;00:00,  1.09it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.00657] / T.Acc[0.99855] / V.Loss[0.36998] / V.Acc[0.90126] / F1[0.82594]</span></span>
<span data-line><span> * New Best Model -> Epoch [8] / best_score : [0.82594]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [12:22&lt;00:00,  1.57s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.00542] / T.Acc[0.99855] / V.Loss[0.37530] / V.Acc[0.89995] / F1[0.82378]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.00463] / T.Acc[0.99868] / V.Loss[0.38403] / V.Acc[0.89811] / F1[0.81544]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:03&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.88it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.00397] / T.Acc[0.99881] / V.Loss[0.38562] / V.Acc[0.90100] / F1[0.82616]</span></span>
<span data-line><span> * New Best Model -> Epoch [11] / best_score : [0.82616]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.00373] / T.Acc[0.99881] / V.Loss[0.38903] / V.Acc[0.90021] / F1[0.82182]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:04&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:02&lt;00:00,  1.91it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.00355] / T.Acc[0.99881] / V.Loss[0.39441] / V.Acc[0.89968] / F1[0.82154]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.00329] / T.Acc[0.99881] / V.Loss[0.39345] / V.Acc[0.89968] / F1[0.81809]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.88it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.00311] / T.Acc[0.99881] / V.Loss[0.39927] / V.Acc[0.89942] / F1[0.82184]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:04&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.00299] / T.Acc[0.99881] / V.Loss[0.39715] / V.Acc[0.90074] / F1[0.82121]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.00286] / T.Acc[0.99881] / V.Loss[0.39832] / V.Acc[0.89995] / F1[0.82018]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:02&lt;00:00,  1.90it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.00281] / T.Acc[0.99881] / V.Loss[0.39851] / V.Acc[0.90021] / F1[0.82028]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:03&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.85it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.00270] / T.Acc[0.99888] / V.Loss[0.40031] / V.Acc[0.89995] / F1[0.82018]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:05&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.00267] / T.Acc[0.99888] / V.Loss[0.40088] / V.Acc[0.90021] / F1[0.82053]</span></span></code></pre></figure>
</li>
</ul>
<p>입력 크기가 384x384이므로 feature 손실이 거의 일어나지 않다고 생각해도 무방하다<br/>
하지만 그럼에도 f1이 잘 올라가지 않는다<br/>
이전 [Class 별 Image Analysis]에서 Class의 분포를 확인한 결과, 매우 불균형한 Dataset임을 알 수 있었다<br/>
따라서 성능을 더 올리기 위해서는 이를 해소할 방법을 찾아야 한다</p>
<p>Focal Loss</p>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Focal Loss for Dense Object Detection </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations.<br/>
<a href="https://arxiv.org/abs/1708.02002" class="external">https://arxiv.org/abs/1708.02002<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>Cross Entropy는 class unbalance를 고려하지 않는다<br/>
Focal Loss는 확률분포상에서 p 값이 낮을때에 비해 p 값이 높아질수록 loss값이 급격하게 낮게 설정된다는 것이다.</p>
<p>Focal Loss 미적용 (224x224)</p>
<ul>
<li>
<p>출력<span class="text-highlight">backbone not freeze</span>, lr 0.001, batch 64, image size 224</p>
<p><strong>Best :</strong> V.Acc[0.85547] / F1[0.75931]</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V0_KHS_SGD_C.tar', save_path='../models/checkpoint/', seed=41, step_enable=True, step_gamma=0.5, step_size=20, target_model='VIT_V0_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.14it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[1.88979] / T.Acc[0.43645] / V.Loss[1.43225] / V.Acc[0.60286] / F1[0.29889]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.29889]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:40&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.10it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[1.13299] / T.Acc[0.70075] / V.Loss[0.96170] / V.Acc[0.73880] / F1[0.46717]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.46717]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.06it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.11it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.79181] / T.Acc[0.79496] / V.Loss[0.73595] / V.Acc[0.79375] / F1[0.54226]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.54226]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:37&lt;00:00,  1.09it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.10it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.60914] / T.Acc[0.83518] / V.Loss[0.62587] / V.Acc[0.80990] / F1[0.56784]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.56784]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:43&lt;00:00,  1.06it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:33&lt;00:00,  1.81it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.49894] / T.Acc[0.86188] / V.Loss[0.54779] / V.Acc[0.82109] / F1[0.58347]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.58347]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:02&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.42728] / T.Acc[0.87790] / V.Loss[0.49307] / V.Acc[0.83646] / F1[0.61411]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.61411]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:47&lt;00:00,  1.47s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.37494] / T.Acc[0.89023] / V.Loss[0.52960] / V.Acc[0.82266] / F1[0.61055]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:23&lt;00:00,  1.62s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.13it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.33490] / T.Acc[0.90157] / V.Loss[0.45111] / V.Acc[0.83698] / F1[0.63438]</span></span>
<span data-line><span> * New Best Model -> Epoch [8] / best_score : [0.63438]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:48&lt;00:00,  1.47s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.30143] / T.Acc[0.90941] / V.Loss[0.43849] / V.Acc[0.83932] / F1[0.63993]</span></span>
<span data-line><span> * New Best Model -> Epoch [9] / best_score : [0.63993]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:55&lt;00:00,  1.50s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.11it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.27647] / T.Acc[0.91759] / V.Loss[0.45354] / V.Acc[0.83906] / F1[0.66146]</span></span>
<span data-line><span> * New Best Model -> Epoch [10] / best_score : [0.66146]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:02&lt;00:00,  1.53s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.13it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.24905] / T.Acc[0.92590] / V.Loss[0.41210] / V.Acc[0.84844] / F1[0.67002]</span></span>
<span data-line><span> * New Best Model -> Epoch [11] / best_score : [0.67002]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:59&lt;00:00,  1.52s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.22626] / T.Acc[0.93447] / V.Loss[0.41260] / V.Acc[0.85391] / F1[0.69689]</span></span>
<span data-line><span> * New Best Model -> Epoch [12] / best_score : [0.69689]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:47&lt;00:00,  1.47s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.12it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.20725] / T.Acc[0.94014] / V.Loss[0.40826] / V.Acc[0.84922] / F1[0.68709]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:59&lt;00:00,  1.52s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.18829] / T.Acc[0.94620] / V.Loss[0.43351] / V.Acc[0.84844] / F1[0.70059]</span></span>
<span data-line><span> * New Best Model -> Epoch [14] / best_score : [0.70059]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:01&lt;00:00,  1.53s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.14it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.17282] / T.Acc[0.95102] / V.Loss[0.40533] / V.Acc[0.85000] / F1[0.73521]</span></span>
<span data-line><span> * New Best Model -> Epoch [15] / best_score : [0.73521]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:58&lt;00:00,  1.51s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.15531] / T.Acc[0.95642] / V.Loss[0.45812] / V.Acc[0.82917] / F1[0.72246]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:51&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.14054] / T.Acc[0.96301] / V.Loss[0.45406] / V.Acc[0.83594] / F1[0.74068]</span></span>
<span data-line><span> * New Best Model -> Epoch [17] / best_score : [0.74068]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:07&lt;00:00,  1.55s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.12it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.12696] / T.Acc[0.96770] / V.Loss[0.41726] / V.Acc[0.85208] / F1[0.74794]</span></span>
<span data-line><span> * New Best Model -> Epoch [18] / best_score : [0.74794]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:57&lt;00:00,  1.51s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:54&lt;00:00,  1.10it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.11474] / T.Acc[0.97112] / V.Loss[0.40845] / V.Acc[0.86276] / F1[0.75076]</span></span>
<span data-line><span> * New Best Model -> Epoch [19] / best_score : [0.75076]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:02&lt;00:00,  1.53s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.10637] / T.Acc[0.97350] / V.Loss[0.41985] / V.Acc[0.85547] / F1[0.75931]</span></span>
<span data-line><span> * New Best Model -> Epoch [20] / best_score : [0.75931]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C.tar</span></span></code></pre></figure>
</li>
</ul>
<p>Focal Loss 적용 (224x224)</p>
<p>Focal Loss 적용 시 확실히 더 좋은 성능을 보여준다</p>
<ul>
<li>
<p>출력<span class="text-highlight">backbone not freeze</span>, lr 0.001, batch 64, image size 224</p>
<p><strong>Best :</strong> V.Acc[0.85599] / F1[0.76157]</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.001, save_name='Weight_VIT_V0_KHS_SGD_C_FL.tar', save_path='../models/checkpoint/', seed=41, step_enable=True, step_gamma=0.5, step_size=20, target_model='VIT_V0_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:57&lt;00:00,  1.51s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[1.34291] / T.Acc[0.47811] / V.Loss[0.88064] / V.Acc[0.64219] / F1[0.34061]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.34061]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:54&lt;00:00,  1.49s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.13it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.65392] / T.Acc[0.73075] / V.Loss[0.53905] / V.Acc[0.76068] / F1[0.50438]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.50438]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:07&lt;00:00,  1.55s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.14it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.41895] / T.Acc[0.81059] / V.Loss[0.39059] / V.Acc[0.79609] / F1[0.55500]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.55500]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.30483] / T.Acc[0.84309] / V.Loss[0.32417] / V.Acc[0.81068] / F1[0.58714]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.58714]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:00&lt;00:00,  1.52s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.24065] / T.Acc[0.86142] / V.Loss[0.27832] / V.Acc[0.82396] / F1[0.61083]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.61083]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:07&lt;00:00,  1.55s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.13it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.20037] / T.Acc[0.87375] / V.Loss[0.24717] / V.Acc[0.83516] / F1[0.63559]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.63559]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:52&lt;00:00,  1.49s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.17200] / T.Acc[0.88443] / V.Loss[0.24000] / V.Acc[0.83073] / F1[0.63941]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.63941]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:49&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.15111] / T.Acc[0.89333] / V.Loss[0.21895] / V.Acc[0.83464] / F1[0.64770]</span></span>
<span data-line><span> * New Best Model -> Epoch [8] / best_score : [0.64770]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:10&lt;00:00,  1.56s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.13467] / T.Acc[0.90170] / V.Loss[0.20378] / V.Acc[0.83516] / F1[0.66274]</span></span>
<span data-line><span> * New Best Model -> Epoch [9] / best_score : [0.66274]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.51s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.12255] / T.Acc[0.90691] / V.Loss[0.19901] / V.Acc[0.84219] / F1[0.70703]</span></span>
<span data-line><span> * New Best Model -> Epoch [10] / best_score : [0.70703]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:00&lt;00:00,  1.52s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.11057] / T.Acc[0.90955] / V.Loss[0.19342] / V.Acc[0.84453] / F1[0.70841]</span></span>
<span data-line><span> * New Best Model -> Epoch [11] / best_score : [0.70841]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:52&lt;00:00,  1.49s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.10140] / T.Acc[0.91759] / V.Loss[0.18816] / V.Acc[0.84479] / F1[0.71606]</span></span>
<span data-line><span> * New Best Model -> Epoch [12] / best_score : [0.71606]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:09&lt;00:00,  1.56s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.14it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.09307] / T.Acc[0.92392] / V.Loss[0.18592] / V.Acc[0.84349] / F1[0.72371]</span></span>
<span data-line><span> * New Best Model -> Epoch [13] / best_score : [0.72371]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:10&lt;00:00,  1.56s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.08579] / T.Acc[0.92919] / V.Loss[0.18219] / V.Acc[0.84583] / F1[0.72470]</span></span>
<span data-line><span> * New Best Model -> Epoch [14] / best_score : [0.72470]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:05&lt;00:00,  1.29s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.12it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.08016] / T.Acc[0.93091] / V.Loss[0.17729] / V.Acc[0.85000] / F1[0.73899]</span></span>
<span data-line><span> * New Best Model -> Epoch [15] / best_score : [0.73899]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29&lt;00:00,  2.02it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.07410] / T.Acc[0.93506] / V.Loss[0.18023] / V.Acc[0.84714] / F1[0.74333]</span></span>
<span data-line><span> * New Best Model -> Epoch [16] / best_score : [0.74333]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30&lt;00:00,  1.97it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.06946] / T.Acc[0.93987] / V.Loss[0.17984] / V.Acc[0.84531] / F1[0.74026]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28&lt;00:00,  2.13it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.06459] / T.Acc[0.94225] / V.Loss[0.17489] / V.Acc[0.85234] / F1[0.75522]</span></span>
<span data-line><span> * New Best Model -> Epoch [18] / best_score : [0.75522]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:39&lt;00:00,  1.08it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29&lt;00:00,  2.05it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.06011] / T.Acc[0.94746] / V.Loss[0.17372] / V.Acc[0.85495] / F1[0.75458]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [03:42&lt;00:00,  1.07it/s]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:27&lt;00:00,  2.18it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.05585] / T.Acc[0.94996] / V.Loss[0.17397] / V.Acc[0.85599] / F1[0.76157]</span></span>
<span data-line><span> * New Best Model -> Epoch [20] / best_score : [0.76157]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGD_C_FL.tar</span></span></code></pre></figure>
</li>
</ul>
<p>Focal Loss + Step10, gamma0.5</p>
<ul>
<li>
<p>출력<span class="text-highlight">backbone not freeze</span>, lr 0.004, batch 64, image size 224</p>
<p><strong>Best :</strong> V.Acc[0.86224] / F1[0.78143]</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --lr=4e-3 --step_size=10 --epochs=30 --save_name=Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=20, img_size=224, lr=0.004, save_name='Weight_VIT_V0_KHS_SGC_C_S10', save_path='../models/checkpoint/', seed=41, step_enable=True, step_gamma=0.5, step_size=10, target_model='VIT_V0_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.50s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:53&lt;00:00,  1.12it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.72190] / T.Acc[0.68836] / V.Loss[0.34943] / V.Acc[0.79557] / F1[0.58657]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.58657]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:06&lt;00:00,  1.55s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.21428] / T.Acc[0.86603] / V.Loss[0.22265] / V.Acc[0.83802] / F1[0.63386]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.63386]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.50s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.13595] / T.Acc[0.89735] / V.Loss[0.18575] / V.Acc[0.85052] / F1[0.71738]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.71738]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.10158] / T.Acc[0.91232] / V.Loss[0.17468] / V.Acc[0.84792] / F1[0.73956]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.73956]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:03&lt;00:00,  1.53s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.07939] / T.Acc[0.92998] / V.Loss[0.18733] / V.Acc[0.83464] / F1[0.73658]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:59&lt;00:00,  1.51s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.06369] / T.Acc[0.94179] / V.Loss[0.18287] / V.Acc[0.83516] / F1[0.76561]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.76561]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:12&lt;00:00,  1.57s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.05203] / T.Acc[0.95174] / V.Loss[0.17255] / V.Acc[0.85859] / F1[0.77612]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.77612]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:59&lt;00:00,  1.52s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.04293] / T.Acc[0.96150] / V.Loss[0.18020] / V.Acc[0.85078] / F1[0.77197]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:06&lt;00:00,  1.55s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.03466] / T.Acc[0.96783] / V.Loss[0.18515] / V.Acc[0.84427] / F1[0.75693]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:03&lt;00:00,  1.54s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.02871] / T.Acc[0.97350] / V.Loss[0.17529] / V.Acc[0.85417] / F1[0.76863]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:05&lt;00:00,  1.54s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.02154] / T.Acc[0.98233] / V.Loss[0.17757] / V.Acc[0.85990] / F1[0.77990]</span></span>
<span data-line><span> * New Best Model -> Epoch [11] / best_score : [0.77990]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:54&lt;00:00,  1.49s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.01896] / T.Acc[0.98536] / V.Loss[0.18180] / V.Acc[0.86224] / F1[0.77990]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:51&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.01707] / T.Acc[0.98741] / V.Loss[0.18875] / V.Acc[0.86224] / F1[0.76702]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:19&lt;00:00,  1.60s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:48&lt;00:00,  1.24it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.01554] / T.Acc[0.98813] / V.Loss[0.18762] / V.Acc[0.86302] / F1[0.77517]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:01&lt;00:00,  1.52s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.01435] / T.Acc[0.98853] / V.Loss[0.19114] / V.Acc[0.86016] / F1[0.77536]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:55&lt;00:00,  1.50s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.01275] / T.Acc[0.99057] / V.Loss[0.19873] / V.Acc[0.86224] / F1[0.78143]</span></span>
<span data-line><span> * New Best Model -> Epoch [16] / best_score : [0.78143]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S10</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.01172] / T.Acc[0.99156] / V.Loss[0.20302] / V.Acc[0.85964] / F1[0.77807]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:09&lt;00:00,  1.56s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.01069] / T.Acc[0.99156] / V.Loss[0.20453] / V.Acc[0.85677] / F1[0.77985]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:53&lt;00:00,  1.49s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.00964] / T.Acc[0.99262] / V.Loss[0.20540] / V.Acc[0.86198] / F1[0.77771]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:49&lt;00:00,  1.47s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.20it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.00880] / T.Acc[0.99308] / V.Loss[0.20924] / V.Acc[0.85469] / F1[0.77885]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:01&lt;00:00,  1.53s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span></code></pre></figure>
</li>
</ul>
<p>Focal Loss + Step5, gamma0.5</p>
<ul>
<li>
<p>출력<span class="text-highlight">backbone not freeze</span>, lr 0.004, batch 64, image size 224</p>
<p><strong>Best :</strong> V.Acc[0.86068] / F1[0.78284]</p>
<p><code>==제출 : Finished f1 : 0.6654 / acc: 73.1587==</code></p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --lr=4e-3 --step_size=</span></span>
<span data-line><span>5 --epochs=30 --save_name=Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>Namespace(batch_size=64, csv_path='../data/train_i.csv', device='cuda', epochs=30, img_size=224, lr=0.004, save_name='Weight_VIT_V0_KHS_SGC_C_S5', save_path='../models/checkpoint/', seed=41, step_enable=True, step_gamma=0.5, step_size=5, target_model='VIT_V0_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:42&lt;00:00,  1.45s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.72190] / T.Acc[0.68836] / V.Loss[0.34943] / V.Acc[0.79557] / F1[0.58657]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.58657]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:08&lt;00:00,  1.55s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.21428] / T.Acc[0.86603] / V.Loss[0.22265] / V.Acc[0.83802] / F1[0.63386]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.63386]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:55&lt;00:00,  1.50s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.13595] / T.Acc[0.89735] / V.Loss[0.18575] / V.Acc[0.85052] / F1[0.71738]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.71738]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:51&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.10158] / T.Acc[0.91232] / V.Loss[0.17468] / V.Acc[0.84792] / F1[0.73956]</span></span>
<span data-line><span> * New Best Model -> Epoch [4] / best_score : [0.73956]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:04&lt;00:00,  1.54s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.07939] / T.Acc[0.92998] / V.Loss[0.18733] / V.Acc[0.83464] / F1[0.73658]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.51s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.06154] / T.Acc[0.94436] / V.Loss[0.16302] / V.Acc[0.85547] / F1[0.76878]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.76878]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:11&lt;00:00,  1.57s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.05433] / T.Acc[0.95121] / V.Loss[0.16818] / V.Acc[0.85677] / F1[0.76885]</span></span>
<span data-line><span> * New Best Model -> Epoch [7] / best_score : [0.76885]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:58&lt;00:00,  1.51s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.19it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.04918] / T.Acc[0.95583] / V.Loss[0.16799] / V.Acc[0.85417] / F1[0.76732]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:10&lt;00:00,  1.56s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.04386] / T.Acc[0.95879] / V.Loss[0.17267] / V.Acc[0.85130] / F1[0.76561]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:01&lt;00:00,  1.53s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.14it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.03987] / T.Acc[0.96288] / V.Loss[0.16654] / V.Acc[0.85599] / F1[0.77333]</span></span>
<span data-line><span> * New Best Model -> Epoch [10] / best_score : [0.77333]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:02&lt;00:00,  1.53s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:49&lt;00:00,  1.21it/s]</span></span>
<span data-line><span>Ep[11] / T.Loss[0.03433] / T.Acc[0.96948] / V.Loss[0.16849] / V.Acc[0.85911] / F1[0.77092]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:51&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[12] / T.Loss[0.03225] / T.Acc[0.97178] / V.Loss[0.17023] / V.Acc[0.86068] / F1[0.77492]</span></span>
<span data-line><span> * New Best Model -> Epoch [12] / best_score : [0.77492]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:52&lt;00:00,  1.15it/s]</span></span>
<span data-line><span>Ep[13] / T.Loss[0.03036] / T.Acc[0.97468] / V.Loss[0.17377] / V.Acc[0.85885] / F1[0.76709]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:20&lt;00:00,  1.60s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[14] / T.Loss[0.02878] / T.Acc[0.97521] / V.Loss[0.17221] / V.Acc[0.86120] / F1[0.77510]</span></span>
<span data-line><span> * New Best Model -> Epoch [14] / best_score : [0.77510]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:00&lt;00:00,  1.52s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.16it/s]</span></span>
<span data-line><span>Ep[15] / T.Loss[0.02755] / T.Acc[0.97607] / V.Loss[0.17549] / V.Acc[0.85807] / F1[0.76886]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:56&lt;00:00,  1.50s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[16] / T.Loss[0.02538] / T.Acc[0.98009] / V.Loss[0.17614] / V.Acc[0.85990] / F1[0.77545]</span></span>
<span data-line><span> * New Best Model -> Epoch [16] / best_score : [0.77545]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:48&lt;00:00,  1.47s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:51&lt;00:00,  1.17it/s]</span></span>
<span data-line><span>Ep[17] / T.Loss[0.02478] / T.Acc[0.97976] / V.Loss[0.17756] / V.Acc[0.85964] / F1[0.77784]</span></span>
<span data-line><span> * New Best Model -> Epoch [17] / best_score : [0.77784]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [06:06&lt;00:00,  1.55s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[18] / T.Loss[0.02405] / T.Acc[0.98128] / V.Loss[0.17941] / V.Acc[0.85833] / F1[0.77824]</span></span>
<span data-line><span> * New Best Model -> Epoch [18] / best_score : [0.77824]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:52&lt;00:00,  1.49s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[19] / T.Loss[0.02328] / T.Acc[0.98207] / V.Loss[0.17868] / V.Acc[0.86146] / F1[0.77820]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [05:50&lt;00:00,  1.48s/it]</span></span>
<span data-line><span>100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:50&lt;00:00,  1.18it/s]</span></span>
<span data-line><span>Ep[20] / T.Loss[0.02275] / T.Acc[0.98253] / V.Loss[0.18019] / V.Acc[0.86068] / F1[0.78284]</span></span>
<span data-line><span> * New Best Model -> Epoch [20] / best_score : [0.78284]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V0_KHS_SGC_C_S5</span></span></code></pre></figure>
</li>
</ul>
<p>Crop을 했지만 여전히 Resize 이후, image feature가 부족한거같다<br/>
Size를 키워서 다시 진행해보았다<br/>
결과는 좋아졌지만 기존 최고 기록을 넘지는 못했다<br/>
Overfitting이 예상된다</p>
<ul>
<li>
<p>출력<span class="text-highlight">backbone not freeze</span>, lr 0.004, batch 64, image size 384</p>
<p><strong>Best :</strong> V.Acc[0.90257] / F1[0.82163]</p>
<p><code>==제출 : Finished f1 : 0.7218 / acc: 78.4286==</code></p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>python trainALL.py --lr=4e-3 --step_size=5 --epochs=10 --save_name=Weight_VIT_V1_KHS_SGC_C_S5 --target_model=&quot;VIT_V1_KHS(False)&quot; --img_size=384 --batch_size=32</span></span>
<span data-line><span>Namespace(batch_size=32, csv_path='../data/train_i.csv', device='cuda', epochs=10, img_size=384, lr=0.004, save_name='Weight_VIT_V1_KHS_SGC_C_S5', save_path='../models/checkpoint/', seed=41, step_enable=True, step_gamma=0.5, step_size=5, target_model='VIT_V1_KHS(False)', validation_ratio=0.2)</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.85it/s]</span></span>
<span data-line><span>Ep[1] / T.Loss[0.25867] / T.Acc[0.85181] / V.Loss[0.11856] / V.Acc[0.87710] / F1[0.77540]</span></span>
<span data-line><span> * New Best Model -> Epoch [1] / best_score : [0.77540]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:09&lt;00:00,  1.29s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.88it/s]</span></span>
<span data-line><span>Ep[2] / T.Loss[0.04351] / T.Acc[0.95587] / V.Loss[0.13384] / V.Acc[0.89837] / F1[0.79012]</span></span>
<span data-line><span> * New Best Model -> Epoch [2] / best_score : [0.79012]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:09&lt;00:00,  1.29s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.84it/s]</span></span>
<span data-line><span>Ep[3] / T.Loss[0.01495] / T.Acc[0.98447] / V.Loss[0.15800] / V.Acc[0.89207] / F1[0.80832]</span></span>
<span data-line><span> * New Best Model -> Epoch [3] / best_score : [0.80832]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:11&lt;00:00,  1.29s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.83it/s]</span></span>
<span data-line><span>Ep[4] / T.Loss[0.00487] / T.Acc[0.99538] / V.Loss[0.15480] / V.Acc[0.89811] / F1[0.80708]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:10&lt;00:00,  1.29s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:05&lt;00:00,  1.81it/s]</span></span>
<span data-line><span>Ep[5] / T.Loss[0.00190] / T.Acc[0.99802] / V.Loss[0.16241] / V.Acc[0.90336] / F1[0.81799]</span></span>
<span data-line><span> * New Best Model -> Epoch [5] / best_score : [0.81799]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5</span></span>
<span data-line><span> 23%|███████████████████████                                                     23%|███████████████████████▎                                                    23%|███████████████████████▌                                                    23%|███████████████████████▋                                                    23%|███████████████████████▉                                                    24%|████████████████████████▏                                                   24%|████████████████████████▎                                                   24%|████████████████████████▌                                                   24%|████████████████████████▊                                                   25%|█████████████████████████                                                   25%|█████████████████████████▏                                                  25%|█████████████████████████▍                                                  25%|█████████████████████████▋                                                  25%|█████████████████████████▉                                                  26%|██████████████████████████                                                  26%|██████████████████████████▎                                                 26%|██████████████████████████▌                                                 26%|██████████████████████████▋                                                 26%|██████████████████████████▉                                                                           | 125/473 [02:39&lt;07:21,  1.27s/it]source /opt/ml/.local/share/virtualenvs/lv1_imageclassification_cv02-bp8_CroY/bin/activate</span></span>
<span data-line><span> 27%|██████████████████████████▋                                                 27%|██████████████████████████▊                                                100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [11:59&lt;00:00,  1.52s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:06&lt;00:00,  1.80it/s]</span></span>
<span data-line><span>Ep[6] / T.Loss[0.00090] / T.Acc[0.99874] / V.Loss[0.16468] / V.Acc[0.90389] / F1[0.82088]</span></span>
<span data-line><span> * New Best Model -> Epoch [6] / best_score : [0.82088]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:13&lt;00:00,  1.30s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.86it/s]</span></span>
<span data-line><span>Ep[7] / T.Loss[0.00070] / T.Acc[0.99894] / V.Loss[0.16688] / V.Acc[0.90179] / F1[0.81186]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:11&lt;00:00,  1.29s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:06&lt;00:00,  1.80it/s]</span></span>
<span data-line><span>Ep[8] / T.Loss[0.00063] / T.Acc[0.99881] / V.Loss[0.17004] / V.Acc[0.90152] / F1[0.81653]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:10&lt;00:00,  1.29s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:03&lt;00:00,  1.87it/s]</span></span>
<span data-line><span>Ep[9] / T.Loss[0.00050] / T.Acc[0.99894] / V.Loss[0.17162] / V.Acc[0.90205] / F1[0.81566]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [10:07&lt;00:00,  1.28s/it]</span></span>
<span data-line><span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:04&lt;00:00,  1.85it/s]</span></span>
<span data-line><span>Ep[10] / T.Loss[0.00042] / T.Acc[0.99894] / V.Loss[0.17372] / V.Acc[0.90257] / F1[0.82163]</span></span>
<span data-line><span> * New Best Model -> Epoch [10] / best_score : [0.82163]</span></span>
<span data-line><span> -> The model has been saved at ../models/checkpoint/Weight_VIT_V1_KHS_SGC_C_S5</span></span></code></pre></figure>
</li>
</ul>
<p>아쉽지만, 대회 기간이 끝나서 추가 작업을 해볼 수는 없었다.</p>
<h3 id="회고">회고<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#회고" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><strong>이번 프로젝트에서 나의 목표는 무엇이었는가?</strong></p>
<p>리더보드 상위권, 직접 코드짜기, 팀원들과 협업, 더 효율적인 훈련</p>
<p><strong>나는 내 학습목표를 달성하기 위해 무엇을 어떻게 했는가?</strong></p>
<p>GitHub Repos 구성 및 팀원들과 공유<br/>
지속적으로 Utility Function Update<br/>
EDA 결과 지속적 공유<br/>
훈련 결과 지속적 공유<br/>
Gender 단일 분류 모델 학습 및 결과 공유<br/>
Age/Gender/Mask 전체 분류 모델 학습 및 결과 공유<br/>
Baseline code 참조, Train 및 Inferrence용 python script 제작 및 공유</p>
<p><strong>나는 어떤 방식으로 모델을 개선했는가?</strong></p>
<p>전체 이미지에 대한 RGB 평균 및 표준편차 시각화<br/>
전체 이미지에 대한 class별 분포 시각화<br/>
전체 이미지에 대한 class별 평균 이미지 시각화 및 전체 이미지의 평균 이미지 시각화<br/>
데이터 불균형을 해소하기 위해 FocalLoss추가<br/>
SGD와 Adam optimizer의 성능 비교<br/>
ViT 사용, 여러 Case(input size, model size, argmentation, ..etc)들에 대하여 훈련 및 결과 비교</p>
<p><strong>내가 한 행동의 결과로 어떤 지점을 달성하고, 어떠한 깨달음을 얻었는가?</strong></p>
<p>Gender, Age, Mask를 각각 분류하는 모델을 만들고 결과를 합치는 방식이 좋은 결과를 도출할 것이라고 생각했다. 개별적으로 훈련을 진행한 뒤, 잘 예측하는 항목은 고정하고 잘 예측하지 못하는 항목에 집중하면 더 좋을 것이라고 생각했기 때문이다. 하지만 결과는 좋지 않았다. 한번에 예측하는 모델이라면 제출해서 정량적으로 판단할 수 있다. 그러나 개별 모델로 나누고 이를 합쳐서 제출하면 어떤 모델을 수정해야하는지 알 수 없게 되어버렸다.<br/>
어떤 계획을 세웠을 때, 실제로 실현이 가능한지 면밀이 검토가 필요하다고 느꼈다.<br/>
또한 개별 분류 방식에서 한번에 분류하는 방식으로 넘어갔을 때, 기록의 중요성을 느꼈다. 개별 분류 방식에서 했던 많은 시행착오를 기록해놓지 않았기 때문에, 0부터 다시 시작하는 기분이었다. 그래서 한번에 분류하는 방식으로 훈련할 때는 지속적으로 기록했다.</p>
<p><strong>전과 비교해서, 내가 새롭게 시도한 변화는 무엇이고, 어떤 효과가 있었는가?</strong></p>
<p>성별 분류 모델은 기본적으로 성능이 잘 나와서 크게 새로운 것은 없었다. 한번에 분류하는 모델의 경우, 처음으로 ViT를 사용해서 분류해보았다. ViT는 Res-net, Efficient-net등과는 구조가 조금 다르다. 그래서 처음에는 어떻게 끝단을 수정해야 할지 알 수 없어서 직접 모델 내부를 분석해 보았다. 결과, timm 라이브러리를 사용하지 않고 torchvision에 있는 ViT의 Head를 직접 수정해서 사용할 수 있었다.<br/>
ViT를 사용해 HyperParameter와 ViT Model Type 및 Argmentation을 변경해가며 실험했고, 이전과 같은 실수를 반복하지 않기 위하여 모두 기록으로 남겼다. 결과, 실험적으로 성능이 올라가는 조건들을 하나씩 찾을 수 있었고, 단일 제출로는 가장 높은 점수를 얻을 수 있었다.</p>
<p><strong>마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?</strong></p>
<p>ViT만 사용해서 실험을 진행한 점이 아쉬웠다. 앞서 성별 분류를 하면서 여러 모델을 사용해보았고, 시간을 많이 소모했다. 따라서 다른 모델을 사용해볼 여유가 없다고 판단했기 때문이다.<br/>
모델의 훈련과정을 WandB등으로 시각화하고 기록하지 않은것이 아쉽다.<br/>
데이터를 직접 확인하지 않은 것이 아쉽다. 데이터 항상 올바르게 라벨링 되어있을 것이라고 생각해서는 안된다.<br/>
Confusion Matrix로 결과를 시각화해보지 못한 것이 아쉽다.</p>
<p><strong>한계/교훈을 바탕으로 다음 프로젝트에서 스스로 새롭게 시도해볼 것은 무엇일까?</strong></p>
<p>가장 기본이 되는 모델로 훈련하여 기본이 될 정보들을 만들어놓겠다. 그리고 그 정보들을 기반으로 여러가지 모델을 실험해보고 성능을 평가하겠다.<br/>
WandB나 Tensorboard를 사용해 파라미터를 기록하고 훈련과정을 시각화해보고 싶다. 또한 여러 파라미터에 대해 자동으로 입력 및 훈련하고, 이를 기록하는 시퀀스를 만들어보겠다.<br/>
데이터 분석을 더 면밀히 하겠다.</p>
<hr/>
<h2 id="torchvision에서-pre-trained-model을-불러왔을-때-초기-required_grad-값은">TorchVision에서 Pre-Trained model을 불러왔을 때, 초기 required_grad 값은?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#torchvision에서-pre-trained-model을-불러왔을-때-초기-required_grad-값은" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Finetuning Torchvision Models - PyTorch Tutorials 1.2.0 documentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Author: Nathan Inkawhich In this tutorial we will take a deeper look at how to finetune and feature extract the torchvision models, all of which have been pretrained on the 1000-class Imagenet dataset.<br/>
<a href="https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#set-model-parameters-requires-grad-attribute" class="external">https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#set-model-parameters-requires-grad-attribute<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>TorchVision에서 미리 학습된 weight를 불러왔을 때, 모델 각 레이어의 required_grad 값은?</p>
<p>공식 홈페이지 검색 → 기본적으로 True로 설정되어 있음</p>
<p>따라서 기존의 feature를 유지하고 싶다면, required_grad = false를 사용해야 한다</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>def set_parameter_requires_grad(model, feature_extracting):</span></span>
<span data-line><span>    if feature_extracting:</span></span>
<span data-line><span>        for param in model.parameters():</span></span>
<span data-line><span>            param.requires_grad = False</span></span></code></pre></figure>
<hr/>
<h3 id="추가-메모">추가 메모<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#추가-메모" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>저장 model이 여러 개가 되니 뭐가 뭔지 알 수가 없다. 좋은 방법이 없을까?<br/>
그래프로 계속 경향을 저장해놓고싶다 어떻게 해야할까?<br/>
code는 cli에서 사용해도 전혀 불편함이 없을 수준으로 자동화해놔야 편리하다<br/>
시간은 짧고 할 일은 많다. 빠르게 시도하고 결과를 확인할 수 있는게 좋다<br/>
성실하게 훈련을 자동화 할 수 있는 코드를 만들어두면, 실패해도 쉽게 복구할 수 있다</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div class="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false,&quot;enableRadial&quot;:false}"></div><button class="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div class="global-graph-outer"><div class="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.2,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true,&quot;enableRadial&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul class="toc-content overflow" id="list-1"><li class="depth-0"><a href="#ai-stages---마스크-착용-상태-분류-대회" data-for="ai-stages---마스크-착용-상태-분류-대회">AI Stages - 마스크 착용 상태 분류 대회</a></li><li class="depth-1"><a href="#age-gender-maskstatus-한번에-예측하기-1" data-for="age-gender-maskstatus-한번에-예측하기-1">Age-Gender-MaskStatus 한번에 예측하기 #1</a></li><li class="depth-1"><a href="#class-별-image-analysis" data-for="class-별-image-analysis">Class 별 Image Analysis</a></li><li class="depth-1"><a href="#age-gender-maskstatus-한번에-예측하기-2" data-for="age-gender-maskstatus-한번에-예측하기-2">Age-Gender-MaskStatus 한번에 예측하기 #2</a></li><li class="depth-1"><a href="#회고" data-for="회고">회고</a></li><li class="depth-0"><a href="#torchvision에서-pre-trained-model을-불러왔을-때-초기-required_grad-값은" data-for="torchvision에서-pre-trained-model을-불러왔을-때-초기-required_grad-값은">TorchVision에서 Pre-Trained model을 불러왔을 때, 초기 required_grad 값은?</a></li><li class="depth-1"><a href="#추가-메모" data-for="추가-메모">추가 메모</a></li><li class="overflow-end"></li></ul></div><div class="backlinks"><h3>Backlinks</h3><ul id="list-2" class="overflow"><li><a href="../../../../../Notion/DB/DB-Blog-Post/Naver-Connect---Boostcamp-AI-Tech-4기/Naver-Connect---Boostcamp-AI-Tech-4기" class="internal">Naver Connect - Boostcamp AI Tech 4기</a></li><li class="overflow-end"></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.5.1</a> © 2025</p><ul><li><a href="https://github.com/404Vector/404Vector">GitHub</a></li></ul></footer></div></div></body><script type="application/javascript">function n(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.getElementsByClassName("callout-content")[0];if(!e)return;let l=t.classList.contains("is-collapsed");e.style.gridTemplateRows=l?"0fr":"1fr"}function c(){let t=document.getElementsByClassName("callout is-collapsible");for(let e of t){let l=e.getElementsByClassName("callout-title")[0],s=e.getElementsByClassName("callout-content")[0];if(!l||!s)continue;l.addEventListener("click",n),window.addCleanup(()=>l.removeEventListener("click",n));let o=e.classList.contains("is-collapsed");s.style.gridTemplateRows=o?"0fr":"1fr"}}document.addEventListener("nav",c);
</script><script type="module">function f(i,e){if(!i)return;function r(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function t(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}i?.addEventListener("click",r),window.addCleanup(()=>i?.removeEventListener("click",r)),document.addEventListener("keydown",t),window.addCleanup(()=>document.removeEventListener("keydown",t))}function y(i){for(;i.firstChild;)i.removeChild(i.firstChild)}var h=class{constructor(e,r){this.container=e;this.content=r;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),r=this.onMouseMove.bind(this),t=this.onMouseUp.bind(this),o=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",r),document.addEventListener("mouseup",t),window.addEventListener("resize",o),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",r),()=>document.removeEventListener("mouseup",t),()=>window.removeEventListener("resize",o))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let r=this.createButton("+",()=>this.zoom(.1)),t=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(t),e.appendChild(o),e.appendChild(r),this.container.appendChild(e)}createButton(e,r){let t=document.createElement("button");return t.textContent=e,t.className="mermaid-control-button",t.addEventListener("click",r),window.addCleanup(()=>t.removeEventListener("click",r)),t}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}zoom(e){let r=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),t=this.content.getBoundingClientRect(),o=t.width/2,n=t.height/2,c=r-this.scale;this.currentPan.x-=o*c,this.currentPan.y-=n*c,this.scale=r,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){this.scale=1;let e=this.content.querySelector("svg");this.currentPan={x:e.getBoundingClientRect().width/2,y:e.getBoundingClientRect().height/2},this.updateTransform()}},C=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],E;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;E||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let r=E.default,t=new WeakMap;for(let n of e)t.set(n,n.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let a=t.get(s);a&&(s.innerHTML=a)}let n=C.reduce((s,a)=>(s[a]=window.getComputedStyle(document.documentElement).getPropertyValue(a),s),{}),c=document.documentElement.getAttribute("saved-theme")==="dark";r.initialize({startOnLoad:!1,securityLevel:"loose",theme:c?"dark":"base",themeVariables:{fontFamily:n["--codeFont"],primaryColor:n["--light"],primaryTextColor:n["--darkgray"],primaryBorderColor:n["--tertiary"],lineColor:n["--darkgray"],secondaryColor:n["--secondary"],tertiaryColor:n["--tertiary"],clusterBkg:n["--light"],edgeLabelBackground:n["--highlight"]}}),await r.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let n=0;n<e.length;n++){let v=function(){let g=l.querySelector("#mermaid-space"),m=l.querySelector(".mermaid-content");if(!m)return;y(m);let w=c.querySelector("svg").cloneNode(!0);m.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new h(g,m)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},c=e[n],s=c.parentElement,a=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(a),L=a.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),f(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../../../../../postscript.js" type="module"></script></html>