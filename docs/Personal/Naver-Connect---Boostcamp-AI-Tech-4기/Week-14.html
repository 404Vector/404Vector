<!DOCTYPE html>
<html lang="en"><head><title>Week 14</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;family=IBM Plex Mono:wght@400;600&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="HyeongSeok Kim's Vault"/><meta property="og:title" content="Week 14"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Week 14"/><meta name="twitter:description" content="Segmentation Overview Segmentation History COCO(Common Objects in COntext) Semantic Segmentation 기초와 이해 Transposed Convolution FCN(Fully Convolutional networks for semantic segmentation) FCN의 성능을 향상시키기 위한 방법 FCN의 한계와 극복한 모델들 1 FCN의 한계점 Decoder를 개선한 Model들 DeconvNet SegNet DeconvNet vs SegNet Skip C..."/><meta property="og:description" content="Segmentation Overview Segmentation History COCO(Common Objects in COntext) Semantic Segmentation 기초와 이해 Transposed Convolution FCN(Fully Convolutional networks for semantic segmentation) FCN의 성능을 향상시키기 위한 방법 FCN의 한계와 극복한 모델들 1 FCN의 한계점 Decoder를 개선한 Model들 DeconvNet SegNet DeconvNet vs SegNet Skip C..."/><meta property="og:image:alt" content="Segmentation Overview Segmentation History COCO(Common Objects in COntext) Semantic Segmentation 기초와 이해 Transposed Convolution FCN(Fully Convolutional networks for semantic segmentation) FCN의 성능을 향상시키기 위한 방법 FCN의 한계와 극복한 모델들 1 FCN의 한계점 Decoder를 개선한 Model들 DeconvNet SegNet DeconvNet vs SegNet Skip C..."/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https://quartz.jzhao.xyz/Personal/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14"/><meta property="twitter:url" content="https://quartz.jzhao.xyz/Personal/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14"/><link rel="icon" href="../../static/icon.png"/><meta name="description" content="Segmentation Overview Segmentation History COCO(Common Objects in COntext) Semantic Segmentation 기초와 이해 Transposed Convolution FCN(Fully Convolutional networks for semantic segmentation) FCN의 성능을 향상시키기 위한 방법 FCN의 한계와 극복한 모델들 1 FCN의 한계점 Decoder를 개선한 Model들 DeconvNet SegNet DeconvNet vs SegNet Skip C..."/><meta name="generator" content="Quartz"/><link href="../../index.css" rel="stylesheet" type="text/css" spa-preserve/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  padding: 2rem;
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,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 */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://quartz.jzhao.xyz/index.xml"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image" content="https://quartz.jzhao.xyz/Personal/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14-og-image.webp"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/Personal/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14-og-image.webp"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/Personal/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14-og-image.webp"/><meta property="og:image:type" content="image/.webp"/></head><body data-slug="Personal/Naver-Connect---Boostcamp-AI-Tech-4기/Week-14"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../..">HyeongSeok Kim's Vault</a></h2><div class="spacer mobile-only"></div><div class="flex-component" style="flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search"><button class="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div class="search-layout" data-preview="true"></div></div></div></div></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="readermode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="readerIcon" fill="currentColor" stroke="currentColor" stroke-width="0.2" stroke-linecap="round" stroke-linejoin="round" width="64px" height="64px" viewBox="0 0 24 24" aria-label="Reader mode"><title>Reader mode</title><g transform="translate(-1.8, -1.8) scale(1.15, 1.2)"><path d="M8.9891247,2.5 C10.1384702,2.5 11.2209868,2.96705384 12.0049645,3.76669482 C12.7883914,2.96705384 13.8709081,2.5 15.0202536,2.5 L18.7549359,2.5 C19.1691495,2.5 19.5049359,2.83578644 19.5049359,3.25 L19.5046891,4.004 L21.2546891,4.00457396 C21.6343849,4.00457396 21.9481801,4.28672784 21.9978425,4.6528034 L22.0046891,4.75457396 L22.0046891,20.25 C22.0046891,20.6296958 21.7225353,20.943491 21.3564597,20.9931534 L21.2546891,21 L2.75468914,21 C2.37499337,21 2.06119817,20.7178461 2.01153575,20.3517706 L2.00468914,20.25 L2.00468914,4.75457396 C2.00468914,4.37487819 2.28684302,4.061083 2.65291858,4.01142057 L2.75468914,4.00457396 L4.50368914,4.004 L4.50444233,3.25 C4.50444233,2.87030423 4.78659621,2.55650904 5.15267177,2.50684662 L5.25444233,2.5 L8.9891247,2.5 Z M4.50368914,5.504 L3.50468914,5.504 L3.50468914,19.5 L10.9478955,19.4998273 C10.4513189,18.9207296 9.73864328,18.5588115 8.96709342,18.5065584 L8.77307039,18.5 L5.25444233,18.5 C4.87474657,18.5 4.56095137,18.2178461 4.51128895,17.8517706 L4.50444233,17.75 L4.50368914,5.504 Z M19.5049359,17.75 C19.5049359,18.1642136 19.1691495,18.5 18.7549359,18.5 L15.2363079,18.5 C14.3910149,18.5 13.5994408,18.8724714 13.0614828,19.4998273 L20.5046891,19.5 L20.5046891,5.504 L19.5046891,5.504 L19.5049359,17.75 Z M18.0059359,3.999 L15.0202536,4 L14.8259077,4.00692283 C13.9889509,4.06666544 13.2254227,4.50975805 12.7549359,5.212 L12.7549359,17.777 L12.7782651,17.7601316 C13.4923805,17.2719483 14.3447024,17 15.2363079,17 L18.0059359,16.999 L18.0056891,4.798 L18.0033792,4.75457396 L18.0056891,4.71 L18.0059359,3.999 Z M8.9891247,4 L6.00368914,3.999 L6.00599909,4.75457396 L6.00599909,4.75457396 L6.00368914,4.783 L6.00368914,16.999 L8.77307039,17 C9.57551536,17 10.3461406,17.2202781 11.0128313,17.6202194 L11.2536891,17.776 L11.2536891,5.211 C10.8200889,4.56369974 10.1361548,4.13636104 9.37521067,4.02745763 L9.18347055,4.00692283 L8.9891247,4 Z"></path></g></svg></button></div></div><div class="explorer" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>node&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-content"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div class="explorer-content" aria-expanded="false"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../Personal/">Personal</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../Personal/Naver-Connect---Boostcamp-AI-Tech-4기/">Naver Connect   Boostcamp AI Tech 4기</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Week 14</a></div></nav><h1 class="article-title">Week 14</h1><p show-comma="true" class="content-meta"><time datetime="2025-06-17T08:48:27.125Z">Jun 17, 2025</time><span>26 min read</span></p></div></div><article class="popover-hint"><ul>
<li><a href="#segmentation-overview" class="internal alias">Segmentation Overview</a>
<ul>
<li><a href="#segmentation-history" class="internal alias">Segmentation History</a></li>
<li><a href="#cococommon-objects-in-context" class="internal alias">COCO(Common Objects in COntext)</a></li>
</ul>
</li>
<li><a href="#semantic-segmentation-%EA%B8%B0%EC%B4%88%EC%99%80-%EC%9D%B4%ED%95%B4" class="internal alias">Semantic Segmentation 기초와 이해</a>
<ul>
<li><a href="#transposed-convolution" class="internal alias">Transposed Convolution</a></li>
<li><a href="#fcnfully-convolutional-networks-for-semantic-segmentation" class="internal alias">FCN(Fully Convolutional networks for semantic segmentation)</a></li>
<li><a href="#fcn%EC%9D%98-%EC%84%B1%EB%8A%A5%EC%9D%84-%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%A4%EA%B8%B0-%EC%9C%84%ED%95%9C-%EB%B0%A9%EB%B2%95" class="internal alias">FCN의 성능을 향상시키기 위한 방법</a></li>
</ul>
</li>
<li><a href="#fcn%EC%9D%98-%ED%95%9C%EA%B3%84%EC%99%80-%EA%B7%B9%EB%B3%B5%ED%95%9C-%EB%AA%A8%EB%8D%B8%EB%93%A4-1" class="internal alias">FCN의 한계와 극복한 모델들 1</a>
<ul>
<li><a href="#fcn%EC%9D%98-%ED%95%9C%EA%B3%84%EC%A0%90" class="internal alias">FCN의 한계점</a></li>
<li><a href="#decoder%EB%A5%BC-%EA%B0%9C%EC%84%A0%ED%95%9C-model%EB%93%A4" class="internal alias">Decoder를 개선한 Model들</a>
<ul>
<li><a href="#deconvnet" class="internal alias">DeconvNet</a></li>
<li><a href="#segnet" class="internal alias">SegNet</a></li>
<li><a href="#deconvnet-vs-segnet" class="internal alias">DeconvNet vs SegNet</a></li>
</ul>
</li>
<li><a href="#skip-connection%EC%9D%84-%EC%A0%81%EC%9A%A9%ED%95%9C-model%EB%93%A4" class="internal alias">Skip Connection을 적용한 Model들</a>
<ul>
<li><a href="#fc-densenet" class="internal alias">FC DenseNet</a></li>
<li><a href="#unet" class="internal alias">Unet</a></li>
</ul>
</li>
<li><a href="#receptive-field%EB%A5%BC-%ED%99%95%EC%9E%A5%EC%8B%9C%ED%82%A8-model%EB%93%A4" class="internal alias">Receptive Field를 확장시킨 Model들</a>
<ul>
<li><a href="#deeplab-v1" class="internal alias">DeepLab v1</a></li>
<li><a href="#dilatednet" class="internal alias">DilatedNet</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#fcn%EC%9D%98-%ED%95%9C%EA%B3%84%EC%99%80-%EA%B7%B9%EB%B3%B5%ED%95%9C-%EB%AA%A8%EB%8D%B8%EB%93%A4-2" class="internal alias">FCN의 한계와 극복한 모델들 2</a>
<ul>
<li><a href="#receptive-field%EB%A5%BC-%ED%99%95%EC%9E%A5%EC%8B%9C%ED%82%A8-model%EB%93%A4" class="internal alias">Receptive Field를 확장시킨 Model들</a>
<ul>
<li><a href="#deeplab-v2" class="internal alias">DeepLab v2</a></li>
<li><a href="#pspnet" class="internal alias">PSPNet</a></li>
<li><a href="#deeplab-v3" class="internal alias">DeepLab v3</a></li>
<li><a href="#deeplab-v3" class="internal alias">DeepLab v3+</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#high-performance%EB%A5%BC-%EC%9E%90%EB%9E%91%ED%95%98%EB%8A%94-unet-%EA%B3%84%EC%97%B4%EC%9D%98-%EB%AA%A8%EB%8D%B8%EB%93%A4" class="internal alias">High Performance를 자랑하는 UNet 계열의 모델들</a>
<ul>
<li><a href="#unet" class="internal alias">UNet</a>
<ul>
<li><a href="#unet-%EA%B0%9C%EC%9A%94" class="internal alias">UNet 개요</a></li>
<li><a href="#unet-architecture" class="internal alias">UNet Architecture</a></li>
<li><a href="#data-augmentation" class="internal alias">Data Augmentation</a></li>
<li><a href="#%ED%95%9C%EA%B3%84%EC%A0%90" class="internal alias">한계점</a></li>
</ul>
</li>
<li><a href="#unet" class="internal alias">UNet++</a>
<ul>
<li><a href="#unet-%EA%B0%9C%EC%9A%94" class="internal alias">UNet++ 개요</a></li>
<li><a href="#%ED%95%9C%EA%B3%84%EC%A0%90" class="internal alias">한계점</a></li>
</ul>
</li>
<li><a href="#unet3" class="internal alias">UNet3+</a>
<ul>
<li><a href="#unet-unet%EC%9D%98-%ED%95%9C%EA%B3%84%EC%A0%90" class="internal alias">UNet, UNet++의 한계점</a></li>
<li><a href="#unet3%EC%97%90-%EC%A0%81%EC%9A%A9%EB%90%9C-techniques" class="internal alias">UNet3+에 적용된 Techniques</a></li>
</ul>
</li>
<li><a href="#another-version-of-the-unet" class="internal alias">Another Version of the UNet</a></li>
</ul>
</li>
<li><a href="#semantic-segmentation-%EB%8C%80%ED%9A%8C%EC%97%90%EC%84%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95%EB%93%A4" class="internal alias">Semantic Segmentation 대회에서 사용하는 방법들</a>
<ul>
<li><a href="#efficientunet-baseline" class="internal alias">EfficientUNet Baseline</a>
<ul>
<li><a href="#model-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0" class="internal alias">Model 불러오기</a></li>
</ul>
</li>
<li><a href="#baseline-%EC%9D%B4%ED%9B%84%EC%97%90-%EC%8B%A4%ED%97%98%ED%95%B4%EB%B4%90%EC%95%BC%ED%95%A0-%EC%82%AC%ED%95%AD%EB%93%A4" class="internal alias">Baseline 이후에 실험해봐야할 사항들</a>
<ul>
<li><a href="#augmentation" class="internal alias">Augmentation</a></li>
<li><a href="#sota-model" class="internal alias">SOTA Model</a></li>
<li><a href="#scheduler" class="internal alias">Scheduler</a></li>
<li><a href="#optimizer--loss" class="internal alias">Optimizer / Loss</a></li>
</ul>
</li>
<li><a href="#ensemble" class="internal alias">Ensemble</a></li>
<li><a href="#pseudo-labeling" class="internal alias">Pseudo Labeling</a></li>
<li><a href="#%EA%B7%B8-%EC%99%B8" class="internal alias">그 외</a></li>
</ul>
</li>
<li><a href="#semantic-segmentation-%EC%97%B0%EA%B5%AC-%EB%8F%99%ED%96%A5" class="internal alias">Semantic Segmentation 연구 동향</a>
<ul>
<li><a href="#hrnet" class="internal alias">HRNet</a>
<ul>
<li><a href="#hrnet%EC%9D%98-%EA%B5%AC%EC%84%B1-%EC%9A%94%EC%86%8C" class="internal alias">HRNet의 구성 요소</a></li>
</ul>
</li>
<li><a href="#wsss" class="internal alias">WSSS</a></li>
</ul>
</li>
</ul>
<hr/>
<h1 id="segmentation-overview">Segmentation Overview<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#segmentation-overview" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="segmentation-history">Segmentation History<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#segmentation-history" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Image Segmentation Using Deep Learning: A Survey </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others.<br/>
<a href="https://arxiv.org/abs/2001.05566" class="external">https://arxiv.org/abs/2001.05566<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-70.png" width="auto" height="auto" alt="Untitled 70.png"/></p>
<h2 id="cococommon-objects-in-context">COCO(Common Objects in COntext)<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cococommon-objects-in-context" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>COCO - Common Objects in Context </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p><a href="https://cocodataset.org/#home" class="external">https://cocodataset.org/#home<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Introduction to the COCO Dataset - OpenCV </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>With applications such as object detection, segmentation, and captioning, the COCO dataset is widely understood by state-of-the-art neural networks.<br/>
<a href="https://opencv.org/introduction-to-the-coco-dataset/" class="external">https://opencv.org/introduction-to-the-coco-dataset/<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="JSON" data-theme="github-dark"><code data-language="JSON" data-theme="github-dark" style="display:grid;"><span data-line> </span>
<span data-line><span>dataset = {</span></span>
<span data-line><span>    “info”: {</span></span>
<span data-line><span>        “year”: 2021,</span></span>
<span data-line><span>        “version”: 1.2,</span></span>
<span data-line><span>        “description:” “Pets dataset”,</span></span>
<span data-line><span>        “contributor”: “Pets inc.”,</span></span>
<span data-line><span>        “url”: “http://sampledomain.org”,</span></span>
<span data-line><span>        “date_created”: “2021/07/19” </span></span>
<span data-line><span>    },</span></span>
<span data-line><span>    “licenses”: [{</span></span>
<span data-line><span>        “id”: 1,</span></span>
<span data-line><span>        “name”: “Free license”,</span></span>
<span data-line><span>        “url:” “http://sampledomain.org”</span></span>
<span data-line><span>    }],</span></span>
<span data-line><span>    “categories”: [</span></span>
<span data-line><span>        {“id”: 1, </span></span>
<span data-line><span>         “name”: ”poodle”, </span></span>
<span data-line><span>         “supercategory”: “dog”, </span></span>
<span data-line><span>         “isthing”: 1, </span></span>
<span data-line><span>         “color”: [1,0,0]},</span></span>
<span data-line><span>        {“id”: 2, </span></span>
<span data-line><span>         “name”: ”ragdoll”, </span></span>
<span data-line><span>         “supercategory”: “cat”, </span></span>
<span data-line><span>         “isthing”: 1, </span></span>
<span data-line><span>         “color”: [2,0,0]}</span></span>
<span data-line><span>    ],</span></span>
<span data-line><span>    “image”: [</span></span>
<span data-line><span>        {</span></span>
<span data-line><span>        “id”: 934,</span></span>
<span data-line><span>        “width”: 640,</span></span>
<span data-line><span>        “height”: 640,</span></span>
<span data-line><span>        “file_name: “84.jpg”,</span></span>
<span data-line><span>        “license”: 1,</span></span>
<span data-line><span>        “date_captured”: “2021-07-19  17:49”</span></span>
<span data-line><span>        }</span></span>
<span data-line><span>        ]</span></span>
<span data-line><span>“annotations”: [</span></span>
<span data-line><span>        {</span></span>
<span data-line><span>        ”segmentation”:</span></span>
<span data-line><span>            {	</span></span>
<span data-line><span>            “counts”: [34, 55, 10, 71]</span></span>
<span data-line><span>            “size”: [240, 480]</span></span>
<span data-line><span>            },</span></span>
<span data-line><span>        “area”: 600.4,</span></span>
<span data-line><span>        “iscrowd”: 1,</span></span>
<span data-line><span>        “Image_id:” 122214,</span></span>
<span data-line><span>        “bbox”: [473.05, 395.45, 38.65, 28.92],</span></span>
<span data-line><span>        “category_id”: 15,</span></span>
<span data-line><span>        “id”: 934</span></span>
<span data-line><span>        },</span></span>
<span data-line><span>        {</span></span>
<span data-line><span>        ”segmentation”: [[34, 55, 10, 71, 76, 23, 98, 43, 11, 8]],</span></span>
<span data-line><span>        “area”: 600.4,</span></span>
<span data-line><span>        “iscrowd”: 1,</span></span>
<span data-line><span>        “Image_id:” 122214,</span></span>
<span data-line><span>        “bbox”: [473.05, 395.45, 38.65, 28.92],</span></span>
<span data-line><span>        “category_id”: 15,</span></span>
<span data-line><span>        “id”: 934</span></span>
<span data-line><span>        }</span></span>
<span data-line><span>        ]</span></span>
<span data-line><span>}</span></span></code></pre></figure>
<h1 id="semantic-segmentation-기초와-이해">Semantic Segmentation 기초와 이해<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#semantic-segmentation-기초와-이해" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="transposed-convolution">Transposed Convolution<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#transposed-convolution" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>딥러닝에서 사용되는 여러 유형의 Convolution 소개 </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>An Introduction to different Types of Convolutions in Deep Learning 을 번역한 글입니다.<br/>
<a href="https://zzsza.github.io/data/2018/02/23/introduction-convolution/" class="external">https://zzsza.github.io/data/2018/02/23/introduction-convolution/<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>14.10. Transposed Convolution - Dive into Deep Learning 1.0.0-beta0 documentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>The CNN layers we have seen so far, such as convolutional layers () and pooling layers (), typically reduce (downsample) the spatial dimensions (height and width) of the input, or keep them unchanged.<br/>
<a href="https://d2l.ai/chapter_computer-vision/transposed-conv.html" class="external">https://d2l.ai/chapter_computer-vision/transposed-conv.html<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>What are deconvolutional layers? </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>I recently read Fully Convolutional Networks for Semantic Segmentation by Jonathan Long, Evan Shelhamer, Trevor Darrell.<br/>
<a href="https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers" class="external">https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-1-51.png" width="auto" height="auto" alt="Untitled 1 51.png"/></p>
<p>2x2 Transposed Convolution (stride = 1)</p>
<p><img src="../../resources/Untitled-2-35.png" width="auto" height="auto" alt="Untitled 2 35.png"/></p>
<p>2x2 Transposed Convolution (stride = 2)</p>
<p>피처맵의 공간 차원을 증가(Up Sampling)을 위해 사용하는 연산</p>
<p>Deconvolution이라는 용어와 혼용하지만, 사실 구분해야 한다</p>
<ul>
<li>왜 ‘Transposed’라고 표현할까? 연산 방식과 깊은 관련이 있다
<ul>
<li>
<p>일반적인 Convolution</p>
<p><img src="../../resources/Untitled-3-25.png" width="auto" height="auto" alt="Untitled 3 25.png"/></p>
</li>
<li>
<p>Transposed Convolution</p>
<p><img src="../../resources/Untitled-4-17.png" width="auto" height="auto" alt="Untitled 4 17.png"/></p>
</li>
</ul>
</li>
</ul>
<h2 id="fcnfully-convolutional-networks-for-semantic-segmentation">FCN(Fully Convolutional networks for semantic segmentation)<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#fcnfully-convolutional-networks-for-semantic-segmentation" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Fully Convolutional Networks for Semantic Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Convolutional networks are powerful visual models that yield hierarchies of features.<br/>
<a href="https://arxiv.org/abs/1411.4038" class="external">https://arxiv.org/abs/1411.4038<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-5-13.png" width="auto" height="auto" alt="Untitled 5 13.png"/></p>
<ul>
<li>
<p>기존의 VGG에서 FC Layer들을 1x1 Conv로 변경</p>
</li>
<li>
<p>마지막에 Transposed Convolution을 사용하여 Upacaling 수행, 입력 크기만큼 복원</p>
</li>
</ul>
<h2 id="fcn의-성능을-향상시키기-위한-방법">FCN의 성능을 향상시키기 위한 방법<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#fcn의-성능을-향상시키기-위한-방법" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Skip Connection 사용</p>
<ul>
<li>
<p>Max Pooling에 의해서 잃어버린 정보를 복원해주는 작업을 진행</p>
</li>
<li>
<p>Upsampled Size를 줄여주기에 좀 더 효율적인 이미지 복원 가능</p>
</li>
<li>
<p>Skip Connection을 사용해 Pooling 전 Feature와 합성후 다시 Upsacaling 수행</p>
<p><img src="../../resources/Untitled-6-10.png" width="auto" height="auto" alt="Untitled 6 10.png"/></p>
<p>FCN-8s Network(<a href="https://www.researchgate.net/figure/Fully-convolutional-neural-network-architecture-FCN-8_fig1_327521314" class="external">https://www.researchgate.net/figure/Fully-convolutional-neural-network-architecture-FCN-8_fig1_327521314<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</p>
</li>
</ul>
<p><img src="../../resources/Untitled-7-7.png" width="auto" height="auto" alt="Untitled 7 7.png"/></p>
<h1 id="fcn의-한계와-극복한-모델들-1">FCN의 한계와 극복한 모델들 1<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#fcn의-한계와-극복한-모델들-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="fcn의-한계점">FCN의 한계점<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#fcn의-한계점" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>객체의 크기가 크거나 작은 경우 예측을 잘 하지 못하는 문제</p>
<ul>
<li>
<p>큰 Object의 경우 지역적인 정보만으로 예측</p>
<ul>
<li>버스의 앞 부분은 버스로 예측하지만, 버스 유리창에 미친 자전거를 보고 자전거로 인식하는 문제도 발생</li>
<li>Convolution Layer의 Receptive Field의 문제로 볼 수 있음</li>
</ul>
</li>
<li>
<p>작은 Object가 무시되는 경향 존재</p>
</li>
</ul>
<p>Object의 디테일한 모습이 사라지는 문제</p>
<ul>
<li>
<p>Upscaling 방식이 너무 간단해 경계를 학습하기 어려움</p>
</li>
</ul>
<h2 id="decoder를-개선한-model들">Decoder를 개선한 Model들<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#decoder를-개선한-model들" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="deconvnet">DeconvNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#deconvnet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Learning Deconvolution Network for Semantic Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>We propose a novel semantic segmentation algorithm by learning a deconvolution network.<br/>
<a href="https://arxiv.org/abs/1505.04366" class="external">https://arxiv.org/abs/1505.04366<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-8-5.png" width="auto" height="auto" alt="Untitled 8 5.png"/></p>
<p>Conv Network는 VGG16 사용</p>
<ul>
<li>
<p>13개의 층</p>
</li>
<li>
<p>ReLU와 Pooling이 Convolution 사이에서 이루어짐</p>
</li>
<li>
<p>7x7 Conv와 1x1 Conv 활용</p>
</li>
</ul>
<p>Deconvolution Network</p>
<p><img src="../../resources/Untitled-9-5.png" width="auto" height="auto" alt="Untitled 9 5.png"/></p>
<ul>
<li>Unpooling
<ul>
<li>디테일한 경계를 포착</li>
<li>Pooling의 경우, 노이즈를 제거하지만 정보가 손실되는 문제가 존재</li>
<li>Unpooling을 통해 Pooling 시 지워진 경계의 정보를 기록했다 복원</li>
<li>학습의 영역이 아니기 때문에 빠른 속도</li>
<li>그러나 Sparse한 Activation Map을 가짐
<ul>
<li>Transposed Convolution이 이를 보완</li>
</ul>
</li>
</ul>
</li>
<li>Deconvolution(Transposed Convolution)
<ul>
<li>전반적인 모습을 포착</li>
</ul>
</li>
<li>ReLU</li>
</ul>
<p><img src="../../resources/Untitled-10-4.png" width="auto" height="auto" alt="Untitled 10 4.png"/></p>
<p>깊은 계층에 대한 deconvolution일수록 디테일한 형상을 갖고 있음</p>
<p><img src="../../resources/Untitled-11-4.png" width="auto" height="auto" alt="Untitled 11 4.png"/></p>
<p>기존 FCN에 비해 더 디테일한 Predict 가능</p>
<p><img src="../../resources/Untitled-12-3.png" width="auto" height="auto" alt="Untitled 12 3.png"/></p>
<p><img src="../../resources/Untitled-13-3.png" width="auto" height="auto" alt="Untitled 13 3.png"/></p>
<h3 id="segnet">SegNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#segnet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet.<br/>
<a href="https://arxiv.org/abs/1511.00561" class="external">https://arxiv.org/abs/1511.00561<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>Road Scene Understanding applications 분야에서 Semantic Segmentation을 수행하기 위해 개발</p>
<ul>
<li>
<p>차량, 도로, 차선, 건물, 보도, 하늘, 사람 들의 Class를 빠르게 구분할 수 있어야 함</p>
</li>
<li>
<p>기존 DeconvNet의 경량화 버전</p>
<p><img src="../../resources/Untitled-14-3.png" width="auto" height="auto" alt="Untitled 14 3.png"/></p>
</li>
</ul>
<h3 id="deconvnet-vs-segnet">DeconvNet vs SegNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#deconvnet-vs-segnet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>

























<div class="table-container"><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>DeconvNet</td><td>SegNet</td></tr><tr><td>Encoder / Decoder Network가 대칭인 구조</td><td>Encoder / Decoder Network가 대칭인 구조</td></tr><tr><td>Encoder Network  <br/>→ VGG 16으로 Encoding  <br/>→ 13개의 층  <br/>→ Conv / BN / ReLU / Pooling  <br/>→  <br/><strong>FC Layer로 7x7 Conv 및 1x1 Conv</strong></td><td>Encoder Network  <br/>→ VGG 16으로 Encoding  <br/>→ 13개의 층  <br/>→ Conv / BN / ReLU / Pooling  <br/>→  <br/><strong>중간의 1x1 Conv 제거</strong></td></tr><tr><td>Decoder Network  <br/>→ Unpooling +  <br/><strong>Deconvolution</strong> + ReLU</td><td>Decoder Network  <br/>→ Unpooling +  <br/><strong>Convolution</strong> + ReLU</td></tr></tbody></table></div>
<h2 id="skip-connection을-적용한-model들">Skip Connection을 적용한 Model들<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#skip-connection을-적용한-model들" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="fc-densenet">FC DenseNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#fc-densenet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Densely Connected Convolutional Networks </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output.<br/>
<a href="https://arxiv.org/abs/1608.06993" class="external">https://arxiv.org/abs/1608.06993<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-15-3.png" width="auto" height="auto" alt="Untitled 15 3.png"/></p>
<p><img src="../../resources/Untitled-16-3.png" width="auto" height="auto" alt="Untitled 16 3.png"/></p>
<p>이전 Layer의 정보를 Concatenation해서 다음 Layer로 전달</p>
<h3 id="unet">Unet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>U-Net: Convolutional Networks for Biomedical Image Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>There is large consent that successful training of deep networks requires many thousand annotated training samples.<br/>
<a href="https://arxiv.org/abs/1505.04597" class="external">https://arxiv.org/abs/1505.04597<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-17-3.png" width="auto" height="auto" alt="Untitled 17 3.png"/></p>
<p>Encoder의 정보를 Skip connection을 통해 decoder로 전달, Concatenation 후 다음 Decoder Layer로 전달</p>
<h2 id="receptive-field를-확장시킨-model들">Receptive Field를 확장시킨 Model들<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#receptive-field를-확장시킨-model들" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="deeplab-v1">DeepLab v1<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#deeplab-v1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection.<br/>
<a href="https://arxiv.org/abs/1412.7062" class="external">https://arxiv.org/abs/1412.7062<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>Receptive Field : 뉴런이 바라보고 있는 입력 영역의 크기</p>
<ul>
<li>
<p>Receptive Field가 클 수록 전체적인 정보를 통해 추론하므로, 더 잘 검출할 수 있을 것이라고 기대 할 수 있음</p>
<p><img src="../../resources/Untitled-18-3.png" width="auto" height="auto" alt="Untitled 18 3.png"/></p>
</li>
<li>
<p>Conv → Maxpooling → Conv → …</p>
<ul>
<li>
<p>메모리 저감</p>
</li>
<li>
<p>노이즈 저감</p>
</li>
<li>
<p>효율적인 Receptive Field 확장</p>
</li>
<li>
<p>하지만 Segmantation에서는 Low Feature resolution을 가지는 문제 발생</p>
</li>
</ul>
</li>
</ul>
<p>Dilated Convolution을 사용, Receptive Field 확장</p>
<ul>
<li>Dilated Convolution[<a href="https://zzsza.github.io/data/2018/02/23/introduction-convolution/" class="external">link<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>]</li>
</ul>
<p><img src="../../resources/Untitled-19-3.png" width="auto" height="auto" alt="Untitled 19 3.png"/></p>
<p>Dense CRF를 사용해 Predict Image Upscaling</p>
<p>Dense CRF</p>
<p><img src="../../resources/Untitled-20-3.png" width="auto" height="auto" alt="Untitled 20 3.png"/></p>
<ul>
<li>
<p>이미지 분할 문제를 해결하기 위해 사용되는 확률 그래픽 모델의 일종</p>
</li>
<li>
<p>CRF를 사용하면 이미지의 색상과 텍스처 등의 특징을 기반으로 이미지의 각 픽셀을 특정 클래스에 할당</p>
</li>
<li>
<p>CRF 모델은 픽셀 간의 관계를 고려하여 이미지의 더 정확한 분할이 가능</p>
</li>
<li>
<p>이미지의 먼 부분 간의 관계를 잡아내기 어려운 전통적인 CRF 모델의 개선 버전으로 제안</p>
</li>
<li>
<p>이미지 내의 모든 픽셀 간의 관계를 모델링하기 위해 완전히 연결된 그래프를 사용</p>
</li>
<li>
<p>이웃 픽셀만 모델링, 픽셀 간의 관계를 더 자세하게 분석할 수 있게 해 세분화 결과의 정확도를 향상</p>
</li>
</ul>
<h3 id="dilatednet">DilatedNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#dilatednet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Multi-Scale Context Aggregation by Dilated Convolutions </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification.<br/>
<a href="https://arxiv.org/abs/1511.07122" class="external">https://arxiv.org/abs/1511.07122<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>Deeplab v1의 개량형 모델</p>
<p><img src="../../resources/Untitled-21-3.png" width="auto" height="auto" alt="Untitled 21 3.png"/></p>
<h1 id="fcn의-한계와-극복한-모델들-2">FCN의 한계와 극복한 모델들 2<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#fcn의-한계와-극복한-모델들-2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="receptive-field를-확장시킨-model들-1">Receptive Field를 확장시킨 Model들<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#receptive-field를-확장시킨-model들-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="deeplab-v2">DeepLab v2<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#deeplab-v2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit.<br/>
<a href="https://arxiv.org/abs/1606.00915" class="external">https://arxiv.org/abs/1606.00915<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>DilatedNet, DeepLab v1 모두 Dilated Convolution을 사용해 Receptive Field를 키웠음</p>
<p>DeepLab v1의 단점</p>
<ul>
<li>
<p>모든 풀링 계층에 대해 단일 속도만 사용하여 Receptive Field의 크기가 제한됨</p>
</li>
<li>
<p>Up Sampling은 간단히 특징 맵을 반복하여 단순하게 수행됨</p>
</li>
</ul>
<p>본 문제들을 해결하기 위해, DeepLab v2는 multi-scale dilated convolution을 사용하는 것을 제안</p>
<p><img src="../../resources/Untitled-22-3.png" width="auto" height="auto" alt="Untitled 22 3.png"/></p>
<p><img src="../../resources/Untitled-23-3.png" width="auto" height="auto" alt="Untitled 23 3.png"/></p>
<ul>
<li>하나의 비율 대신, DeepLab v2는 각 레이어마다 비율이 증가하는 multiple rates를 사용</li>
<li>더 큰 Receptive Field를 갖게 하여 더 나은 segmentation 성능 달성 가능</li>
<li>단순한 Up Sampling 대신 Up Sampling에 bilinear interpolation을 사용</li>
</ul>
<h3 id="pspnet">PSPNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#pspnet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Pyramid Scene Parsing Network </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Scene parsing is challenging for unrestricted open vocabulary and diverse scenes.<br/>
<a href="https://arxiv.org/abs/1612.01105" class="external">https://arxiv.org/abs/1612.01105<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Object Detectors Emerge in Deep Scene CNNs </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.<br/>
<a href="https://arxiv.org/abs/1412.6856" class="external">https://arxiv.org/abs/1412.6856<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>도입 배경</p>
<p><img src="../../resources/Untitled-24-3.png" width="auto" height="auto" alt="Untitled 24 3.png"/></p>
<ul>
<li>Msimatched relationship
<ul>
<li>객체들간의 특징을 catch하지 못함</li>
<li>ex : 호수 주변에 boat가 있는데, 기존 모델(FCN)은 Car로 예측</li>
<li>idea : 주변의 특징을 고려</li>
</ul>
</li>
<li>Confusion Categories</li>
<li>Inconspicuous Classes
<ul>
<li>
<p>작아서 예측을 하지 못함</p>
</li>
<li>
<p>ex : 배개와 이불이 같은 무늬일 때, 기존 모델(FCN)은 배개를 이불로 예측</p>
</li>
<li>
<p>idea : 작은 객체들도 global contextual information 사용</p>
</li>
</ul>
</li>
</ul>
<p>Receptive Filed가 큰데, 왜 이런 문제가 발생할까?</p>
<ul>
<li>OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS의 저자는 이론적인 Receptive Field의 크기와 실제 Receptive Field의 크기가 다르다고 주장
<ul>
<li>실험결과, Pooling이 진행될수록 오차가 커짐</li>
</ul>
</li>
</ul>
<p><img src="../../resources/Untitled-25-3.png" width="auto" height="auto" alt="Untitled 25 3.png"/></p>
<p>PSP Net의 Idea</p>
<p><img src="../../resources/Untitled-26-3.png" width="auto" height="auto" alt="Untitled 26 3.png"/></p>
<ul>
<li>
<p>Global Average Pooling을 사용</p>
</li>
</ul>
<p>Gloabl Average Pooling</p>
<ul>
<li>
<p>다양한 양의 비율의 풀링을 연결하여 발생된 각 Feature Map을 객체 간의 상관관계를 잡아낼 수 있게 함</p>
</li>
<li>
<p>따라서 객체간의 관계를 잡아낼 수 있게되고, 또한 작은 객체들도 고려할 수 있게 됨</p>
</li>
<li>
<p>이를 통해 이미지 픽셀 분할을 모델링하기 위해 완전히 연결된 그래프를 사용하는 CRF 모델의 단점을 극복할 수 있게 됨</p>
</li>
</ul>
<h3 id="deeplab-v3">DeepLab v3<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#deeplab-v3" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Rethinking Atrous Convolution for Semantic Image Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter’s field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation.<br/>
<a href="https://arxiv.org/abs/1706.05587" class="external">https://arxiv.org/abs/1706.05587<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-27-2.png" width="auto" height="auto" alt="Untitled 27 2.png"/></p>
<ul>
<li>DeepLabV2 모델의 개선 버전
<ul>
<li>기존 DeepLab v2 버전에서 Atrous Spatial Pyramid pooling을 수정, Global Average Pooling이 추가</li>
</ul>
</li>
<li>확장된 Receptive Field를 사용하고 이미지 분할의 정확도를 향상시키기 위해 Dilated Convolution을 사용</li>
<li>Encoder-Decoder 구조</li>
<li>Atrous Convolutional Neural Network (CNN)을 사용해 결과물의 해상도를 높임</li>
<li>Multi-Scale Feature Learning과 간단하면서도 효과적인 Decoder Module을 소개</li>
<li>Fully Connected Conditional Random Field (CRF)을 사용하여 분할 결과를 더욱 개선</li>
</ul>
<h3 id="deeplab-v3-1">DeepLab v3+<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#deeplab-v3-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task.<br/>
<a href="https://arxiv.org/abs/1802.02611" class="external">https://arxiv.org/abs/1802.02611<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-28-2.png" width="auto" height="auto" alt="Untitled 28 2.png"/></p>
<ul>
<li>Encoder에서 Spatial diemsion의 축소로 인해 손실된 정모를 Decoder에서 점진적으로 복원</li>
<li>Encoder
<ul>
<li>
<p>수정된 Xception을 backbone으로 사용</p>
<p><img src="../../resources/Untitled-29-2.png" width="auto" height="auto" alt="Untitled 29 2.png"/></p>
</li>
<li>
<p>Atrous separable convolution을 적용한 ASPP 사용</p>
</li>
<li>
<p>Backbone 내 low-level feature와 ASPP 모듈 출력을 모두 decoder에 전달</p>
</li>
</ul>
</li>
<li>Decoder
<ul>
<li>ASPP 모듈의 출력을 Up Sampling(bilinear)해서 Low-level feature와 결합</li>
<li>결합된 정보는 Conv 연산 후 Up Sampling되어 최종 결과 도출</li>
<li>기존의 단순한 Up Sampling 연산을 개선시켜 detail을 유지하도록 함</li>
</ul>
</li>
</ul>
<h1 id="high-performance를-자랑하는-unet-계열의-모델들">High Performance를 자랑하는 UNet 계열의 모델들<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#high-performance를-자랑하는-unet-계열의-모델들" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="unet-1">UNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>U-Net: Convolutional Networks for Biomedical Image Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>There is large consent that successful training of deep networks requires many thousand annotated training samples.<br/>
<a href="https://arxiv.org/abs/1505.04597" class="external">https://arxiv.org/abs/1505.04597<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<h3 id="unet-개요">UNet 개요<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet-개요" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>의료계열은 데이터를 확보하기 어려움</p>
<p>기본적인 DL 모델들은 파라미터 수가 많고 네트워크가 깊어 많은 data 필요</p>
<p>Cell segmentation의 경우, 같은 클래스가 인점해 있는 셀 사이의 경계 구분 필요</p>
<h3 id="unet-architecture">UNet Architecture<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet-architecture" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../../resources/Untitled-30-2.png" width="auto" height="auto" alt="Untitled 30 2.png"/></p>
<h3 id="data-augmentation">Data Augmentation<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#data-augmentation" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Random Elastic deformation을 통해 augmentation 수행</p>
<h3 id="한계점">한계점<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#한계점" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>기본적으로 깊이가 4로 고정되어 있음</p>
<ul>
<li>데이터셋마다 최고의 성능을 보장하지 못함</li>
<li>최적깊이 탐색비용 상승</li>
</ul>
<p>단순한 Skip Connection</p>
<ul>
<li>동일 깊이를 가지는 Encoder와 Decoder만 연결되는 제한적인 구조</li>
</ul>
<h2 id="unet-2">UNet++<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet-2" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>UNet++: A Nested U-Net Architecture for Medical Image Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation.<br/>
<a href="https://arxiv.org/abs/1807.10165" class="external">https://arxiv.org/abs/1807.10165<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<h3 id="unet-개요-1">UNet++ 개요<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet-개요-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../../resources/Untitled-31-2.png" width="auto" height="auto" alt="Untitled 31 2.png"/></p>
<p>UNet의 한계점을 극복하기 위해 새로운 아키텍처 제시</p>
<ul>
<li>Encoder를 공유하는 다양한 깊이의 UNet 생성</li>
<li>동일한 깊이에서의 Feature Map들이 모두 결합되도록 Skip Connection 수행
<ul>
<li>Ensemble 효과</li>
</ul>
</li>
<li>Deep Supervision Loss 사용
<ul>
<li>X^(0,1) &amp; X^(0,2) &amp; X^(0,3) &amp; X^(0,4)에 대하여 각각 Loss 계산</li>
<li>최종 Loss는 위 4개의 Loss의 평균</li>
</ul>
</li>
</ul>
<h3 id="한계점-1">한계점<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#한계점-1" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>복잡한 Connection으로 인한 Parameter 증가</li>
<li>많은 Connection으로 인한 메모리 증가</li>
<li>Encoder-Decoder 사이에서의 Connection이 동일한 크기를 갖는 Feature map에서만 진행됨
<ul>
<li>Full scale에서 충춘한 정보를 탐색하지 못해 위치와 경계를 명시적으로 학습하지 못함</li>
</ul>
</li>
</ul>
<h2 id="unet3">UNet3+<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet3" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Recently, a growing interest has been seen in deep learning-based semantic segmentation.<br/>
<a href="https://arxiv.org/abs/2004.08790" class="external">https://arxiv.org/abs/2004.08790<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<h3 id="unet-unet의-한계점">UNet, UNet++의 한계점<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet-unet의-한계점" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>UNet
<ul>
<li>Decoder를 구성하는 방법이 같은 level의 encoder layer로 부터 feature map을 받는 simple skip connection을 사용</li>
</ul>
</li>
<li>UNet++
<ul>
<li>Nested and dense skip connection을 사용해서 encoder-decoder 사이의 semantic gap을 줄임
<ul>
<li>파라미터 증가, 메모리 증가, full scale에서의 충분한 정보 탐색을 잘 못함(예측위치와 경계를 명시적으로 잘 학습하지 못함)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="unet3에-적용된-techniques">UNet3+에 적용된 Techniques<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#unet3에-적용된-techniques" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../../resources/Untitled-32-2.png" width="auto" height="auto" alt="Untitled 32 2.png"/></p>
<p><img src="../../resources/Untitled-33-2.png" width="auto" height="auto" alt="Untitled 33 2.png"/></p>
<p>Full-Scale Connections : [Conventional + Inter + Intra] skip connection</p>
<ul>
<li>Decoder Feature Map
<ul>
<li>(Conventional)encoder layer로부터 same-scale의 feature maps를 받음</li>
<li>(Inter)encoder layer로부터 smaller-scale의 low-level feature maps를 받음
<ul>
<li>풍부한 공간 정보를 통해 경계 강조</li>
</ul>
</li>
<li>(Intra)decoder layer로부터 lager-scale의 high-level feature maps를 받음
<ul>
<li>
<p>어디 위치하는지의 위치 정보 구현</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Classification-guided Module(CGM)</p>
<p><img src="../../resources/Untitled-34-2.png" width="auto" height="auto" alt="Untitled 34 2.png"/></p>
<ul>
<li>low-level layer에 남아있는 background의 noise로 인해 많은 False-Positive 문제 발생
<ul>
<li>정확도를 높이고자 exrtra classfication task 즌행</li>
<li>high-level feature maps인 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0972em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4169em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">5</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span> 를 활용
<ul>
<li>
<p>Dropout, 1x1 Conv, Adaptive max pool, sigmoid 통과</p>
</li>
<li>
<p>argmax를 통해 organ이 없으면 0, 있으면 1로 출력</p>
</li>
<li>
<p>위에서 얻은 결과의 각 low-layer마다 나온 결과를 곱</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Full-scale Deep Supervision</p>
<ul>
<li>경계부분을 잘 학습하기 위해서 Loss 여러가지 결합
<ul>
<li>Focal Loss : class 불균형 해소</li>
<li>ms-ssim Loss : Boundary 인식 강화</li>
<li>iou : 픽셀의 분류 정확도 상승</li>
</ul>
</li>
</ul>
<h2 id="another-version-of-the-unet">Another Version of the UNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#another-version-of-the-unet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>
<p>Residual-UNet</p>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Road Extraction by Deep Residual U-Net <br/>
Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis.<br/>
<a href="https://arxiv.org/abs/1711.10684" class="external">https://arxiv.org/abs/1711.10684<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p></div>
                  
                </div>
</blockquote>
<ul>
<li>Encoder 및 Decoder 부분의 Block마다 residual unit with identity mapping 적용</li>
</ul>
</li>
<li>
<p>Mobile-UNet</p>
<ul>
<li>
<p>MobileNet 을 backbone으로 사용</p>
</li>
</ul>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Info</p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p><a href="https://www.researchgate.net/publication/341749157_Mobile-Unet_An_efficient_convolutional_neural_network_for_fabric_defect_detection" class="external">https://www.researchgate.net/publication/341749157_Mobile-Unet_An_efficient_convolutional_neural_network_for_fabric_defect_detection<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
</li>
<li>
<p>Eff-UNet</p>
<ul>
<li>
<p>EfficientNet을 backbone으로 사용</p>
</li>
</ul>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Info</p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p><a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w22/Baheti_Eff-UNet_A_Novel_Architecture_for_Semantic_Segmentation_in_Unstructured_Environment_CVPRW_2020_paper.pdf" class="external">https://openaccess.thecvf.com/content_CVPRW_2020/papers/w22/Baheti_Eff-UNet_A_Novel_Architecture_for_Semantic_Segmentation_in_Unstructured_Environment_CVPRW_2020_paper.pdf<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
</li>
</ul>
<h1 id="semantic-segmentation-대회에서-사용하는-방법들">Semantic Segmentation 대회에서 사용하는 방법들<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#semantic-segmentation-대회에서-사용하는-방법들" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="efficientunet-baseline">EfficientUNet Baseline<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#efficientunet-baseline" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="model-불러오기">Model 불러오기<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#model-불러오기" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Segmentation Models[<a href="https://github.com/qubvel/segmentation_models.pytorch" class="external">link<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>]</p>
<ul>
<li>High Level API</li>
<li>Architectures
<ul>
<li>UNet, UNet++, Manet, Linknet, FPN, PSPNet, PAN, DeepLabV3, DeepLabV3+</li>
</ul>
</li>
<li>Encoders
<ul>
<li>ResNet, ResNeXt, ResNeSt, Res2Ne(X)t, RegNet(x,y) SE-Ne(X)t, DenseNet, Inception, EfficientNet, MobileNet, VGG</li>
</ul>
</li>
</ul>
<p>Latest version from source:</p>
<p><code>$ pip install git+https://github.com/qubvel/segmentation_models.pytorch</code></p>
<h2 id="baseline-이후에-실험해봐야할-사항들">Baseline 이후에 실험해봐야할 사항들<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#baseline-이후에-실험해봐야할-사항들" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="augmentation">Augmentation<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#augmentation" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Cutout : 이미지를 일부 가려서 일반화 성능의 향상 → Object와 상관 없는 영역을 가릴 수 있음</p>
<p>GridDropout : 전체적인 Grid를 가림</p>
<p><img src="../../resources/Untitled-35-2.png" width="auto" height="auto" alt="Untitled 35 2.png"/></p>
<p>Cutmix : object 일부를 다른 object 이미지로 대치</p>
<p><img src="../../resources/Untitled-36-2.png" width="auto" height="auto" alt="Untitled 36 2.png"/></p>
<p>Snapmix : CAM을 이용해 이미지 및 라벨을 Mixing하는 방법</p>
<p><img src="../../resources/Untitled-37-2.png" width="auto" height="auto" alt="Untitled 37 2.png"/></p>
<p>CropNonEmptyMaskIfExist : Object가 존재하는 부분을 중심으로 Crop 할 수 있다면 model의 학습을 효율적으로 할 수 있음</p>
<h3 id="sota-model">SOTA Model<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#sota-model" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>HRNet</p>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Deep High-Resolution Representation Learning for Visual Recognition </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection.<br/>
<a href="https://arxiv.org/abs/1908.07919" class="external">https://arxiv.org/abs/1908.07919<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<h3 id="scheduler">Scheduler<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#scheduler" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>CosineAnnealingLR
<ul>
<li>LR의 최대-최소 값을 정해, 그 범위의 학습율을 Cosine 함수를 이용해 스케줄링 하는 방법</li>
<li>Saddle Point를 빠르게 벗어나게 함</li>
</ul>
</li>
<li>ReduceLROnPlateau
<ul>
<li>Metric의 성능이 향상되지 않을 때 LR을 조절</li>
<li>Local Minima에 빠졌을 때, LR를 조절하여 효과적으로 빠져나옴</li>
</ul>
</li>
<li>Gradual Warmup
<ul>
<li>학습 시작 시 매우 작은 LR로 시작해서 특정 값에 도달할 때 까지 LR을 서서히 증가시키는 방법</li>
<li>이 방식을 사용하면 Weight가 불한정한 초반에도 비교적 안정적으로 학습 수행 가능</li>
<li>Backbone 네트워크 사용 시, Weight가 망가지는 것을 방지</li>
</ul>
</li>
</ul>
<h3 id="optimizer--loss">Optimizer / Loss<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#optimizer--loss" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>Normalization techniques are a boon for modern deep learning.<br/>
<a href="https://arxiv.org/abs/2006.08217" class="external">https://arxiv.org/abs/2006.08217<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p><img src="../../resources/Untitled-38-2.png" width="auto" height="auto" alt="Untitled 38 2.png"/></p>
<ul>
<li>Adam</li>
<li>AdamW</li>
<li>AdamP</li>
<li>Radam</li>
<li>Lookahead Optimizer
<ul>
<li>
<p>Adam이나 SGD를 사용해 K번 업데이트 후, 처음 시작했던 point 방향으로 1 step back 후, 그 지점에서 다시 시작하는 방법</p>
</li>
<li>
<p>Adam이나 SGD로 빠져나오기 힘든 Local Minima를 빠져나올 수 있음</p>
</li>
</ul>
</li>
</ul>
<h2 id="ensemble">Ensemble<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#ensemble" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>5 Fold Ensemble</li>
<li>Epoch Ensemble</li>
<li>SWA(Stochastic Weight Averaging)</li>
<li>Seed Ensemble</li>
<li>Resize Ensemble</li>
<li>TTA(Test time Augmentation) Ensemble
<ul>
<li>TTA Library : ttach</li>
</ul>
</li>
</ul>
<h2 id="pseudo-labeling">Pseudo Labeling<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#pseudo-labeling" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>label이 없는 데이터에 대해서 labeling 후, confidence가 높은 데이터를 학습에 참여시키는 방법</p>
<h2 id="그-외">그 외<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#그-외" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>학습 이미지가 많고 큰 경우에는 네트워크를 한 번 학습하는 데 시간이 오래 걸림, 충분한 실험이 어려움</p>
<ul>
<li>
<p>AMP</p>
</li>
<li>
<p>가벼운 상황으로 실험</p>
</li>
<li>
<p>Params가 적은 모델로 실험, 최종은 성능이 잘 나오는 모델로 실험</p>
</li>
</ul>
<p>Sliding Window를 사용</p>
<p>Label Smoothing</p>
<ul>
<li>Hard target → Soft target</li>
<li>CrossEntropyLoss / BCE Loss → SoftCrossEntropyLoss / SoftBCELoss</li>
</ul>
<h1 id="semantic-segmentation-연구-동향">Semantic Segmentation 연구 동향<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#semantic-segmentation-연구-동향" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="hrnet">HRNet<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#hrnet" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<blockquote class="callout info" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Deep High-Resolution Representation Learning for Visual Recognition </p></div>
                  
                </div>
<div class="callout-content">
<div class="callout-content-inner">
<p>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection.<br/>
<a href="https://arxiv.org/abs/1908.07919" class="external">https://arxiv.org/abs/1908.07919<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</div>
</div>
</blockquote>
<p>Image Classification 모델이 해상도를 줄여나가는 이유</p>
<ul>
<li>특정 물체 분류시 많은 경우, 이미지 내 모든 특징점이 필요하지 않음</li>
<li>해상도가 줄어들어 효율적인 연산이 가능, 넓은 Receptive Field를 갖게 됨</li>
<li>주요 특징만을 추출하여 과적합 방지</li>
</ul>
<p>이미지 분류 모델을 사용하여 얻은 저해상도 특징은 모든 픽셀에 대해 정확한 분류를 수행하기에는 부족한 정보를 가짐</p>
<p>따라서 segmantation 모델들은 해상도를 적게줄이면서 receptive field를 키울 필요가 있음</p>
<h3 id="hrnet의-구성-요소">HRNet의 구성 요소<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#hrnet의-구성-요소" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../../resources/Untitled-39-2.png" width="auto" height="auto" alt="Untitled 39 2.png"/></p>
<p>저해상도 → 고해상도 복원이 아닌, 고해상도 정보를 계속 유지</p>
<ul>
<li>
<p>입력 이미지의 해상도를 strided convolution Article 을 이용해 해상도를 1/4로 줄임</p>
</li>
<li>
<p>전체 구조에서 해당 해상도 유지</p>
</li>
<li>
<p>중간중간 분기하여 고해상도부터 저해상도까지 다양한 해상도의 feature를 계산</p>
</li>
<li>
<p>중간중간 서로의 feature를 서로에게 전달</p>
<p><img src="../../resources/Untitled-40-2.png" width="auto" height="auto" alt="Untitled 40 2.png"/></p>
</li>
</ul>
<p>high → low에서 strided conv를 사용한 이유 : 정보 손실 최소화</p>
<p>low → high에서 bilinear upsampling 사용한 이유 : time complexity</p>
<h2 id="wsss">WSSS<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#wsss" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Segmentation Labeling은 시간이 오래 걸림</p>
<p>Weak Supervision</p>
<ul>
<li>
<p>Test시 요구하는 output보다 학습 시에 더 간단한 annotation을 이용하여 학습</p>
</li>
<li>
<p>image level label을 활용하기 위해 Classification Model 학습</p>
</li>
<li>
<p>학습한 Classification model을 통해 CAM, Grad-CAM 혹은 attention 추출, label로 사용</p>
</li>
</ul>
<p>CAM &amp; Grad-CAM 문제점</p>
<ul>
<li>
<p>pseudo-mask의 결과가 좋지 않음</p>
</li>
<li>
<p>CAM의 문제점</p>
<ul>
<li>마지막 레이어는 꼭 GAP를 가져야 함</li>
<li>마지막 레이어에서만 CAM을 만들 수 있음</li>
</ul>
</li>
<li>
<p>결과가 Sharp하지 않음</p>
</li>
</ul>
<p>Sharp하게 만들기 위한 접근</p>
<ul>
<li>CRF를 사용</li>
<li>Object-ness(saliency)를 학습한 모델을 전이학습</li>
<li>Self-supervised Learning 이용
<ul>
<li>
<p>동일 Image를 input size와 관계 없이 CAM 모양이 같아지도록 L1 Loss 사용</p>
</li>
</ul>
</li>
</ul>
<p>CAM Output의 특징</p>
<ul>
<li>
<p>분류 시, 다른 Class임을 확실하게 알 수 있도록 특징적인 부분에 집중(타조의 몸통보다 얼굴에 집중)</p>
</li>
<li>
<p>같은 Class여도 서로 다른 모습을 보이므로 공통적으로 보이는 특징에 의존</p>
</li>
</ul>
<p>CAM 영역 확장을 위한 접근</p>
<ul>
<li>CAM 영역을 지워가며 반복 학습
<ul>
<li>입력 이미지를 Classification Network1를 사용해 CAM 추출</li>
<li>Classification Network1에서 만들 결과 영역 제거</li>
<li>제거한 이미지로 Classification Network2 학습</li>
<li>동일하게 Classification Network3 학습</li>
<li>합치면, object 영역을 얻을 수 있음</li>
<li>Output별로 다른 모델을 학습해야 하는 번거로움 존재</li>
<li>반복하다 보면 object 영역이 아닌 부분에 집중해버리는 Over-erasing 현상 존재</li>
</ul>
</li>
<li>입력 이미지에서 random하게 patch를 지워 최대한 다양한 영역에서 feature를 추출해야 하도록 강제
<ul>
<li>CAM 영역이 확장됨</li>
<li>Over-erasing 해결됨</li>
</ul>
</li>
<li>CAM 영역을 지워가며 반복학습 하는 것을 네트워크 하나로 수행
<ul>
<li>Adversarial Complementary Learning for Weakly supervised Object localization, cvpr 2018</li>
</ul>
</li>
<li>다양한 receptive field 사용
<ul>
<li>Revisiting Dilated Convolution: A Simple Approch for weakly-and semi-supervised semantic segmentation, cvpr 2018</li>
</ul>
</li>
<li>Mixup</li>
</ul></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div class="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false,&quot;enableRadial&quot;:false}"></div><button class="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div class="global-graph-outer"><div class="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.2,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true,&quot;enableRadial&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul class="toc-content overflow" id="list-1"><li class="depth-0"><a href="#segmentation-overview" data-for="segmentation-overview">Segmentation Overview</a></li><li class="depth-1"><a href="#segmentation-history" data-for="segmentation-history">Segmentation History</a></li><li class="depth-1"><a href="#cococommon-objects-in-context" data-for="cococommon-objects-in-context">COCO(Common Objects in COntext)</a></li><li class="depth-0"><a href="#semantic-segmentation-기초와-이해" data-for="semantic-segmentation-기초와-이해">Semantic Segmentation 기초와 이해</a></li><li class="depth-1"><a href="#transposed-convolution" data-for="transposed-convolution">Transposed Convolution</a></li><li class="depth-1"><a href="#fcnfully-convolutional-networks-for-semantic-segmentation" data-for="fcnfully-convolutional-networks-for-semantic-segmentation">FCN(Fully Convolutional networks for semantic segmentation)</a></li><li class="depth-1"><a href="#fcn의-성능을-향상시키기-위한-방법" data-for="fcn의-성능을-향상시키기-위한-방법">FCN의 성능을 향상시키기 위한 방법</a></li><li class="depth-0"><a href="#fcn의-한계와-극복한-모델들-1" data-for="fcn의-한계와-극복한-모델들-1">FCN의 한계와 극복한 모델들 1</a></li><li class="depth-1"><a href="#fcn의-한계점" data-for="fcn의-한계점">FCN의 한계점</a></li><li class="depth-1"><a href="#decoder를-개선한-model들" data-for="decoder를-개선한-model들">Decoder를 개선한 Model들</a></li><li class="depth-2"><a href="#deconvnet" data-for="deconvnet">DeconvNet</a></li><li class="depth-2"><a href="#segnet" data-for="segnet">SegNet</a></li><li class="depth-2"><a href="#deconvnet-vs-segnet" data-for="deconvnet-vs-segnet">DeconvNet vs SegNet</a></li><li class="depth-1"><a href="#skip-connection을-적용한-model들" data-for="skip-connection을-적용한-model들">Skip Connection을 적용한 Model들</a></li><li class="depth-2"><a href="#fc-densenet" data-for="fc-densenet">FC DenseNet</a></li><li class="depth-2"><a href="#unet" data-for="unet">Unet</a></li><li class="depth-1"><a href="#receptive-field를-확장시킨-model들" data-for="receptive-field를-확장시킨-model들">Receptive Field를 확장시킨 Model들</a></li><li class="depth-2"><a href="#deeplab-v1" data-for="deeplab-v1">DeepLab v1</a></li><li class="depth-2"><a href="#dilatednet" data-for="dilatednet">DilatedNet</a></li><li class="depth-0"><a href="#fcn의-한계와-극복한-모델들-2" data-for="fcn의-한계와-극복한-모델들-2">FCN의 한계와 극복한 모델들 2</a></li><li class="depth-1"><a href="#receptive-field를-확장시킨-model들-1" data-for="receptive-field를-확장시킨-model들-1">Receptive Field를 확장시킨 Model들</a></li><li class="depth-2"><a href="#deeplab-v2" data-for="deeplab-v2">DeepLab v2</a></li><li class="depth-2"><a href="#pspnet" data-for="pspnet">PSPNet</a></li><li class="depth-2"><a href="#deeplab-v3" data-for="deeplab-v3">DeepLab v3</a></li><li class="depth-2"><a href="#deeplab-v3-1" data-for="deeplab-v3-1">DeepLab v3+</a></li><li class="depth-0"><a href="#high-performance를-자랑하는-unet-계열의-모델들" data-for="high-performance를-자랑하는-unet-계열의-모델들">High Performance를 자랑하는 UNet 계열의 모델들</a></li><li class="depth-1"><a href="#unet-1" data-for="unet-1">UNet</a></li><li class="depth-2"><a href="#unet-개요" data-for="unet-개요">UNet 개요</a></li><li class="depth-2"><a href="#unet-architecture" data-for="unet-architecture">UNet Architecture</a></li><li class="depth-2"><a href="#data-augmentation" data-for="data-augmentation">Data Augmentation</a></li><li class="depth-2"><a href="#한계점" data-for="한계점">한계점</a></li><li class="depth-1"><a href="#unet-2" data-for="unet-2">UNet++</a></li><li class="depth-2"><a href="#unet-개요-1" data-for="unet-개요-1">UNet++ 개요</a></li><li class="depth-2"><a href="#한계점-1" data-for="한계점-1">한계점</a></li><li class="depth-1"><a href="#unet3" data-for="unet3">UNet3+</a></li><li class="depth-2"><a href="#unet-unet의-한계점" data-for="unet-unet의-한계점">UNet, UNet++의 한계점</a></li><li class="depth-2"><a href="#unet3에-적용된-techniques" data-for="unet3에-적용된-techniques">UNet3+에 적용된 Techniques</a></li><li class="depth-1"><a href="#another-version-of-the-unet" data-for="another-version-of-the-unet">Another Version of the UNet</a></li><li class="depth-0"><a href="#semantic-segmentation-대회에서-사용하는-방법들" data-for="semantic-segmentation-대회에서-사용하는-방법들">Semantic Segmentation 대회에서 사용하는 방법들</a></li><li class="depth-1"><a href="#efficientunet-baseline" data-for="efficientunet-baseline">EfficientUNet Baseline</a></li><li class="depth-2"><a href="#model-불러오기" data-for="model-불러오기">Model 불러오기</a></li><li class="depth-1"><a href="#baseline-이후에-실험해봐야할-사항들" data-for="baseline-이후에-실험해봐야할-사항들">Baseline 이후에 실험해봐야할 사항들</a></li><li class="depth-2"><a href="#augmentation" data-for="augmentation">Augmentation</a></li><li class="depth-2"><a href="#sota-model" data-for="sota-model">SOTA Model</a></li><li class="depth-2"><a href="#scheduler" data-for="scheduler">Scheduler</a></li><li class="depth-2"><a href="#optimizer--loss" data-for="optimizer--loss">Optimizer / Loss</a></li><li class="depth-1"><a href="#ensemble" data-for="ensemble">Ensemble</a></li><li class="depth-1"><a href="#pseudo-labeling" data-for="pseudo-labeling">Pseudo Labeling</a></li><li class="depth-1"><a href="#그-외" data-for="그-외">그 외</a></li><li class="depth-0"><a href="#semantic-segmentation-연구-동향" data-for="semantic-segmentation-연구-동향">Semantic Segmentation 연구 동향</a></li><li class="depth-1"><a href="#hrnet" data-for="hrnet">HRNet</a></li><li class="depth-2"><a href="#hrnet의-구성-요소" data-for="hrnet의-구성-요소">HRNet의 구성 요소</a></li><li class="depth-1"><a href="#wsss" data-for="wsss">WSSS</a></li><li class="overflow-end"></li></ul></div><div class="backlinks"><h3>Backlinks</h3><ul id="list-2" class="overflow"><li><a href="../../Personal/Naver-Connect---Boostcamp-AI-Tech-4기/Naver-Connect---Boostcamp-AI-Tech-4기" class="internal">Naver Connect - Boostcamp AI Tech 4기</a></li><li class="overflow-end"></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.5.1</a> © 2025</p><ul><li><a href="https://github.com/404Vector">GitHub</a></li></ul></footer></div></div></body><script type="application/javascript">function n(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.getElementsByClassName("callout-content")[0];if(!e)return;let l=t.classList.contains("is-collapsed");e.style.gridTemplateRows=l?"0fr":"1fr"}function c(){let t=document.getElementsByClassName("callout is-collapsible");for(let e of t){let l=e.getElementsByClassName("callout-title")[0],s=e.getElementsByClassName("callout-content")[0];if(!l||!s)continue;l.addEventListener("click",n),window.addCleanup(()=>l.removeEventListener("click",n));let o=e.classList.contains("is-collapsed");s.style.gridTemplateRows=o?"0fr":"1fr"}}document.addEventListener("nav",c);
</script><script type="module">function f(i,e){if(!i)return;function r(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function t(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}i?.addEventListener("click",r),window.addCleanup(()=>i?.removeEventListener("click",r)),document.addEventListener("keydown",t),window.addCleanup(()=>document.removeEventListener("keydown",t))}function y(i){for(;i.firstChild;)i.removeChild(i.firstChild)}var h=class{constructor(e,r){this.container=e;this.content=r;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),r=this.onMouseMove.bind(this),t=this.onMouseUp.bind(this),o=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",r),document.addEventListener("mouseup",t),window.addEventListener("resize",o),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",r),()=>document.removeEventListener("mouseup",t),()=>window.removeEventListener("resize",o))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let r=this.createButton("+",()=>this.zoom(.1)),t=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(t),e.appendChild(o),e.appendChild(r),this.container.appendChild(e)}createButton(e,r){let t=document.createElement("button");return t.textContent=e,t.className="mermaid-control-button",t.addEventListener("click",r),window.addCleanup(()=>t.removeEventListener("click",r)),t}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}zoom(e){let r=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),t=this.content.getBoundingClientRect(),o=t.width/2,n=t.height/2,c=r-this.scale;this.currentPan.x-=o*c,this.currentPan.y-=n*c,this.scale=r,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){this.scale=1;let e=this.content.querySelector("svg");this.currentPan={x:e.getBoundingClientRect().width/2,y:e.getBoundingClientRect().height/2},this.updateTransform()}},C=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],E;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;E||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let r=E.default,t=new WeakMap;for(let n of e)t.set(n,n.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let a=t.get(s);a&&(s.innerHTML=a)}let n=C.reduce((s,a)=>(s[a]=window.getComputedStyle(document.documentElement).getPropertyValue(a),s),{}),c=document.documentElement.getAttribute("saved-theme")==="dark";r.initialize({startOnLoad:!1,securityLevel:"loose",theme:c?"dark":"base",themeVariables:{fontFamily:n["--codeFont"],primaryColor:n["--light"],primaryTextColor:n["--darkgray"],primaryBorderColor:n["--tertiary"],lineColor:n["--darkgray"],secondaryColor:n["--secondary"],tertiaryColor:n["--tertiary"],clusterBkg:n["--light"],edgeLabelBackground:n["--highlight"]}}),await r.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let n=0;n<e.length;n++){let v=function(){let g=l.querySelector("#mermaid-space"),m=l.querySelector(".mermaid-content");if(!m)return;y(m);let w=c.querySelector("svg").cloneNode(!0);m.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new h(g,m)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},c=e[n],s=c.parentElement,a=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(a),L=a.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),f(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../../postscript.js" type="module"></script></html>